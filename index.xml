<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Mao Song(毛松)'s Homepage</title><link>https://maosong2022.github.io/</link><description>Recent content on Mao Song(毛松)'s Homepage</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 01 Nov 2025 15:32:30 +0800</lastBuildDate><atom:link href="https://maosong2022.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Mixstral 8x7B</title><link>https://maosong2022.github.io/p/mixstral-8x7b/</link><pubDate>Sat, 01 Nov 2025 15:32:30 +0800</pubDate><guid>https://maosong2022.github.io/p/mixstral-8x7b/</guid><description>&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>作者在本文中提出了 Mixtral 8x7B, 一个 MoE 模型，模型上下文为 32K. 作者还对模型进行 finetune 得到了 Mixtral 8x7B-Instruct, finetuning 包含 SFT 和 DPO 两个阶段。&lt;/p>
&lt;h2 id="method">Method
&lt;/h2>&lt;p>模型架构与 &lt;a class="link" href="https://maosong.website/p/mixstral-7b/" target="_blank" rel="noopener"
>Mistral-7B&lt;/a> 基本相同，参数如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Parameter&lt;/th>
&lt;th>Value&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>dim&lt;/code>&lt;/td>
&lt;td>4096&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>n_layers&lt;/code>&lt;/td>
&lt;td>32&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>head_dim&lt;/code>&lt;/td>
&lt;td>128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>hidden_dim&lt;/code>&lt;/td>
&lt;td>14336&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>n_heads&lt;/code>&lt;/td>
&lt;td>32&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>n_kv_heads&lt;/code>&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>window_size&lt;/code>&lt;/td>
&lt;td>4096&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>context_len&lt;/code>&lt;/td>
&lt;td>32768&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>vocab_size&lt;/code>&lt;/td>
&lt;td>32000&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>num_experts&lt;/code>&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>top_k_experts&lt;/code>&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>MoE 的架构与 &lt;a class="link" href="https://maosong.website/p/gshard/" target="_blank" rel="noopener"
>GShard&lt;/a> 基本一致&lt;/p>
&lt;h2 id="results">Results
&lt;/h2>&lt;p>作者探究了专家的 specialization, 结果有三点发现：&lt;/p>
&lt;ol>
&lt;li>不同专家对于不同 domain 的数据并没有出现 specialization&lt;/li>
&lt;li>在 math domain 上，专家的分布有一个明显的区别。&lt;/li>
&lt;li>连续的 token 往往会被分配到同一个专家上&lt;/li>
&lt;/ol>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者在本文提出了 Mistral 8x7B, 一个 MoE 大语言模型&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2401.04088" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Mixstral 7B</title><link>https://maosong2022.github.io/p/mixstral-7b/</link><pubDate>Sat, 01 Nov 2025 15:28:19 +0800</pubDate><guid>https://maosong2022.github.io/p/mixstral-7b/</guid><description>&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>作者主要强调 Mistral 7B 的表现超过了 LLaMA 2 7B 和 LLaMA 34B 的表现。&lt;/p>
&lt;p>Mistral 7B 主要使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-gqa/" target="_blank" rel="noopener"
>GQA&lt;/a> 以及 SWA 两个方法来加速推理和减少内存占用，进而提高 batch size 和 throughput.&lt;/p>
&lt;h2 id="method">Method
&lt;/h2>&lt;p>Mistral 7B 的模型配置如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Parameter&lt;/th>
&lt;th>Value&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>dim&lt;/code>&lt;/td>
&lt;td>4096&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>n_layers&lt;/code>&lt;/td>
&lt;td>32&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>head_dim&lt;/code>&lt;/td>
&lt;td>128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>hidden_dim&lt;/code>&lt;/td>
&lt;td>14336&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>n_heads&lt;/code>&lt;/td>
&lt;td>32&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>n_kv_heads&lt;/code>&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>window_size&lt;/code>&lt;/td>
&lt;td>4096&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>context_len&lt;/code>&lt;/td>
&lt;td>8192&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>vocab_size&lt;/code>&lt;/td>
&lt;td>32000&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>SWA 的示意图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/mixstral-7b/Mistral-7B-SWA.png"
width="1047"
height="387"
srcset="https://maosong2022.github.io/p/mixstral-7b/Mistral-7B-SWA_hu6372088832831988140.png 480w, https://maosong2022.github.io/p/mixstral-7b/Mistral-7B-SWA_hu8868497716526501398.png 1024w"
loading="lazy"
alt="Sliding Window Attention"
class="gallery-image"
data-flex-grow="270"
data-flex-basis="649px"
>&lt;/p>
&lt;p>对于第 $k$ 层，模型可以感知到 $W\times k$ 的 tokens, 进而可以在提高训练效率的同时保持模型的表现。作者发现，在 &lt;a class="link" href="https://maosong.website/p/notes-on-flashattention/" target="_blank" rel="noopener"
>flash attention&lt;/a> 中使用 SWA 之后，模型的效率提升了 2 倍左右。&lt;/p>
&lt;p>并且使用了 SWA 之后，我们的 kv cache 也就随之固定了，因此我们可以使用一个 rolling buff cache, 其大小为 $W$, 对于第 $i$ 个 token, 我们将其保存在 cache 中的第 $i % M$ 个位置。&lt;/p>
&lt;p>作者还进一步将 sequence 分割为多个 chunk, 每个 chunk 的大小都是 window size $M$, 这样在计算 attention 的时候，对于当前的 chunk, 我们使用 self-attention, 对于 cache 中的 attention, 我们使用 SWA, 然后对于 past token, 由于这部分不在 sliding window 内因此不参与计算&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/mixstral-7b/Mixtral-7B-prefill-and-chunking.png"
width="748"
height="307"
srcset="https://maosong2022.github.io/p/mixstral-7b/Mixtral-7B-prefill-and-chunking_hu1455448342047297388.png 480w, https://maosong2022.github.io/p/mixstral-7b/Mixtral-7B-prefill-and-chunking_hu8665872249214920433.png 1024w"
loading="lazy"
alt="Prefilling and Chunking"
class="gallery-image"
data-flex-grow="243"
data-flex-basis="584px"
>&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 Mistral 7B, 一个基于 GQA 和 SWA 的大语言模型。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2310.06825v1" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>ST-MoE</title><link>https://maosong2022.github.io/p/st-moe/</link><pubDate>Sat, 01 Nov 2025 15:23:58 +0800</pubDate><guid>https://maosong2022.github.io/p/st-moe/</guid><description>&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>已有的模型如 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseekmoe/" target="_blank" rel="noopener"
>DeepSeek-MoE&lt;/a>, &lt;a class="link" href="https://maosong.website/p/mixstral-8x7b/" target="_blank" rel="noopener"
>Mixtral MoE&lt;/a>, &lt;a class="link" href="https://maosong.website/search/?keyword=qwen1.5" target="_blank" rel="noopener"
>Qwen1.5&lt;/a> 等 MoE 模型基本只开源权重。也有一些开源的模型，比如 OpenMoE 等，但是开源信息不全。基于这个目的，作者提出了 olmoe 模型系列，包括 olmoe-7B-A1B 和 olmoe-7B-A1B-instruct 两个版本。&lt;/p>
&lt;h2 id="method">Method
&lt;/h2>&lt;h3 id="pretraining">Pretraining
&lt;/h3>&lt;p>模型的架构如下图所示，MoE 架构与 dense 架构不同的地方在于 decoder layer 中的 FFN 被替换为了 MoE layer.&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/st-moe/olmoe-MoE_architecture.png"
width="1356"
height="912"
srcset="https://maosong2022.github.io/p/st-moe/olmoe-MoE_architecture_hu7056192022238791486.png 480w, https://maosong2022.github.io/p/st-moe/olmoe-MoE_architecture_hu14138954601143358094.png 1024w"
loading="lazy"
alt="Architecture of olmoe"
class="gallery-image"
data-flex-grow="148"
data-flex-basis="356px"
>&lt;/p>
&lt;p>模型的配置如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/st-moe/olmoe-config.png"
width="900"
height="1201"
srcset="https://maosong2022.github.io/p/st-moe/olmoe-config_hu6222181885177926068.png 480w, https://maosong2022.github.io/p/st-moe/olmoe-config_hu5779946575980998108.png 1024w"
loading="lazy"
alt="Model configuration"
class="gallery-image"
data-flex-grow="74"
data-flex-basis="179px"
>&lt;/p>
&lt;p>训练的目标函数为&lt;/p>
$$
\mathcal{L} = \mathcal{L}_{CE} +\alpha\mathcal{L}_{LB} +\beta\mathcal{L}_{RZ}
$$&lt;p>其中 $\alpha,\beta$ 为系数， $\mathcal{L}&lt;em>{CE}$, $\mathcal{L}&lt;/em>{LB}$ 以及 $\mathcal{L}_{RZ}$ 分别代表 cross-entropy loss, load balancing loss 以及 routing Z loss.&lt;/p>
&lt;p>预训练数据包括 DCLM 和 Dolma1.7 两个数据集的混合，作者将预训练数据集称为&lt;strong>olmoe-mix&lt;/strong>. 数据集的配比如下&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/st-moe/olmoe-composition-pretraining-data.png"
width="1147"
height="441"
srcset="https://maosong2022.github.io/p/st-moe/olmoe-composition-pretraining-data_hu6221260531721970525.png 480w, https://maosong2022.github.io/p/st-moe/olmoe-composition-pretraining-data_hu18237277662731378337.png 1024w"
loading="lazy"
alt="Composition of the pretraining data"
class="gallery-image"
data-flex-grow="260"
data-flex-basis="624px"
>&lt;/p>
&lt;h3 id="post-training">Post-training
&lt;/h3>&lt;p>在 post-training 时，作者将训练分为 instruction tuning 和 preference tuning 两个阶段，在 instruction dataset 中，作者加入了更多的代码和数学数据来提高对应的能力。数据集如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/st-moe/olmoe-composition-post-training-data.png"
width="1208"
height="367"
srcset="https://maosong2022.github.io/p/st-moe/olmoe-composition-post-training-data_hu10322022158395140932.png 480w, https://maosong2022.github.io/p/st-moe/olmoe-composition-post-training-data_hu15587205861081401911.png 1024w"
loading="lazy"
alt="Post-training data"
class="gallery-image"
data-flex-grow="329"
data-flex-basis="789px"
>&lt;/p>
&lt;h2 id="ablation-study">Ablation Study
&lt;/h2>&lt;h3 id="moe-settings">MoE Settings
&lt;/h3>&lt;h4 id="moe-vs-dense">MoE vs. Dense
&lt;/h4>&lt;p>作者对比了 MoE 模型和 dense 模型的训练效率，为了方便对比，作者使用 olmo-7B 和 olmo-1B 作为 baseline, 最终 olmoe 的总参数为 6.9B, 激活参数为 1.3B. 实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/st-moe/olmoe-MoE-vs-Dense-training-efficiency.png"
width="1155"
height="626"
srcset="https://maosong2022.github.io/p/st-moe/olmoe-MoE-vs-Dense-training-efficiency_hu15631484222114504499.png 480w, https://maosong2022.github.io/p/st-moe/olmoe-MoE-vs-Dense-training-efficiency_hu2616226216353592856.png 1024w"
loading="lazy"
alt="MoE vs. Dense"
class="gallery-image"
data-flex-grow="184"
data-flex-basis="442px"
>&lt;/p>
&lt;p>实验结果发现，MoE 模型所需要的 token 或者 FLOPs 是 dense 模型的 $1/3$, 但是由于 MoE 模型需要额外的内存开销，因此从训练时间上来看，MoE 模型训练时间仅比 dense 模型快 $2$ 倍左右。&lt;/p>
&lt;h4 id="expert-granularity">Expert Granularity
&lt;/h4>&lt;p>&lt;a class="link" href="https://maosong.website/p/notes-on-deepseekmoe/" target="_blank" rel="noopener"
>DeepSeekMoE&lt;/a> 提出使用细粒度的专家来提供更多的组合可能性。作者探究了不同的粒度对模型表现的影响，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/st-moe/olmoe-expert-granularity.png"
width="1446"
height="592"
srcset="https://maosong2022.github.io/p/st-moe/olmoe-expert-granularity_hu11423684369230573666.png 480w, https://maosong2022.github.io/p/st-moe/olmoe-expert-granularity_hu8462329692609096647.png 1024w"
loading="lazy"
alt="Expert granularity"
class="gallery-image"
data-flex-grow="244"
data-flex-basis="586px"
>&lt;/p>
&lt;p>结果显示，当专家粒度从 8E-1A 扩展到 32E-4A 时，模型在 HellaSwag 上的表现提升了 $10%$, 但是进一步扩展到 64E-8A 时，模型的表现提升不到 $2%$, 这说明了无限制提升粒度对模型的提升越来越有限。在本文中，作者使用了 64 个专家。&lt;/p>
&lt;h4 id="shared-experts">Shared Experts
&lt;/h4>&lt;p>&lt;a class="link" href="https://maosong.website/p/notes-on-deepseekmoe/" target="_blank" rel="noopener"
>DeepSeekMoE&lt;/a> 提出使用共享专家来学习 common knowledge, 作者对这种方法进行了实验，结果如下图所示。&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/st-moe/olmoe-shared-experts.png"
width="1156"
height="477"
srcset="https://maosong2022.github.io/p/st-moe/olmoe-shared-experts_hu17801868202021072630.png 480w, https://maosong2022.github.io/p/st-moe/olmoe-shared-experts_hu1657113941603463913.png 1024w"
loading="lazy"
alt="Shared experts"
class="gallery-image"
data-flex-grow="242"
data-flex-basis="581px"
>&lt;/p>
&lt;p>可以看到，加入一个 shared expert 之后，模型的表现没有变化，作者认为减少 routed expert 之后，模型的组合可能性降低为原来的 $10%$ 左右。因此作者认为没有必要使用共享专家，因此作者在 olmoe 中没有采用共享专家这个方法。&lt;/p>
&lt;h4 id="expert-choice-vs-token-choice">Expert Choice vs. Token Choice
&lt;/h4>&lt;p>作者探究了 routing 的策略，一个是 expert choice (EC), 另一种是 token choice (TC), 分别代表了每个 expert 选取固定的 token 数和每个 token 选取固定的 expert 数这两种情况。实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/st-moe/olmoe-routing-strategy.png"
width="1157"
height="450"
srcset="https://maosong2022.github.io/p/st-moe/olmoe-routing-strategy_hu14576683236097856598.png 480w, https://maosong2022.github.io/p/st-moe/olmoe-routing-strategy_hu12605903787078733490.png 1024w"
loading="lazy"
alt="Expert choice (EC) vs. token choice (TC)"
class="gallery-image"
data-flex-grow="257"
data-flex-basis="617px"
>&lt;/p>
&lt;p>可以看到，token choice 的表现明显更好。EC 虽然可以实现负载均衡。但是因为自回归模型在生成时是无法提前确定生成的 token 数的，因此 EC 很可能导致算力资源浪费或者是 token dropping. 在本文中，作者采用了 TC 这种策略。&lt;/p>
&lt;h4 id="sparse-upcycling">Sparse Upcycling
&lt;/h4>&lt;p>作者还对比了从零开始训练 MoE 与基于 dense model upcycling 的方式训练 MoE，sparse upcycling 的相关工作有 MiniCPM, &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2/" target="_blank" rel="noopener"
>Qwen2&lt;/a> 以及 &lt;a class="link" href="https://maosong.website/p/mixstral-8x7b/" target="_blank" rel="noopener"
>Mixtral MoE&lt;/a>.结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/st-moe/olmoe-sparse-upcycling.png"
width="1440"
height="680"
srcset="https://maosong2022.github.io/p/st-moe/olmoe-sparse-upcycling_hu17851074497901335192.png 480w, https://maosong2022.github.io/p/st-moe/olmoe-sparse-upcycling_hu11258886835187859490.png 1024w"
loading="lazy"
alt="Sparse upcycling"
class="gallery-image"
data-flex-grow="211"
data-flex-basis="508px"
>&lt;/p>
&lt;p>结果发现，upcycling 确实可以提高训练效率，但是这种方法的缺陷在于：&lt;/p>
&lt;ol>
&lt;li>upcycling 受 dense model 的超参数限制&lt;/li>
&lt;li>upcycling 的训练不是很稳定&lt;/li>
&lt;/ol>
&lt;p>因此在本文中作者没有采取 upcycling 的做法。&lt;/p>
&lt;h4 id="load-balancing-loss">Load Balancing Loss
&lt;/h4>&lt;p>作者还探究 Load Balancing loss 对模型表现的影响，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/st-moe/olmoe-load-balancing-loss.png"
width="1450"
height="532"
srcset="https://maosong2022.github.io/p/st-moe/olmoe-load-balancing-loss_hu18064338514520462513.png 480w, https://maosong2022.github.io/p/st-moe/olmoe-load-balancing-loss_hu3485135402316134026.png 1024w"
loading="lazy"
alt="Impact of applying a load balancing loss"
class="gallery-image"
data-flex-grow="272"
data-flex-basis="654px"
>&lt;/p>
&lt;p>可以看到，加入 load balancing loss 之后，模型的表现均超过了不加时的表现。&lt;/p>
&lt;p>作者进一步分析了不同专家在加/不加 load balancing loss 时的激活情况，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/st-moe/ollmoe-load-balancing-expert-assignment.png"
width="1443"
height="566"
srcset="https://maosong2022.github.io/p/st-moe/ollmoe-load-balancing-expert-assignment_hu10314573392730023514.png 480w, https://maosong2022.github.io/p/st-moe/ollmoe-load-balancing-expert-assignment_hu15306812845288590871.png 1024w"
loading="lazy"
alt="Expert assignment during training"
class="gallery-image"
data-flex-grow="254"
data-flex-basis="611px"
>&lt;/p>
&lt;p>结果显示，load balancing loss 确实可以让不同专家的激活概率大致相当。&lt;/p>
&lt;h4 id="router-z-loss">Router Z-loss
&lt;/h4>&lt;p>&lt;a class="link" href="https://maosong.website/p/st-moe/" target="_blank" rel="noopener"
>ST-MoE&lt;/a> 提出了 Router z-loss 来提高 MOE 训练的稳定性和表现。其表达式如下所示&lt;/p>
$$
\mathcal{L}_{RZ}(x) = \frac{1}{B}\sum_{i=1}^B\left(\log\sum_{j=1}^{N_E}\exp(x_i^{(i)})\right)^2
$$&lt;p>实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/st-moe/olmoe-routing-z-loss.png"
width="1162"
height="419"
srcset="https://maosong2022.github.io/p/st-moe/olmoe-routing-z-loss_hu7327018522268719766.png 480w, https://maosong2022.github.io/p/st-moe/olmoe-routing-z-loss_hu15269971373468008426.png 1024w"
loading="lazy"
alt="Router z-loss"
class="gallery-image"
data-flex-grow="277"
data-flex-basis="665px"
>&lt;/p>
&lt;p>可以看到，加入 router Z-loss 之后，模型训练的稳定性有所提升。因此在本文中作者使用了这个 loss.&lt;/p>
&lt;h3 id="general-pre-training-settings">General Pre-training Settings
&lt;/h3>&lt;h4 id="initialization">Initialization
&lt;/h4>&lt;p>作者探究了不同初始化策略对模型训练的影响，结果发现使用 truncate normal initialization 的训练稳定性更高&lt;/p>
&lt;h4 id="qk-norm">QK-Norm
&lt;/h4>&lt;p>作者探究了 &lt;a class="link" href="https://maosong.website/p/notes-on-qk-norm/" target="_blank" rel="noopener"
>QK-norm&lt;/a> 对模型训练的影响，结果发现 QK-norm 可以提高模型训练的稳定性，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/st-moe/olmoe-QK-norm-ablation.png"
width="1157"
height="306"
srcset="https://maosong2022.github.io/p/st-moe/olmoe-QK-norm-ablation_hu1341256823894447241.png 480w, https://maosong2022.github.io/p/st-moe/olmoe-QK-norm-ablation_hu14266747158761727550.png 1024w"
loading="lazy"
alt="ablation study on QK-Norm"
class="gallery-image"
data-flex-grow="378"
data-flex-basis="907px"
>&lt;/p>
&lt;h4 id="adamw-epsilon">AdamW Epsilon
&lt;/h4>&lt;p>作者发现，在 &lt;a class="link" href="https://maosong.website/p/notes-on-adamw/" target="_blank" rel="noopener"
>AdamW&lt;/a> 优化器中，使用更小的 &lt;code>eps&lt;/code> 可以提高模型的表现，因此作者将 &lt;code>eps&lt;/code> 设置为 $1e-8$.&lt;/p>
&lt;h4 id="adaptation-settings">Adaptation Settings
&lt;/h4>&lt;p>在 post-training 阶段，作者在三个方面进行了实验：&lt;/p>
&lt;ol>
&lt;li>是否加入 load balancing loss: 结论是不加，因为负载均衡在 pre-training 阶段已经实现了&lt;/li>
&lt;li>是否使用 annealing: 结论是使用，因为效果更好&lt;/li>
&lt;li>使用 DPO 还是 KTO, 结论是两种方法结果差不多&lt;/li>
&lt;/ol>
&lt;p>实验结果如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/st-moe/olmoe-adaptation-experiments.png"
width="1105"
height="588"
srcset="https://maosong2022.github.io/p/st-moe/olmoe-adaptation-experiments_hu12941725841537986665.png 480w, https://maosong2022.github.io/p/st-moe/olmoe-adaptation-experiments_hu5486252117953152034.png 1024w"
loading="lazy"
alt="Adaptation experiments"
class="gallery-image"
data-flex-grow="187"
data-flex-basis="451px"
>&lt;/p>
&lt;h4 id="load-balancing-precision">Load Balancing Precision
&lt;/h4>&lt;p>&lt;a class="link" href="https://maosong.website/p/switch-transformer/" target="_blank" rel="noopener"
>Switch Transformer&lt;/a> 中提出使用 &lt;code>float32&lt;/code> 精度来进行 routing 的计算，作者通过实验发现，这一方法并不能提高模型训练的稳定性，因此作者没有采用这一策略。&lt;/p>
&lt;h2 id="moe-analysis">MoE Analysis
&lt;/h2>&lt;h3 id="router-saturation">Router Saturation
&lt;/h3>&lt;p>作者探究了训练过程中激活的专家和训练结束后激活的专家的匹配程度，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/st-moe/olmoe-router-saturation.png"
width="1149"
height="393"
srcset="https://maosong2022.github.io/p/st-moe/olmoe-router-saturation_hu7667431516082233346.png 480w, https://maosong2022.github.io/p/st-moe/olmoe-router-saturation_hu5058911977226073561.png 1024w"
loading="lazy"
alt="Router saturation"
class="gallery-image"
data-flex-grow="292"
data-flex-basis="701px"
>&lt;/p>
&lt;p>结果发现，训练 $1%$ 的数据之后，就有 $40%$ 的 routing 和训练完毕的 routing 一致，当训练 $40%$ 的数据之后，这个比例提升到了 $80%$.&lt;/p>
&lt;p>作者还发现，later layers 比 early layers 饱和更快，early layer, 特别是 layer 0, 饱和的非常慢。作者认为，这是 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseekmoe/" target="_blank" rel="noopener"
>DeepSeekMoE&lt;/a> 放弃在第一层使用 MoE layer 的原因，因为 load balancing loss 收敛更慢。&lt;/p>
&lt;h3 id="expert-co-activation">Expert Co-activation
&lt;/h3>&lt;p>作者分析了 expert 之间的互相依赖程度，作者通过可视化发现，不同的 expert 之间 co-activation 的比例比较小，说明 expert redundancy 比较低&lt;/p>
&lt;h3 id="domain-specialization">Domain Specialization
&lt;/h3>&lt;p>作者还探究了不同 expert 对于不同 domain 的 specialization 程度，作者发现对于 specialized domain 的数据，expert 会出现一定程度的 specialization, 但是对于通用 domain 的数据，expert 的 specialization 程度比较低。这个结论与 &lt;a class="link" href="https://maosong.website/p/mixstral-8x7b/" target="_blank" rel="noopener"
>Mixtral MoE&lt;/a> 的结论不同，作者认为这个原因是 &lt;a class="link" href="https://maosong.website/p/mixstral-8x7b/" target="_blank" rel="noopener"
>Mixtral MoE&lt;/a> 使用了 upcycling 的方式，这会限制模型的表现。因此，作者进一步强调 MoE 从零开始训练是一个更好的训练方式。&lt;/p>
&lt;h3 id="vocabulary-specialization">Vocabulary Specialization
&lt;/h3>&lt;p>作者还探究了 vocabulary 中不同 token index 与激活专家之间的关系，结果发现 later layers 的 specialization 程度更高，这与 saturation 的趋势一致&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 olmoe, 一个全开源的 moe 大模型系列，作者详细介绍了针对 MoE 架构和通用架构的设计，为后来的模型架构设计提供了基础。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2409.02060" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>GShard</title><link>https://maosong2022.github.io/p/gshard/</link><pubDate>Wed, 29 Oct 2025 11:22:39 +0800</pubDate><guid>https://maosong2022.github.io/p/gshard/</guid><description>&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>作者认为，训练大规模的模型存在如下问题：&lt;/p>
&lt;ol>
&lt;li>缺乏有效的 Model parallelism 算法&lt;/li>
&lt;li>随着设备数的增加，训练时间与 model size 呈现超线性增长的关系&lt;/li>
&lt;li>(tensorflow) 在 nodes 数增多时，构建所需要的时间也大幅度增长&lt;/li>
&lt;li>在多个设备上 partition model 比较困难&lt;/li>
&lt;/ol>
&lt;p>作者在本文中构建了一个基于 sparse MoE 架构的 600B 模型。为了解决这些问题，作者作出了如下贡献：&lt;/p>
&lt;ol>
&lt;li>作者提出了基于 MoE 架构的模型，来减少计算和通信开销&lt;/li>
&lt;li>作者提出了 Gshard, 来自动化实现并行&lt;/li>
&lt;li>作者提出了 SPMD (single program multiple data) 来减少计算表示和编译的难度&lt;/li>
&lt;/ol>
&lt;h2 id="method">Method
&lt;/h2>&lt;p>架构如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/gshard/GShard-model-architecture.png"
width="1118"
height="713"
srcset="https://maosong2022.github.io/p/gshard/GShard-model-architecture_hu18193360422981598966.png 480w, https://maosong2022.github.io/p/gshard/GShard-model-architecture_hu1150641733643218422.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="156"
data-flex-basis="376px"
>&lt;/p>
&lt;p>其中激活专家个数为 2 个，MoE layer 和 FFN layer 交替出现。&lt;/p>
&lt;p>作者对 GATE 函数进行了如下优化：&lt;/p>
&lt;ol>
&lt;li>expert capacity. 每个 expert 都有一个处理 token 的上限值，超过该上限值之后 GATE 直接输出 0 进入下一层，假设 batch size 为 $N$, 专家个数为 $E$, 则该阈值定义为 $N/E$&lt;/li>
&lt;li>group dispatching. 将 token 拆分为 $G$ 个 group, 每个 group 并行处理，每个 group 里每个专家处理的 token 个数上限为 $N/(GE)$.&lt;/li>
&lt;li>Auxiliary loss. 作者使用了 &lt;a class="link" href="Load%20Balancing%20loss.md" >Load Balancing loss&lt;/a> 来实现负载均衡&lt;/li>
&lt;li>Random routing. 作者选取了 top-2 的专家，当第二个专家的权重太小是，作者直接忽略第二个专家，简化为选取 top-1 的专家&lt;/li>
&lt;/ol>
&lt;p>算法运行如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/gshard/GShard-expert-computation.png"
width="1124"
height="917"
srcset="https://maosong2022.github.io/p/gshard/GShard-expert-computation_hu10173219915397521593.png 480w, https://maosong2022.github.io/p/gshard/GShard-expert-computation_hu2905974338988159778.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="122"
data-flex-basis="294px"
>&lt;/p>
&lt;h2 id="parallel-implementation">Parallel Implementation
&lt;/h2>&lt;p>第一步是将算法转化为线性代数的方式，算法的代码如下所示&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">gates&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">softmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">einsum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;GSM, ME-&amp;gt;GSE&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">inputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">wg&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">combine_weights&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dispatch_mask&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Top2Gating&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">gates&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">dispatched_expert_inputs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">einsum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;GSEC, GSM-&amp;gt;EGCM&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dispatch_mask&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">reshaped_inputs&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">h&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">einsum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;EGCM, EMH-&amp;gt;EGCH&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dispatched_expert_inputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">wi&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">h&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">relu&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">h&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">expert_outputs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">einsum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;EGCH, EHM-&amp;gt;GECM&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">h&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">wo&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">outputs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">einsum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;GSEC, GECM-&amp;gt;GSM&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">combine_weights&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">expert_outputs&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>第二步是通过 API 来实现并行执行&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Partition inputs along group (G) dim. &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">inputs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">split&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">inputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">D&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Replicate the gating weights&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">wg&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">replicate&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">wg&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">gates&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">softmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">einsum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;GSM, ME-&amp;gt;GSE&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">inputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">wg&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">combine_weights&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dispatch_mask&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Top2Gating&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">gating_logits&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">dispatched_expert_inputs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">einsum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;GSEC, GSM-&amp;gt;EGCM&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dispatch_mask&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">reshaped_inputs&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Partition dispatched inputs along expert (E) dim.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">dispatched_expert_inputs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">split&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dispatched_expert_inputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">D&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">h&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">einsum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;EGCM, EMH-&amp;gt;EGCH&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dispatched_expert_inputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">wi&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>第三部就是基于 compiler infra 来基于 sharding annotation 自动化分割一个 computation graph.&lt;/p>
&lt;h2 id="experiments">Experiments
&lt;/h2>&lt;p>作者在机器翻译的任务上训练了若干模型，结果发现：&lt;/p>
&lt;ol>
&lt;li>层数更多的模型表现更好&lt;/li>
&lt;li>提高 expert capacity 有效提高模型的表现&lt;/li>
&lt;li>使用更多的 expert 可以在 high-resourced 任务上提高表现&lt;/li>
&lt;li>dense 模型相比于 MoE 模型拥有更强的迁移能力&lt;/li>
&lt;/ol>
&lt;p>从训练效率上来看&lt;/p>
&lt;ol>
&lt;li>层数更多的模型的 sample efficiency 也更高&lt;/li>
&lt;li>600B 的模型也可以在 4 天之内训练完毕&lt;/li>
&lt;/ol>
&lt;p>从内存使用效率上来看&lt;/p>
&lt;ol>
&lt;li>层数相同时，weight memory 以及 activation memory 不随专家个数增加而增加&lt;/li>
&lt;li>专家个数比较少（128）时，模型可以达到 roofline performance 的 $70%$, 专家个数比较多（2048）时，模型依然可以达到 roofline performance 的 $48%$.&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://maosong2022.github.io/p/gshard/GShard-execution-time-breakdown.png"
width="1064"
height="521"
srcset="https://maosong2022.github.io/p/gshard/GShard-execution-time-breakdown_hu4516039562048509286.png 480w, https://maosong2022.github.io/p/gshard/GShard-execution-time-breakdown_hu5230886350146491299.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="204"
data-flex-basis="490px"
>&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者探究了如何提高大规模模型的训练效率，作者提出了基于 MoE 架构的模型，然后作者还设计了 GShard, 一个自动化分割大规模模型的深度学习模块。实现结果发现，增加模型 size 可以提高模型的表现。作者还发现，SPMD 是一个更好的提高计算效率的方法。&lt;/p>
&lt;h2 id="reference">Reference
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2006.16668" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>ST-MoE</title><link>https://maosong2022.github.io/p/st-moe/</link><pubDate>Wed, 29 Oct 2025 11:19:37 +0800</pubDate><guid>https://maosong2022.github.io/p/st-moe/</guid><description>&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>已有的工作如 &lt;a class="link" href="https://maosong.website/p/GShard.md" target="_blank" rel="noopener"
>GShard&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/switch-transformer/" target="_blank" rel="noopener"
>Switch Transformer&lt;/a> 均验证了 MoE 模型的有效性，但是他们的问题在于训练不稳定。&lt;/p>
&lt;p>因此，在本文中，作者就提出了 ST-MoE 来解决这个问题。作者的主要贡献如下：&lt;/p>
&lt;ol>
&lt;li>探究了如何平衡模型的表现与训练稳定性&lt;/li>
&lt;li>提出了 router Z-loss 来解决训练的不稳定性&lt;/li>
&lt;li>探究了如何设定 MoE 模型 fine tuning 时的超参数&lt;/li>
&lt;li>对 MoE 的性质进行了分析&lt;/li>
&lt;/ol>
&lt;h2 id="training-stability">Training Stability
&lt;/h2>&lt;p>作者发现，直接训练 MoE 模型会面临训练不稳定性的问题，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/st-moe/ST-MoE-training-instabilities.png"
width="1040"
height="420"
srcset="https://maosong2022.github.io/p/st-moe/ST-MoE-training-instabilities_hu7326905665646279246.png 480w, https://maosong2022.github.io/p/st-moe/ST-MoE-training-instabilities_hu5976367835112795528.png 1024w"
loading="lazy"
alt="Training instabilities for sparse models."
class="gallery-image"
data-flex-grow="247"
data-flex-basis="594px"
>&lt;/p>
&lt;p>可以看到不同的实验，一次训练崩溃，一次训练正常。&lt;/p>
&lt;p>作者接下来探究了如何提高训练的稳定性，作者有三点发现：&lt;/p>
&lt;ol>
&lt;li>大多数方法都可以提高训练稳定性，但是也会是的模型表现更差&lt;/li>
&lt;li>router Z-loss 可以在不损失模型表现的情况下提高训练的稳定性&lt;/li>
&lt;li>multiplicative components 如 RMSNorm 等可以提高模型表现，但是训练稳定性更差&lt;/li>
&lt;/ol>
&lt;p>作者构建了一个 baseline 的 MoE 模型，总专家个数为 $N=32$, 激活专家个数为 $K=2$, Transformer block 按照 4 个 block 为一组，每组里包含一个 MoE layer. 为了避免初始化带来的误差，作者使用 6 个随机数种子进行训练。&lt;/p>
&lt;h3 id="multiplicative-components">Multiplicative Components
&lt;/h3>&lt;p>multiplicative components 指的是 GEGLU 和 RMSNorm 这些操作，其实验结果如下所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Method&lt;/th>
&lt;th>Fraction Stable&lt;/th>
&lt;th>Quality&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Baseline&lt;/td>
&lt;td>4/6&lt;/td>
&lt;td>$-1.755 \pm0.02$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Remove GEGLU&lt;/td>
&lt;td>3/3&lt;/td>
&lt;td>$-1.849 \pm0.02$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Remove RMS Norm. Scale Param&lt;/td>
&lt;td>3/3&lt;/td>
&lt;td>$-2.020 \pm0.06$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>结果发现，移除掉 multiplicative components 之后，模型训练变得稳定，但是效果也变差了。&lt;/p>
&lt;h3 id="adding-noise">Adding Noise
&lt;/h3>&lt;p>接下来作者尝试了 &lt;a class="link" href="https://maosong.website/p/switch-transformer/" target="_blank" rel="noopener"
>Switch Transformer&lt;/a> 中的 jitter noise 和 dropoout 方法，实验结果如下图所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Method&lt;/th>
&lt;th>Fraction Stable&lt;/th>
&lt;th>Quality&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Baseline&lt;/td>
&lt;td>4/6&lt;/td>
&lt;td>$-1.755 ± 0.02$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Input jitter ($10^{-2}$)&lt;/td>
&lt;td>3/3&lt;/td>
&lt;td>$-1.777 \pm 0.03$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Dropout (0.1)&lt;/td>
&lt;td>3/3&lt;/td>
&lt;td>$-1.822 \pm0.11$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>实验结果显示，加入噪声对模型的表现存在负面影响。&lt;/p>
&lt;h3 id="constraining-activations">Constraining Activations
&lt;/h3>&lt;p>作者接下来分析了以下 &lt;a class="link" href="https://maosong.website/p/switch-transformer/" target="_blank" rel="noopener"
>Switch Transformer&lt;/a> 中 router 存在的问题，作者发现尽管在 router 中使用 &lt;code>float32&lt;/code> 可以提高训练稳定性，但是这还不够，因此作者使用了如下的 router Z-loss:&lt;/p>
$$
\mathcal{L}_z(x) = \frac1B\sum_{i=1}^B\left(\log\sum_{j=1}^N e^{x_j^{(i)}}\right)^2
$$&lt;p>其中 $B, N$ 分别是 batch size 和专家个数，$x_j^{(i)}$ 代表了第 $j$ 个专家对 $i$ 个 token 的激活 logits. 通过增加这个 loss, 我们可以让 routing layer 的 logits 都在一个合理的范围内。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Method&lt;/th>
&lt;th>Fraction Stable&lt;/th>
&lt;th>Quality ($\uparrow$)&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Baseline&lt;/td>
&lt;td>$4/6$&lt;/td>
&lt;td>$-1.755 \pm 0.02$&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Update clipping (clip = 0.1)&lt;/td>
&lt;td>$3/3$&lt;/td>
&lt;td>$-4.206 \pm 0.17$&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Router Z-Loss&lt;/td>
&lt;td>$3/3$&lt;/td>
&lt;td>$-1.741 \pm 0.02$&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，加入 router Z-loss 之后，模型的稳定性和表现都有了提升。&lt;/p>
&lt;p>因此，在本文中，作者使用的损失函数为&lt;/p>
$$
\mathcal{L} = \mathcal{L}_{CE} + \alpha \mathcal{L}_B + \beta \mathcal{L}_z
$$&lt;p>其中 $\mathcal{L}_{CE}, \mathcal{L}_B$ 分别是 cross-entropy loss 和 Load Balancing loss, $\alpha,\beta$ 是对应 loss 的权重。&lt;/p>
&lt;h3 id="numerical-precision">Numerical Precision
&lt;/h3>&lt;p>接下来作者探究了数值精度对训练效率和稳定性的影响，作者发现使用低精度进行训练的优势有：&lt;/p>
&lt;ol>
&lt;li>通信开销更小&lt;/li>
&lt;li>计算消耗更小&lt;/li>
&lt;li>内存需求更小&lt;/li>
&lt;/ol>
&lt;p>但是低精度进行训练的问题是存在严重的 roundoff error, 其示意图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/st-moe/ST-MoE-numerical-precision-and-roundoff.png"
width="925"
height="249"
srcset="https://maosong2022.github.io/p/st-moe/ST-MoE-numerical-precision-and-roundoff_hu1402581372307421845.png 480w, https://maosong2022.github.io/p/st-moe/ST-MoE-numerical-precision-and-roundoff_hu14172820709588307084.png 1024w"
loading="lazy"
alt="Numerical precision formats and roundoff errors."
class="gallery-image"
data-flex-grow="371"
data-flex-basis="891px"
>&lt;/p>
&lt;h2 id="fine-tuning">Fine-tuning
&lt;/h2>&lt;p>作者在本节探究了如何 fine-tune 一个 MoE 模型。&lt;/p>
&lt;p>作者首先通过实验给出了 MoE 模型容易过拟合的结论，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/st-moe/ST-MoE-overfitting-of-MoE.png"
width="1138"
height="456"
srcset="https://maosong2022.github.io/p/st-moe/ST-MoE-overfitting-of-MoE_hu15053576913876449027.png 480w, https://maosong2022.github.io/p/st-moe/ST-MoE-overfitting-of-MoE_hu11759343686158783167.png 1024w"
loading="lazy"
alt="Sparse models are prone to overfit."
class="gallery-image"
data-flex-grow="249"
data-flex-basis="598px"
>&lt;/p>
&lt;p>可以看到，在两个人呢误伤，MoE 模型相比于 dense 模型都更容易过拟合。&lt;/p>
&lt;p>接下来作者探究了超参数特别是 batch size 和 learning rate 对模型表现的影响，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/st-moe/ST-MoE-hyperparameter-sensitivity.png"
width="1036"
height="416"
srcset="https://maosong2022.github.io/p/st-moe/ST-MoE-hyperparameter-sensitivity_hu2550488555205211608.png 480w, https://maosong2022.github.io/p/st-moe/ST-MoE-hyperparameter-sensitivity_hu9603778225196677401.png 1024w"
loading="lazy"
alt="Batch size and learning rate sensitivity"
class="gallery-image"
data-flex-grow="249"
data-flex-basis="597px"
>&lt;/p>
&lt;p>可以看到，与 dense 模型相反，MoE 模型需要更大的 batch size 以及更小的 learning rate.&lt;/p>
&lt;h2 id="design-sparse-models">Design Sparse Models
&lt;/h2>&lt;p>首先作者分析了专家个数 $N$ 的设置，作者认为当专家个数超过 $256$ 之后，带来的收益就微乎其微了。并且，当专家个数增加之后，虽然算力没有变化，但是通信开销以及计算优化会变得更加困难。作者还认为，在 TPU 上，每个 core 应该进放置一个 expert&lt;/p>
&lt;p>作者还对 capacity factor 进行了实验，实验结果显示，提升 capacity factor 可以提高模型的表现。但是同时也会带来计算开销，从而让模型训练更慢，实验结果如下图所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Train CF&lt;/th>
&lt;th>Step Time (s) ($\downarrow$)&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>ST-MoE-L&lt;/td>
&lt;td>1.25&lt;/td>
&lt;td>$2.397$&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ST-MoE-L&lt;/td>
&lt;td>2.0&lt;/td>
&lt;td>$2.447 (+7%)$&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ST-MoE-32B&lt;/td>
&lt;td>1.25&lt;/td>
&lt;td>$4.244$&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ST-MoE-32B&lt;/td>
&lt;td>2.0&lt;/td>
&lt;td>$4.819 (+14%)$&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>最终结论如下：&lt;/p>
&lt;blockquote>
&lt;ol>
&lt;li>使用 top-2 routing, 即激活 $K=2$ 个专家，capacity factor 设置为 $1.25$, 每个 core 上最多放置一个专家&lt;/li>
&lt;li>在评估时可以动态调整 capacity factor&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;h2 id="tracing-tokens-through-the-model">Tracing Tokens through the Model
&lt;/h2>&lt;p>在这一节里，作者分析了 MoE 模型专家的性质。&lt;/p>
&lt;p>首先，作者发现，每一层里，至少有一个专家对于 sentinel token 比较敏感。并且，encoder 里的专家 specialization 更强，比如某些专家会负责 punctuation, verbs, proper names 等&lt;/p>
&lt;p>接下来，作者发现，专家的 specialization 在 decoder 里几乎没有，作者认为这是因为 target token distribution 不同导致的，即：&lt;/p>
&lt;ol>
&lt;li>decoding 是只有一小部分 token 被一个专家处理&lt;/li>
&lt;li>decoding 过程中大部分 token 都是 sentinel token&lt;/li>
&lt;/ol>
&lt;p>因此，每个 group 相比于 encoder 来说通常只 cover 一小部分的 semantic space&lt;/p>
&lt;p>encoder 和 decoder 专家的 specialization 实验结果如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Layer 1&lt;/th>
&lt;th>Layer 2&lt;/th>
&lt;th>Layer 3&lt;/th>
&lt;th>Layer 4&lt;/th>
&lt;th>Layer 5&lt;/th>
&lt;th>Layer 6&lt;/th>
&lt;th>Uniform (32-experts)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Encoder&lt;/td>
&lt;td>2.2&lt;/td>
&lt;td>1.8&lt;/td>
&lt;td>1.6&lt;/td>
&lt;td>1.7&lt;/td>
&lt;td>1.7&lt;/td>
&lt;td>1.2&lt;/td>
&lt;td>3.5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Decoder&lt;/td>
&lt;td>3.4&lt;/td>
&lt;td>3.4&lt;/td>
&lt;td>3.4&lt;/td>
&lt;td>3.4&lt;/td>
&lt;td>3.4&lt;/td>
&lt;td>3.4&lt;/td>
&lt;td>3.5&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，decoder 的专家分布和均匀分布差不多，这说明 decoder 里的专家 specialization 更弱。&lt;/p>
&lt;p>作者还探究了不同专家对于不同语言的敏感程度，结果发现，专家对于不同语言并没有出现明显的 specialization 现象，作者分析原因认为，输入一般只含有一种语言，因此各个专家均需要去处理对应语言的 token, 这就导致了专家对于不同语种的数据处理基本上没有太大差别。&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>在本文中，作者提出了 ST-MoE 系列 MoE 大语言模型，作者通过实验优化了 MoE 模型训练的不稳定性问题。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2202.08906" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Switch Transformer</title><link>https://maosong2022.github.io/p/switch-transformer/</link><pubDate>Tue, 28 Oct 2025 09:38:12 +0800</pubDate><guid>https://maosong2022.github.io/p/switch-transformer/</guid><description>&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>&lt;a class="link" href="https://maosong.website/p/kaplan-scaling-law/" target="_blank" rel="noopener"
>Kaplan scaling law&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/chinchilla-scaling-law/" target="_blank" rel="noopener"
>Chinchilla scaling law&lt;/a> 已经给出了针对 dense transformer model 的 scaling law, 即在给定算力下如何找到最优的 model size 和 dataset size. 已有的工作如 T5 主要是扩展 dense model 来达到更高的表现, 但是他们的问题是对算力要求比较高。&lt;/p>
&lt;p>为了解决这个问题，作者尝试在不增加算力的情况下提升模型的参数量，为了达到这个目的，作者构建了基于 MoE 架构的模型 Switch Transformer.&lt;/p>
&lt;p>作者的主要贡献如下：&lt;/p>
&lt;ol>
&lt;li>提出了基于 MoE 架构的 Switch Transformer model&lt;/li>
&lt;li>探究了针对 MoE 架构的 scaling law&lt;/li>
&lt;li>将 MoE model 的能力蒸馏到 small dense model 里去&lt;/li>
&lt;li>若干提升训练效率和稳定性的技巧&lt;/li>
&lt;/ol>
&lt;h2 id="architecture">Architecture
&lt;/h2>&lt;p>Switch Transformer 的架构如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-Architecture.png"
width="1130"
height="571"
srcset="https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-Architecture_hu15565212143753599310.png 480w, https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-Architecture_hu6275654350693265782.png 1024w"
loading="lazy"
alt="Architecture of Switch Transformer"
class="gallery-image"
data-flex-grow="197"
data-flex-basis="474px"
>&lt;/p>
&lt;h3 id="moe">MoE
&lt;/h3>&lt;p>MoE 的定义见 &lt;a class="link" href="MoE%20tutorial.md" >MoE tutorial&lt;/a>, 我们假设有 $N$ 个专家，其中激活 $K$ 个专家。&lt;/p>
&lt;p>之前的工作认为我们只有在激活 $&amp;gt;2$ 个专家时，模型才能够比较好的训练，但是在本文中，作者决定只使用 1 个 expert, 也就是 $K=1$. 作者将激活一个专家的 layer 称为 &lt;strong>Switch layer&lt;/strong>.&lt;/p>
&lt;p>作者认为 Switch Layer 有三个优势：&lt;/p>
&lt;ol>
&lt;li>router computation 现在只需要将每个 token route 到 1 个 expert&lt;/li>
&lt;li>每个专家的 capacity 更小，负载更加均衡&lt;/li>
&lt;li>routing 的实现更简单，且通信开销也降低了&lt;/li>
&lt;/ol>
&lt;h3 id="efficient-sparse-routing">Efficient Sparse Routing
&lt;/h3>&lt;p>作者首先定义了&lt;strong>expert capacity&lt;/strong>, 也就是每个 expert 处理的 token 数量，其定义如下&lt;/p>
$$
\text{expert capacity} = \left(\frac{\text{tokens per batch}}{\text{number of experts}}\right) * \text{capacity factor}
$$&lt;p>其示意图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-expert-capacity.png"
width="1269"
height="479"
srcset="https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-expert-capacity_hu15802103571420683711.png 480w, https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-expert-capacity_hu1304176036435719856.png 1024w"
loading="lazy"
alt="Token routing dynamics"
class="gallery-image"
data-flex-grow="264"
data-flex-basis="635px"
>&lt;/p>
&lt;p>提升 capacity factor 可以减少 token overflow 的概率，但同时也会导致计算和内存的浪费。作者通过实验发现应该尽可能降低 dropped token 比例。&lt;/p>
&lt;p>为了平衡每个 expert 处理 token 的个数，作者设计了 load balancing loss, 见 &lt;a class="link" href="Load%20Balancing%20loss.md" >Load Balancing loss&lt;/a> 来要求每个 expert 处理的 token 数基本一致。&lt;/p>
&lt;h2 id="parallelism">Parallelism
&lt;/h2>&lt;p>作者在本节介绍了针对 MoE 模型的并行策略，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-Parallelism.png"
width="1268"
height="746"
srcset="https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-Parallelism_hu5410496410627554406.png 480w, https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-Parallelism_hu10984838773348886305.png 1024w"
loading="lazy"
alt="Data and weight partitioning strategies"
class="gallery-image"
data-flex-grow="169"
data-flex-basis="407px"
>&lt;/p>
&lt;p>这里，我们给定 notation 如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Term&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$B$&lt;/td>
&lt;td>Number of tokens in the batch.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$N$&lt;/td>
&lt;td>Number of total cores.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$n$&lt;/td>
&lt;td>Number of ways for data-parallelism sharding.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$m$&lt;/td>
&lt;td>Number of ways for model-parallelism sharding.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$E$&lt;/td>
&lt;td>Number of experts in Switch layers.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$C$&lt;/td>
&lt;td>Expert capacity, the batch size of each expert.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>作者将一个 Logical mesh 分为两个维度，一个是 data-parallel sharding, 用 $n$ 表示，另一个是 model-parallel sharding, 用 $m$ 表示，从而总的 cores 数为 $N=n\times m$.&lt;/p>
&lt;h3 id="data-parallelism">Data Parallelism
&lt;/h3>&lt;p>对于 data prallelism 来说，每个 core 上的模型是一样的，而一个 batch 的数据被切分为了 $B/n$ 份，此时 $n=N$, $m=1$. 这种并行策略的好处是仅在 forward 和 backward 完成之后才会进行一次通信。&lt;/p>
&lt;h3 id="model-parallelism">Model Parallelism
&lt;/h3>&lt;p>对于 model parallelism, 每个 core 上的数据都是全部数据（通过拷贝得到），模型被 shard 到所有 core 上，此时 $n=1,m=N$, 这种情况下，每次 forward 或者 backward，都需要进行 $N$ 次通信（假设我们使用 pipeline parallelism）， 这种并行策略的好处是每个 core 上的计算压力小了，但是缺点是通信开销多了。&lt;/p>
&lt;h3 id="model-and-data-parallelism">Model and Data Parallelism
&lt;/h3>&lt;p>第三种总策略是同时进行 model parallelism 和 data parallelism, 这种情况下，每个 core 上保存 $B/n$ 的数据以及 shard 为 $m$ 份的 sharding weight.&lt;/p>
&lt;h3 id="expert-and-data-parallelism">Expert and Data Parallelism
&lt;/h3>&lt;p>第四种策略是同时进行 expert parallelism 和 data parallelism, 作者在这里假设 $n=N$ 且 $E=n=N$, 也就是每个 core 上保留 $B/n$ 的数据, 然后还有一个对应的专家。&lt;/p>
&lt;p>首先，对于输入大小为 $[B,d]$ 的 tensor, 我们会进行拆分得到大小为 $[n, B/n, d]$ 的 tensor, 代表 $n$ 个设备上分别存储 $[B/n, d]$ 的数据，首先我们计算路由权重，得到&lt;/p>
$$
[n,B/n, d]\times [d, E] \to [n, B/n, E]
$$&lt;p>权重的每个值 $[i,j,k]$ 代表了第 $i$ 个 core 上,第 $j$ 个 token 分配一个第 $k$ 个专家的概率，我们对其进行 softmax 与 top-K 操作之后，就得到了对应的专家（注意 Switch Transformer 中的 $K=1$, 我们通过 one-hot 编码将其输出转化为一个二进制矩阵，大小为 $[n,B/n, E, C]$, 这里的每个值 $[i,j,k,l]$ 代表了第 $i$ 个 core 上,第 $j$ 个 token 分配给第 $k$ 个专家第 $l$ 个 token, 接下来我们再基于输入 &lt;code>[n,B/n,d]&lt;/code> 计算 core 到 expert 的数据，其大小为 &lt;code>[n,E,C,d]&lt;/code>, 计算方式为&lt;/p>
$$
\mathrm{einsum}([n,B/n, d], [n,B/n,E,C], \mathrm{dim}=[B/n])
$$&lt;p>里面的元素 &lt;code>[i,j,k,:]&lt;/code> 代表了第 $i$ 个设备路由到第 $j$ 个专家的第 $k$ ($k&amp;lt;C$) 个 token.&lt;/p>
&lt;p>然后我们就可以执行 &lt;code>all-to-all&lt;/code> 通信来把对应的 token 传输给对应专家所在的设备上了，通信完成之后，每个设备上的专家对收集到的输入 token 进行计算，计算完之后，再通过 &lt;code>all-to-all&lt;/code> 通信来把输出传输给输入的设备。这样就完成了 MoE 模块的计算与通信&lt;/p>
&lt;h3 id="expert-model-and-data-parallelism">Expert, Model and Data Parallelism
&lt;/h3>&lt;p>目前我们只是通过增加专家个数来提高模型的表现，但是 FLOPs 并没有增加，如果我们希望通过增加 FLOPs 来提高模型表现的话，我们需要增加 expert layer 的 model size, 这就涉及到了 Model parallelism, 此时的做法和前面提到的 Expert and Data Parallelism 一样，我们在收集到对应的设备的输入之后，再执行 model parallelism 就可以了。&lt;/p>
&lt;h2 id="results">Results
&lt;/h2>&lt;p>基于以上改进，作者构建了 Switch Transformer, 模型训练的数据集为 C4, 训练的目标为 masked language modeling, 训练时，作者将 $15%$ 的 token 替换为 &lt;code>[mask]&lt;/code> token.&lt;/p>
&lt;p>模型的配置如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-model-configuration.png"
width="1256"
height="252"
srcset="https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-model-configuration_hu4357538381573383876.png 480w, https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-model-configuration_hu3038916745392518968.png 1024w"
loading="lazy"
alt="Model configuration"
class="gallery-image"
data-flex-grow="498"
data-flex-basis="1196px"
>&lt;/p>
&lt;p>实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-performance.png"
width="1206"
height="526"
srcset="https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-performance_hu13682634193827039586.png 480w, https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-performance_hu4383386043902142845.png 1024w"
loading="lazy"
alt="Performance of Switch Transformer"
class="gallery-image"
data-flex-grow="229"
data-flex-basis="550px"
>&lt;/p>
&lt;p>实验结果发现&lt;/p>
&lt;ol>
&lt;li>Switch Transformer 的表现和训练效率都超过了 Dense model&lt;/li>
&lt;li>Switch transformer 的训练效率稍微优于使用 2 个 expert 的 MoE-Base 模型&lt;/li>
&lt;li>Switch transformer 在 low capacity factor 的场景下效果更好&lt;/li>
&lt;/ol>
&lt;h3 id="scaling">Scaling
&lt;/h3>&lt;p>作者对比了 MoE 模型的 scaling law, 结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-scaling-law.png"
width="1244"
height="497"
srcset="https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-scaling-law_hu8777599275504326134.png 480w, https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-scaling-law_hu16075252639264525468.png 1024w"
loading="lazy"
alt="Scaling law of MoE model"
class="gallery-image"
data-flex-grow="250"
data-flex-basis="600px"
>&lt;/p>
&lt;p>可以看到，当我们增加专家个数的时候，模型的表现是持续提升的。并且当我们增加专家个数之后，模型的训练效率也有所提升。&lt;/p>
&lt;p>作者接下来在训练时间上进行了对比，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-speed-comparison.png"
width="839"
height="672"
srcset="https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-speed-comparison_hu9106963091176582103.png 480w, https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-speed-comparison_hu6038297728123734119.png 1024w"
loading="lazy"
alt="Speed comparison of MoE model"
class="gallery-image"
data-flex-grow="124"
data-flex-basis="299px"
>&lt;/p>
&lt;p>实验结果说明，switch transformer 的训练效率比 dense model 快 7 倍左右&lt;/p>
&lt;p>作者进一步对比 switch transformer 和更大的 dense 模型 (T5 large, $3.5\times$ FLOPs), 实验结果证明 Switch transformer 的训练效率仍然更高。&lt;/p>
&lt;h3 id="switch-for-attention">Switch for Attention
&lt;/h3>&lt;p>作者还探究了在 attention layer 加入 MoE 的表现，结果发现尽管效果有提升，但是训练不稳定。&lt;/p>
&lt;h3 id="no-token-left-behind">No Token Left behind
&lt;/h3>&lt;p>由于 tensorflow 为一个静态计算框架，tensor 的形状必须预先定义好，因此必须为每个 expert 设定 capacity, 但是这样就会导致有些 token 被 drop 掉。&lt;/p>
&lt;p>因此，作者提出了 &amp;ldquo;No token left behind&amp;rdquo; 这种方法，来避免出现 token overflow 的情况。具体做法就是先按照正常的 router 逻辑进行计算，如果 router 选取出来的 top-K expert （本文中 $K=1$） 都已经满载了，则选取 &lt;code>top-(K+1)&lt;/code> expert 进行计算，作者发现这种方式可以保证大部分 token 都不会被 drop. 作者通过尝试之后发现，这种方法并没有带来提升。&lt;/p>
&lt;h3 id="encouraging-exploration-across-experts">Encouraging Exploration Across Experts
&lt;/h3>&lt;p>作者还探究了不同选取 top-Kexpert 的方式，作者对比了以下三种方法：&lt;/p>
&lt;ol>
&lt;li>argmax&lt;/li>
&lt;li>sampling from the softmax distribution&lt;/li>
&lt;li>input dropout on the incoming representation&lt;/li>
&lt;li>multiplicative jitter noise on the incoming representation&lt;/li>
&lt;/ol>
&lt;p>实验结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model Quality&lt;/th>
&lt;th>(Neg. Log Perp.) (↑)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Argmax&lt;/td>
&lt;td>-1.471&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Sample softmax&lt;/td>
&lt;td>-1.570&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Input dropout&lt;/td>
&lt;td>-1.480&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Input jitter&lt;/td>
&lt;td>-1.468&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>作者发现，jitter 的效果最好，因此在本文中作者使用 jitter 来加入 noise.&lt;/p>
&lt;h3 id="ablation-on-few-experts">Ablation on Few Experts
&lt;/h3>&lt;p>作者还使用了更少的专家进行实验，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-few-experts.png"
width="846"
height="666"
srcset="https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-few-experts_hu12167909709105131407.png 480w, https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-few-experts_hu9734617699320083614.png 1024w"
loading="lazy"
alt="Switch Transformer with few experts"
class="gallery-image"
data-flex-grow="127"
data-flex-basis="304px"
>&lt;/p>
&lt;p>实验结果显示，即使我们只使用少数 experts, 其模型表现仍然超过了 dense model 的表现，说明了 MoE 架构的有效性。&lt;/p>
&lt;h3 id="downstream-model-performance">Downstream Model Performance
&lt;/h3>&lt;p>作者还进一步探究了模型的预训练表现与 downstream task 任务上表现的关系，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-downstream-performance-scaling.png"
width="1257"
height="499"
srcset="https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-downstream-performance-scaling_hu1602937172753393933.png 480w, https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-downstream-performance-scaling_hu14741610769347968667.png 1024w"
loading="lazy"
alt="Upstream pre-trained quality to downstream model quality."
class="gallery-image"
data-flex-grow="251"
data-flex-basis="604px"
>&lt;/p>
&lt;p>实验结果显示，不管是 baseline 还是 Switch Transformer 其预训练的表现与下游任务上的表现都是正相关的。但是，作者也发现，MoE 模型在微调之后，其表现并不总是与预训练的表现正相关。因此，作者认为进一步探究这个机制是有必要的。&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 Switch Transformer，一个基于 MoE 架构的 Transformer 模型。作者通过改进 MoE 算法，大幅度提高了计算和通信效率，结果发现模型比 dense model 有更高的训练效率。&lt;/p>
&lt;p>作者认为，未来的工作有：&lt;/p>
&lt;ol>
&lt;li>提升大规模模型的训练稳定性&lt;/li>
&lt;li>解决 MoE 模型微调之后效果不如预期的问题&lt;/li>
&lt;li>探究针对 MoE 模型的 scaling law&lt;/li>
&lt;li>支持异构架构的 MoE 模型&lt;/li>
&lt;li>在 FFN 模块意外应用 MoE 架构&lt;/li>
&lt;li>将 Switch Transformer 扩展到其他的模态&lt;/li>
&lt;/ol>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2101.03961" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>MoE tutorial</title><link>https://maosong2022.github.io/p/moe-tutorial/</link><pubDate>Thu, 23 Oct 2025 16:13:29 +0800</pubDate><guid>https://maosong2022.github.io/p/moe-tutorial/</guid><description>&lt;h2 id="介绍">介绍
&lt;/h2>&lt;p>MoE 模型是一个将 transformer block 中 FFN 替换为 MoE layer 的方法，通过 MoE，我们可以让模型在相同的激活参数下，达到更好的性能。&lt;/p>
&lt;p>本文中，我们基于主流的 MoE 模型学习一下 MoE 的方法与进展，更多细节请参阅参考文献。&lt;/p>
&lt;h2 id="方法">方法
&lt;/h2>&lt;p>MoE 模型和 dense 模型的示意图如下，图源 [[olmoe]]&lt;/p>
&lt;p>![[MoE_architecture.png]]&lt;/p>
&lt;p>一个 MoE layer 包括两个模块：&lt;/p>
&lt;ol>
&lt;li>Router：Router 负责为 token 指定合适的专家&lt;/li>
&lt;li>Expert：Expert 负责处理 token&lt;/li>
&lt;/ol>
&lt;p>对于输入 $x\in\mathbb{R}^d$, 我们假设有 $N$ 个 Expert，router 一般是一个 linear layer 再加上一个 softmax，其构建了 $\mathbb{R}^d\to\mathbb{R}^N$ 的映射，其定义为：&lt;/p>
$$
G(x) = \mathrm{softmax}(W_gx + b)\in\mathbb{R}^N
$$&lt;p>其中 $W_g\in\mathbb{R}^{N\times d}$, $b\in\mathbb{R}^N$ 是可学习的参数。$G(x)\in\mathbb{R}^N$ 是一个概率分布，$G_{i}$ 代表了第 $i$ 个 Expert 对于当前 token $x$ 的重要性.&lt;/p>
&lt;p>一般来说，Expert 会使用和 dense 模型一样的 MLP，即使用 SwiGLU 激活函数的 FFN，见 [[Assignment 1]] ， 我们记为&lt;/p>
$$
E_i(x) = FFN(x), i = 1,\dots,N
$$&lt;p>接下来，基于 $G(x)$ 和 $E_i(x)$, 我们会使用合适的方法来挑选 $K&amp;lt;N$ 个 Expert 出来，其中 $K&amp;gt;0$ 是给定的超参数，我们记挑选出来的 $K$ 个 Expert 的 index 为 $e_1,\dots,e_K$, 则我们最终 MoE layer 的输出为&lt;/p>
$$
y = \sum_{i=1}^K\mathrm{Normalize}(G_{e_i}) E_{e_i}(x)
$$&lt;p>这里 $\mathrm{Normalize}(\cdot)$ 代表我们对于输出进行归一化，即&lt;/p>
$$
\mathrm{Normalize}(G_{e_i}) = \frac{\exp(G_{e_i})}{\sum_{i=1}^K \exp(G_{e_i})}
$$&lt;h2 id="代码">代码
&lt;/h2>&lt;p>我们这里展示基于 [[olmoe]] 的代码，代码如下&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;span class="lnt">49
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">OlmoeSparseMoeBlock&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="fm">__init__&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_experts&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_experts&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">top_k&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_experts_per_tok&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">norm_topk_prob&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">norm_topk_prob&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">gate&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_experts&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">experts&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ModuleList&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">OlmoeMLP&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_experts&lt;/span>&lt;span class="p">)])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">batch_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sequence_length&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_dim&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># hidden_states: (batch * sequence_length, hidden_dim)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># router_logits: (batch * sequence_length, n_experts)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">router_logits&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">gate&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">routing_weights&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">F&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">softmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">router_logits&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># routing_weights: (batch * sequence_length, top_k)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># selected_experts: indices of top_k experts&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">routing_weights&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">selected_experts&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">topk&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">routing_weights&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">top_k&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">norm_topk_prob&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">routing_weights&lt;/span> &lt;span class="o">/=&lt;/span> &lt;span class="n">routing_weights&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">keepdim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># we cast back to the input dtype&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">routing_weights&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">routing_weights&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dtype&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">final_hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zeros&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">(&lt;/span>&lt;span class="n">batch_size&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">sequence_length&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_dim&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dtype&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">device&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">device&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># One hot encode the selected experts to create an expert mask&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># this will be used to easily index which expert is going to be selected&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">expert_mask&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">functional&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">one_hot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">selected_experts&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_classes&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_experts&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">permute&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Loop over all available experts in the model and perform the computation on each expert&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">expert_idx&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_experts&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">expert_layer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">experts&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">expert_idx&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">idx&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">top_x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">where&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">expert_mask&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">expert_idx&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Index the correct hidden states and compute the expert hidden state for&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># the current expert. We need to make sure to multiply the output hidden&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># states by `routing_weights` on the corresponding tokens (top-1 and top-2)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">current_state&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">top_x&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reshape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">current_hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">expert_layer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">current_state&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">routing_weights&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">top_x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">idx&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># However `index_add_` only support torch tensors for indexing so we&amp;#39;ll use&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># the `top_x` tensor here.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">final_hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">index_add_&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">top_x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">current_hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dtype&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">final_hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">final_hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reshape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">batch_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sequence_length&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">final_hidden_states&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">router_logits&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>【TODO】理解后面代码优化的部分&lt;/p>
&lt;h2 id="variant">Variant
&lt;/h2>&lt;p>在构建 MoE Layer 的过程中，有以下设计方法。&lt;/p>
&lt;h3 id="routing-type">Routing Type
&lt;/h3>&lt;p>在为专家分配 token 的时候，我们有如下方式：&lt;/p>
&lt;ol>
&lt;li>为每个 token 选取若干个专家&lt;/li>
&lt;li>为每个专家选取若个个 token&lt;/li>
&lt;li>动态分配 token 与专家之间的关系&lt;/li>
&lt;/ol>
&lt;p>三种选择方式如下图所示，图源 &lt;a class="link" href="https://arxiv.org/pdf/2209.01667" target="_blank" rel="noopener"
>MoE survey&lt;/a>&lt;/p>
&lt;p>![[MoE_routing.png]]&lt;/p>
&lt;p>图源：【参考文献 2】&lt;/p>
&lt;ol>
&lt;li>Token Choice: 每个 token 选取 top-k 的专家，好处是每个 token 都会被处理，缺点是负载不均衡&lt;/li>
&lt;li>Expert Choice: 每个专家选取 top-k 的 token，此时每个专家处理的 token 个数是相同的，这个方法的好处是 load balance。缺点是自回归生成的方式没有完整序列长度的信息，从而导致 token dropping，也就是某些 token 不会被任何专家处理，某些 token 会被多个专家处理&lt;/li>
&lt;li>Global Choice: 全局分配决定 token 和专家的匹配关系&lt;/li>
&lt;/ol>
&lt;p>现在几乎所有的模型都选择方式 1，即每个 token 选取 top-k 的专家。 [[olmoe]] 对比了以下方式 1 和方式 2 的表现，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/moe-tutorial/olmoe-routing-strategy.png"
width="1157"
height="450"
srcset="https://maosong2022.github.io/p/moe-tutorial/olmoe-routing-strategy_hu14576683236097856598.png 480w, https://maosong2022.github.io/p/moe-tutorial/olmoe-routing-strategy_hu12605903787078733490.png 1024w"
loading="lazy"
alt="MoE routing strategy EC v.s. TC"
class="gallery-image"
data-flex-grow="257"
data-flex-basis="617px"
>&lt;/p>
&lt;p>可以看到，加入 load balancing loss 之后，相比于 Expert Choice, Token Choice 的表现更好。但是，expert choice 更加高效，作者认为 expert choice 更适用于多模态，因为丢掉 noise image tokens 对 text token 影响会比较小。因此，在 olmoe 中，作者使用 token choice 作为 routing 策略&lt;/p>
&lt;h3 id="granularity-of-expert">Granularity of Expert
&lt;/h3>&lt;p>[[DeepSeekMoE]]&lt;/p>
&lt;p>[[olmoe]]&lt;/p>
&lt;h3 id="shared-expert">Shared Expert
&lt;/h3>&lt;p>Shared Expert 由 [[DeepSeekMoE]] 提出，其基本思想为，固定某几个专家，响应所有的 token，这样可以让某些专家学习到共有的知识，而让其他的专家学习到特定的知识。这个方法随后被 [[Qwen1.5]], [[Qwen2]] , [[Qwen2.5]] 以及 [[DeepSeek-V3]] 所采用。&lt;/p>
&lt;p>[[DeepSeekMoE]] 给出的实验结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/moe-tutorial/DeepSeekMoE-ablation-experts.png"
width="1197"
height="552"
srcset="https://maosong2022.github.io/p/moe-tutorial/DeepSeekMoE-ablation-experts_hu13488373378225854754.png 480w, https://maosong2022.github.io/p/moe-tutorial/DeepSeekMoE-ablation-experts_hu6581578019190079093.png 1024w"
loading="lazy"
alt="DeepSeek-Moe shared experts ablation study"
class="gallery-image"
data-flex-grow="216"
data-flex-basis="520px"
>&lt;/p>
&lt;p>作者发现，当使用 shared experts 之后，模型在大部分 benchmark 上的表现都有所提升。&lt;/p>
&lt;p>[[olmoe]] 在 32 个专家下进行了实验，比较了 4 个激活专家和 3 个激活专家 +1 个共享专家两种设置的表现，结果如下：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/moe-tutorial/olmoe-shared-experts.png"
width="1156"
height="477"
srcset="https://maosong2022.github.io/p/moe-tutorial/olmoe-shared-experts_hu17801868202021072630.png 480w, https://maosong2022.github.io/p/moe-tutorial/olmoe-shared-experts_hu1657113941603463913.png 1024w"
loading="lazy"
alt="Olmoe shared experts performance"
class="gallery-image"
data-flex-grow="242"
data-flex-basis="581px"
>&lt;/p>
&lt;p>作者认为，加入 shared experts 之后，组合的可能性有所减少，这会降低模型的泛化性。因此，在 olmoe 中，作者没有使用 shared experts.&lt;/p>
&lt;p>虽然 [[Qwen1.5]], [[Qwen2]] 和 [[Qwen2.5]] 都使用了 shared experts, 但是后续的 [[Qwen3]] 中却并没有使用，作者并未解释原因。&lt;/p>
&lt;h2 id="training">Training
&lt;/h2>&lt;p>训练的时候，我们必须保证 sparsity，但是 sparsity 意味着不可微，为了解决不可微的问题，现有解决方法：&lt;/p>
&lt;ol>
&lt;li>基于 RL 的算法&lt;/li>
&lt;li>随机扰动&lt;/li>
&lt;li>balancing loss&lt;/li>
&lt;/ol>
&lt;h3 id="backpropogation">Backpropogation
&lt;/h3>&lt;p>我们假设损失函数为 $\mathcal{L}=g(y)$, 则&lt;/p>
$$
\frac{\partial \mathcal{L}}{\partial W_g} = \frac{\partial \mathcal{L}}{\partial g}\left(\sum_{i=1}^K E_{e_i}(x)\frac{\partial G_{e_i}}{\partial W_g}+\sum_{i=1}^K G_{e_i}(x)\frac{\partial E_{e_i}}{\partial W_g}\right)
$$&lt;p>其中，第二部分关于专家部分的反向传播是可微的，我们直接忽略。在第一项中，我们发现， $\frac{\partial G_{e_i}}{\partial W_g}$ 是不可微的, 因此我们需要解决这个不可微的问题。&lt;/p>
&lt;p>解决方案一般有以下几种&lt;/p>
&lt;h4 id="reinforce">REINFORCE
&lt;/h4>&lt;h4 id="straight-through-estimator">Straight Through Estimator
&lt;/h4>&lt;h4 id="noisy-top-k-gating">Noisy Top-k Gating
&lt;/h4>&lt;h4 id="differentiable-top-k-relaxations">Differentiable Top-k Relaxations
&lt;/h4>&lt;p>Gumbel-Softmax (or Concrete Distribution)&lt;/p>
&lt;h3 id="load-balancing-loss">Load Balancing Loss
&lt;/h3>&lt;p>见 &lt;a class="link" href="Load%20Balancing%20loss.md" >Load Balancing loss&lt;/a>&lt;/p>
&lt;p>[[olmoe]]&lt;/p>
&lt;h3 id="router-z-loss">Router Z-loss
&lt;/h3>&lt;p>Router z-loss 用于提升 MoE 模型训练的稳定性和最终表现，z-loss 会惩罚 gating 网络中较大的 logits，因为这些较大的 logits 会导致数值溢出，给定一个 batch $B$, 对于 router layer 输入的 logits $x_i$, 其定义如下：&lt;/p>
$$
\mathcal{L}_{z}(x) = \frac{1}{B}\sum_{i=1}^B\left(\log \sum_{j=1}^K \exp(x_j^{(i)})\right)^2
$$&lt;p>即再求和之前，先计算对应的数值，然后给较大的数值一个更大的惩罚，这样可以让每个 token 对专家的 logits 分布更加平滑，而不是仅关注少数几个专家&lt;/p>
&lt;p>实验结果【olmoe 图 11】&lt;/p>
&lt;p>[[olmoe]]&lt;/p>
&lt;h3 id="upcycling">Upcycling
&lt;/h3>&lt;p>upsampling 是一个将 dense model 转化为 MoEmodel 的方法，具体做法就是我们复制 dense model 中的 FFN layer 得到对应 MoE layer 中的 Expert，然后我们再结合 router 训练，这样可以提高整体的训练效率。&lt;/p>
&lt;p>但是这样做的问题是，MoE 模型会被 dense 模型的一些超参数所限制&lt;/p>
&lt;p>实验结果【olmoe 图 8】&lt;/p>
&lt;p>[[MiniCPM]]&lt;/p>
&lt;p>[[Qwen1.5]]&lt;/p>
&lt;p>[[Mixtral MoE]]&lt;/p>
&lt;h2 id="pros-and-cons">Pros and Cons
&lt;/h2>&lt;p>优点&lt;/p>
&lt;ul>
&lt;li>MoE 在训练和推理效率等方面具有优势&lt;/li>
&lt;li>相同的激活参数下，MoE 模型表现的更好&lt;/li>
&lt;/ul>
&lt;p>缺点：&lt;/p>
&lt;ul>
&lt;li>训练不稳定&lt;/li>
&lt;li>在相同存储量下的模型性能以及下游任务小样本微调的表现上存在劣势&lt;/li>
&lt;li>更高的内存占用&lt;/li>
&lt;/ul>
&lt;p>Dense 模型：&lt;/p>
&lt;ul>
&lt;li>相同总参数量下稠密模型的性能更强，对于探索模型能力上限的意义更为重大&lt;/li>
&lt;/ul>
&lt;h2 id="moe-模型">MoE 模型
&lt;/h2>&lt;p>[[LLaMA4]]&lt;/p>
&lt;p>[[Mistral-7B]]&lt;/p>
&lt;p>[[DeepSeekMoE]]&lt;/p>
&lt;p>[[DeepSeek-V3]]&lt;/p>
&lt;p>[[olmoe]]&lt;/p>
&lt;p>[[Grok系列]]&lt;/p>
&lt;h2 id="结论">结论
&lt;/h2>&lt;p>在本文中，我们系统性回顾了 MoE 的相关概念，MoE 模型已经是现在大语言模型的主流架构，比如商业模型 [[Gemini2.5]], 开源领先的模型 [[DeepSeek-V3]] , [[LLaMA4]] 以及 [[Qwen3]] 等都采用了 MoE 的架构，如何进一步优化 MoE 的训练方式是当前研究的一个重点方向。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2101.03961" target="_blank" rel="noopener"
>Switch Transformer&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2209.01667" target="_blank" rel="noopener"
>MoE Survey&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://openreview.net/forum?id=xXTkbTBmqq" target="_blank" rel="noopener"
>olmoe&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2006.16668" target="_blank" rel="noopener"
>GShard&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.cnblogs.com/rossiXYZ/p/18800825" target="_blank" rel="noopener"
>blog&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2501.16352v1" target="_blank" rel="noopener"
>MoE a big data perspective&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://huggingface.co/blog/moe" target="_blank" rel="noopener"
>Mixture of Experts Explained&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Chinchilla Scaling Law</title><link>https://maosong2022.github.io/p/chinchilla-scaling-law/</link><pubDate>Wed, 22 Oct 2025 14:39:23 +0800</pubDate><guid>https://maosong2022.github.io/p/chinchilla-scaling-law/</guid><description>&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>本文中关注的研究问题为：&lt;/p>
&lt;blockquote>
&lt;p>给定一个 FLOPs budget, 如何平衡 model size 和 dataset size 之间的关系？&lt;/p>
&lt;/blockquote>
&lt;p>即，我们希望求解如下优化问题：&lt;/p>
$$
N_{opt}(C), D_{opt}(C) =\arg\min_{N,D,\ \mathrm{s.t.}\ FLOPs(N,D)=C} L(N,D)
$$&lt;p>作者通过训练 400 多个模型，构建了对应的 scaling law.&lt;/p>
&lt;p>已有工作如 &lt;a class="link" href="https://maosong.website/p/kaplan-scaling-law/" target="_blank" rel="noopener"
>Kaplan scaling law&lt;/a> 已经发现模型参数和大语言模型表现之间的关系，一个结论就是计算最优并不代表达到最优的 loss. 在本文中，作者也有相同结论，但是作者认为大模型应该使用比 &lt;a class="link" href="https://maosong.website/p/kaplan-scaling-law/" target="_blank" rel="noopener"
>Kaplan scaling law&lt;/a> 推荐的更多的 training token. 基于这个发现，作者训练了 Chinchilla, 一个 70B 的 LLM, Chinchilla 相比 Gopher 表现有了大幅度的提升。&lt;/p>
&lt;h2 id="scaling-law">Scaling Law
&lt;/h2>&lt;h3 id="fix-model-size-and-very-dataset-size">Fix Model Size and Very Dataset Size
&lt;/h3>&lt;p>这个方法中，作者通过改变训练步数，来研究 FLOPs 与模型表现之间的关系，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/chinchilla-scaling-law/Chinchilla-Training-curve-envelope.png"
width="3018"
height="1304"
srcset="https://maosong2022.github.io/p/chinchilla-scaling-law/Chinchilla-Training-curve-envelope_hu10567536964506360272.png 480w, https://maosong2022.github.io/p/chinchilla-scaling-law/Chinchilla-Training-curve-envelope_hu16266655799417657314.png 1024w"
loading="lazy"
alt="Training Curve envelope"
class="gallery-image"
data-flex-grow="231"
data-flex-basis="555px"
>&lt;/p>
&lt;p>通过对实验结果进行拟合，作者发现存在关系 $N_{opt}(C)\propto C^a$ 以及 $D_{opt}(C)\propto C^b$, 拟合的结果为 $a=b=0.5$.&lt;/p>
&lt;h3 id="isoflops-profiles">IsoFLOPS Profiles
&lt;/h3>&lt;p>这个方法中，作者使用了不同的模型大小以及算力来构建最优模型参数量与算力之间的关系。作者给定 9 个算力配置，然后选取不同参数量的模型，训练的 token 数由算力和模型参数量决定，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/chinchilla-scaling-law/Chinchilla-IsoFLOP-curves.png"
width="3018"
height="1396"
srcset="https://maosong2022.github.io/p/chinchilla-scaling-law/Chinchilla-IsoFLOP-curves_hu12964383356718639356.png 480w, https://maosong2022.github.io/p/chinchilla-scaling-law/Chinchilla-IsoFLOP-curves_hu11328230484318679329.png 1024w"
loading="lazy"
alt="IsoFLOP curves"
class="gallery-image"
data-flex-grow="216"
data-flex-basis="518px"
>&lt;/p>
&lt;p>结果显示，不同大小的模型的表现 (loss) 随算力上升先下降后上升。因此给定算力，存在一个最优的 model size. 作者基于拟合出来的曲线得到了 Gopher 使用的算力配置下的最优 model size 和 training tokens. 同样的，作者得到 $a=0.49,b=0.51$.&lt;/p>
&lt;h3 id="fitting-a-parametric-loss-function">Fitting a Parametric Loss Function
&lt;/h3>&lt;p>这个方法中，作者对 $L(N,D)$ 进行建模，作者使用了如下的公式&lt;/p>
$$
L(N,D) = E + \frac{A}{N^{\alpha}} + \frac{B}{D^{\beta}}
$$&lt;p>第一项代表了建模的误差，第二项代表了数据集充分大损失与模型参数之间的关系，第三项代表了当模型充分训练时，损失与数据集大小之间的关系。&lt;/p>
&lt;p>为了求解 $(A,B,E,\alpha,\beta)$, 作者基于训练收集到的数据 $L(N_i,D_i)$, 通过 L-BFGS 算法来最小化 Huber loss 进行求解，结果得到 $(A,B,E,\alpha,\beta)=(1.69, 406.4, 410.7, 0.34, 0.28)$.&lt;/p>
&lt;p>将结果带入带上面的表达式中，然后求出梯度为 0 的点，就得到&lt;/p>
$$
N_{opt}(C) = G\left(\frac C6\right)^a, D_{opt}(C) = G^{-1}\left(\frac C6\right)^b, \text{ where }G=\left(\frac{\alpha A}{\beta B}\right)^{1/(\alpha+\beta)}, a=\frac{\beta}{\alpha+\beta}, b=\frac{\alpha}{\alpha+\beta}
$$&lt;p>带入数值之后就得到 $a=0.46$, $b=0.54$. 作者对结果可视化如下图所示，左图是拟合曲线的 Contour plot, 右图对左图的一个切片&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/chinchilla-scaling-law/Chinchilla-Parametric-fit.png"
width="2980"
height="1284"
srcset="https://maosong2022.github.io/p/chinchilla-scaling-law/Chinchilla-Parametric-fit_hu11197797298351948532.png 480w, https://maosong2022.github.io/p/chinchilla-scaling-law/Chinchilla-Parametric-fit_hu10910264691183016639.png 1024w"
loading="lazy"
alt="Parametric fit"
class="gallery-image"
data-flex-grow="232"
data-flex-basis="557px"
>&lt;/p>
&lt;h3 id="optimal-model-scaling">Optimal Model Scaling
&lt;/h3>&lt;p>作者将三种方法的结果以及 &lt;a class="link" href="https://maosong.website/p/kaplan-scaling-law/" target="_blank" rel="noopener"
>Kaplan scaling law&lt;/a> 的结果总结放在下表中，作者假设 $N_{opt}(C)\propto C^a$ 以及 $D_{opt}(C)\propto C^b$&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Approach&lt;/th>
&lt;th>$a$&lt;/th>
&lt;th>$b$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Kaplan&lt;/td>
&lt;td>0.73&lt;/td>
&lt;td>0.26&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Approach 1&lt;/td>
&lt;td>0.50&lt;/td>
&lt;td>0.50&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Approach 2&lt;/td>
&lt;td>0.49&lt;/td>
&lt;td>0.51&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Approach 3&lt;/td>
&lt;td>0.46&lt;/td>
&lt;td>0.54&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>结果表明，三种方法的结论差不多：model size 和 dataset size 增长 debility 差不多。&lt;/p>
&lt;p>作者因此给出来的不同模型大小所需要的算力以及 token, 结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Parameters&lt;/th>
&lt;th>Approach 1&lt;/th>
&lt;th>&lt;/th>
&lt;th>Approach 2&lt;/th>
&lt;th>&lt;/th>
&lt;th>Approach 3&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>FLOPs&lt;/td>
&lt;td>Tokens&lt;/td>
&lt;td>FLOPs&lt;/td>
&lt;td>Tokens&lt;/td>
&lt;td>FLOPs&lt;/td>
&lt;td>Tokens&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>400 M&lt;/td>
&lt;td>1.92e+19&lt;/td>
&lt;td>8.0 B&lt;/td>
&lt;td>1.84e+19&lt;/td>
&lt;td>7.7 B&lt;/td>
&lt;td>1.84e+19&lt;/td>
&lt;td>7.7 B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1 B&lt;/td>
&lt;td>1.21e+20&lt;/td>
&lt;td>20.2 B&lt;/td>
&lt;td>1.20e+20&lt;/td>
&lt;td>20.0 B&lt;/td>
&lt;td>1.20e+20&lt;/td>
&lt;td>20.0 B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10 B&lt;/td>
&lt;td>1.23e+22&lt;/td>
&lt;td>205.1 B&lt;/td>
&lt;td>1.32e+22&lt;/td>
&lt;td>219.5 B&lt;/td>
&lt;td>1.32e+22&lt;/td>
&lt;td>219.5 B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>67 B&lt;/td>
&lt;td>5.76e+23&lt;/td>
&lt;td>1.5 T&lt;/td>
&lt;td>6.88e+23&lt;/td>
&lt;td>1.7 T&lt;/td>
&lt;td>6.88e+23&lt;/td>
&lt;td>1.7 T&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>175 B&lt;/td>
&lt;td>3.85e+24&lt;/td>
&lt;td>3.7 T&lt;/td>
&lt;td>4.54e+24&lt;/td>
&lt;td>4.3 T&lt;/td>
&lt;td>4.54e+24&lt;/td>
&lt;td>4.3 T&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>280 B&lt;/td>
&lt;td>9.90e+24&lt;/td>
&lt;td>5.9 T&lt;/td>
&lt;td>1.18e+25&lt;/td>
&lt;td>7.1 T&lt;/td>
&lt;td>1.18e+25&lt;/td>
&lt;td>7.1 T&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>520 B&lt;/td>
&lt;td>3.43e+25&lt;/td>
&lt;td>11.0 T&lt;/td>
&lt;td>4.19e+25&lt;/td>
&lt;td>13.4 T&lt;/td>
&lt;td>4.19e+25&lt;/td>
&lt;td>13.4 T&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1 T&lt;/td>
&lt;td>1.27e+26&lt;/td>
&lt;td>21.2 T&lt;/td>
&lt;td>1.59e+26&lt;/td>
&lt;td>26.5 T&lt;/td>
&lt;td>1.59e+26&lt;/td>
&lt;td>26.5 T&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10 T&lt;/td>
&lt;td>1.30e+28&lt;/td>
&lt;td>216.2 T&lt;/td>
&lt;td>1.75e+28&lt;/td>
&lt;td>292.0 T&lt;/td>
&lt;td>1.75e+28&lt;/td>
&lt;td>292.0 T&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>作者基于发现的 scaling law, 对已有模型进行了探究，发现现有的大模型都存在 under-training 的现象，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/chinchilla-scaling-law/Chinchilla-Overlaied-predictions.png"
width="2422"
height="1284"
srcset="https://maosong2022.github.io/p/chinchilla-scaling-law/Chinchilla-Overlaied-predictions_hu5657720287792187032.png 480w, https://maosong2022.github.io/p/chinchilla-scaling-law/Chinchilla-Overlaied-predictions_hu2560641788537084569.png 1024w"
loading="lazy"
alt="Overlaid predictions"
class="gallery-image"
data-flex-grow="188"
data-flex-basis="452px"
>&lt;/p>
&lt;p>实验结果显示，现有的大模型的 size 应该更小（或者需要更大的算力）。作者最终的结论就是，现有的比较小的模型，需要更多的算力才能达到更好的表现。&lt;/p>
&lt;h2 id="chinchilla">Chinchilla
&lt;/h2>&lt;p>基于上一节的发现，作者提出了 Chinchilla, 一个 70B 的模型，训练使用了 1.4T token. 训练的数据集为 MassiveText 的扩展版本，训练使用的优化器为 AdamW, tokenizer 为 SentencePiece.&lt;/p>
&lt;p>模型配置如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Layers&lt;/th>
&lt;th>Number Heads&lt;/th>
&lt;th>Key/Value Size&lt;/th>
&lt;th>dmodel&lt;/th>
&lt;th>Max LR&lt;/th>
&lt;th>Batch Size&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Gopher 280B&lt;/td>
&lt;td>80&lt;/td>
&lt;td>128&lt;/td>
&lt;td>128&lt;/td>
&lt;td>16384&lt;/td>
&lt;td>$4\times 10^{-5}$&lt;/td>
&lt;td>$3M\to6M$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Chinchilla 70B&lt;/td>
&lt;td>80&lt;/td>
&lt;td>64&lt;/td>
&lt;td>128&lt;/td>
&lt;td>8192&lt;/td>
&lt;td>$1\times 10^{-5}$&lt;/td>
&lt;td>$1.5M\to3M$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="ablation-study">Ablation Study
&lt;/h3>&lt;p>&lt;strong>learning rate schedule&lt;/strong>
作者还通过 ablation study 发现，cosine learning rate cycle length 应该和训练步数差不多，当 cycle length 太长时，模型表现会下降。&lt;/p>
&lt;p>&lt;strong>Optimizer&lt;/strong>
作者对比了 &lt;a class="link" href="https://maosong.website/p/notes-on-adam/" target="_blank" rel="noopener"
>Adam&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/notes-on-adamw/" target="_blank" rel="noopener"
>AdamW&lt;/a> 的表现，结果发现，AdamW 的表现优于 Adam.&lt;/p>
&lt;p>&lt;strong>High Precision&lt;/strong>
训练时，作者使用了高精度也就是 &lt;code>float32&lt;/code> 来保存梯度的状态，结果显示，不管是 Adam 还是 AdamW, 使用高精度都可以提高模型的表现&lt;/p>
&lt;p>&lt;strong>Comparison with Kaplan&lt;/strong>
作者还对比了 Chinchilla 和 Kaplan 的预测结果，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/chinchilla-scaling-law/Chinchilla-Comparison-with-Kaplan.png"
width="2684"
height="1436"
srcset="https://maosong2022.github.io/p/chinchilla-scaling-law/Chinchilla-Comparison-with-Kaplan_hu507971005612149960.png 480w, https://maosong2022.github.io/p/chinchilla-scaling-law/Chinchilla-Comparison-with-Kaplan_hu5865122634259890514.png 1024w"
loading="lazy"
alt="Comparison with Kaplan"
class="gallery-image"
data-flex-grow="186"
data-flex-basis="448px"
>&lt;/p>
&lt;p>结果显示，基于 Chinchilla 预测得到的模型训练效果比 Kaplan 的更好。&lt;/p>
&lt;p>&lt;strong>Curvature of the FLOPs-frontier&lt;/strong>
作者发现，FLOP-minimal loss frontier 存在 curvature, 也就是小模型和大模型预测出来的曲线是不一样的，作者将结果展示在下图中&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/chinchilla-scaling-law/Chinchilla-curvature-of-FLOPs-frontier.png"
width="3012"
height="1570"
srcset="https://maosong2022.github.io/p/chinchilla-scaling-law/Chinchilla-curvature-of-FLOPs-frontier_hu14241032175010404049.png 480w, https://maosong2022.github.io/p/chinchilla-scaling-law/Chinchilla-curvature-of-FLOPs-frontier_hu704732338348992847.png 1024w"
loading="lazy"
alt="Curvature of the FLOPs-frontier"
class="gallery-image"
data-flex-grow="191"
data-flex-basis="460px"
>&lt;/p>
&lt;p>结果显示，从小模型拟合出来的结果比大模型拥有更高的算力使用效率，作者认为这是未来的一个研究方向。&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>本文中作者重新探究了针对 LLM 的 scaling law, 作者发现已有的大模型都存在 under-training 的现象，也就是说，模型需要更多的训练 token, 具体来讲，model size scaling 和 dataset scaling 应该处于同一水平。作者基于这个结论，提出了 Chinchilla, 一个 70B 的 LLM, 其表现超过了 280B 的 LLM.&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2203.15556" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Kaplan Scaling Law</title><link>https://maosong2022.github.io/p/kaplan-scaling-law/</link><pubDate>Wed, 22 Oct 2025 14:10:52 +0800</pubDate><guid>https://maosong2022.github.io/p/kaplan-scaling-law/</guid><description>&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>作者首先就总结了本文的发现，主要有以下几点：&lt;/p>
&lt;ol>
&lt;li>损失与模型的 scale , 即除开 embedding 的模型参数，数据集大小以及算力强相关，与 model shape 比如 depth 或者 width 关系不大&lt;/li>
&lt;li>scaling law 是非常光滑的，意味着 scaling law 是一个可预测的模型&lt;/li>
&lt;li>overfitting 普遍存在，当参数和数据集大小同时增加时，模型的表现会增加，但是当其中一个量固定时，提升就比较小。并且当我们将模型参数提升 8 倍时，我们只需要将数据集的大小提升 5 倍就可以避免过拟合&lt;/li>
&lt;li>训练的损失函数曲线与 model size 无关，因此我们可以预测给定大小模型的表现&lt;/li>
&lt;li>模型在测试集和训练集上的表现高度相关，因此我们可以基于训练集的损失来预测模型的表现&lt;/li>
&lt;li>大模型比小模型拥有更高的 sample efficiency, 即更小的训练步数就可以达到相同的表现&lt;/li>
&lt;li>convergence 不能说明一切，我们可以通过 early-stopping 来提高算力使用效率，避免模型花费过多的算力在较小的提升上&lt;/li>
&lt;li>最优的 batch size 与 loss 呈一个 power law 的关系&lt;/li>
&lt;/ol>
&lt;h3 id="notation">Notation
&lt;/h3>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Symbol&lt;/th>
&lt;th>Notation&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$L$&lt;/td>
&lt;td>cross entropy loss&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$N$&lt;/td>
&lt;td>non-embedding parametters&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$B$&lt;/td>
&lt;td>batch size&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$S$&lt;/td>
&lt;td>number of training steps&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$C\approx 6NBS$&lt;/td>
&lt;td>estimate of total training compute&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$D$&lt;/td>
&lt;td>dataset size in tokens&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$B_{crit}$&lt;/td>
&lt;td>critical batch size&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$C_{\min}$&lt;/td>
&lt;td>estimate of the minimum compute to reach a given value of loss&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$S_{\min}$&lt;/td>
&lt;td>estimate of the minimum steps to reach a given value of loss&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\alpha_X$&lt;/td>
&lt;td>power-law exponents for the scaling law of loss&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$d$&lt;/td>
&lt;td>hidden size of the model&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>为了简便，后续未经特殊说明，我们说模型参数均指的是 non-embedding 的参数&lt;/p>
&lt;h3 id="scaling-law-overview">Scaling Law Overview
&lt;/h3>&lt;ol>
&lt;li>当数据集 $D$ 足够大时，损失与模型参数大小 $N$ 之间的关系为&lt;/li>
&lt;/ol>
$$
L(N) = \left(\frac{N_C}{N}\right)^{\alpha_N}, \alpha_N\sim 0.076, N_C\sim 8.8\times 10^{13}
$$&lt;ol start="2">
&lt;li>给定模型参数大小 $N$ , 损失与数据集大小 $D$ 之间的关系为&lt;/li>
&lt;/ol>
$$
L(D) = \left(\frac{D_C}{D}\right)^{\alpha_D}, \alpha_D\sim 0.095, D_C\sim 5.4\times 10^{13}
$$&lt;ol start="3">
&lt;li>给定足够大的数据集 $D$ 和最优模型大小 $N$ 时，损失与算力 $C$ 之间的关系为&lt;/li>
&lt;/ol>
$$
L(C_{\min}) = \left(\frac{C_C^{\min}}{C_{\min}}\right)^{\alpha_D}, \alpha_C^{\min}\sim 0.050, C_C^{\min}\sim 3.1\times 10^{8}
$$&lt;p>作者在不同大小的数据集，算力，模型大小下进行了测试，结果发现 scaling 与模型的 shape, transformer 的超参数之间的关系比较小。$\alpha_N,\alpha_D,\alpha_C^{\min}$ 等决定了当我们 scale up 数据及大小，模型大小和算力时损失的变化情况。比如当我们将模型参数提升至 2 倍时，模型的损失会降低至原来的 $0.95$.&lt;/p>
&lt;p>基于发现 1 和 2, 作者发现当我们将模型的 size 提升至原来的 2 倍时，模型的数据集大小应该提升至原来的 $1.67$ 倍，具体关系为 $D\sim N^{0.74}$.&lt;/p>
&lt;p>作者使用了一个统一的公式来描述损失与数据及大小和模型参数大小之间的关系&lt;/p>
$$
L(N,D) = \left[\left(\frac{N_c}{N}\right)^{\frac{\alpha_N}{\alpha_D}}+ \frac{D_c}{D}\right]^{\alpha_D}
$$&lt;ol start="4">
&lt;li>当数据集充分大时，损失与模型参数大小以及更新步数 $S$ 的关系如下&lt;/li>
&lt;/ol>
$$
L(N, S) = \left(\frac{N_C}{N}\right)^{\alpha_N} +\left(\frac{S_C}{S_{\min}(S)}\right)^{\alpha_S}
$$&lt;p>这里 $S_C\approx 2.1\times 10^3$, $\alpha_S\approx 0.76$, $S_{\min}(S)$ 是估计出来的最小优化步数&lt;/p>
&lt;ol start="5">
&lt;li>最优的 batch size 与损失函数之间的关系如下&lt;/li>
&lt;/ol>
$$
B_{crit}(L) = \frac{B_*}{L^{1/\alpha_B}}, B_*\sim 2*10^8 \text{ tokens}, \alpha_B\sim 0.21
$$&lt;ol start="6">
&lt;li>给定算力 $C$ 且无其他限制时，模型参数，数据及大小，batch size 和更新参数与算力之间的关系如下&lt;/li>
&lt;/ol>
$$
N\propto C^{\alpha_C^{\min}/\alpha_N}, B\propto C^{\alpha_C^{\min}/\alpha_B}, S\propto C^{\alpha_C^{\min}/\alpha_S}, D=BS
$$&lt;p>其中&lt;/p>
$$
\alpha_C^{\min} = \frac{1}{\frac{1}{\alpha_S}+\frac{1}{\alpha_B}+\frac{1}{\alpha_N}}
$$&lt;p>实验的结果为 $N\propto C_{\min}^{0.73}$, $B\propto C_{\min}^{0.24}$ , $S\propto C_{\min}^{0.03}$. 也就是说，当我们提升算力时，提升模型的参数大小带来的收益是最高的。&lt;/p>
&lt;h2 id="background">Background
&lt;/h2>&lt;p>首先，transformer 的参数量通过计算可以得到&lt;/p>
$$
N\approx 2dn(2d+d_{ff}) = 12nd^2
$$&lt;p>这里 $d$ 是 hidden size, $n$ 是 layer 个数，$d_{ff}$ 是 MLP 的 hidden size, 这里我们 假设 $d_{ff}=4d$. 计算时我们丢掉了 bias 以及 LayerNorm 的参数量。具体计算过程见 &lt;a class="link" href="https://maosong.website/p/llm-parameter-computation/" target="_blank" rel="noopener"
>LLM parameter analysis&lt;/a>&lt;/p>
&lt;p>transformer 一次前向计算的 operations 数量大概为&lt;/p>
$$
C_{forward}\approx 2N + 2nLd
$$&lt;p>这里 $L$ 是输入的 token 长度。&lt;/p>
&lt;p>由于反向传播所需要的 FLOPs 是前向传播两倍，因此 transformer 的计算量为&lt;/p>
$$
C = C_{backward} + C_{forward} = 3C_{forward}\approx 6N
$$&lt;p>具体计算过程见 &lt;a class="link" href="https://maosong.website/p/llm-flops-computation/" target="_blank" rel="noopener"
>LLM FLOPs analysis&lt;/a>。也就是说，对于参数量为 $N$ 的 transformer model, 每个 token 所需要的 FLOPs 为 $C\approx 6N$&lt;/p>
&lt;h2 id="empirical-results-and-basic-power-laws">Empirical Results and Basic Power Laws
&lt;/h2>&lt;h3 id="transformer-shape-and-hyper-parameter-independence">Transformer Shape and Hyper-parameter Independence
&lt;/h3>&lt;p>作者基于 $N=12nd^2$, 在保持总参数量 $N$ 不变的情况下，分别调整 $n$, $d_{ff}$ 和 number of attention heads 的个数 （变化 $d$ 用于维持总参数量不变），结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-model-shape-ablation.png"
width="1264"
height="501"
srcset="https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-model-shape-ablation_hu17900088031382528453.png 480w, https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-model-shape-ablation_hu13532320540752824526.png 1024w"
loading="lazy"
alt="Ablation study on model shape"
class="gallery-image"
data-flex-grow="252"
data-flex-basis="605px"
>&lt;/p>
&lt;p>实验结果发现，损失对于 $d_{ff}/d$, $d/n$, $d/n_h$ 都比较 robust, 说明&lt;strong>模型的损失对模型的 shape 依赖性比较低。&lt;/strong>&lt;/p>
&lt;h3 id="non-embedding-parameter-count">Non-embedding Parameter Count
&lt;/h3>&lt;p>作者探究了以下 model size 对损失的影响，作者使用了不同的 $n$ 和 $d$, 然后训练得到的损失情况如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-ablation-model-size.png"
width="1285"
height="547"
srcset="https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-ablation-model-size_hu9271102985560927496.png 480w, https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-ablation-model-size_hu7924636881673413467.png 1024w"
loading="lazy"
alt="Ablation study on model size"
class="gallery-image"
data-flex-grow="234"
data-flex-basis="563px"
>&lt;/p>
&lt;p>作者发现，当包含 embedding parameter 时，损失不仅依赖于模型参数量，还依赖于 layer 层数 $n$, 但是&lt;strong>当我们排除 embedding parameter 时，模型的损失便与 layer 层数 $n$ 关系不大&lt;/strong>。这个趋势可以用以下模型来表示&lt;/p>
$$
L(N) \approx \left(\frac{N_c}{N}\right)^{\alpha_N}
$$&lt;p>最终拟合的曲线如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-parameter-scaling-law-curve.png"
width="404"
height="383"
srcset="https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-parameter-scaling-law-curve_hu13168868596518131083.png 480w, https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-parameter-scaling-law-curve_hu5245681849934050002.png 1024w"
loading="lazy"
alt="Scaling law with respect to parameters"
class="gallery-image"
data-flex-grow="105"
data-flex-basis="253px"
>&lt;/p>
&lt;h3 id="comparing-to-lstms-and-universal-transformers">Comparing to LSTMs and Universal Transformers
&lt;/h3>&lt;p>作者比较了 LSTM 和 Transformer 结构的损失，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-ablation-LSTM.png"
width="1268"
height="467"
srcset="https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-ablation-LSTM_hu1136860397266232918.png 480w, https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-ablation-LSTM_hu10097496253072581529.png 1024w"
loading="lazy"
alt="Ablation study on LSTM"
class="gallery-image"
data-flex-grow="271"
data-flex-basis="651px"
>&lt;/p>
&lt;p>可以看到，&lt;strong>transformer 比 LSTM 拥有更强的学习能力&lt;/strong>， LSTM 架构对于 early context 表现比较好，但是随着 context 增加，LSTM 的表现逐渐弱于 transformer. &lt;strong>即 transformer 的长上下文能力强于 LSTM 架构&lt;/strong>。&lt;/p>
&lt;h3 id="generalization-among-data-distributions">Generalization Among Data Distributions
&lt;/h3>&lt;p>模型是在 WebText2 数据集上训练的，作者进一步在其他数据集上评估了以下模型的泛化性，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-parameter-scaling-law-generalization.png"
width="1266"
height="591"
srcset="https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-parameter-scaling-law-generalization_hu4153651653083461167.png 480w, https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-parameter-scaling-law-generalization_hu2666664858330362694.png 1024w"
loading="lazy"
alt="Generalization performance"
class="gallery-image"
data-flex-grow="214"
data-flex-basis="514px"
>&lt;/p>
&lt;p>结果发现，模型在其他数据集上的泛化性很好。并且，&lt;strong>模型的泛化性能仅与训练阶段的表现相关（validation loss），而与训练阶段（是否收敛）无关&lt;/strong>。&lt;/p>
&lt;p>作者还评估了 model depth 对模型泛化性的影响，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-ablation-depth-to-generalization.png"
width="658"
height="421"
srcset="https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-ablation-depth-to-generalization_hu3862238373047661970.png 480w, https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-ablation-depth-to-generalization_hu15572690613863877083.png 1024w"
loading="lazy"
alt="Ablation study on depth"
class="gallery-image"
data-flex-grow="156"
data-flex-basis="375px"
>&lt;/p>
&lt;p>实验结果显示，&lt;strong>model depth 对模型泛化性基本没有影响&lt;/strong>。&lt;/p>
&lt;h3 id="performance-with-data-size-and-compute">Performance with Data Size and Compute
&lt;/h3>&lt;p>作者探究了损失与 dataset size $D$ 之间的关系。作者固定一个模型，然后当 test loss 不再下降时停止训练，结果发现 test loss 与 dataset size $D$ 之间存在如下关系&lt;/p>
$$
L(D) \approx \left(\frac{D_c}{D}\right)^{\alpha_D}
$$&lt;p>拟合结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-dataset-size-scaling-law.png"
width="419"
height="384"
srcset="https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-dataset-size-scaling-law_hu14553376488015908882.png 480w, https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-dataset-size-scaling-law_hu7074269476621454165.png 1024w"
loading="lazy"
alt="Scaling law with respect to dataset"
class="gallery-image"
data-flex-grow="109"
data-flex-basis="261px"
>&lt;/p>
&lt;p>接下来，基于前面计算的结果，我们有 $C\approx 6ND=6NBS$, 这里 $B$ 是 batch size, $S$ 是训练步数。给定 $C$, 作者使用不同大小的模型进行训练，batch size $B$ 保持不懂，训练步数设置为 $S=C/6BS$,实验结果显示损失与算力 $C$ 之间满足如下关系&lt;/p>
$$
L(C) \approx \left(\frac{C_c}{C}\right)^{\alpha_C}
$$&lt;p>拟合结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-compute-scaling-law.png"
width="451"
height="384"
srcset="https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-compute-scaling-law_hu10913539496339843682.png 480w, https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-compute-scaling-law_hu13622587868165539120.png 1024w"
loading="lazy"
alt="Scaling law with respect to compute"
class="gallery-image"
data-flex-grow="117"
data-flex-basis="281px"
>&lt;/p>
&lt;p>作者进一步探究了 sample efficiency 与 model size 之间的关系，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-sample-efficiency-vs-model-size.png"
width="1273"
height="513"
srcset="https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-sample-efficiency-vs-model-size_hu12183487265673740051.png 480w, https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-sample-efficiency-vs-model-size_hu10795984610073713790.png 1024w"
loading="lazy"
alt="Sample efficiency with respect to model size"
class="gallery-image"
data-flex-grow="248"
data-flex-basis="595px"
>&lt;/p>
&lt;p>结果显示，&lt;strong>随着 model size 增加，sample efficiency 也在增加&lt;/strong>&lt;/p>
&lt;h2 id="charting-the-infinite-data-limit-and-overfitting">Charting the Infinite Data Limit and Overfitting
&lt;/h2>&lt;p>作者在本节探讨了同时变化 $N$ 和 $D$ 对损失变化的影响。&lt;/p>
&lt;h3 id="proposed-equation">Proposed Equation
&lt;/h3>&lt;p>作者基于三个原则进行建模：&lt;/p>
&lt;ol>
&lt;li>改变 vocabulary size 或者 tokenization 会 rescale loss&lt;/li>
&lt;li>固定 $D$ 并且令 $N\to\infty$, 则最终损失应该接近 $L(D)$. 反之固定 $N$, 令 $D\to\infty$, 最终损失应该接近 $L(N)$&lt;/li>
&lt;li>$L(N,D)$ 在 $D=\infty$ 处应该是可解析的&lt;/li>
&lt;/ol>
&lt;p>基于以上三条原则，将模型选择为如下形式&lt;/p>
$$
L(N,D) = \left[\left(\frac{N_c}{N}\right)^{\frac{\alpha_N}{\alpha_D}}+ \frac{D_c}{D}\right]^{\alpha_D}
$$&lt;p>作者基于不同配置进行训练，基于实验结果你和得到的参数如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Parameter&lt;/th>
&lt;th>$\alpha_N$&lt;/th>
&lt;th>$\alpha_D$&lt;/th>
&lt;th>$N_c$&lt;/th>
&lt;th>$D_c$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Value&lt;/td>
&lt;td>0.076&lt;/td>
&lt;td>0.103&lt;/td>
&lt;td>$6.4\times 10^{13}$&lt;/td>
&lt;td>$1.8\times 10^{13}$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>接下来，作者探究了模型的过拟合程度，作者定义如下 metric&lt;/p>
$$
\delta L(N,D) := \frac{L(N,D)}{L(N,\infty)} - 1
$$&lt;p>带入 $L(N,D)$ 定义就得到&lt;/p>
$$
\delta L(N,D) = \left(1 + \left(\frac{N}{N_c}\right)^{\frac{\alpha_N}{\alpha_D}}\frac{D_c}{D}\right) - 1
$$&lt;p>通过测试不同的模型，作者发现 $\delta L$ 的值在 $0.02$ 左右，将实验结果带入到上面的公式就得到&lt;/p>
$$
D \geq (5\times 10^3)N^{0.7379}
$$&lt;p>也就是说对于参数量为 $N$ 的模型，需要 data size $D \geq (5\times 10^3)N^{0.7379}$ 才能避免过拟合。&lt;/p>
&lt;h2 id="scaling-laws-with-model-size-and-training-time">Scaling Laws with Model Size and Training time
&lt;/h2>&lt;p>作者在本节构建了损失函数与 model size $N$ 以及训练时间的 scaling law&lt;/p>
&lt;h3 id="adjustment-for-training-at-critical-batch-size">Adjustment for Training at Critical Batch Size
&lt;/h3>&lt;p>已有结论说明，存在一个 critical batch size $B_{crit}$, 当 batch size 接近 $B_{crit}$ 时，增加 batch size 对计算效率影响比较小，但是当 batch size 大于 $B_{crit}$ 时，带来的提升比较小。另一方面，batch size 会影响梯度的噪声程度。因此，训练步数 $S$ 和处理的样本数 $E=BS$ 应该满足：&lt;/p>
$$
\left(\frac{S}{S_{\min}}-1\right)\left(\frac{E}{E_{\min}}-1\right) = 1
$$&lt;p>这里 $S_{\min}$ 是达到损失 $L$ 所需要的最小训练步数，而 $E_{\min}$ 是最小的训练样本数量。&lt;/p>
&lt;p>作者的实验结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-critical-batch-size-relation.png"
width="1264"
height="499"
srcset="https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-critical-batch-size-relation_hu14217156308397777485.png 480w, https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-critical-batch-size-relation_hu15617822732658331116.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="253"
data-flex-basis="607px"
>&lt;/p>
&lt;p>作者将 critical batch size 定义为&lt;/p>
$$
B_{crit}(L) := \frac{E_{\min}}{S_{\min}}
$$&lt;p>使用 critical batch size 进行训练可以在计算效率和算力之间达到一个平衡。&lt;/p>
&lt;p>作者基于上面的实验结果探究了 critical batch size 和 model performance 之间的关系，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-critical-batch-size-vs-performance.png"
width="946"
height="620"
srcset="https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-critical-batch-size-vs-performance_hu9889472903970816050.png 480w, https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-critical-batch-size-vs-performance_hu12246504925174554762.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="152"
data-flex-basis="366px"
>&lt;/p>
&lt;p>可以看到，critical batch size 与 model size 的关系不大，仅与损失 $L$ 有关。作者通过以下模型拟合 critical batch size:&lt;/p>
$$
B_{crit}(L) \approx \frac{B_*}{L^{1/\alpha_B}}
$$&lt;p>这里 $B_*\approx 2\times 10^8$, $\alpha_B\approx 0.21$.&lt;/p>
&lt;p>给定一个 target loss $L$, 当 batch size $B&amp;raquo; B_{crit}$ 时，作者定义最小训练步数为&lt;/p>
$$
S_{\min}(S) := \frac{S}{1+B_{crit}(L)/B}
$$&lt;p>给定 target loss $L$ 和 model size $N$, 当 batch size $B&amp;laquo; B_{crit}$ 时，作者定义最小算力为&lt;/p>
$$
C_{\min}(C) := \frac{C}{1+B_{crit}(L)/B}
$$&lt;h3 id="performance-with-model-size-and-compute">Performance with Model Size and Compute
&lt;/h3>&lt;p>作者使用如下公式来探究损失与 model size 和 computer 之间的关系&lt;/p>
$$
L(N, S_{\min}) = \left(\frac{N_C}{N}\right)^{\alpha_N} +\left(\frac{S_C}{S_{\min}(S)}\right)^{\alpha_S}
$$&lt;p>拟合结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Parameter&lt;/th>
&lt;th>$\alpha_N$&lt;/th>
&lt;th>$\alpha_S$&lt;/th>
&lt;th>$N_c$&lt;/th>
&lt;th>$S_c$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Value&lt;/td>
&lt;td>0.077&lt;/td>
&lt;td>0.76&lt;/td>
&lt;td>$6.5\times 10^{13}$&lt;/td>
&lt;td>$2.1\times 10^{3}$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>基于这个拟合结果，作者得到了下图的结果&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-loss-vs-model-size-and-training-steps.png"
width="613"
height="347"
srcset="https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-loss-vs-model-size-and-training-steps_hu4947734139461952958.png 480w, https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-loss-vs-model-size-and-training-steps_hu13550666530688042403.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="176"
data-flex-basis="423px"
>&lt;/p>
&lt;p>作者还使用了不同的可视化方式，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-performance-vs-compute-budget-and-steps.png"
width="1273"
height="560"
srcset="https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-performance-vs-compute-budget-and-steps_hu1733705209026617313.png 480w, https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-performance-vs-compute-budget-and-steps_hu17479915844925270087.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="227"
data-flex-basis="545px"
>&lt;/p>
&lt;p>实验结果显示，上面的公式拟合的很好。&lt;/p>
&lt;h3 id="lower-bound-on-early-stopping-step">Lower Bound on Early Stopping step
&lt;/h3>&lt;p>作者还探究了以下 early step 与模型大小以及数据集之间的关系，作者通过分析得到如下结果&lt;/p>
$$
S_{stop}(N,D) \gtrsim \frac{S_c}{[L(N,D)-L(N,\infty)]^{1/\alpha_S}}
$$&lt;p>其中 $L(N,\infty)$ 是在充分大数据集上的收敛损失。作者对实验结果进行了拟合，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-early-stop-resultes.png"
width="1277"
height="618"
srcset="https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-early-stop-resultes_hu5805543562140428933.png 480w, https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-early-stop-resultes_hu1969578989997728403.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="206"
data-flex-basis="495px"
>&lt;/p>
&lt;h2 id="optimal-allocation-of-the-compute-budget">Optimal Allocation of the Compute Budget
&lt;/h2>&lt;p>作者在本节探究了最优算力与 model size $N$ 和训练数据 $2B_{crit}S_{\min}$ 之间的关系&lt;/p>
&lt;h3 id="optimal-performance-and-allocations">Optimal Performance and Allocations
&lt;/h3>&lt;p>作者首先基于&lt;/p>
$$
C_{\min}(C) := \frac{C}{1+B_{crit}(L)/B}
$$&lt;p>绘制了如下曲线图&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-loss-vs-optimal-compute.png"
width="664"
height="439"
srcset="https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-loss-vs-optimal-compute_hu2624240290987422757.png 480w, https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-loss-vs-optimal-compute_hu3026233000259484535.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="151"
data-flex-basis="363px"
>&lt;/p>
&lt;p>作者发现，相比于 loss 与算力 $C$ 之间的关系，使用 $C_{\min}$ 进行拟合效果更好。&lt;/p>
&lt;p>接下来，作者基于 $L(C_{\min})$ 进一步探究了给定算力如何决定最优的 model size $N(C_{\min})$. 其实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-optimal-model-size-given-compute.png"
width="616"
height="405"
srcset="https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-optimal-model-size-given-compute_hu6766491787863314706.png 480w, https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-optimal-model-size-given-compute_hu13721565017444583805.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="152"
data-flex-basis="365px"
>&lt;/p>
&lt;p>实验结果显示，model size 和算力之间有如下关系&lt;/p>
$$
N(C_{\min}) \propto (C_{\min})^{0.73}
$$&lt;p>作者进一步探究了对于非最优模型大小与算力之间的关系，作者先构建了如下的关系&lt;/p>
$$
\frac{C(N, N_{\mathrm{eff}})}{C(N_{\mathrm{eff}}, N_{\mathrm{eff}})}
= \frac{N}{N_{\mathrm{eff}}}
\left[
1 + \frac{\alpha_S}{\alpha_N}
\left( 1 - \left( \frac{N_{\mathrm{eff}}}{N} \right)^{\alpha_N} \right)
\right]^{-\!1 / \alpha_S}.
$$&lt;p>对应的示意图为&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-suboptimal-model-efficiency.png"
width="1256"
height="600"
srcset="https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-suboptimal-model-efficiency_hu3403526838017537494.png 480w, https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-suboptimal-model-efficiency_hu18252056355401133900.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="209"
data-flex-basis="502px"
>&lt;/p>
&lt;p>实现结果发现，大小为最优模型的 $0.6\sim 2.2$ 倍只需要额外 $20%$ 的算力。作者强调，这个实验结果对于超大模型不一定适用。&lt;/p>
&lt;p>作者进一步推导了 $S_{\min}$ 和 $C_{\min}$ 之间的关系，由于 $C_{\min}=6NB_{crit}S$, 且我们前面已经有 $B\propto L^{-4.8}$ 和 $L\propto C_{\min}^{-0.05}$, 因此 我们有&lt;/p>
$$
B_{crit}\propto L^{-4.8} \propto (C_{\min})^{-0.05\times (-4.8)}\propto (C_{\min})^{0.24}
$$&lt;p>以及&lt;/p>
$$
S_{\min} \propto \frac{C_{\min}}{6B_{crit}N(C_{\min})} \propto (C_{\min})^{0.03}
$$&lt;p>拟合的结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-optimal-step-size-vs-compute.png"
width="620"
height="403"
srcset="https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-optimal-step-size-vs-compute_hu17458715235703264140.png 480w, https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-optimal-step-size-vs-compute_hu15917017864681204094.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="153"
data-flex-basis="369px"
>&lt;/p>
&lt;p>因此，基于上面的结果，当我们增加算力时，我们的主要精力应该放在增加模型大小和提高 batch size 上，而训练步数基本可以保持不变。&lt;/p>
&lt;h3 id="another-way-of-derivation">Another way of Derivation
&lt;/h3>&lt;p>作者还给出了另一种建模 $L(C_{\min})$ 的方式，即从 $L(N,S_{\min})$ 中进行推导，作者将 $B_{crit}$ 和 $S_{\min}$ 的表达式带入到 $L(N,S_{\min})$ 然后求解最小值就得到&lt;/p>
$$
L(C_{\min})= \left(\frac{C_C^{\min}}{C_{\min}}\right)^{\alpha_C^{\min}}
$$&lt;p>其中&lt;/p>
$$
\alpha_C^{\min} = \frac{1}{\frac{1}{\alpha_S}+\frac{1}{\alpha_B}+\frac{1}{\alpha_N}} \approx 0.054
$$&lt;p>这和前面的结果基本是吻合的，进一步进行推导得到&lt;/p>
$$
N(C_{\min})\propto (C_{\min})^{\alpha_C^{\min}/\alpha_N}\approx (C_{\min})^{0.71}
$$&lt;p>这个结果也和上面的差不多。&lt;/p>
&lt;h3 id="contradiction-and-a-conjecture">Contradiction and a Conjecture
&lt;/h3>&lt;p>作者发现，尽管拟合的 scaling law 曲线非常好，但是由于自然语言不可能达到 zero entropy, 因此该曲线最终一定会失效。作者基于更大的模型进行了实验，结果发现，模型在某一点开始就比预测的损失曲线下降的更慢。作者认为这是因为 transformer 模型已经达到了 maximal performance 导致的。&lt;/p>
&lt;p>通过前面的分析，我们发现 $L(C_{\min})$ 比 $L(D)$ 下降的快，因此两者必然在某一点相交。&lt;/p>
&lt;p>在前面的章节中，我们基于以下关系来决定数据集大小&lt;/p>
$$
D\propto N^{0.74}\propto (C_{\min})^{0.74*0.73}\propto (C_{\min})^{0.54}
$$&lt;p>这里我们利用了 $N(C_{\min})$ 的结果&lt;/p>
&lt;p>另一方面，我们有&lt;/p>
$$
D(C_{\min}) = \frac{2C_{\min}}{6N(C_{\min})}\propto (C_{\min})^{0.26}
$$&lt;p>可以看到，基于训练最优导出的数据集大小相比于拟合出来的数据集大小，实际上存在过拟合。&lt;/p>
&lt;p>作者进一步分析出了 $L(D(C_{\min}))$ 和 $L(C_{\min})$ 这两条曲线的交点，结果得到&lt;/p>
$$
C^*\approx 10^4 \text{ PF-Days}, N^*\approx 10^{12}\text{ parameters}, D^*\approx 10^12\text{ tokens}, L^*\approx 1.7\text{1.7nats/token}
$$&lt;p>作者认为出现这种原因有以下几种情况：&lt;/p>
&lt;ol>
&lt;li>$L^*$ 给出了自然语言的 entropy 的一个估计，因此当模型充分大之后，模型可能已经获取到了数据中的所有知识&lt;/li>
&lt;li>$L(C_{\min})$ 可以作为数据集噪声的一个量化表现，其衡量了数据集的质量&lt;/li>
&lt;/ol>
&lt;h2 id="learning-rate-schedule">Learning Rate Schedule
&lt;/h2>&lt;p>附录中，作者还探究了 learning rate 与损失之间的关系，作者使用了不同 learning rate schedule 对模型损失的影响，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-learning-rate-schedule-results.png"
width="1278"
height="530"
srcset="https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-learning-rate-schedule-results_hu13606754496324356115.png 480w, https://maosong2022.github.io/p/kaplan-scaling-law/Kaplan-learning-rate-schedule-results_hu4951923550844399163.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="241"
data-flex-basis="578px"
>&lt;/p>
&lt;p>实验结果显示，&lt;strong>只要 learning rate 下降的不会太快，模型的表现基本上差不太多&lt;/strong>。&lt;/p>
&lt;p>作者基于实验结果得到了学习率和模型参数之间的关系如下&lt;/p>
$$
\text{lr}(N)\approx 0.003239 - 0.0001395\log N
$$&lt;p>也就是说，小模型用比较大的学习率，大模型用较小的学习率。&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者在本文中训练了大量不同配置的大模型，然后构建了损失（损失）与模型参数，数据及大小以及算力之间的关系。实验结果发现，损失与架构和优化参数之间的关系比较小，主要由模型参数量决定，更大的模型拥有更高的采样效率。&lt;/p>
&lt;p>作者认为，本文的局限在于损失函数不一定能够反应模型在其他语言任务上的表现。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2001.08361" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>LLM FLOPs Computation</title><link>https://maosong2022.github.io/p/llm-flops-computation/</link><pubDate>Wed, 15 Oct 2025 16:33:39 +0800</pubDate><guid>https://maosong2022.github.io/p/llm-flops-computation/</guid><description>&lt;p>本文中，我们介绍如何计算基于 transformer 架构的 LLM 的 FLOPs, 计算完成之后，我们可以推导出算力 $C$ 与模型参数量 $N$，数据集大小 $D$ 之间的关系，即 $C\approx 6ND$.&lt;/p>
&lt;h2 id="background">Background
&lt;/h2>&lt;h3 id="flops">FLOPs
&lt;/h3>&lt;p>FLOPs，floating point operations，表示浮点数运算次数，一般计算 FLOPs 仅考虑加法和乘法。&lt;/p>
&lt;blockquote>
&lt;p>假设 $A\in\mathbb{R}^{m\times p}$, $B\in\mathbb{R}^{p\times n}$, 则 计算 $C=AB$ 的过程中一共需要进行 $mnp$ 次乘法运算和 $mnp$ 次加法运算，因此总的 FLOPs 为 $2mnp$.&lt;/p>
&lt;/blockquote>
&lt;h3 id="assumption">Assumption
&lt;/h3>&lt;blockquote>
&lt;p>假设 $A\in\mathbb{R}^{m\times p}$, $B\in\mathbb{R}^{p\times n}$, $C\in\mathbb{R}^{m\times n}$ 则 计算 $D=AB+C$ 的过程中一共需要进行 $mnp$ 次乘法运算和 $mnp+mn$ 次加法运算，因此总的 FLOPs 为 $2mnp+mn\approx 2mnp$.&lt;/p>
&lt;/blockquote>
&lt;p>基于上述结论，我们计算 FLOPs 时，忽略 element-wise 的操作，即我们做如下假设：&lt;/p>
&lt;ol>
&lt;li>忽略 normalization 中的小常数项运算&lt;/li>
&lt;li>忽略 residual connection 和 bias term 的加法&lt;/li>
&lt;li>忽略注意力的 MASK 和 softmax，因为这两者都是 element-wise operation.&lt;/li>
&lt;li>使用 look-up 计算 embedding layer&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>注，基于以上假设，我们的结果与 Chinchilla Scaling law 的结果稍有不同，但结论不变。&lt;/p>
&lt;/blockquote>
&lt;h3 id="notation">Notation
&lt;/h3>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Math Variable&lt;/th>
&lt;th>Code Variable&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$n$&lt;/td>
&lt;td>&lt;code>num_hidden_layers&lt;/code>&lt;/td>
&lt;td>Transformer block 个数&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\vert V\vert$&lt;/td>
&lt;td>&lt;code>vocab_size&lt;/code>&lt;/td>
&lt;td>词表大小&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$d$&lt;/td>
&lt;td>&lt;code>hidden_size&lt;/code>&lt;/td>
&lt;td>token embedding 的维度&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$d_{ff}$&lt;/td>
&lt;td>&lt;code>intermediate_size&lt;/code>&lt;/td>
&lt;td>MLP 的中间层的维度&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$h$&lt;/td>
&lt;td>&lt;code>num_attention_heads&lt;/code>&lt;/td>
&lt;td>query head 的个数&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$s$&lt;/td>
&lt;td>&lt;code>seq_len&lt;/code>&lt;/td>
&lt;td>length of token sequence&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;blockquote>
&lt;p>注：为了避免混淆，我们使用 $n$ 来表示 decode layer 的个数。&lt;/p>
&lt;/blockquote>
&lt;h2 id="computation">Computation
&lt;/h2>&lt;p>我们计算训练阶段的总 FLOPs, 记为 $C$, &lt;a class="link" href="https://maosong.website/p/kaplan-scaling-law/" target="_blank" rel="noopener"
>Kaplan scaling law&lt;/a> 用 PF-days 作为单位，$1\text{ PF-Days}=10^{15}\times 24\times 3600\ FLOPs$. 训练阶段包括前向阶段 (forward pass) 和反向传播阶段 (backward pass). 因此&lt;/p>
$$
C = FLOPs(\text{forward}) + FLOPs(\text{backward})
$$&lt;h3 id="forward">Forward
&lt;/h3>&lt;p>decoder-only transformer 的模型架构包含三个模块：&lt;/p>
&lt;ol>
&lt;li>1 层 embedding layer&lt;/li>
&lt;li>$n$ 层 decoder layer&lt;/li>
&lt;li>1 层 lm head layer&lt;/li>
&lt;/ol>
&lt;p>因此模型总的 FLOPs 为&lt;/p>
$$
FLOPs(\text{forward}) = FLOPs(\text{embedding}) + n*FLOPs(\mathrm{decode\_layer})+FLOPs(\mathrm{lm\_head})
$$&lt;h4 id="embedding--lm-head">Embedding &amp;amp; Lm Head
&lt;/h4>&lt;p>首先，对于 embedding layer, embedding layer 本质上是一个 look up table, 计算过程中不涉及浮点数运算，因此 $\boxed{FLOPs(\text{embedding})=0}$.&lt;/p>
&lt;p>接下来，对于 &lt;code>lm_head&lt;/code>, 这是一个 linear layer, 其权重大小为 $W\in\mathbb{R}^{d\times |V|}$, 输入为 $x\in\mathbb{R}^{s\times d}$, 因此 $\boxed{FLOPs(\mathrm{lm_head})=2sd|V|}$.&lt;/p>
&lt;p>因此，我们有&lt;/p>
$$
FLOPs(\text{forward}) = n*FLOPs(\mathrm{decode\_layer})+ 2sd|V|
$$&lt;h4 id="decode-layer">Decode Layer
&lt;/h4>&lt;p>对于 &lt;code>decode_layer&lt;/code>, 其又包含了四个模块：&lt;/p>
&lt;ol>
&lt;li>pre-normalization&lt;/li>
&lt;li>attention&lt;/li>
&lt;li>post-normalization&lt;/li>
&lt;li>FFN&lt;/li>
&lt;/ol>
&lt;p>pre-normalization 和 post-normalization 一般是一样的，因此&lt;/p>
$$
\begin{aligned}
FLOPs(\mathrm{decode\_layer}) &amp;= FLOPs(\mathrm{pre\_normoalization}) + FLOPs(\mathrm{Attention}) + FLOPs(\mathrm{post\_normoalization}) +FLOPs(\mathrm{FFN})\\
&amp;= 2*FLOPs(\mathrm{normoalization}) + FLOPs(\mathrm{Attention})+FLOPs(\mathrm{FFN})
\end{aligned}
$$&lt;h4 id="normalization">Normalization
&lt;/h4>&lt;p>现在有两种常见的 normalization, 也就是 LayerNorm 和 RMSNorm&lt;/p>
&lt;p>LayerNorm 定义如下&lt;/p>
$$
\mathrm{LayerNorm}(x) = \frac{x-\mathbb{E}[x]}{\sqrt{\mathrm{var}[x]+\epsilon}}\odot \beta + \gamma
$$&lt;p>其中 $\beta,\gamma\in\mathbb{R}^d$ 是可学习的参数。&lt;/p>
&lt;p>对输入 $x\in\mathbb{R}^{s\times d}$, 均值需要约 $sd$ 次 FLOPs，方差 $\mathrm{var}[x]$ 只需要约 $3sd$ 次 FLOPs，这两者的计算都可以忽略。接下来就是 scaling 和 shift, 这两者都是 element-wise 操作，我们这里，因此总的 FLOPs 为&lt;/p>
$$
\boxed{FLOPs(\mathrm{normoalization}) = 4sd}
$$&lt;p>RMSNorm 的作用和 LayerNorm 是一样的，但是实现上更简单&lt;/p>
$$
\mathrm{RMSNorm}(x) = \frac{x}{\sqrt{\|x\|_2^2+\epsilon}}\odot \gamma
$$&lt;p>其中 $\gamma\in\mathbb{R}^d$ 是可学习的参数&lt;/p>
&lt;p>对于 RMSNorm，其分析方式与 LayerNorm 基本一致，因此总的 FLOPs 为&lt;/p>
$$
\boxed{FLOPs(\mathrm{normoalization}) = 4sd}
$$&lt;p>总之，不管使用哪种 normalization，其 FLOPs 都是&lt;/p>
$$
\boxed{FLOPs(\mathrm{normoalization}) = 4sd}
$$&lt;h4 id="attention">Attention
&lt;/h4>&lt;p>Attention 定义如下&lt;/p>
$$
\mathrm{Attention}(X) = \mathrm{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V\in\mathbb{R}^{m\times d}
$$&lt;p>其中 $X\in\mathbb{R}^{s\times d}$, $W_Q,W_K,W_V\in\mathbb{R}^{d\times d}$&lt;/p>
$$
Q = W_QX\in\mathbb{R}^{s\times d},\quad
K =W_KX\in\mathbb{R}^{s\times d},\quad
V = W_VX\in\mathbb{R}^{s\times d}
$$&lt;p>$Q,K,V$ 计算的 FLOPs 为 $6*sd^2$. $QK^T$ 的 FlOPs 为 $2s^2d$, $\mathrm{softmax}(\cdot)V$ 的 FLOPs 为 $2s^2d$, 最后对于 multi-head attention 还有一个 output projection layer, 其权重为 $W_O\in\mathbb{R}^{d\times d}$, 因此 FLOPs 为 $2sd^2$. 故 attention 最终的 FLOPs 为&lt;/p>
$$
FLOPs(\mathrm{Attention})=6sd^2+2s^2d+2s^2d+2sd^2=\boxed{8sd^2+4s^2d}
$$&lt;h4 id="ffn">FFN
&lt;/h4>&lt;p>对于 FFN，有两种常见的形式，一种基于 ReLU 激活函数，其定义如下&lt;/p>
$$
y = \max(xW_1+b_1, 0)W_2 + b_2
$$&lt;p>其中 $W_1\in\mathbb{R}^{d_{ff}\times d}$, $W_2\in\mathbb{R}^{d\times d_{ff}}$. $b_1\in\mathbb{R}^{d_{ff}}$, $b_2\in\mathbb{R}^{d}$.&lt;/p>
&lt;p>对输入 $x\in\mathbb{R}^{s\times d}$, 其 FLOPs 为&lt;/p>
$$
FLOPs(\mathrm{FFN_{ReLU}}) = 2sdd_{ff} + 2sd_{ff}d =\boxed{4sdd_{ff}}
$$&lt;p>其中第一项和第二项分别为为 $xW_1$ 与 $\max(xW_1+b_1, 0)W_2$ 的 FLOPs.&lt;/p>
&lt;p>另一种基于 SwiGLU 激活函数，其定义为&lt;/p>
$$
\mathrm{SwiGLU}(x) = x\odot \sigma(x)
$$&lt;p>其中 $\sigma(\cdot)$ 是 sigmoid 函数&lt;/p>
&lt;p>FFN 的定义为&lt;/p>
$$
y = W_2(W_3x\odot \mathrm{SwiGLU}(W_1x))
$$&lt;p>其中 $W_3,W_1\in\mathbb{R}^{d_{ff}\times d}$, $W_2\in\mathbb{R}^{d\times d_{ff}}$.&lt;/p>
&lt;p>对输入 $x\in\mathbb{R}^{s\times d}$, 其 FLOPs 为&lt;/p>
$$
FLOPs(\mathrm{FFN_{SwiGLU}}) = 2sdd_{ff} + 2sdd_{ff} + 2sd_{ff}d = \boxed{6sdd_{ff}}
$$&lt;h4 id="summary">Summary
&lt;/h4>&lt;p>最终，decoder-only transformer 的 FLOPs 计算量为 （我们假设使用 multi-head attention, 基于 SwiGLU 的 MLP）&lt;/p>
$$
\begin{aligned}
FLOPs(\text{forward}) &amp;= FLOPs(\text{embedding}) + n*FLOPs(\mathrm{decode\_layer})+FLOPs(\mathrm{lm\_head})\\
&amp;= n*FLOPs(\mathrm{decode\_layer})+2sd|V|\\
&amp;= n*(2*FLOPs(\mathrm{normoalization}) + FLOPs(\mathrm{Attention})+FLOPs(\mathrm{FFN}))+2sd|V|\\
&amp;= n*(8sd+8sd^2+4s^2d+6sdd_{ff})+2sd|V|\\
&amp;= nsd^2\left(\frac8d + 8+\frac{4s}{d}+\frac{6d_{ff}}{d}+\frac{2|V|}{nd}\right)\\
&amp;\approx \boxed{nsd^2\left(8+\frac{4s}{d}+\frac{6d_{ff}}{d}+\frac{2|V|}{nd}\right)}
\end{aligned}
$$&lt;p>这里我们丢弃了 normalization 项，因为 normalization 的 FLOPs 是一个低阶项&lt;/p>
&lt;h3 id="backward">Backward
&lt;/h3>&lt;p>首先，我们有如下结论：&lt;/p>
&lt;blockquote>
&lt;p>神经网络 backward 过程的计算量（FLOPs）为 forward 过程的两倍&lt;/p>
&lt;/blockquote>
&lt;p>我们使用一个简单的例子来证明这个结论，考虑线性层 $h=Wx$, 其中 $W\in\mathbb{R}^{m\times d}$, 对于输入 $x\in\mathbb{R}^{d\times 1}$ 其 forward 过程的计算量为 $2md$.&lt;/p>
&lt;p>对于反向过程，我们需要分别计算损失 $L$ 对权重和输入的梯度，即&lt;/p>
$$
\frac{\partial L}{\partial x} = W^T\frac{\partial L}{\partial h}\in\mathbb{R}^{d\times 1}, \quad\frac{\partial L}{\partial W} = \frac{\partial L}{\partial h}\otimes x^T\in{m\times d},
$$&lt;p>这里 $\frac{\partial L}{\partial h}\in\mathbb{R}^{m\times 1}$ 为损失对输出 $h$ 的梯度。因此反向传播的总计算量为&lt;/p>
$$
2dm + 2md = 4md
$$&lt;p>这里的两项分别是对 $x$ 和 $W$ 求梯度的 FLOPs.&lt;/p>
&lt;h3 id="overall">Overall
&lt;/h3>&lt;p>将前向传播和反向传播的计算量汇总我们就得到一次前向传播和一次反向传播过程中，对于长度为 $s$ 的 token, 其 FLOPs 为 (multi-head attention, SwiGLU-FFN)&lt;/p>
$$
\begin{aligned}
C &amp;= FLOPs(\text{forward}) + FLOPs(\text{backward})\\
&amp;= 3FLOPs(\mathrm{forward}) \\
&amp;\approx \boxed{3nsd^2\left(8+\frac{4s}{d}+\frac{6d_{ff}}{d}+\frac{2V}{nd}\right)}
\end{aligned}
$$&lt;h3 id="extension">Extension
&lt;/h3>&lt;h4 id="gqa">GQA
&lt;/h4>&lt;p>GQA 与 MHA 不同的地方在于通过共享 key 和 value 来降低 KV cache 的占用，我们假设 group number 为 $g$, 则 key 和 value 的 FLOPs 现在变成了&lt;/p>
$$
2sdd_h\frac{h}{g}+2sdd_h\frac{h}{g}=4sdd_h\frac{h}{g}
$$&lt;p>因此 attention 部分总的 FLOPs 变成了&lt;/p>
$$
FLOPs(\mathrm{Attention})=4sd^2+2s^2d+2s^2d+4sdd_h\frac{h}{g}=4sd^2+4s^2d+4sdd_h\frac{h}{g}
$$&lt;p>当 $g=h$ 时，GQA 就变成了 MHA, 此时的 FLOPs 也一致。&lt;/p>
&lt;h4 id="moe">MoE
&lt;/h4>&lt;p>MoE 是针对 Dense FFN 的一个改进，介绍见 &lt;a class="link" href="MoE.md" >MoE&lt;/a>, 我们假设一共有 $e$ 个路由专家，其中激活 $k$ 个。&lt;/p>
&lt;p>Gate layer 一般是一个 linear layer, 其权重矩阵大小为 $W_{G}\in\mathbb{R}^{d\times e}$, 因此 $FLOPs(\text{router})= 2sde$.&lt;/p>
&lt;p>Expert layer 和前面提到的 FFN 一致，我们每次挑选出 $k$ 个专家进行计算，因此 expert 部分 $FLOPs(\text{expert})=6ksdd_{ff}$.&lt;/p>
&lt;p>从而对于 MoE 来说，FFN 部分的 FLOPs 为&lt;/p>
$$
FLOPs(\text{MoE}) = FLOPs(\text{router})+FLOPs(\text{expert})= \boxed{2sde+6ksdd_{ff}}
$$&lt;h3 id="simplification">Simplification
&lt;/h3>&lt;p>我们已经得到了 transformer 的 FLOPs 计算表达式，但是其表达式比较繁琐，因此，在研究 scaling law 时，一般会进行简化。&lt;/p>
&lt;p>首先，在 &lt;a class="link" href="https://maosong.website/p/llm-parameter-computation/" target="_blank" rel="noopener"
>LLM parameter analysis&lt;/a> 中，我们已经给出了 LLM 参数量 $N$ （基于 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a>）的计算结果&lt;/p>
$$
N=n*(4d+3dd_{ff}+2hh_{d}d + 2h_{kv}h_dd) + d(2|V|+1)
$$&lt;p>我们这里对其进行简化，一般来说 $d_{ff}=8/3d$, $h_{kv}=h$, $h_d=d/h$, 带入就得到&lt;/p>
$$
N = n(4d+8d^2+2d^2+2d^2) + d(2|V|+1)=n(12d^2+4d)+ d(2|V|+1)
$$&lt;p>我们忽略关于 $d$ 的一阶项，并且我们假设 $|V| &amp;laquo; 12nd$, 则最终模型参数量可以近似为&lt;/p>
$$
\boxed{N \approx 12nd^2}
$$&lt;p>接下来，我们基于上面的配置简化 FLOPs 表达式&lt;/p>
$$
\begin{aligned}
C &amp;=
3nsd^2\left(8+\frac{4s}{d}+\frac{6d_{ff}}{d}+\frac{2|V|}{nd}\right) \\
&amp;= 3nsd^2\left(24+\frac{4s}{d}+\frac{2|V|}{nd}\right)\\
&amp;\approx 72nsd^2 \\
&amp;= 6sN
\end{aligned}
$$&lt;p>这里我们利用了前面的 $|V| &amp;laquo; 12nd$ 假设，为了简便我们还舍弃了 $4s/d$.&lt;/p>
&lt;p>注意到 $s$ 代表 token 序列长度，如果训练集的总 token 个数为 $D$, 则最终对于包含 $D$ tokens 的数据集和包含 $N$ 参数量的 LLM, 其训练总 FLOPs 可以近似估计为&lt;/p>
$$
\boxed{C\approx 6ND}
$$&lt;h2 id="experiments">Experiments
&lt;/h2>&lt;h3 id="setting">Setting
&lt;/h3>&lt;p>接下来我们定量分析一些模型的 FLOPs. 我们基于 &lt;a class="link" href="https://maosong.website/p/chinchilla-scaling-law/" target="_blank" rel="noopener"
>Chinchilla scaling law&lt;/a> 给出的实验配置 (Table A9), 我们筛掉 &lt;code>kv_size * n_heads != d_model&lt;/code> 的配置，$|V|=32,000$.&lt;/p>
&lt;p>各部分的 FLOPs 计算代码如下&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">compute_flops&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">V&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d_ff&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">lm_head_flops&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">V&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attention_flops&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">8&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">s&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">4&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">s&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">s&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">feed_forward_flops&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">4&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">s&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d_ff&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">lm_head_flops&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">attention_flops&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">feed_forward_flops&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>首先我们看一下不同大小模型的 FLOPs 分布情况&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/llm-flops-computation/LLM-FLOPs-flops_distribution.png"
width="1200"
height="600"
srcset="https://maosong2022.github.io/p/llm-flops-computation/LLM-FLOPs-flops_distribution_hu5685535018397506133.png 480w, https://maosong2022.github.io/p/llm-flops-computation/LLM-FLOPs-flops_distribution_hu12467466346776368488.png 1024w"
loading="lazy"
alt="FLOPs distribution against model size"
class="gallery-image"
data-flex-grow="200"
data-flex-basis="480px"
>&lt;/p>
&lt;p>可以看到，当模型越来越大，FFN 层的算力占比越来越高，这也是为什么后来采用 MoE 架构的一个原因。&lt;/p>
&lt;p>接下来，我们看一下模型 FLOPs 分布情况随上下文长度变化的情况&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/llm-flops-computation/LLM-FLOPs-flops_vs_context.png"
width="1200"
height="600"
srcset="https://maosong2022.github.io/p/llm-flops-computation/LLM-FLOPs-flops_vs_context_hu14091423642077256204.png 480w, https://maosong2022.github.io/p/llm-flops-computation/LLM-FLOPs-flops_vs_context_hu1892100057245072770.png 1024w"
loading="lazy"
alt="FLOPs distribution aginst context length"
class="gallery-image"
data-flex-grow="200"
data-flex-basis="480px"
>&lt;/p>
&lt;p>可以看到，随 context length 增加，attention 的算力占比逐渐上升，这符合 attention 是一个平方复杂度的算法的结论。并且，当上下文足够长之后，计算还出现了 overflow (图像右端的突然下降)。&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>在本文中，我们介绍了如何计算基于 decoder-only transformer LLM 的 FLOPs, 并推导除了 Kaplan scaling law 中使用的公式 $C=6ND$, 这为后面的 infra 和 scaling law 的学习提供了基础。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2203.15556" target="_blank" rel="noopener"
>Chinchilla Scaling law&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Embedding.html" target="_blank" rel="noopener"
>pytorch embedding layer&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.adamcasson.com/posts/transformer-flops" target="_blank" rel="noopener"
>transformer flops&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Keye-VL 1.5</title><link>https://maosong2022.github.io/p/notes-on-keye-vl-1.5/</link><pubDate>Thu, 11 Sep 2025 11:33:31 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-keye-vl-1.5/</guid><description>&lt;p>快手提出了 Keye-VL 1.5, 一个强调 reasoning, video understanding 的 8B 多模态大模型。作者提出了 slow-fast video encoding strategy 来提高模型的视频理解能力，作者通过在预训练和后训练提高了模型的长上下文能力和 reasoning 能力&lt;/p>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>作者回顾了多模态大模型的进展，然后提到视频理解能力仍然是一个挑战。为了解决这个问题，作者提出了 Keye-VL-1.5, 一个 8B 的视频理解多模态大模型。&lt;/p>
&lt;p>Keye-VL-1.5 主要做了三点改进：&lt;/p>
&lt;ol>
&lt;li>在架构上，使用了 Slow-Fast Video Encoding&lt;/li>
&lt;li>在预训练阶段，使用多个 stage 来提升模型的长上下文能力&lt;/li>
&lt;li>在 post-training 阶段，提高模型的 reasoning 能力和 alignment 表现&lt;/li>
&lt;/ol>
&lt;h2 id="method">Method
&lt;/h2>&lt;h3 id="architecture">Architecture
&lt;/h3>&lt;p>Keye-VL 1.5 的架构与 &lt;a class="link" href="https://maosong.website/p/notes-on-keye-vl/" target="_blank" rel="noopener"
>Keye-VL&lt;/a> 一致。&lt;/p>
&lt;p>Keye-VL 1.5 主要做出的改进点为针对视频的 encoding 方式&lt;/p>
&lt;p>作者回顾了已有 MLLM 处理视频的方式，比如 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5-vl/" target="_blank" rel="noopener"
>Qwen2.5-VL&lt;/a> 使用 3D convolution 来 merge 相邻的两帧，&lt;a class="link" href="https://maosong.website/p/notes-on-seed1.5-vl/" target="_blank" rel="noopener"
>Seed1.5-VL&lt;/a> 采用了 Dynamic Frame-Resolution Sampling 技巧，来根据 budget 和处理的任务来动态调整采样率 (frame) 和每一帧的图片精度 (resolution)。&lt;/p>
&lt;p>但是这些方法很难进行泛化。因此，作者在本文中就提出了 &lt;strong>SlowFast video encoding stratrgy&lt;/strong>:&lt;/p>
&lt;ol>
&lt;li>Slow Pathway: 空间信息丰富（high resolution），时间信息简略 (low number of frames)&lt;/li>
&lt;li>Fast Pathway: 时间信息丰富（high number of frames），空间信息简略 (low resolution)&lt;/li>
&lt;/ol>
&lt;p>为了区分 slow/fast frames, 作者提出了一个基于 patch similarity 的 metric:&lt;/p>
&lt;ol>
&lt;li>第一帧始终定义为 slow frame&lt;/li>
&lt;li>接下来的每一帧，如果其和上一帧的相似度超过 $95%$, 则定义为 fast frame; 反之则定义为 slow frame.&lt;/li>
&lt;/ol>
&lt;p>得到 slow/fast frames 之后，作者将 fast-frame 的 token budget 限制为 slow frame token budget 的 $30%$ 来平衡时间信息以及空间信息。接下来，作者使用二分搜索来决定 slow frame 的 token budget. 为了区分 slow frame 和 fast frame 的 token, 作者使用了特殊的 token 来进行分离。&lt;/p>
&lt;p>最终的处理结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-video-encoding.png"
width="1364"
height="335"
srcset="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-video-encoding_hu3751451477354070023.png 480w, https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-video-encoding_hu15836414842667305534.png 1024w"
loading="lazy"
alt="SlowFast Video Encoding"
class="gallery-image"
data-flex-grow="407"
data-flex-basis="977px"
>&lt;/p>
&lt;h3 id="pre-training">Pre-training
&lt;/h3>&lt;p>预训练的数据和 Keye-VL 基本一致，我们主要介绍改进的点&lt;/p>
&lt;p>对于 Image caption 数据，作者认为这批数据可能会损害模型的指令跟随和 reasoning 能力，因此作者对数据进行了增广，主要是调整了数据的格式：&lt;/p>
&lt;ol>
&lt;li>QA, 数据格式为 &lt;code>&amp;lt;image, caption, [eos], question, answer&amp;gt;&lt;/code>&lt;/li>
&lt;li>reverse QA, 数据格式为 &lt;code>&amp;lt;image, question, answer, [eos], caption&amp;gt;&lt;/code>&lt;/li>
&lt;li>instruction following: 随机给一批数据作为输入，然后让模型基于特定 image 输出 caption&lt;/li>
&lt;/ol>
&lt;p>作者还构建了一个 trap question 来提高模型的 robustness 以及 faithfulness.&lt;/p>
&lt;p>OCR 数据在 Keye-VL 的基础上加入了两点：&lt;/p>
&lt;ol>
&lt;li>Structured Document and Code Understanding: 基于 markdown 和 HTML 等数据来获取 code OCR 数据&lt;/li>
&lt;li>Instruction Following OCR: 基于特定指令进行 OCR&lt;/li>
&lt;/ol>
&lt;p>对于 grounding 数据，作者进一步加入了 temporal grounding 数据，作者首先使用 TEMPURA&lt;/p>
&lt;p>来将短视频分割成若干个 video clips. 然后作者使用 SOTA MLLM 来过滤数据，最后作者基于 Gemini2.5 来生成对应的 QA.&lt;/p>
&lt;p>预训练和 Keye-VL 一样，包含 3 个 stage&lt;/p>
&lt;p>前两个 stage，作者将模型的上下文限制为 8K, 使用了 DP 和 Zero-2 来减少内存开销。在 stage 3, 作者将模型的上下文从 8K 扩展到 128K, 对应的 base frequency 从 1M 提升到 8M. 训练数据包括长视频，长文本和大规模图片。作者将优化策略调整为 Zero-1, CP 和 PP 来支持 long-context 的训练。训练时数据分布为 video:images:text=24:50:26.&lt;/p>
&lt;h3 id="post-training">Post-training
&lt;/h3>&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-post-training-pipeline.png"
width="1360"
height="449"
srcset="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-post-training-pipeline_hu4088885002935100923.png 480w, https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-post-training-pipeline_hu14194521321835783259.png 1024w"
loading="lazy"
alt="Post-Training Pipeline"
class="gallery-image"
data-flex-grow="302"
data-flex-basis="726px"
>&lt;/p>
&lt;p>SFT 阶段使用了 &lt;strong>7.5M&lt;/strong> 多模态 QA 样本进行训练。&lt;/p>
&lt;p>MPO 阶段的数据相比 Keye-VL 有所减少，包含：&lt;/p>
&lt;ol>
&lt;li>250K 开源样本&lt;/li>
&lt;li>150K 纯文本数据&lt;/li>
&lt;li>26K 人类标注数据&lt;/li>
&lt;/ol>
&lt;p>对于 reward model 的训练，作者使用了 SFT 和 RL 两个阶段。SFT 阶段的数据包括 R1-Reward 和 MMPR, 训练之后作者还是用比较短的 good response 来避免产生较长的回答&lt;/p>
&lt;p>在 SFT 和 MPO 阶段之后，作者使用 LongCoT code-start 初步激活模型的 reasoning 能力。&lt;/p>
&lt;p>作者构建了一个 5 部的自动化数据生成 pipeline, 如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-LongCoT-data-generation-pipeline.png"
width="1362"
height="389"
srcset="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-LongCoT-data-generation-pipeline_hu10802193350758216375.png 480w, https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-LongCoT-data-generation-pipeline_hu15923562071597966044.png 1024w"
loading="lazy"
alt="LongCoT data generation pipeline"
class="gallery-image"
data-flex-grow="350"
data-flex-basis="840px"
>&lt;/p>
&lt;p>步骤如下：&lt;/p>
&lt;ol>
&lt;li>Multi-Source Data Collection and Enhancement：收集数据&lt;/li>
&lt;li>Multi-Path Reasoning Generation with Confidence Quantification: 基于 confidence 来挑选数据&lt;/li>
&lt;li>Comprehensive Two-Level Quality Assessment: 基于答案和过程的正确性来提高数据质量&lt;/li>
&lt;li>Human-in-the-Loop Quality Enhancement: 对于中等质量的数据请人类进一步进行标注&lt;/li>
&lt;li>Dynamic Quality Scoring and Data Utilization Strategy: 对数据进行打分，高质量数据进行上采样&lt;/li>
&lt;/ol>
&lt;p>接下来就是 General RL 过程。&lt;/p>
&lt;p>对于通用的 RLVR 训练，作者使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-gspo/" target="_blank" rel="noopener"
>GSPO&lt;/a> 算法来进行训练。&lt;/p>
&lt;p>在训练过程中，作者采取了 progressive hint sampling 方式，也就是提供不同程度的 hint 来提高模型的训练效率。作者将 hint 分为五个等级：&lt;/p>
&lt;ol>
&lt;li>Level 1 (Concept / Observation)&lt;/li>
&lt;li>Level 2 (Strategy / Method)&lt;/li>
&lt;li>Level 3 (Tools / Formula)&lt;/li>
&lt;li>Level 4 (Steps / Calculation)&lt;/li>
&lt;li>Level 5 (Complete Solution)&lt;/li>
&lt;/ol>
&lt;p>来提供不同程度的辅助，作者使用 Keye-VL 1.5 来确定最小的 hint level, 然后基于这个 level 提供信息进行 RL 的训练&lt;/p>
&lt;p>为了进一步提高模型的表现，作者采用了一个和 &lt;a class="link" href="https://maosong.website/p/notes-on-seed1.5-vl/" target="_blank" rel="noopener"
>Seed1.5-VL&lt;/a> 一样的迭代式训练策略，即反复进行 SFT 和 RL 来降低训练成本，提高训练效率。&lt;/p>
&lt;p>最后，再通用 RL 阶段之后，作者加入了 alignment RL 的训练来进行对齐，reward 包括三个方面：&lt;/p>
&lt;ol>
&lt;li>rule-based reward&lt;/li>
&lt;li>generative reward&lt;/li>
&lt;li>model-based reward&lt;/li>
&lt;/ol>
&lt;p>任务主要包括三个方面：&lt;/p>
&lt;ol>
&lt;li>instruction following&lt;/li>
&lt;li>format adherence&lt;/li>
&lt;li>preference alignment&lt;/li>
&lt;/ol>
&lt;p>数据介绍如下：&lt;/p>
&lt;ol>
&lt;li>instruction following:25 类硬约束，20 类软约束，数据包括 17K 多模态数据和 23K 纯文本数据，奖励包括 rule-based reward 和 generative reward&lt;/li>
&lt;li>reasoning: 12K 数学和逻辑推理数据&lt;/li>
&lt;li>RAG: 提高模型的搜索能力，作者使用 GSPO 算法进行训练&lt;/li>
&lt;/ol>
&lt;h2 id="experiments">Experiments
&lt;/h2>&lt;p>模型表现如下&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-Performance.png"
width="1366"
height="988"
srcset="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-Performance_hu12685109521298202886.png 480w, https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-Performance_hu1121153072311157701.png 1024w"
loading="lazy"
alt="Performance of Keye-VL 1.5"
class="gallery-image"
data-flex-grow="138"
data-flex-basis="331px"
>&lt;/p>
&lt;p>接下来作者进行了消融实验。&lt;/p>
&lt;p>首先是不同训练阶段对模型表现的影响，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-training-stage.png"
width="1369"
height="458"
srcset="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-training-stage_hu14045805214300333895.png 480w, https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-training-stage_hu10663261281529444295.png 1024w"
loading="lazy"
alt="Ablation study on training strategy"
class="gallery-image"
data-flex-grow="298"
data-flex-basis="717px"
>&lt;/p>
&lt;p>实验结果显示，提高 SFT 训练数据可以有效提高模型在数学推理，逻辑推理和 OCR 任务上的表现。MPO 可以进一步提高模型的表现，Long CoT cold start 可以有效提高模型的 reasoning 表现。&lt;/p>
&lt;p>作者还探究了 model merging 对模型表现的影响，作者首先基于 base model 和 OCR 数据训练得到 OCR expert, 然后进行 merge, 实验结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-model-merging.png"
width="1244"
height="340"
srcset="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-model-merging_hu5485713103224105634.png 480w, https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-model-merging_hu4736239008410953171.png 1024w"
loading="lazy"
alt="Ablation study on model merging"
class="gallery-image"
data-flex-grow="365"
data-flex-basis="878px"
>&lt;/p>
&lt;p>实验结果显示，model merging 可以有效提高模型在 special domain 上的表现，并且还可以维持模型的通用能力&lt;/p>
&lt;p>作者还发现：&lt;/p>
&lt;ol>
&lt;li>expert model 训练时间过长会影响最终 merge model 的表现&lt;/li>
&lt;li>expert mode 训练的学习率应该要设置比较小&lt;/li>
&lt;/ol>
&lt;p>接下来，作者探究了 alignment RL 对模型表现的影响，&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-alignment-RL.png"
width="1385"
height="275"
srcset="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-alignment-RL_hu16960946704897994390.png 480w, https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-alignment-RL_hu16484448956614414637.png 1024w"
loading="lazy"
alt="Ablation on alignment RL"
class="gallery-image"
data-flex-grow="503"
data-flex-basis="1208px"
>&lt;/p>
&lt;p>实验结果说明，alignment RL 可以在保持模型 reasoning 能力的同时提高模型的指令跟随能力&lt;/p>
&lt;p>作者还探究了 hint 对模型表现的影响，使用不同 level hint 进行训练对模型表现影响的结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-hint.png"
width="1209"
height="310"
srcset="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-hint_hu5531435168214557685.png 480w, https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-hint_hu1703494621381694086.png 1024w"
loading="lazy"
alt="Ablation on hint level"
class="gallery-image"
data-flex-grow="390"
data-flex-basis="936px"
>&lt;/p>
&lt;p>实验结果显示，使用 high level 的 hint 可以有效提高模型输出的正确率。&lt;/p>
&lt;p>最后作者探究了 rejection sampling 对模型表现的影响，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-rejection-sampling.png"
width="1384"
height="340"
srcset="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-rejection-sampling_hu12066184384347437079.png 480w, https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-rejection-sampling_hu1521405363756042548.png 1024w"
loading="lazy"
alt="Ablation on rejection sampling"
class="gallery-image"
data-flex-grow="407"
data-flex-basis="976px"
>&lt;/p>
&lt;p>结果发现，通过 rejection sampling，模型的表现有了进一步的提升。因此作者采用了 ST-RL-(RFT-SFT)-(RFT-RL) 的训练方式来进行训练&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 Keye-VL1.5, 一个强调 video understanding 和 reasoning 的 MLLM. Keye-VL 1.5 使用了 SlowFast video encoding strategy 来提高模型的效率和视频理解能力。作者详细介绍了模型的数据，训练和评估。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2509.01563" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on AdamW</title><link>https://maosong2022.github.io/p/notes-on-adamw/</link><pubDate>Thu, 04 Sep 2025 10:27:03 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-adamw/</guid><description>&lt;p>作者提出了一个针对 &lt;a class="link" href="https://maosong.website/p/notes-on-adam/" target="_blank" rel="noopener"
>Adam&lt;/a> 优化器的 weight decay 方法&lt;/p>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>作者首先回顾了动态梯度算法如 AdaGrad, RMSProp, &lt;a class="link" href="https://maosong.website/p/notes-on-adam/" target="_blank" rel="noopener"
>Adam&lt;/a> 的进展。已有工作表明动态梯度算法的泛化性要比 SGD with momentum 要差。作者在本文中探究了在 SGD 和 Adam 中使用 L2 regularization 和 weight decay 对最终模型表现的影响。结果表明，模型泛化性较差的原因在于对于 Adam, L2 regularization 的效果要比 SGD 差。&lt;/p>
&lt;p>作者有如下发现：&lt;/p>
&lt;ol>
&lt;li>L2 regularization 和 weight decay 不等价。在 SGD 中，L2 regularization 是等价的，但是在 Adam 中这个结论不成立。具体来说，L2 regularization 对历史参数的惩罚要小于 weight decay&lt;/li>
&lt;li>L2 regularization 对 Adam 效果提升有效&lt;/li>
&lt;li>weight decay 对于 SGD 和 AdamW 都很有效，在 SGD 中，weight decay 与 L2 regularization 等价&lt;/li>
&lt;li>最优的 weight decay 取决于 batch, batch 越大，最优的 weight decay 越小&lt;/li>
&lt;li>通过 learning rate scheduler 可以进一步提高 Adam 的表现&lt;/li>
&lt;/ol>
&lt;p>作者在本文中的主要贡献是通过解耦梯度更新中的 weight decay 来提高 Adam 的 regularization.&lt;/p>
&lt;p>作者的主要 motivation 是提升 Adam 表现，让其可以和 SGD with momentum 相比&lt;/p>
&lt;h2 id="method">Method
&lt;/h2>&lt;p>Weight decay 的定义如下&lt;/p>
$$
\theta_{t+1} = (1-\lambda)\theta_t - \alpha \nabla f_t(\theta_t) \tag{1}
$$&lt;p>其中 $\lambda$ 是 weight decay rate, $\nabla f_t(\theta_t)$ 是第 $t$ 个 batch 的梯度，$\alpha$ 是学习率。&lt;/p>
&lt;p>首先，对于标准的 SGD 来说，weight decay 与 L2 regularization 等价&lt;/p>
&lt;blockquote>
&lt;p>Proposition 1
对于标准的 SGD 来说，对损失函数 $f_t(\theta_t)$ 执行 weight decay （公式 $(1)$）与对损失函数 $f_t(\theta_t)+\lambda&amp;rsquo;/2|\theta_t|_2^2$ 执行梯度下降算法是等价的，这里 $\lambda&amp;rsquo;=\lambda/\alpha$。&lt;/p>
&lt;/blockquote>
&lt;p>证明比较简单，只需要写出损失函数的梯度下降更新公式即可。&lt;/p>
&lt;p>基于这个结论，大部分优化算法都将 L2 regularization 和 weight decay 看做是等价的。但实际上，这个结论对于 adaptive gradient 方法来说是不成立的。结论如下&lt;/p>
&lt;blockquote>
&lt;p>Proposition 2
令 $O$ 为一个 optimizer, 其目标函数为 $f_t(\theta)$, 当不考虑 weight decay 时，梯度更新过程为 $\theta_{t+1}\gets \theta_t-\alpha M_t\nabla f_t(\theta_t)$. 当考虑 weight decay 时，梯度更新过程为 $\theta_{t+1}\gets (1-\lambda)\theta_t-\alpha M_t\nabla f_t(\theta_t)$. 如果 $M_t\neq kI$, 则不存在 $\lambda&amp;rsquo;$, 使得 $O$ 在优化目标函数 $f_t^{reg}(\theta)=f_t(\theta)+\lambda&amp;rsquo;/2|\theta|_2^2$ 时，不考虑 weight decay 的梯度更新与 $O$ 在优化目标函数 $f_t(\theta)$ 时，考虑 weight decay 的梯度更新等价。&lt;/p>
&lt;/blockquote>
&lt;p>证明比较简单，只需要写出两个目标函数对应的梯度更新公式即可。&lt;/p>
&lt;p>作者通过分析发现，在 adaptive gradient 方法中，对于 L2 regularization，梯度和 regularization 是打包在一起考虑的。而 weight decay 是分开考虑的。这就导致了对于梯度比较大的权重，L2 regularization 的学习率较小，从而 regularization 效应减弱。而 weight decay 中，这种效应则不存在。因此 weight decay 的 regularization 效应更强。&lt;/p>
&lt;p>作者通过这个分析，给出了一个 weight decay 与 L2 regularization 相等的条件&lt;/p>
&lt;blockquote>
&lt;p>Proposition 3
令 $O$ 为一个 optimizer, 其目标函数为 $f_t(\theta)$, 当不考虑 weight decay 时，梯度更新过程为 $\theta_{t+1}\gets \theta_t-\alpha M_t\nabla f_t(\theta_t)$. 当考虑 weight decay 时，梯度更新过程为 $\theta_{t+1}\gets (1-\lambda)\theta_t-\alpha M_t\nabla f_t(\theta_t)$. 如果 $M_t= \mathrm{diag}(s)^{-1}$ ($s_i&amp;gt;0,\forall i$), 则 $O$ 在优化目标函数&lt;/p>
$$
> f_t^{reg}(\theta)=f_t(\theta)+\frac{\lambda'}{2\alpha}\|\theta\odot \sqrt{s}\|_2^2
> $$&lt;p>时，不考虑 weight decay 的梯度更新与 $O$ 在优化目标函数 $f_t(\theta)$ 时，考虑 weight decay 的梯度更新等价。&lt;/p>
&lt;/blockquote>
&lt;p>上面的结论显示，对于比较大的 preconditioner $s_i$, 其在相比于 L2 regularization 被 regularized 的效应更强。&lt;/p>
&lt;p>为了解耦这两个参数，作者提出了 SGDW 算法，其 weight decay 和梯度更新同时进行，算法如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-adamw/AdamW-SGDW-algorithm.png"
width="1163"
height="514"
srcset="https://maosong2022.github.io/p/notes-on-adamw/AdamW-SGDW-algorithm_hu1610815020791727695.png 480w, https://maosong2022.github.io/p/notes-on-adamw/AdamW-SGDW-algorithm_hu16327860896185970169.png 1024w"
loading="lazy"
alt="SGDW algorithm"
class="gallery-image"
data-flex-grow="226"
data-flex-basis="543px"
>&lt;/p>
&lt;p>在算法中，为了支持同时给 $\alpha$ 和 $\lambda$ 做 scheduling, 作者提出了一个 scaling factor $\eta_t$, $\eta_t$ 由用户定义的 scheduler &lt;code>SetScheduleMultiplier(t)&lt;/code> 决定。此时，针对 SGD with momentum 的 weight decay 与 L2 regularization 是等价的&lt;/p>
&lt;p>同理，我们也可以对 Adam 算法实行同样的操作，算法如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-adamw/AdamW-AdamW-algorithm.png"
width="1156"
height="560"
srcset="https://maosong2022.github.io/p/notes-on-adamw/AdamW-AdamW-algorithm_hu2828248833593717468.png 480w, https://maosong2022.github.io/p/notes-on-adamw/AdamW-AdamW-algorithm_hu17928827955348721462.png 1024w"
loading="lazy"
alt="AdamW algorithm"
class="gallery-image"
data-flex-grow="206"
data-flex-basis="495px"
>&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者在本文中分析了 adaptive gradient 方法中 L2 regularization 与 weight decay 的不一致性。基于分析，作者提出了 SGDW 和 AdamW 两个优化算法。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/1711.05101" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Adam</title><link>https://maosong2022.github.io/p/notes-on-adam/</link><pubDate>Thu, 04 Sep 2025 10:11:55 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-adam/</guid><description>&lt;p>作者提出了 Adam, 一个一阶的优化方法，Adam 更加高效，且具有 scaling invariant 的性质。&lt;/p>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>作者首先回顾了一下已有优化器的进展，其中主要是 SGD. 在本文中，作者提出了 Adam, 一个针对高维参数空间的一阶优化器，Adam 基于 gradient 的一阶和二阶信息为不同的参数安排不同的学习率。Adam 的来源是 &lt;em>adaptive moment estimation&lt;/em>. Adam 主要是结合了 AdaGrad 和 RMSProp 两个算法的优点。&lt;/p>
&lt;p>Adam 与 RMSProp 的区别在于：&lt;/p>
&lt;ol>
&lt;li>RMSProp 在 rescaled gradient 上进行 momentum 的计算然后更新，而 Adam 直接使用一阶和二阶矩来进行估计&lt;/li>
&lt;li>RMSProp 没有 bias-correction 项&lt;/li>
&lt;/ol>
&lt;p>Adam 的主要优势为：&lt;/p>
&lt;ol>
&lt;li>参数更新的量级与 gradient 的 scaling 无关&lt;/li>
&lt;li>步长被 stepsize 超参数限制&lt;/li>
&lt;li>不要求目标函数 stationary&lt;/li>
&lt;li>对于稀疏梯度 work 的比较好&lt;/li>
&lt;li>优化器自带 annealing&lt;/li>
&lt;/ol>
&lt;h2 id="algorithm">Algorithm
&lt;/h2>&lt;p>Adam 的算法如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-adam/Adam-optimizer-algorithm.png"
width="1163"
height="725"
srcset="https://maosong2022.github.io/p/notes-on-adam/Adam-optimizer-algorithm_hu11795896431055486830.png 480w, https://maosong2022.github.io/p/notes-on-adam/Adam-optimizer-algorithm_hu16982606774958661824.png 1024w"
loading="lazy"
alt="Adam Algorithm"
class="gallery-image"
data-flex-grow="160"
data-flex-basis="384px"
>&lt;/p>
&lt;p>我们优化的目标函数如下&lt;/p>
$$
\min_{\theta}\quad f(\theta)
$$&lt;p>这里 $f$ 一般是一个神经网络。我们记 $f(\theta)$ 在 $\theta_t$ 处的梯度为 $g_t=\nabla_{\theta}f(\theta_t)$.&lt;/p>
&lt;p>算法运行时，会更新梯度 $m_t$ 以及梯度二阶矩 $v_t$ 的 exponential moving average. 超参数 $\beta_1,\beta_2$ 负责控制 exponential decay rates. 这里 $m_t$ 和 $v_t$ 分别是一阶动量（均值）和二阶动量（未中心化的 variance）的估计。由于 $m_t$ 和 $v_t$ 的初始化都是 0, 因此他们会引入 bias, 作者在后续通过修正解决了这个问题。&lt;/p>
&lt;p>假设 $\epsilon=0$, 如果除了当前时刻 $t$ 之外，之前所有时刻的梯度 $g_i=0,i&amp;lt;t$, 此时&lt;/p>
$$
m_t = (1-\beta_t)g_t, v_t=(1-\beta_2)g_t^2
$$&lt;p>修正后的一阶和二阶矩分别为&lt;/p>
$$
\Delta_t = \alpha \frac{(1-\beta_1)\sqrt{1-\beta_2^t}}{(1-\beta_1^t)\sqrt{1-\beta_2}}
$$&lt;p>当 $t$ 足够大的时候， $\beta_1^t\to0, \beta_2^t\to0$, 此时&lt;/p>
$$
\Delta_t = \alpha \frac{1-\beta_1}{\sqrt{1-\beta_2}}
$$&lt;p>如果之前所有时刻的梯度不全为 0, 则依据 Cauchy-Schwarz 不等式，我们有 $(\mathbb{E}[XY])^2\leq \mathbb{E}[X^2]\mathbb{E}[Y^2]$. 令 $X=1$, $Y=g$, 则我们有&lt;/p>
$$
(\mathbb{E}[g])^2\leq \mathbb{E}[g^2] \Rightarrow \frac{|\mathbb{E}[g]|}{\sqrt{\mathbb{E}[g^2]}}\leq 1
$$&lt;p>此时，我们有&lt;/p>
$$
\mathbb{E}[g_t] = \hat{m}_t, \mathbb{E}[g_t^2] = \hat{v}_t
$$&lt;p>因此，&lt;/p>
$$
|\Delta_t| = \left|\alpha\frac{\hat{m}_t}{\sqrt{\hat{v}_t}}\right|=\alpha \left|\frac{|\mathbb{E}[g]|}{\sqrt{\mathbb{E}[g^2]}}\right|\leq\alpha
$$&lt;p>从而我们有&lt;/p>
$$
|\Delta_t| \leq\begin{cases}
\alpha \frac{1-\beta_1}{\sqrt{1-\beta_2}} &amp; \text{ if }1-\beta_1>\sqrt{1-\beta_2}\\
\alpha &amp;\text{ otherwise}
\end{cases}
$$&lt;p>实际上，$\Delta_t$ 可以理解为一个 trust region, 可以用来保证当前更新的参数不会离原始参数太远。&lt;/p>
&lt;p>作者定义 signal-noise ratio (SNR) 为&lt;/p>
$$
SNR = \frac{\hat{m}_t}{\sqrt{\hat{v}_t}}
$$&lt;p>当 SNR 较小时，说明此时的不确定性比较大，因此 $\Delta_t$ 也比较小。这就避免了模型朝错误的方向更新。也就是&lt;em>automatic annealing&lt;/em>.&lt;/p>
&lt;p>$\Delta_t$ 还对 gradient 的 scaling 有不变的性质，这是因为，&lt;/p>
$$
\frac{c\cdot\hat{m}_t}{\sqrt{c^2\cdot\hat{v}_t}} = \frac{\hat{m}_t}{\sqrt{\hat{v}_t}}
$$&lt;h2 id="bias-correction">Bias Correction
&lt;/h2>&lt;p>上一节提到，Adam 算法的初始化是存在 bias 的，作者在本届就对齐进行了分析。令 $g$ 为目标函数 $f$ 的梯度，我们希望估计其二阶动量的期望.令 $g_1,\dots,g_T$ 分别为 $\theta_1,\dots,\theta_T$ 处的梯度估计，其中 $g_t\sim p(g_t)$ 是对应时刻梯度的分布。令 $v_0=0$, 在 $t$ 时刻，我们有&lt;/p>
$$
v_t = \beta_2 v_{t-1} + (1-\beta_2)g_t^2 = (1-\beta_2)\sum_{i=1}^t\beta_2^{t-i} g_i^2
$$&lt;p>我们希望计算 $\mathbb{E}[v_t]$ 与 $\mathbb{E}[g_t^2]$ 之间的关系，我们有&lt;/p>
$$
\begin{aligned}
\mathbb{E}[v_t] &amp;= \left[(1-\beta_2)\sum_{i=1}^t\beta_2^{t-i} g_i^2\right]\\
&amp;= \mathbb{E}[g_t^2]\cdot (1-\beta_2)\sum_{i=1}^t\beta_2^{t-i}+\zeta\\
&amp;= \mathbb{E}[g_t^2](1-\beta_2^t)+\zeta
\end{aligned}
$$&lt;p>其中当 $\mathbb{E}[g_i^2]$ 为 stationary 时，$\zeta=0$, 否则我们可以通过控制 $\beta_2$ 来让 past gradient 保持在一个较小的规模。最后，我们剩下的就是 $1-\beta_2^t$, 这也是我们在算法中进行修正的地方。&lt;/p>
&lt;p>对于一阶动量 $m_t$ 的修正也是同理。&lt;/p>
&lt;h2 id="convergence-analysis">Convergence Analysis
&lt;/h2>&lt;p>作者在本节中使用了 online learning framework 来分写 Adam 的收敛性。给定一系列 convex cost function $f_1(\theta),\dots,f_T(\theta)$. 在 $t$ 时刻，我们的目标是基于上一个 cost function $f_t(\theta)$ 来预测 $\theta_t$.&lt;/p>
&lt;p>作者在这里使用 regret 来分析，记 $f_t(\theta^*)$ 为 $t$ 时刻最优的参数对应的 cost function, regret 定义为&lt;/p>
$$
R(T) = \sum_{t=1}^T [f_t(\theta_t) - f_t(\theta^*)]
$$&lt;p>其中，&lt;/p>
$$
\theta^* = \arg\min_{\theta\in\mathcal{X}}\sum_{t=1}^Tf_t(\theta)
$$&lt;p>则我们有如下的结论&lt;/p>
&lt;blockquote>
&lt;p>Theorem 1
假设&lt;/p>
&lt;ol>
&lt;li>函数 $f_t$ 的梯度是有界的，即 $|\nabla f_t(\theta)|&lt;em>2\leq G$, $|\nabla f_t(\theta)|&lt;/em>{\infty}\leq G_{\infty}$ 对任意 $\theta\in\mathbb{R}^d$ 都成立&lt;/li>
&lt;li>${\theta_1,\dots,\theta_T}$ 中任意两个参数的距离都是有界的，即 $|\theta_m-\theta_m|&lt;em>2\leq D$, $|\theta_m-\theta_n|&lt;/em>{\infty}\leq D_{\infty}$ 对任意 $m,n\in{1,\dots,T}$ 都成立&lt;/li>
&lt;li>$\beta_1,\beta_2\in[0,1)$ 满足 $\frac{\beta_1^2}{\sqrt{\beta_2}}&amp;lt;1$
令 $\alpha_t=\alpha/\sqrt{t}$, $\beta_{1,t}=\beta_1\lambda^{t-1}$, $\lambda\in(0,1)$, 则我们有&lt;/li>
&lt;/ol>
$$
> R(T)\leq \frac{D^2}{2\alpha(1-\beta_1)}\sum_{i=1}^d\sqrt{T\hat{v}_{T,i}}+\frac{\alpha(1+\beta_1)G_{\infty}}{(1-\beta_1)\sqrt{1-\beta_2}(1-\gamma)^2}\sum_{i=1}^d\|g_{1:T,i}\|_2+\sum_{i=1}^d\frac{D_{\infty}^2G_{\infty}\sqrt{1-\beta_2}}{2\alpha(1-\beta_1)(1-\lambda)^2}
> $$&lt;/blockquote>
&lt;p>结果说明，当我们的 data feature 稀疏且梯度有界时我们有&lt;/p>
$$
\sum_{i=1}^d\|g_{1:T,i}\|_2&lt;&lt; dG_{\infty}\sqrt{T}
$$&lt;p>以及&lt;/p>
$$
\sum_{i=1}^d\sqrt{T\hat{v}_{T,i}}&lt;&lt; dG_{\infty}\sqrt{T}
$$&lt;p>实际上，对于 Adam 以及 Adamgrad，这个上界可以优化到 $O(\log d\sqrt{T})$.&lt;/p>
&lt;p>最终，我们可以证明 Adam 的收敛性&lt;/p>
&lt;blockquote>
&lt;p>Corollary 1
假设&lt;/p>
&lt;ol>
&lt;li>函数 $f_t$ 的梯度是有界的，即 $|\nabla f_t(\theta)|&lt;em>2\leq G$, $|\nabla f_t(\theta)|&lt;/em>{\infty}\leq G_{\infty}$ 对任意 $\theta\in\mathbb{R}^d$ 都成立&lt;/li>
&lt;li>${\theta_1,\dots,\theta_T}$ 中任意两个参数的距离都是有界的，即 $|\theta_m-\theta_m|&lt;em>2\leq D$, $|\theta_m-\theta_n|&lt;/em>{\infty}\leq D_{\infty}$ 对任意 $m,n\in{1,\dots,T}$ 都成立
则对 $T\geq1$, 我们有&lt;/li>
&lt;/ol>
$$
> \frac{R(T)}{T}=O\left(\frac{1}{\sqrt{T}}\right)
> $$&lt;/blockquote>
&lt;h2 id="experiment">Experiment
&lt;/h2>&lt;p>作者在 logistic regression, MLP, CNN 等三种模型架构上进行了实验。&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 Adam optimizer， 一个基于 AdaGrad 和 RMSProp 优点的优化器，作者通过理论验证了 Adam 的收敛性，然后通过实验验证了 Adam 的有效性。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/1412.6980" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on RNoPE-SWA</title><link>https://maosong2022.github.io/p/notes-on-rnope-swa/</link><pubDate>Tue, 02 Sep 2025 11:24:10 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-rnope-swa/</guid><description>&lt;p>作者系统性分析了已有的 attention 机制，然后作者提出了混合的 attention 机制，来提高模型在长上下文的表现以及维持模型在短上下文场景下的表现。&lt;/p>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>作者首先强调了提升 LLM 上下文长度面临的问题：&lt;/p>
&lt;ol>
&lt;li>如何有效处理长上下文输入&lt;/li>
&lt;li>如何训练长上下文 LLM&lt;/li>
&lt;li>如何降低长上下文 LLM 在 inference 时的 latency 以及 memory usage&lt;/li>
&lt;/ol>
&lt;p>对于建模长上下文输入，我们可以从 attention 机制或者位置编码来入手。前者类似的工作有 Landmark Attention 和 Focused Transformer, 但是这些方法的问题在于训练不稳定。 &lt;a class="link" href="https://maosong.website/p/notes-on-qk-norm/" target="_blank" rel="noopener"
>QK norm&lt;/a> 可以比较好解决 softmax 分布过于极端的问题，但是其问题在于训练时的数值不稳定性，并且可能会影响模型的长上下文能力。&lt;/p>
&lt;p>另一方面，对于位置编码，已有的工作如 APE, AliBi, &lt;a class="link" href="https://maosong.website/p/notes-on-position-encoding/" target="_blank" rel="noopener"
>RoPE&lt;/a> 等都可以提供位置信息。但是，这些方法很有可能回影响模型最终的 attention score 分布。另外，NoPE 探究了移除 position encoding 的可能性。&lt;/p>
&lt;p>还有一些工作目的是降低 softmax attention 的时间复杂度和空间复杂度。比如 sliding window attention, sparse attention, &lt;a class="link" href="https://maosong.website/p/notes-on-streamingllm/" target="_blank" rel="noopener"
>attention sink&lt;/a> 等都可以降低整体的时间/空间复杂度。但是这些方法最终的表现都有所下降。&lt;/p>
&lt;h2 id="observation">Observation
&lt;/h2>&lt;p>作者首先对比了以下不同方法对模型长上下文能力的影响。&lt;/p>
&lt;p>作者训练了一个 8B 的模型，然后分别对比了三种方法：&lt;/p>
&lt;ol>
&lt;li>RoPE: base frequency 设置为 10,000, SFT 阶段扩展到 2M&lt;/li>
&lt;li>QK-Norm: 在 RoPE 的基础上，对 query 和 key 先进行 normalization 再进行 RoPE&lt;/li>
&lt;li>NoPE: 移除 attention 中的位置编码信息&lt;/li>
&lt;/ol>
&lt;p>作者分别评估了三种方法的表现，实验结果如下表&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Val Loss&lt;/th>
&lt;th>MMLU&lt;/th>
&lt;th>HellaSwag&lt;/th>
&lt;th>CommonsenseQA&lt;/th>
&lt;th>ARC-E&lt;/th>
&lt;th>ARC-C&lt;/th>
&lt;th>Needles 65k&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>RoPE&lt;/td>
&lt;td>1.52&lt;/td>
&lt;td>48.55&lt;/td>
&lt;td>73.74&lt;/td>
&lt;td>68.30&lt;/td>
&lt;td>81.05&lt;/td>
&lt;td>39.13&lt;/td>
&lt;td>9.82&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>QK-Norm&lt;/td>
&lt;td>1.53&lt;/td>
&lt;td>48.21&lt;/td>
&lt;td>73.68&lt;/td>
&lt;td>68.23&lt;/td>
&lt;td>80.54&lt;/td>
&lt;td>38.98&lt;/td>
&lt;td>7.93&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>NoPE&lt;/td>
&lt;td>1.58&lt;/td>
&lt;td>47.61&lt;/td>
&lt;td>72.16&lt;/td>
&lt;td>66.42&lt;/td>
&lt;td>76.94&lt;/td>
&lt;td>37.12&lt;/td>
&lt;td>9.03&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>实验结果显示，对于通用任务，RoPE 和 QK-Norm 的表现差不多，而 NoPE 的表现较差。对于长上下文任务，QK-Norm 的表现最差。&lt;/p>
&lt;p>接下来，作者分析了三种方法的 attention 分布情况。作者将上下文分为四个 segments：&lt;/p>
&lt;ul>
&lt;li>begin: 开始的 10 个 token&lt;/li>
&lt;li>needle: 与 needle 相关的 tokens&lt;/li>
&lt;li>context: 通用的上下文 token&lt;/li>
&lt;li>qc: question/completion token, 语文题答案相关的 token&lt;/li>
&lt;/ul>
&lt;p>作者将 needle 放置在 50% 深度的位置。评测的实验结果如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Context Length&lt;/th>
&lt;th>Model Variants&lt;/th>
&lt;th>begin&lt;/th>
&lt;th>needle&lt;/th>
&lt;th>context&lt;/th>
&lt;th>qc&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>8k&lt;/td>
&lt;td>RoPE&lt;/td>
&lt;td>0.3863&lt;/td>
&lt;td>0.0328&lt;/td>
&lt;td>0.3809&lt;/td>
&lt;td>0.2000&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>QK-Norm&lt;/td>
&lt;td>0.0242&lt;/td>
&lt;td>0.0173&lt;/td>
&lt;td>0.8020&lt;/td>
&lt;td>0.1565&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>NoPE&lt;/td>
&lt;td>0.3058&lt;/td>
&lt;td>0.0454&lt;/td>
&lt;td>0.4501&lt;/td>
&lt;td>0.1987&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>32k&lt;/td>
&lt;td>RoPE&lt;/td>
&lt;td>0.3541&lt;/td>
&lt;td>0.0201&lt;/td>
&lt;td>0.4343&lt;/td>
&lt;td>0.1915&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>QK-Norm&lt;/td>
&lt;td>0.0064&lt;/td>
&lt;td>0.0056&lt;/td>
&lt;td>0.8517&lt;/td>
&lt;td>0.1364&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>NoPE&lt;/td>
&lt;td>0.2807&lt;/td>
&lt;td>0.0325&lt;/td>
&lt;td>0.4981&lt;/td>
&lt;td>0.1886&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>128k&lt;/td>
&lt;td>RoPE&lt;/td>
&lt;td>0.3463&lt;/td>
&lt;td>0.0010&lt;/td>
&lt;td>0.4751&lt;/td>
&lt;td>0.1776&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>QK-Norm&lt;/td>
&lt;td>0.0010&lt;/td>
&lt;td>0.0004&lt;/td>
&lt;td>0.8993&lt;/td>
&lt;td>0.0994&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>NoPE&lt;/td>
&lt;td>0.0846&lt;/td>
&lt;td>0.0073&lt;/td>
&lt;td>0.8156&lt;/td>
&lt;td>0.0925&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>实验结果显示：&lt;/p>
&lt;ol>
&lt;li>NoPE 是最关注 needle token 信息的，RoPE 次之，QK-Norm 最差&lt;/li>
&lt;li>QK-Norm 更关注上下文信息，对其他的信息关注度较少&lt;/li>
&lt;/ol>
&lt;p>作者发现 QK-Norm 对初始的 token 信息关注度较少，作者认为这是因为 normalization 会让模型更关注邻近的 token 信息。为了验证这个观点，作者在不同的上下文长度下，分别计算了不同方法的 attention 分布情况，为了避免噪声，作者将开始的 10 个 token 以及最后的 $3%$ token 排除在外，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-rnope-swa/RNoPE-SWA-8k-attention.png"
width="700"
height="496"
srcset="https://maosong2022.github.io/p/notes-on-rnope-swa/RNoPE-SWA-8k-attention_hu7284298001029033696.png 480w, https://maosong2022.github.io/p/notes-on-rnope-swa/RNoPE-SWA-8k-attention_hu14355922510337803786.png 1024w"
loading="lazy"
alt="Attention distribution on 8K context length"
class="gallery-image"
data-flex-grow="141"
data-flex-basis="338px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-rnope-swa/RNoPE-SWA-32k-attention.png"
width="692"
height="482"
srcset="https://maosong2022.github.io/p/notes-on-rnope-swa/RNoPE-SWA-32k-attention_hu3669404194925672875.png 480w, https://maosong2022.github.io/p/notes-on-rnope-swa/RNoPE-SWA-32k-attention_hu16261772671243833744.png 1024w"
loading="lazy"
alt="Attention distribution on 32K context length"
class="gallery-image"
data-flex-grow="143"
data-flex-basis="344px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-rnope-swa/RNoPE-SWA-128k-attention.png"
width="693"
height="564"
srcset="https://maosong2022.github.io/p/notes-on-rnope-swa/RNoPE-SWA-128k-attention_hu2695619081909157132.png 480w, https://maosong2022.github.io/p/notes-on-rnope-swa/RNoPE-SWA-128k-attention_hu2021364689251498381.png 1024w"
loading="lazy"
alt="Attention distribution on 128K context length"
class="gallery-image"
data-flex-grow="122"
data-flex-basis="294px"
>&lt;/p>
&lt;p>实验结果说明，相比于 softmax attention, QK-Norm 的 attention score 分布更平滑，其对于 need token 的注意力不如 RoPE, 这也说明了为什么 QK-Norm 的长上下文能力比较差。并且，QK-Norm 的 recency bias 更严重。&lt;/p>
&lt;p>作者通过计算 RoPE 和 QK-Norm 的 attention distribution 的 entropy 来进一步说明这一点，结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>8k&lt;/th>
&lt;th>32k&lt;/th>
&lt;th>128k&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>RoPE&lt;/td>
&lt;td>6.02&lt;/td>
&lt;td>6.95&lt;/td>
&lt;td>7.62&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>QK-Norm&lt;/td>
&lt;td>10.71&lt;/td>
&lt;td>12.46&lt;/td>
&lt;td>14.14&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>结果显示 QK-Norm 的熵更高，这意味着 QK-Norm 的 attention score 分布更分散，也就证明了 Qk-Norm retrieval 能力比较差。&lt;/p>
&lt;h2 id="method">Method
&lt;/h2>&lt;p>考虑到 NoPE 和 RopE 各自的优点，作者提出了一个混合架构，来结合 NoPE 与 RoPE. 具体做法就是 NoPE layer 和 RoPE layer 交替进行。作者将这个模型架构记为 RNoPE.&lt;/p>
&lt;p>RNoPE 不同 layer 与不同 base frequency 产生的 attention score 分布如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>NoPE Layers - begin&lt;/th>
&lt;th>NoPE Layers - needle&lt;/th>
&lt;th>NoPE Layers - context&lt;/th>
&lt;th>NoPE Layers - qc&lt;/th>
&lt;th>RoPE Layers - begin&lt;/th>
&lt;th>RoPE Layers - needle&lt;/th>
&lt;th>RoPE Layers - context&lt;/th>
&lt;th>RoPE Layers - qc&lt;/th>
&lt;th>needles-128k&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>RoPE&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>0.3541&lt;/td>
&lt;td>0.0201&lt;/td>
&lt;td>0.4343&lt;/td>
&lt;td>0.1915&lt;/td>
&lt;td>7.395&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RNoPE-10k&lt;/td>
&lt;td>0.3275&lt;/td>
&lt;td>0.0765&lt;/td>
&lt;td>0.5672&lt;/td>
&lt;td>0.0287&lt;/td>
&lt;td>0.0049&lt;/td>
&lt;td>0.0004&lt;/td>
&lt;td>0.6805&lt;/td>
&lt;td>0.3142&lt;/td>
&lt;td>8.036&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RNoPE-100k&lt;/td>
&lt;td>0.3263&lt;/td>
&lt;td>0.0778&lt;/td>
&lt;td>0.5633&lt;/td>
&lt;td>0.0327&lt;/td>
&lt;td>0.0241&lt;/td>
&lt;td>0.0005&lt;/td>
&lt;td>0.6782&lt;/td>
&lt;td>0.2972&lt;/td>
&lt;td>7.461&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RNoPE-2M&lt;/td>
&lt;td>0.3250&lt;/td>
&lt;td>0.0712&lt;/td>
&lt;td>0.5735&lt;/td>
&lt;td>0.0303&lt;/td>
&lt;td>0.1111&lt;/td>
&lt;td>0.0046&lt;/td>
&lt;td>0.6233&lt;/td>
&lt;td>0.2611&lt;/td>
&lt;td>7.022&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RNoPE-4M&lt;/td>
&lt;td>0.3486&lt;/td>
&lt;td>0.0369&lt;/td>
&lt;td>0.5981&lt;/td>
&lt;td>0.0165&lt;/td>
&lt;td>0.0960&lt;/td>
&lt;td>0.0039&lt;/td>
&lt;td>0.6774&lt;/td>
&lt;td>0.2227&lt;/td>
&lt;td>6.203&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RNoPE-10k-swa&lt;/td>
&lt;td>0.3303&lt;/td>
&lt;td>0.0742&lt;/td>
&lt;td>0.5634&lt;/td>
&lt;td>0.0321&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>9.562&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>实验结果显示，在 RNoPE 架构中，&lt;/p>
&lt;ol>
&lt;li>提升 base frequency 带来的增益逐渐递减&lt;/li>
&lt;li>NoPE layer 的 retrieval 能力比较强，表现在 attention sink 现象以及对于 needle token 的 spike. 但是其 recency bias 比较弱&lt;/li>
&lt;li>RoPE 的 retrieval 能力比较弱，但是其 attention sink 现象比较小&lt;/li>
&lt;li>当 base frequency 增加止呕，RoPE 的 recency bias 会降低，表现在 qc token 的权重降低&lt;/li>
&lt;/ol>
&lt;p>作者发现，RoPE 的 receptive field 会影响 NoPE layer 的 retrieval 能力。作者总结得到两个 insight&lt;/p>
&lt;ol>
&lt;li>NoPE layer 对于 retrieval 任务比较擅长，而 RoPE layer 对于处理 local Information 比较擅长&lt;/li>
&lt;li>限制 RoPE 的 receptive field 可以保证 NoPE layer 的 retrieval 能力&lt;/li>
&lt;/ol>
&lt;p>基于这两个 insight, 作者构建了 RNoPE-SWA 架构，RNoPE-SWA 相比于 RNoPE, 将 full attention 变成了 sliding window attention, 来避免 RoPE layer 对下游的 NoPE layer 产生影响。&lt;/p>
&lt;p>最终，作者基于 Command R+ 构建了模型，作者去除了 QK-Norm, 对于 NoPE layer, 作者使用了 full attention, 对于 RoPE layer, 作者使用了 window size 为 4096 的 sliding window attention. 作者通过实验验证了 NoPE layer 和 RoPElayer 的比例，实验结果发现 $1:3$ 的比例是最优的。最终每 4 个 layer 为一组，前三组为 SWA, 最后一层为 full attention.&lt;/p>
&lt;p>最终，在通用任务上评测的结果如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>MMLU&lt;/th>
&lt;th>HellaSwag&lt;/th>
&lt;th>ARC-E&lt;/th>
&lt;th>ARC-C&lt;/th>
&lt;th>SATEn&lt;/th>
&lt;th>SATMath&lt;/th>
&lt;th>GSM8K&lt;/th>
&lt;th>Winogrande&lt;/th>
&lt;th>MBPP&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Baseline&lt;/td>
&lt;td>57.5&lt;/td>
&lt;td>75.8&lt;/td>
&lt;td>84.6&lt;/td>
&lt;td>48.5&lt;/td>
&lt;td>70.0&lt;/td>
&lt;td>30.9&lt;/td>
&lt;td>40.9&lt;/td>
&lt;td>68.5&lt;/td>
&lt;td>39.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RNope-SWA&lt;/td>
&lt;td>59.5&lt;/td>
&lt;td>76.2&lt;/td>
&lt;td>82.5&lt;/td>
&lt;td>48.8&lt;/td>
&lt;td>71.9&lt;/td>
&lt;td>30.5&lt;/td>
&lt;td>42.7&lt;/td>
&lt;td>69.5&lt;/td>
&lt;td>39.3&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>在 Ruler retrieval 上评测的结果如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>8k&lt;/th>
&lt;th>16k&lt;/th>
&lt;th>32k&lt;/th>
&lt;th>64k&lt;/th>
&lt;th>128k&lt;/th>
&lt;th>256k&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Baseline&lt;/td>
&lt;td>96.6&lt;/td>
&lt;td>94.4&lt;/td>
&lt;td>95.1&lt;/td>
&lt;td>89.1&lt;/td>
&lt;td>83.0&lt;/td>
&lt;td>57.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RNope-SWA&lt;/td>
&lt;td>96.1&lt;/td>
&lt;td>96.1&lt;/td>
&lt;td>94.9&lt;/td>
&lt;td>92.0&lt;/td>
&lt;td>90.0&lt;/td>
&lt;td>74.8&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>在 Ruler QA 上评测的结果如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>8k&lt;/th>
&lt;th>16k&lt;/th>
&lt;th>32k&lt;/th>
&lt;th>64k&lt;/th>
&lt;th>128k&lt;/th>
&lt;th>256k&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Baseline&lt;/td>
&lt;td>53.5&lt;/td>
&lt;td>50.0&lt;/td>
&lt;td>52.5&lt;/td>
&lt;td>45.5&lt;/td>
&lt;td>36.0&lt;/td>
&lt;td>30.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RNope-SWA&lt;/td>
&lt;td>55.5&lt;/td>
&lt;td>52.5&lt;/td>
&lt;td>55.5&lt;/td>
&lt;td>49.0&lt;/td>
&lt;td>46.0&lt;/td>
&lt;td>42.5&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>实验结果说明，模型在通用任务任务上与 baseline 模型表现差不多，且长上下文能力更强&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者提出了 RNope-SWA, 一个混合 NoPE, RoPE position embedding 和 sliding window attention 的 attention 机制。RNope-SWA 可以在保持模型表现的同时提高计算效率和降低 KV cache 占用。&lt;/p>
&lt;p>尽管本文和已有的工作如 YoCo, Jamba-1.5 和 MiniMax-01 均证明了混合 attention 架构的有效性。但是，目前对于其工作机制尚缺乏一个比较系统性的理解。作者认为这是一个需要探究的方向。另外，作者还认为如何更好的汇总上下文信息与位置信息也是一个可以探究的方向。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2501.18795" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on InternVL3.5</title><link>https://maosong2022.github.io/p/notes-on-internvl3.5/</link><pubDate>Mon, 01 Sep 2025 11:30:50 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-internvl3.5/</guid><description>&lt;p>上海 AI LAB 提出了 InternVL 3.5 系列多模态大模型，InternVL 3.5 主要强调了模型的 reasoning 能力以及 inference 效率。&lt;/p>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>作者首先介绍了已有多模态大模型的进展，然后提到有两个未解决的问题：&lt;/p>
&lt;ol>
&lt;li>如何构建一个 stable, effective, scalable 的 RL 框架来训练 MLLM&lt;/li>
&lt;li>如何降低 MLLM 在长上下文场景下的计算开销过高的问题&lt;/li>
&lt;/ol>
&lt;p>为了解决这两个问题，作者提出了 InternVL3.5 多模态大模型系列，相比于 InternVL3, 模型在 reasoning 能力和 efficiency 上均进行了提升。作者主要通过 Cascade RL 框架来提高模型的 reasoning 表现，以及使用 Visual Resolution Router (ViR) 和 Decoupled Vision Language Deployment (DvD) 来提高模型的推理效率。&lt;/p>
&lt;p>总结起来，InternVL3.5 模型的贡献如下：&lt;/p>
&lt;ol>
&lt;li>开源了 InternVL3.5 系列多模态大模型，模型拥有先进的 reasoning 能力以及推理效率&lt;/li>
&lt;li>提出了 Cascade RL, ViR 以及 DvD 等模块来提高模型的表现/推理效率&lt;/li>
&lt;li>系统性评估了模型的表现&lt;/li>
&lt;/ol>
&lt;h2 id="method">Method
&lt;/h2>&lt;h3 id="architecture">Architecture
&lt;/h3>&lt;p>InternVL3.5 的架构与 InternVL3 的架构基本一致，包括一个 ViT, 一个 MLP 和一个 LLM, 模型的架构如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-architecture.png"
width="1295"
height="419"
srcset="https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-architecture_hu13393200266970951905.png 480w, https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-architecture_hu14009408115549874616.png 1024w"
loading="lazy"
alt="Architecture of InternVl3.5"
class="gallery-image"
data-flex-grow="309"
data-flex-basis="741px"
>&lt;/p>
&lt;p>模型配置如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-model-configuration.png"
width="1227"
height="484"
srcset="https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-model-configuration_hu18167488677887436257.png 480w, https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-model-configuration_hu9269566815942944406.png 1024w"
loading="lazy"
alt="Configuration of InternVL3.5"
class="gallery-image"
data-flex-grow="253"
data-flex-basis="608px"
>&lt;/p>
&lt;p>InternVl3.5 延续了 InternVL 系列的做法，即 20B 以下的模型使用 InternViT-300M, 20B 以上的模型使用 InternViT-6B 作为 Visual encoder. 大语言模型基于 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/notes-on-gpt-oss/" target="_blank" rel="noopener"
>gpt-oss&lt;/a>.&lt;/p>
&lt;p>在 InternVL3.5 的基础上，作者还构建了 InternVL3.5-Flash 模型，InternVL3.5-Flash 不同的地方在于，其在 MLP 之前加入了一个 Visual Resolution Router (ViR) 模块，来处理不同分辨率的图片。在 InternVL3.5 中，每个 image patch 由 1024 个视觉 token 来表示，然后通过 pixel shuffle 压缩至 256 个。 在 InternVL3.5-Flash 中，ViR 首先会判断每个 patch 的 semantic richness, 然后基于 semantic richness 来将对应的 token 路由到不同的 pixel shuffle 模块中，最高可以将视觉 token 压缩到 64 个，这样就可以大幅度提高模型的推理效率。&lt;/p>
&lt;h3 id="pre-training">Pre-training
&lt;/h3>&lt;p>Pre-training 使用 next-token-prediction loss 来更新模型权重，给定多模态 sequence $x=(x_1,\dots,x_L)$, 损失函数定义为&lt;/p>
$$
\mathcal{L}(\theta) = -\sum_{x_i\text{ is text token}} \log p_{\theta}(x_i
\mid x_1,\dots,x_{i-1})
$$&lt;p>与 InternVL2.5 一样，作者还对不同长度的 sequence 进行了加权，避免模型产生 bias, 加权后的 loss 为&lt;/p>
$$
\mathcal{L}(\theta) = -\sum_{x_i\text{ is text token}}\frac{w_i}{\sum_j w_j} \log p_{\theta}(x_i
\mid x_1,\dots,x_{i-1}),\quad w_i = \frac{1}{N^{0.5}}
$$&lt;p>其中 $N$ 是训练样本中需要计算损失的 token 个数。&lt;/p>
&lt;p>训练数据蛀牙包含两部分：&lt;/p>
&lt;ol>
&lt;li>多模态数据，这部分数据基于 InternVL3&lt;/li>
&lt;li>纯文本数据，基于 InternLM 系列和开源的数据集&lt;/li>
&lt;/ol>
&lt;p>最终，预训练数据一共包含&lt;strong>116M&lt;/strong> 样本，&lt;strong>250B&lt;/strong> token. 纯文本数据和多模态数据的比例为 $1:2.5$. 模型上下文长度为 $32K$.&lt;/p>
&lt;h3 id="post-training">Post-training
&lt;/h3>&lt;p>Post-training 包含三个阶段：&lt;/p>
&lt;ol>
&lt;li>SFT： 使用高质量对话数据提高模型表现&lt;/li>
&lt;li>Cascade RL: 提高模型的 reasoning 能力&lt;/li>
&lt;li>Visual Consistency Learning: 训练 ViR 模块，提高模型处理动态分辨率图片的能力&lt;/li>
&lt;/ol>
&lt;p>训练 pipeline 如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-post-training-pipeline.png"
width="1290"
height="335"
srcset="https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-post-training-pipeline_hu6577490402609280168.png 480w, https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-post-training-pipeline_hu10277305099796222533.png 1024w"
loading="lazy"
alt="Post-training pipeline of InternVL3.5"
class="gallery-image"
data-flex-grow="385"
data-flex-basis="924px"
>&lt;/p>
&lt;p>SFT 阶段的训练数据包括三个方面：&lt;/p>
&lt;ol>
&lt;li>InternVL3 的指令跟随数据&lt;/li>
&lt;li>多模态 reasoning 数据&lt;/li>
&lt;li>能力扩展数据，包括 GUI 交互，具身交互以及 SVG 理解与生成的数据&lt;/li>
&lt;/ol>
&lt;p>Cascade RL 阶段结合了 offline RL 和 online RL 的优点。基本思想是先使用 offline RL 来提高模型的能力，降低训练成本。然后再使用 online RL 进一步提高模型的 reasoning 表现。&lt;/p>
&lt;p>offline RL 阶段使用的是 InternVL-MPO 算法，其损失函数为&lt;/p>
$$
\mathcal{L}_{MPO} = w_p\mathcal{L}_p+w_q\mathcal{L}_q+w_g\mathcal{L}_g
$$&lt;p>其中，$\mathcal{L}_p$ 为 DPO 的损失函数，$\mathcal{L}_q$ 为 Quality loss, $\mathcal{L}_g$ 为 SFT 使用的 next-token-prediction loss.&lt;/p>
&lt;p>online RL 阶段使用的是 &lt;a class="link" href="https://maosong.website/p/notes-on-gspo/" target="_blank" rel="noopener"
>GSPO&lt;/a> 算法，核心思想是在 sample 层面做归一化。这个阶段的目的是进一步提高模型的表现。&lt;/p>
&lt;p>Cascade RL 的优势在于：&lt;/p>
&lt;ol>
&lt;li>训练更稳定：offline RL 可以避免模型产生 reward hacking 现象，online RL 阶段可以进一步提高模型的表现&lt;/li>
&lt;li>训练效率更高：offline RL 可以有效提高采样效率&lt;/li>
&lt;li>表现更好：先 MPO 再 RL 可以达到更好的表现&lt;/li>
&lt;/ol>
&lt;p>Visual Consistency Learning (ViCO) 的目的是降低 InternVL3.5 的推理开销。这个阶段主要训练 ViR 模块&lt;/p>
&lt;p>ViCO 包括两个 stage:&lt;/p>
&lt;p>Stage 1: Consistency training&lt;/p>
&lt;p>这一阶段的目的是使得不同压缩率处理的视觉 token 对应的输出与 ground truth 尽可能一致，作者使用另外一个 InternVL3.5 模型作为 reference model, 然后损失函数为&lt;/p>
$$
\mathcal{L}_{ViCO} = \mathbb{E}_{\xi\sim\mathcal{R}}\left[\frac{1}{N}\sum_{i=1}^N\mathrm{KL}\left(\pi_{\mathrm{ref}}(y_i\mid y_{&lt;i}, I)\ \Vert\ \pi_{\theta}(y_i\mid y_{&lt;i}, I_{\xi})\right)\right]
$$&lt;p>$\xi\in{1/4,1/16}$ 为 compression rate, 训练是随机采样。$\xi=1/4$ 代表最终会有 256 个视觉 token, $\xi=1/16$ 代表最终会有 64 个 token.&lt;/p>
&lt;p>Stage 2: Router training&lt;/p>
&lt;p>这个阶段的目的是训练 ViR 来选取合适的精度/压缩率。ViR 此时作为一个 binary classifier 来进行训练，训练的损失函数为 cross-entropy loss. 模型其他部分参数冻结，仅训练 ViR 模块，首先，我们将计算压缩 16 倍后的损失与压缩 4 倍的损失的比例&lt;/p>
$$
r_i = \frac{\mathcal{L}_{ViCO}(y_i\mid I_{1/16})}{\mathcal{L}_{ViCO}(y_i\mid I_{1/4})}
$$&lt;p>该比例衡量了压缩 token 之后带来的性能下降程度，接下来，作者设定了一个阈值 $\tau$, 当性能下降超过阈值 $\tau$, 则认为应该使用高精度，也就是 $\xi=1/4$, 反之则说明不需要过度视觉 token, 可以使用低精度，也就是 $\xi=1/16$. 总结得到&lt;/p>
$$
y_i^{router} = \begin{cases}
0, &amp;r_i&lt;\tau\\
1, &amp;r_i>\tau
\end{cases}
$$&lt;p>训练时，作者基于 sliding window 中的 $r_i$ 来动态调整 $\tau$ 的值。&lt;/p>
&lt;p>post-training 的训练数据如下：&lt;/p>
&lt;ol>
&lt;li>SFT 训练数据包括&lt;strong>56M&lt;/strong> 样本，&lt;strong>130B&lt;/strong> token, 纯文本数据与多模态数据的比例为 $1:3.5$.&lt;/li>
&lt;li>Cascade RL 的训练数据主要基于 MMPR, 包含 200K 左右的样本，通过过滤最终得到&lt;strong>70K&lt;/strong>左右的样本。online RL 同样使用这批数据进行训练&lt;/li>
&lt;li>ViCO 的训练数据与 SFT 阶段的训练数据一致。主要是 OCR 以及 VQA 数据&lt;/li>
&lt;/ol>
&lt;h3 id="test-time-scaling">Test-time Scaling
&lt;/h3>&lt;p>test time scaling 主要基于两个方面：&lt;/p>
&lt;ol>
&lt;li>deep thinking: 就是 reasoning mode&lt;/li>
&lt;li>parallel thinking: 基于 VisualPRM 进行多次采样，然后基于 BoN 策略得到最终的输出。&lt;/li>
&lt;/ol>
&lt;h3 id="infra">Infra
&lt;/h3>&lt;p>模型使用 XTuner 框架进行训练，使用了包括 FSDP, data packing, FP8, flashattention3 等策略。&lt;/p>
&lt;p>作者还提出了 DvD 的策略来提高推理效率，核心思想就是将 ViT-MLP 模块放在一个 server 上，然后将 LLM 放在另一个 server 上，这样就也可以提高整体的通信效率。框架示意图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-DvD.png"
width="1297"
height="580"
srcset="https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-DvD_hu5225375899242682523.png 480w, https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-DvD_hu2947329161828488830.png 1024w"
loading="lazy"
alt="DvD of InternVL3.5"
class="gallery-image"
data-flex-grow="223"
data-flex-basis="536px"
>&lt;/p>
&lt;h2 id="experiments">Experiments
&lt;/h2>&lt;p>模型整体表现如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-model-performance.png"
width="964"
height="1113"
srcset="https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-model-performance_hu14525544515045453778.png 480w, https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-model-performance_hu5961582846464991529.png 1024w"
loading="lazy"
alt="Performance of InternVL3.5"
class="gallery-image"
data-flex-grow="86"
data-flex-basis="207px"
>&lt;/p>
&lt;h3 id="ablation-study">Ablation Study
&lt;/h3>&lt;p>作者首先探究了 Cascade RL 对模型表现的影响&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-ablation-cascade-RL.png"
width="1299"
height="331"
srcset="https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-ablation-cascade-RL_hu1333440004729449373.png 480w, https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-ablation-cascade-RL_hu14908798937935143641.png 1024w"
loading="lazy"
alt="Ablation study on Cascade RL"
class="gallery-image"
data-flex-grow="392"
data-flex-basis="941px"
>&lt;/p>
&lt;p>实验结果显示，SFT, MPO 和 offline RL 每个阶段都可以提高模型的多模态 reasoning 表现。&lt;/p>
&lt;p>作者进一步探究了不同阶段的投入产出比，也就是训练时间与最终模型表现的变化情况，如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-training-efficiency-effectiveness.png"
width="1237"
height="213"
srcset="https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-training-efficiency-effectiveness_hu5719553098653551965.png 480w, https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-training-efficiency-effectiveness_hu5489765521449568152.png 1024w"
loading="lazy"
alt="Training efficiency and effectiveness"
class="gallery-image"
data-flex-grow="580"
data-flex-basis="1393px"
>&lt;/p>
&lt;p>实验结果显示，尽管 GSPO 的效果提升比较明显，但是训练所需要的时间比较长。如果先 MPO 再进行 GSPO 的话，我们可以在较短的时间里取得较好的表现。&lt;/p>
&lt;p>接下来，作者探究了 ViR 的有效性和效率。作者首先对比了 InternVL3.5 以及 InternVL3.5-flash 的表现。结果显示，大部分情况下，这两个模型的表现没有较大差距。说明 ViR 不会损害模型的性能。&lt;/p>
&lt;p>作者进一步探究了 ViR 和 DvD 对提升模型效率的影响，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-ablation-DvD.png"
width="877"
height="397"
srcset="https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-ablation-DvD_hu383731560352212400.png 480w, https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-ablation-DvD_hu14394963354227445597.png 1024w"
loading="lazy"
alt="Ablation study on DvD and ViR"
class="gallery-image"
data-flex-grow="220"
data-flex-basis="530px"
>&lt;/p>
&lt;p>实验结果说明，对于高精度图片输入，DvD 和 ViR 均可以提高模型的推理效率。&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>在本文中，作者提出了 InternVL3.5 系列多模态大模型，作者提出了 Cascade RL 框架，该框架结合了 offline RL 以及 online RL 来提高模型的 reasoning 表现以及训练效率。作者还提出了 ViR 以及 DvD 模块来提高模型处理高分辨率图片的效率。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2508.18265" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Ovis2.5 MLLM with stronger perception and reasoning capability</title><link>https://maosong2022.github.io/p/ovis2.5-mllm-with-stronger-perception-and-reasoning-capability/</link><pubDate>Sat, 30 Aug 2025 17:34:44 +0800</pubDate><guid>https://maosong2022.github.io/p/ovis2.5-mllm-with-stronger-perception-and-reasoning-capability/</guid><description>&lt;p>作者提出了 Ovis2.5, 一个基于 Ovis 改进的多模态大模型系列，包括 2B 和 9B 两个 size，Ovis2.5 主要强调了支持不同分辨率图片输入以及深度思考这两个 feature&lt;/p>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>作者首先回顾了 &lt;a class="link" href="https://maosong.website/p/ovis-discrete-visual-embedding/" target="_blank" rel="noopener"
>Ovis&lt;/a>, Ovis 主要是解决 text embedding 以及 visual embedding 对齐程度比较低的问题。&lt;/p>
&lt;p>接下来，作者介绍了以下 Ovis 的两个问题：&lt;/p>
&lt;ol>
&lt;li>只能支持固定大小的图片输入&lt;/li>
&lt;li>缺乏深度思考能力&lt;/li>
&lt;/ol>
&lt;p>为了解决这两个问题，作者提出了 Ovis 2.5, Ovis 主要做出了两点改进：&lt;/p>
&lt;ol>
&lt;li>使用了 NaViT 来处理不同分辨率图片的输入&lt;/li>
&lt;li>作者通过训练提高了模型的深度思考能力&lt;/li>
&lt;/ol>
&lt;p>最终 Ovis2.5 主要有以下 feature&lt;/p>
&lt;ol>
&lt;li>支持动态分辨率图片输入&lt;/li>
&lt;li>深度思考能力&lt;/li>
&lt;li>SOTA 的表现&lt;/li>
&lt;li>高效的训练方式&lt;/li>
&lt;/ol>
&lt;h2 id="method">Method
&lt;/h2>&lt;h3 id="architecture">Architecture
&lt;/h3>&lt;p>Ovis2.5 的架构如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/ovis2.5-mllm-with-stronger-perception-and-reasoning-capability/Ovis-2-5-architecture.png"
width="855"
height="895"
srcset="https://maosong2022.github.io/p/ovis2.5-mllm-with-stronger-perception-and-reasoning-capability/Ovis-2-5-architecture_hu8524208080120573636.png 480w, https://maosong2022.github.io/p/ovis2.5-mllm-with-stronger-perception-and-reasoning-capability/Ovis-2-5-architecture_hu1291699860300207421.png 1024w"
loading="lazy"
alt="Architecture of Ovis2.5"
class="gallery-image"
data-flex-grow="95"
data-flex-basis="229px"
>&lt;/p>
&lt;p>Ovis 包括三个模块：&lt;/p>
&lt;ol>
&lt;li>visual tokenizer： ViT 架构，&lt;/li>
&lt;li>visual embedding table: 类似 LLM 中的 text embedding table, 见 &lt;a class="link" href="https://maosong.website/p/ovis-discrete-visual-embedding/" target="_blank" rel="noopener"
>Ovis&lt;/a>&lt;/li>
&lt;li>LLM: 基于 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a>&lt;/li>
&lt;/ol>
&lt;p>作者在架构上进行了如下改进：&lt;/p>
&lt;ol>
&lt;li>动态分辨率图片输入处理：作者使用了 NaViT 来支持动态分辨率图片输入&lt;/li>
&lt;li>LLM: 作者使用了 Qwen3 来进一步提高模型的表现&lt;/li>
&lt;/ol>
&lt;h3 id="training">Training
&lt;/h3>&lt;p>模型训练包括 pre-training 和 post-training 两个大的 stage, 其中 pre-training 又包含 3 个小的 stage, post-training 包含 2 个 stage. 训练过程如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/ovis2.5-mllm-with-stronger-perception-and-reasoning-capability/Ovis-2-5-training-process.png"
width="930"
height="292"
srcset="https://maosong2022.github.io/p/ovis2.5-mllm-with-stronger-perception-and-reasoning-capability/Ovis-2-5-training-process_hu17880204471185657436.png 480w, https://maosong2022.github.io/p/ovis2.5-mllm-with-stronger-perception-and-reasoning-capability/Ovis-2-5-training-process_hu8911771560249119512.png 1024w"
loading="lazy"
alt="Training Process of Ovis2.5"
class="gallery-image"
data-flex-grow="318"
data-flex-basis="764px"
>&lt;/p>
&lt;p>pre-training 阶段的数据包括 COYO, Laion, Wukong, DataComp, SAM 等。作者介绍了几个部分的数据：&lt;/p>
&lt;ol>
&lt;li>OCR 数据，作者基于 MLLM 来标注数据和合成 QA&lt;/li>
&lt;li>Grounding 数据，作者使用了 RefCoCo 等数据集以及先进的 MLLM 来标注数据&lt;/li>
&lt;li>Reasoning 数据，作者收集了数据然后使用 MLLM 来合成 Reasoning path&lt;/li>
&lt;/ol>
&lt;p>训练时，&lt;/p>
&lt;ol>
&lt;li>VET pretraining: 训练 VET, 作者基于 SigLIP 来初始模型的参数，然后仅训练最后一层 ViT layer, visual head 以及 VET, 图片精度为 448-896. 作者采用了动态 position embedding&lt;/li>
&lt;li>Multimodal pretraining: 这阶段全量微调所有参数，主要目的是使用对话格式的数据。图片精度为 448-1792&lt;/li>
&lt;li>multimodal instruction tuning: 这阶段训练所有参数，主要提高模型跟随多模态指令的能力&lt;/li>
&lt;/ol>
&lt;p>post-training 包括 DPO 和 GRPO 两个阶段。&lt;/p>
&lt;ol>
&lt;li>DPO: 训练所有参数，使用 pre-training checkpoint 来多次采样&lt;/li>
&lt;li>GRPO: 使用 RLVR 数据集进行训练&lt;/li>
&lt;/ol>
&lt;h2 id="infra">Infra
&lt;/h2>&lt;p>infra 方面，作者主要强调了 data packing 以及多种并行策略融合。&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 Ovis2.5, 一个基于 Ovis 架构的多模态大模型，作者主要强调了模型的动态图片输入处理能力以及深度思考能力。&lt;/p>
&lt;p>作者提出了几个未来的方向：&lt;/p>
&lt;ol>
&lt;li>将输入图片精度提升到 4K&lt;/li>
&lt;li>处理长视频输入并进行 temporal reasoning&lt;/li>
&lt;li>在 Reasoning 过程中加入 tool-use.&lt;/li>
&lt;/ol>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2508.11737" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Ovis-discrete visual embedding</title><link>https://maosong2022.github.io/p/ovis-discrete-visual-embedding/</link><pubDate>Sat, 30 Aug 2025 17:32:22 +0800</pubDate><guid>https://maosong2022.github.io/p/ovis-discrete-visual-embedding/</guid><description>&lt;p>作者提出了 Ovis，一个离散化表示 visual encder 输出特征的方法，来更好对齐 LLM 的视觉输入和文本输入&lt;/p>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>作者分析了已有多模态大模型的架构，已有多模态大模型的输入对于文本来说是离散的 (text token), 对于图片来说是连续的 (visual embedding)。作者认为这种连续 - 离散的输入可能会影响模型最终的表现。&lt;/p>
&lt;p>为了解决这个问题，作者构建了一个 visual embedding table, 将 visual embedding 也转换成离散的 token 表示形式，进而统一 LLM 输出的粒度。&lt;/p>
&lt;h2 id="method">Method
&lt;/h2>&lt;p>模型的架构如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/ovis-discrete-visual-embedding/Ovis-architecure.png"
width="1174"
height="964"
srcset="https://maosong2022.github.io/p/ovis-discrete-visual-embedding/Ovis-architecure_hu605041715381310912.png 480w, https://maosong2022.github.io/p/ovis-discrete-visual-embedding/Ovis-architecure_hu426897017600916110.png 1024w"
loading="lazy"
alt="Architecture of Ovis"
class="gallery-image"
data-flex-grow="121"
data-flex-basis="292px"
>&lt;/p>
&lt;p>我们首先会构建一个 visual vocabulary ${e_k}&lt;em>{k=1}^K$, 其大小为 $K$, 然后对于 ViT 输出的 $n$ 个 visual feature ${r_i}&lt;/em>{i=1}^n$, 我们会加入一个 linear head 以及一个 softmax 来构建一个 vocabulary 上的分布，即&lt;/p>
$$
v_i = \mathrm{softmax}(Wr_i), W\in\mathbb{R}^{K\times d}
$$&lt;p>这里 $v_i\in\Delta^K$ 是 visual vocabulary 上的概率分布。最终，视觉模块的输入是 vocabulary 中 visual token 的一个加权求和&lt;/p>
$$
V_i = \sum_{k=1}^K v_{i,k}e_k\in\mathbb{R}^{d'}
$$&lt;p>训练分为三个阶段：&lt;/p>
&lt;ul>
&lt;li>Stage 1: 训练 $W$, visual encoder 最后一个 block 以及 visual vocabulary&lt;/li>
&lt;li>Stage 2: 训练 $W$, visual vocabulary 以及 visual encoder&lt;/li>
&lt;li>Stage 3: multimodal SFT, 提高模型的指令跟随能力，模型所有参数都参与训练&lt;/li>
&lt;/ul>
&lt;p>训练数据分布如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/ovis-discrete-visual-embedding/Ovis-training-dataset-distribution.png"
width="1023"
height="750"
srcset="https://maosong2022.github.io/p/ovis-discrete-visual-embedding/Ovis-training-dataset-distribution_hu18014811884689103569.png 480w, https://maosong2022.github.io/p/ovis-discrete-visual-embedding/Ovis-training-dataset-distribution_hu12359662282283660716.png 1024w"
loading="lazy"
alt="Statistics of Ovis training dataset"
class="gallery-image"
data-flex-grow="136"
data-flex-basis="327px"
>&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者提出了 Ovis，一个离散化表示 visual encder 输出特征的方法，来更好对齐 LLM 的视觉输入和文本输入&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2405.20797" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on DeepSeekMoE</title><link>https://maosong2022.github.io/p/notes-on-deepseekmoe/</link><pubDate>Fri, 29 Aug 2025 11:03:12 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-deepseekmoe/</guid><description>&lt;p>DeepSeek 在 2024 年 1 月发布了 DeepSeekMoE, 一个解决 MoE 模型 specialization 不足以及 redundancy 问题的大模型系列。&lt;/p>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>作者首先回顾了已有 MoE 模型的不足，主要有两点：&lt;/p>
&lt;ol>
&lt;li>knowledge hybridity: 已有 MoE 模型的专家个数比较少，这就导致每个专家需要掌握更多样化的知识，提高了训练难度&lt;/li>
&lt;li>knowledge redundancy: 不同专家掌握的知识可能有重叠，从而导致了模型参数的 redundancy&lt;/li>
&lt;/ol>
&lt;p>为了解决这两个问题，作者提出了 DeepSeek-MoE, DeepSeek-MoE 主要做出了两点改变：&lt;/p>
&lt;ol>
&lt;li>Fine-Grained Expert Segmentation: 作者使用了更多的专家，来提高每个专家的 specialization, 降低训练成本&lt;/li>
&lt;li>Shared Expert Isolation: 作者在 Routing expert 的基础上，加入了 shared expert 来学习 common knowledge.&lt;/li>
&lt;/ol>
&lt;p>作者在 2B-A0.6B 的模型进行了实验，结果显示模型表现超过了 &lt;a class="link" href="https://maosong.website/p/GShard.md" target="_blank" rel="noopener"
>GShard&lt;/a>, 说明了 DeepSeekMoE 模型架构的有效性。&lt;/p>
&lt;p>作者还进一步将模型 scale 到了 16B-A2.8B 和 145B-A22B, 实验结果均验证了模型的 scaling 效果。&lt;/p>
&lt;h2 id="method">Method
&lt;/h2>&lt;h3 id="preliminary">Preliminary
&lt;/h3>&lt;p>作者首先回顾了 transformer 架构，transformer 的第 $\ell$ 层 decode layer 可以表示为&lt;/p>
$$
\begin{aligned}
u_{1:T}^{\ell} &amp;= \mathrm{self\_attention}(h_{1:T}^{\ell-1}) + h_{1:T}^{\ell}\\
h_t^\ell &amp;= \mathrm{FFN}(u_t^{\ell}) + u_t^{\ell}
\end{aligned}
$$&lt;p>其中 $T$ 是 sequence length, $h_{1:T}^{\ell-1}$ 是第 $\ell-1$ 层 decoder layer 输出的 hidden states.&lt;/p>
&lt;p>接下来，我们可以将 dense 架构转换为 MoE 架构，MoE 架构与 dense 架构不同的地方在与 $\mathrm{FFN}$ 不再是 MLP, 而是一个 MoE 模块，其表达式如下&lt;/p>
$$
\begin{aligned}
h_t^\ell &amp;= \sum_{i=1}^N\left(g_{i,t}\mathrm{FFN}_i(u_t^{\ell})\right) + u_t^{\ell}\\
g_{i,t} &amp;= \begin{cases}
s_{i,t,}, &amp;s_{i,t}\in\mathrm{Topk}(\{s_{j,t}\mid 1\leq j \leq N\},K)\\
0, &amp;\text{otherwise}
\end{cases}\\
s_{i,t,} &amp;= \mathrm{softmax}_i({u_t^{\ell}}^Te_i^{\ell})
\end{aligned}
$$&lt;p>这里 $N$ 是专家的总个数， $K$ 是激活专家个数， $e_{i}^{\ell}$ 是 routing layer 的权重矩阵，$\mathrm{FFN}_i$ 是每个专家对应的 FFN.&lt;/p>
&lt;h3 id="deepseekmoe-architecutre">DeepSeekMoE Architecutre
&lt;/h3>&lt;p>DeepSeekMoE 架构如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-deepseekmoe/DeepSeekMoE-architecture.png"
width="1200"
height="586"
srcset="https://maosong2022.github.io/p/notes-on-deepseekmoe/DeepSeekMoE-architecture_hu4792723050699036467.png 480w, https://maosong2022.github.io/p/notes-on-deepseekmoe/DeepSeekMoE-architecture_hu1111125453942995081.png 1024w"
loading="lazy"
alt="Architecture of DeepSeekMoE"
class="gallery-image"
data-flex-grow="204"
data-flex-basis="491px"
>&lt;/p>
&lt;p>相比于其他 MoE 架构，DeepSeekMoE 主要做了以下几点改变。&lt;/p>
&lt;h4 id="fine-grained-expert-segmentation">Fine-Grained Expert Segmentation
&lt;/h4>&lt;p>作者首先解决了每个专家学习内容过多的问题。作者的做法就是将每个 expert FFN 分割为 $m$ 个更小的专家，具体做法就是将 FFN intermediate hidden size 降低为原来的 $1/m$. 这样的话就可以在不增加模型参数量的情况下提高模型的表现。修正后的 MoE 模块为&lt;/p>
$$
\begin{aligned}
h_t^\ell &amp;= \sum_{i=1}^{mN}\left(g_{i,t}\mathrm{FFN}_i(u_t^{\ell})\right) + u_t^{\ell}\\
g_{i,t} &amp;= \begin{cases}
s_{i,t,}, &amp;s_{i,t}\in\mathrm{Topk}(\{s_{j,t}\mid 1\leq j \leq mN\},mK)\\
0, &amp;\text{otherwise}
\end{cases}\\
s_{i,t,} &amp;= \mathrm{softmax}_i({u_t^{\ell}}^Te_i^{\ell})
\end{aligned}
$$&lt;p>可以看到，现在我们一共有 $mN$ 个专家，激活专家个数为 $mK$ 个。作者认为，通过提高专家的粒度，我们可以有效增加专家组合的可能性，这就提高了最终的组合多样性。&lt;/p>
&lt;h4 id="shared-expert-isolation">Shared Expert Isolation
&lt;/h4>&lt;p>接下来，作者介绍了解决不同专家学习到重复知识的问题，作者的做法是在 routing Expert 的基础上加入 Shared expert. 也就是说，有固定几个专家始终都会被激活，这部分专家复杂学习通用知识，从而减少知识冗余。增加 shared expert 之后的 MoE 模块为&lt;/p>
$$
\begin{aligned}
h_t^\ell &amp;= \sum_{i=1}^{K_s}\mathrm{FFN}_i(u_t^{\ell})+\sum_{i=K_s+1}^{mN}\left(g_{i,t}\mathrm{FFN}_i(u_t^{\ell})\right) + u_t^{\ell}\\
g_{i,t} &amp;= \begin{cases}
s_{i,t,}, &amp;s_{i,t}\in\mathrm{Topk}(\{s_{j,t}\mid K_s+1\leq j \leq mN\},mK-K_s)\\
0, &amp;\text{otherwise}
\end{cases}\\
s_{i,t,} &amp;= \mathrm{softmax}_i({u_t^{\ell}}^Te_i^{\ell})
\end{aligned}
$$&lt;p>此时，模型中一共包含 $K_s$ 个共享专家，$mN-K_s$ 个 routing expert, 其中激活专家个数为 $mK-K_s$.&lt;/p>
&lt;h4 id="load-balancing-loss">Load Balancing Loss
&lt;/h4>&lt;p>接下来，作者解决了训练时的 load imbalance 问题，作者提出了两个 loss 来分别解决不同层面的 load imbalance 问题。&lt;/p>
&lt;p>首先，在 expert 层面，作者使用了如下的 load balancing loss:&lt;/p>
$$
\mathcal{L} = \alpha_1\sum_{i=1}^{N'}f_iP_i
$$&lt;p>其中 $\alpha_1$ 是超参数，&lt;/p>
$$
f_i = \frac{N'}{K'T}\sum_{i=1}^{N'}\mathbb{1}(\text{Token }i \text{ selects Expert }i),\quad P_i = \frac{1}{T}\sum_{t=1}^Ts_{i,t}
$$&lt;p>分别为以及分配给第 $i$ 个专家的 token 比例以及概率之和。$N&amp;rsquo;=mN-K_s$, $K&amp;rsquo;=mK-K_s$. $\mathbb{1}(\cdot)$ 是 indicator function.&lt;/p>
&lt;p>其次，在 device 层面，作者也是用了 load balancing loss 来减少不同设备之间不必要的通信。作者将 routed experts 分为 $D$ 个 group $\mathcal{E}_1,\dots,\mathcal{E}_D$, 然后每个设备部署一个 group, group level 的 load balancing loss 定义如下：&lt;/p>
$$
\mathcal{L} = \alpha_2\sum_{i=1}^D f_i' P_i'
$$&lt;p>其中 $\alpha_2$ 是超参数，&lt;/p>
$$
f_i' = \frac{1}{|\mathcal{E}_i|}\sum_{j\in\mathcal{E}_i}f_i,\quad P_i' = \sum_{j\in\mathcal{E}_i}P_i
$$&lt;p>实际中，作者使用了一个较小的 $\alpha_1$ 来避免 routing collapse, 使用了一个较大的 $\alpha_2$ 来提高 Device 层面的负载均衡。&lt;/p>
&lt;h3 id="training">Training
&lt;/h3>&lt;p>作者使用了中英文数据进行训练，tokenizer 基于 BPE 算法，vocab size 为 8K. 模型训练基于 HAI-LLM.训练时使用了 TP, DP, PP, EP 等并行策略。&lt;/p>
&lt;p>2B, 16B, 145B 模型的参数如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>2B&lt;/th>
&lt;th>16B&lt;/th>
&lt;th>145B&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>total params&lt;/td>
&lt;td>2B&lt;/td>
&lt;td>16.4B&lt;/td>
&lt;td>144.6B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>activated params&lt;/td>
&lt;td>0.3B&lt;/td>
&lt;td>2.8B&lt;/td>
&lt;td>22.2B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>hidden size&lt;/td>
&lt;td>1280&lt;/td>
&lt;td>2048&lt;/td>
&lt;td>4096&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>layers&lt;/td>
&lt;td>9&lt;/td>
&lt;td>28&lt;/td>
&lt;td>62&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>attention heads&lt;/td>
&lt;td>10&lt;/td>
&lt;td>16&lt;/td>
&lt;td>32&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>head dimension&lt;/td>
&lt;td>128&lt;/td>
&lt;td>128&lt;/td>
&lt;td>128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>routed experts&lt;/td>
&lt;td>63&lt;/td>
&lt;td>64&lt;/td>
&lt;td>128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>activated experts&lt;/td>
&lt;td>7&lt;/td>
&lt;td>6&lt;/td>
&lt;td>12&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>shared experts&lt;/td>
&lt;td>1&lt;/td>
&lt;td>2&lt;/td>
&lt;td>4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>training tokens&lt;/td>
&lt;td>100B&lt;/td>
&lt;td>2T&lt;/td>
&lt;td>245B&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="experiments">Experiments
&lt;/h2>&lt;h3 id="alignment">Alignment
&lt;/h3>&lt;p>作者针对 DeepseekMoE 16B 进行了微调，微调使用了 &lt;strong>1.4M&lt;/strong> 的训练样本，覆盖了 math, code, QA, reasoning 等任务。&lt;/p>
&lt;h3 id="ablation-study">Ablation Study
&lt;/h3>&lt;p>作者在 2B 的模型上进行了 ablation study.&lt;/p>
&lt;p>首先，作者探究了细粒度专家和共享专家的有效性，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-deepseekmoe/DeepSeekMoE-ablation-experts.png"
width="1197"
height="552"
srcset="https://maosong2022.github.io/p/notes-on-deepseekmoe/DeepSeekMoE-ablation-experts_hu13488373378225854754.png 480w, https://maosong2022.github.io/p/notes-on-deepseekmoe/DeepSeekMoE-ablation-experts_hu6581578019190079093.png 1024w"
loading="lazy"
alt="Ablation on experts"
class="gallery-image"
data-flex-grow="216"
data-flex-basis="520px"
>&lt;/p>
&lt;p>实验结果显示，与 &lt;a class="link" href="https://maosong.website/p/GShard.md" target="_blank" rel="noopener"
>GShard&lt;/a> 相比，&lt;strong>使用共享专家可以有效提高模型的表现&lt;/strong>。并且，&lt;strong>使用更细粒度的专家也可以进一步提高模型的表现&lt;/strong>&lt;/p>
&lt;p>作者还探究了共享专家与路由专家的比例，作者分别使用不同的比例进行实验，结果发现共享专家：路由专家个数为 1：3 的时候模型效果最好。&lt;/p>
&lt;p>作者还探究了模型的泛化性，作者 mask 掉一部分概率最高的 routing expert, 然后从剩下的专家里进行 topK 的挑选，然后作者比较模型和 GShard 的表现，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-deepseekmoe/DeepSeekMoE-ablation-expert-specialization.png"
width="737"
height="526"
srcset="https://maosong2022.github.io/p/notes-on-deepseekmoe/DeepSeekMoE-ablation-expert-specialization_hu1048559941586353742.png 480w, https://maosong2022.github.io/p/notes-on-deepseekmoe/DeepSeekMoE-ablation-expert-specialization_hu16111375923387789940.png 1024w"
loading="lazy"
alt="Ablation study on expert specialization"
class="gallery-image"
data-flex-grow="140"
data-flex-basis="336px"
>&lt;/p>
&lt;p>实验结果显示，&lt;strong>DeepSeekMoE 对于 mask 操作更敏感，这说明 DeepSeekMoE 模型中专家的 specialization 更强。&lt;/strong>&lt;/p>
&lt;p>作者还探究了 mask 掉共享专家对模型表现的影响，结果显示&lt;strong>共享专家与路由专家之间的 overlap 很小，去掉共享专家之后，模型表现会变差。&lt;/strong>&lt;/p>
&lt;p>作者进一步分析了共享专家与路由专家组合的有效性。作者探究 DeepSeekMoE 是否可以使用更少的路由专家来获取知识。作者通过使用不同的 activated routed experts 来进行实验，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-deepseekmoe/DeepSeekMoE-ablation-number-activated-experts.png"
width="730"
height="524"
srcset="https://maosong2022.github.io/p/notes-on-deepseekmoe/DeepSeekMoE-ablation-number-activated-experts_hu3482294733356635794.png 480w, https://maosong2022.github.io/p/notes-on-deepseekmoe/DeepSeekMoE-ablation-number-activated-experts_hu10658320898924064401.png 1024w"
loading="lazy"
alt="Ablation study on activated routed experts"
class="gallery-image"
data-flex-grow="139"
data-flex-basis="334px"
>&lt;/p>
&lt;p>实验结果显示，DeepSeekMoE 仅需激活 4 个路由专家，就可以达到与 GShard 相同的表现。&lt;strong>这说明了 DeepSeekMoE 模型中每个专家可以学习到更准确的知识。&lt;/strong>&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 DeepSeekMoE 架构，来提高 MoE 模型中专家的利用效率。为了达到这一点，作者首先使用了更细粒度的专家，降低每个专家的学习成本，然后，作者使用了共享专家，来降低不同专家之间的知识冗余。作者先在 2B 的模型上验证了方法的有效性，然后作者将模型 scale 到了 16B 和 125B。结果显示模型的效果均超过了以前的工作。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2401.06066" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on DeepSeek-LLM</title><link>https://maosong2022.github.io/p/notes-on-deepseek-llm/</link><pubDate>Tue, 26 Aug 2025 10:53:10 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-deepseek-llm/</guid><description>&lt;p>DeepSeek 在 2024 年 1 月 5 日发布了 DeepSeek LLM, 包括 7B 和 67B 两个 size, 作者主要强调了对于 scaling law 的探究&lt;/p>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>已有的 scaling law 如 &lt;a class="link" href="https://maosong.website/p/kaplan-scaling-law/" target="_blank" rel="noopener"
>Kaplan&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/chinchilla-scaling-law/" target="_blank" rel="noopener"
>Chinchilla&lt;/a> 介绍了 model size, dataset size, compute budget 与模型表现之间的关系。在本文中，作者进一步探究了 learning rate 和 batch size 等超参数与模型表现之间的关系。基于发现的 scaling law, 作者为不同大小的模型设置了最优的超参数。并且，作者还发现不同数据集与模型表现之间的关系。&lt;/p>
&lt;p>最终，基于这些实验结果，作者提出了 DeepSeek LLM, 模型使用 &lt;strong>2T token&lt;/strong> 进行预训练，使用 1M samples 进行后训练，后训练包括 SFT 以及 DPO.&lt;/p>
&lt;h2 id="pre-training">Pre-training
&lt;/h2>&lt;h3 id="architecture">Architecture
&lt;/h3>&lt;p>DeepSeek-LLM 的架构与 LLaMA 基本相同，作者在 67B 的模型上使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-gqa/" target="_blank" rel="noopener"
>GQA&lt;/a> 来提高 inference 效率。最终模型的配置如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Params&lt;/th>
&lt;th>7B&lt;/th>
&lt;th>67B&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$n_{\text{layers}}$&lt;/td>
&lt;td>$30$&lt;/td>
&lt;td>$95$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$d_{\text{model}}$&lt;/td>
&lt;td>$4096$&lt;/td>
&lt;td>$8192$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$n_{\text{heads}}$&lt;/td>
&lt;td>$32$&lt;/td>
&lt;td>$64$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$n_{\text{kv_heads}}$&lt;/td>
&lt;td>$32$&lt;/td>
&lt;td>$8$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Context Length&lt;/td>
&lt;td>$4096$&lt;/td>
&lt;td>$4096$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Sequence Batch Size&lt;/td>
&lt;td>$2304$&lt;/td>
&lt;td>$4608$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Learning Rate&lt;/td>
&lt;td>$4.2e-4$&lt;/td>
&lt;td>$3.2e-4$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Tokens&lt;/td>
&lt;td>2T&lt;/td>
&lt;td>2T&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="data">Data
&lt;/h3>&lt;p>作者主要从 Common Crawl 构建预训练数据，数据处理过程包括：去重，过滤以及 remixing 三个步骤。&lt;/p>
&lt;p>对于 tokenizer, 作者使用了 &lt;a class="link" href="https://maosong.website/p/hands-on-llm1-tokenizer/" target="_blank" rel="noopener"
>BBPE&lt;/a> 算法，tokenizer 的大小设置为 100,000, 最终的 tokenizer 大小为 102400.&lt;/p>
&lt;h3 id="hyper-parameters">Hyper Parameters
&lt;/h3>&lt;p>作者主要对比了一下不同 learning rate schedule 的表现：&lt;/p>
&lt;ol>
&lt;li>cosine learning schedule&lt;/li>
&lt;li>multi-step learning rate schedule: 包含三个 Stage, 第一个 stage 保持最大学习率，第二个 stage 将学习率降低为最大学习率的 $31.6%$, 第三个 stage 降低为最大学习率的 $10%$.&lt;/li>
&lt;/ol>
&lt;p>对比的实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-deepseek-llm/DeepSeek-LLM-learning-rate-ablation.png"
width="1077"
height="381"
srcset="https://maosong2022.github.io/p/notes-on-deepseek-llm/DeepSeek-LLM-learning-rate-ablation_hu7127490498946913523.png 480w, https://maosong2022.github.io/p/notes-on-deepseek-llm/DeepSeek-LLM-learning-rate-ablation_hu3030451669268181506.png 1024w"
loading="lazy"
alt="Comparison of different learning schedulers"
class="gallery-image"
data-flex-grow="282"
data-flex-basis="678px"
>&lt;/p>
&lt;p>实验结果显示，multi-step learning rate scheduler 的表现与 cosine learning rate 表现差不多。并且，multi-step learning rate scheduler 对于 continue pretraining 支持更好。因此在本文中作者使用了 multi-step learning rate scheduler.&lt;/p>
&lt;h3 id="infra">Infra
&lt;/h3>&lt;p>作者使用了数据并行，张量并行，序列并行以及 1F1B pipeline 并行。作者还使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-flashattention/" target="_blank" rel="noopener"
>flash attention&lt;/a> 来提高硬件利用率。&lt;/p>
&lt;h2 id="scaling-law">Scaling Law
&lt;/h2>&lt;p>本节中，作者分析了 scaling law, 主要有以下三点：&lt;/p>
&lt;ol>
&lt;li>构建了针对 learning rate 和 batch size 的 scaling law&lt;/li>
&lt;li>作者使用 non-embedding FLOPs/token $M$ 来表示 model scale&lt;/li>
&lt;li>预训练数据的质量对最后中的 scaling 影响很大&lt;/li>
&lt;/ol>
&lt;p>作者首先构建了针对 batch size 和 learning rate 的 scaling law, 结果显示最优的 learning rate 和 batch size 范围都比较广，这个结论与 &lt;a class="link" href="https://maosong.website/p/kaplan-scaling-law/" target="_blank" rel="noopener"
>Kaplan&lt;/a> 一致。&lt;/p>
&lt;p>接下来，作者构建了 batch size $B$, learning rate $\eta$ 与 compute budget $C 之间的关系，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-deepseek-llm/DeepSeek-LLM-scaling-curve-bsz-lr.png"
width="1066"
height="379"
srcset="https://maosong2022.github.io/p/notes-on-deepseek-llm/DeepSeek-LLM-scaling-curve-bsz-lr_hu9017218681376297400.png 480w, https://maosong2022.github.io/p/notes-on-deepseek-llm/DeepSeek-LLM-scaling-curve-bsz-lr_hu13578224903628364776.png 1024w"
loading="lazy"
alt="Scaling curves of batch size and learning rate"
class="gallery-image"
data-flex-grow="281"
data-flex-basis="675px"
>&lt;/p>
&lt;p>拟合得到的曲线为&lt;/p>
$$
\begin{aligned}
\eta_{opt} &amp;= 0.3118* C^{-0.1250}\\
B_{opt} &amp;= 0.2920 * C^{0.3271}
\end{aligned}
$$&lt;p>可以看到，随着 compute budget 增加，$B_{opt}$ 也逐渐增加，而 $\eta_{opt}$ 逐渐减小。并且，最优参数的范围都比较广。&lt;/p>
&lt;p>接下来，作者进一步探究了 batch size 与 generalization error $L$ 之间的关系。作者希望找到 model scale $N$, data scale $D$ 与 compute budget $C$ 之间的关系，即&lt;/p>
$$
N_{opt} \varpropto C^a,D_{opt} \varpropto C^b
$$&lt;p>compute budget 与 model scale, data scale 之间的关系可以近似表示为 $C=6ND$, 这个公式的推导见 &lt;a class="link" href="https://maosong.website/p/llm-flops-computation/" target="_blank" rel="noopener"
>LLM FLOPs computation&lt;/a>。我们用 $N_1,N_2$ 分别表示模型的 non-embedding parameter 以及 complete parameters, 则我们可以用 $6N_1$ 或者 $6N_2$ 来近似 model scale, 但是 $6N_1$ 和 $6N_2$ 均没有考虑 attention 的计算开销，因此这两种近似的误差都比较大。&lt;/p>
&lt;p>为了解决这个问题，作者提出了一个新的 model scale 表示形式，即 non-embedding FLOPS/token $M$, 其中 $M$ 包含 attention 的计算开销但是不包含 vocabulary computation. 基于这种表示，compute budget 可以近似表示为 $C=MD$. $M$ 与 $6N_1,6N_2$ 的区别表示如下所示&lt;/p>
$$
\begin{aligned}
6N_1 &amp;= 72nd^2\\
6N_2 &amp;= 72nd^2 + 6Vd\\
M &amp;= 72nd^2+12ndl
\end{aligned}
$$&lt;p>其中, $d$ 是 hidden size, $n$ 是 layers 个数, $V$ 是 vocabulary size, $l$ 是 sequence length. 作者在不同 scale 的模型上比较了三种表示方式，结果发现 $6N_1$ 和 $6N_2$ 要么低估，要么高估了模型的参数量。&lt;/p>
&lt;p>基于 model scale 的表示方式，作者构建了如下的优化问题&lt;/p>
$$
M_{opt}(C), D_{opt}(C) = {\arg\min}_{M,D\ s.t.\ C=MD} L(N,D)
$$&lt;p>作者使用了 &lt;a class="link" href="https://maosong.website/p/chinchilla-scaling-law/" target="_blank" rel="noopener"
>Chinchilla&lt;/a> 提出来的 IsoFLOP 曲线进行拟合，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-deepseek-llm/DeepSeek-LLM-IsoFLOP-curve.png"
width="1070"
height="398"
srcset="https://maosong2022.github.io/p/notes-on-deepseek-llm/DeepSeek-LLM-IsoFLOP-curve_hu9939380833919642003.png 480w, https://maosong2022.github.io/p/notes-on-deepseek-llm/DeepSeek-LLM-IsoFLOP-curve_hu3158434771816979870.png 1024w"
loading="lazy"
alt="IsoFLOP curve and optimal model/data allocation"
class="gallery-image"
data-flex-grow="268"
data-flex-basis="645px"
>&lt;/p>
&lt;p>拟合的曲线为&lt;/p>
$$
M_{opt}(C) = 0.1715*C^{0.5243}, D_{opt}(C) = 5.8316*C^{0.4757}
$$&lt;p>作者还进一步拟合了 compute budget 与 optimal generalization error 之间的关系，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-deepseek-llm/DeepSeek-LLM-performance-scaling-curve.png"
width="662"
height="414"
srcset="https://maosong2022.github.io/p/notes-on-deepseek-llm/DeepSeek-LLM-performance-scaling-curve_hu4460428567910772515.png 480w, https://maosong2022.github.io/p/notes-on-deepseek-llm/DeepSeek-LLM-performance-scaling-curve_hu2255234009678671508.png 1024w"
loading="lazy"
alt="Performance Scaling curve"
class="gallery-image"
data-flex-grow="159"
data-flex-basis="383px"
>&lt;/p>
&lt;p>实验结果显示，作者提出的 scaling law 可以很好预测模型的表现。&lt;/p>
&lt;p>最后，作者探究了以下不同数据集的 scaling law, 作者使用 early in-house data, current in-house data 以及 OpenWebText2 来将进行实验，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-deepseek-llm/DeepSeek-LLM-dataset-ablation.png"
width="746"
height="285"
srcset="https://maosong2022.github.io/p/notes-on-deepseek-llm/DeepSeek-LLM-dataset-ablation_hu5807511465671654580.png 480w, https://maosong2022.github.io/p/notes-on-deepseek-llm/DeepSeek-LLM-dataset-ablation_hu6482466502783940283.png 1024w"
loading="lazy"
alt="Comparison of different dataset scaling"
class="gallery-image"
data-flex-grow="261"
data-flex-basis="628px"
>&lt;/p>
&lt;p>结果显示，scaling law 与数据质量高度相关。当数据质量提升时，model scaling exponent $a$ 逐步提升，data scaling exponent $b$ 逐步下降，说明 compute budget 更多由模型参数量决定。因此，作者认为提升 compute budget 之后，我们应该优先提高模型的 model size.&lt;/p>
&lt;h2 id="post-training">Post-training
&lt;/h2>&lt;p>作者构建了 1.5M 的中英文指令数据。其中安全性的数据有 300K, 有帮助性的数据有 1.2M, 其中包括 $31.2%$ 的通用数据，$46.6%$ 的数学相关数据，$22.2%$ 的代码数据。&lt;/p>
&lt;p>post-training 包含两个阶段：&lt;/p>
&lt;ol>
&lt;li>SFT:7B 的模型训练了 4 个 epoch, 67B 的模型训练了 2 个 epoch, 作者发信进一步训练 67B 的模型会导致过拟合。作者发现，模型在训练过程中会出现重复输出的情况，特别是数学 SFT 数据，为了解决这个问题，作者使用了一个两阶段的 SFT 以及 DPO.&lt;/li>
&lt;li>DPO: 提高模型的能力，作者发现 DPO 可以提高模型 open-ended generation skill.&lt;/li>
&lt;/ol>
&lt;h2 id="evaluation">Evaluation
&lt;/h2>&lt;p>我们主要关注一下消融实验。&lt;/p>
&lt;p>首先作者探究了分阶段 SFT 对模型表现的影响。作者发现，小模型在 math 和 code 数据集上需要训练更长时间，但是这也损害了模型的对话能力。为了解决这个问题，作者使用两阶段的训练模式，第一个阶段使用所有的数据进行训练，第二个阶段仅使用对话数据进行训练，实验结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>HumanEval&lt;/th>
&lt;th>GSM8K&lt;/th>
&lt;th>Repetition&lt;/th>
&lt;th>IFEval&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>DeepSeek LLM 7B Chat Stage1&lt;/td>
&lt;td>48.2&lt;/td>
&lt;td>63.9&lt;/td>
&lt;td>0.020&lt;/td>
&lt;td>38.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DeepSeek LLM 7B Chat Stage2&lt;/td>
&lt;td>48.2&lt;/td>
&lt;td>63.0&lt;/td>
&lt;td>0.014&lt;/td>
&lt;td>41.2&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，经过第二阶段训练之后，模型的表现有所提升&lt;/p>
&lt;p>接下来，作者探究了 Multi-choice question 对模型表现的影响，MCQ 要求模型不仅需要有相关的知识，还要理解每个选项的含义。作者使用 20M 中文 MCQ 来进行消融实验，结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>MMLU&lt;/th>
&lt;th>CEval&lt;/th>
&lt;th>CMMLU&lt;/th>
&lt;th>TriviaQA&lt;/th>
&lt;th>ChineseQA&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>DeepSeek LLM 7B Chat&lt;/td>
&lt;td>49.4&lt;/td>
&lt;td>47.0&lt;/td>
&lt;td>49.7&lt;/td>
&lt;td>57.9&lt;/td>
&lt;td>75.0&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DeepSeek LLM 7B Chat + MC&lt;/td>
&lt;td>60.9&lt;/td>
&lt;td>71.3&lt;/td>
&lt;td>73.8&lt;/td>
&lt;td>57.9&lt;/td>
&lt;td>74.4&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>实验结果显示，MCQ 确实可以提高模型在上述几个 benchmark 上的表现，但是其泛化性会下降。因此，作者在 pre-training 和 fine-tuning 阶段并没有使用 MCQ 数据进行训练。&lt;/p>
&lt;p>作者还探究了在 pre-training 阶段加入 instruction data, 来提高 base model 在下游 benchmark 上的表现。结果发现，base model 的表现提升优先。作者认为，尽管 instruction data 可以提高 base model 表现，但是如果 Instruction data 数量过少，则模型表现不太可能学习到有用的知识。因此，作者的做法是不在 pretraining 阶段加入 Instruction data.&lt;/p>
&lt;p>最后，作者探究了 system prompt 对模型表现的影响。受 LLaMA 2 启发，作者也尝试在输入中加入 system prompt. 实验结果如下所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>MT Bench&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>DeepSeek LLM 7B Chat&lt;/td>
&lt;td>7.15&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DeepSeek LLM 7B Chat + System Prompt&lt;/td>
&lt;td>7.11&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DeepSeek LLM 67B Chat&lt;/td>
&lt;td>8.35&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DeepSeek LLM 67B Chat + System Prompt&lt;/td>
&lt;td>8.58&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，7B 的模型加入 system prompt 之后，模型表现有所下降；67B 的模型加入 system prompt 之后，模型表现有所提升。作者认为，大模型更容易理解 system prompt 的意图，而小模型的指令跟随能力则较差，因此 system prompt 反而会影响模型表现。&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 DeepSeek LLM 系列大语言模型，作者详细介绍了超参数的选择以及 scaling law 等。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2401.02954" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on MFA</title><link>https://maosong2022.github.io/p/notes-on-mfa/</link><pubDate>Sat, 23 Aug 2025 16:04:34 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-mfa/</guid><description>&lt;p>阶跃星辰等提出了 Multi-matrix Factorization Attention (MFA), 一个新型注意力机制，用于在 KV cache 限制下最大化模型的表现。&lt;/p>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>multi-head attention (MHA) 的问题在于，其 KV cache 的内存占用（memory footprint）随 sequence length 以及 batch size 线性增长，从而成为了 LLM 在 decoding 阶段的瓶颈。&lt;/p>
&lt;p>为了解决 MHA 的内存占用过高问题，已有的工作如 &lt;a class="link" href="https://maosong.website/p/notes-on-mqa/" target="_blank" rel="noopener"
>MQA&lt;/a>, &lt;a class="link" href="https://maosong.website/p/notes-on-gqa/" target="_blank" rel="noopener"
>GQA&lt;/a> 等通过共享 key, value projection 来降低 KV cache size. 而 &lt;a class="link" href="DeepSeek-V3.md" >DeepSeek-V3&lt;/a> 提出的 MLA 则是通过对 key, value projection 进行 low-rank compression, 然后只存储 latents 的方法来降低 KV cache size.&lt;/p>
&lt;p>但是，已有的这些方法的问题在于，当我们设置 KV cache budget 之后，它们的表现就比标准的 MHA 要差。&lt;/p>
&lt;p>基于以上这些发现，作者首先分析了已有 attention 机制的 modeling capacity, 然后使用一个统一的框架来表示这些 attention 机制。作者发现，attention heads 的个数以及 dimension 对模型表现有较大影响。&lt;/p>
&lt;p>基于这个发现，作者提出了 &lt;strong>Multi-matrix Factorization Attention (MFA)&lt;/strong>, 以及其变体 &lt;strong>MFA-Key-Reuse (MFA-KR)&lt;/strong>. MFA 的主要目的是在有限的 KV cache size 下提高模型的表现。&lt;/p>
&lt;h2 id="background">Background
&lt;/h2>&lt;p>作者首先介绍了 GMHA 的概念，GMHA 由三部分组成：&lt;/p>
&lt;ol>
&lt;li>QK circuit: 决定了信息之间如何交互&lt;/li>
&lt;li>valueoutput (VO) circuits：决定了信息如何传递&lt;/li>
&lt;li>per-head softmax attention.&lt;/li>
&lt;/ol>
&lt;p>接下来，作者介绍了 Fully Parameterized Bilinear Attention (FPBA), FPBA 的定义如下：&lt;/p>
$$
O = \sum_{c=1}^d\left(\sum_{j=1}^N\phi\left(\frac{xW_cx_j}{H}\right)x_jU_c\right)
$$&lt;p>其中 $\phi$ 是 softmax 函数，$d$ 是模型的 hidden dimension, $N$ 是 sequence length, $W_c,U_c\in\mathbb{R}^{d\times d}$ 每个 channel 上的参数矩阵&lt;/p>
&lt;ol>
&lt;li>每个 channel 都有各自的参数 $W_c, U_c$ 来获取 $x_i$ 与 $x_j$ 之间的信息&lt;/li>
&lt;li>提高泛化性，所有 channel 的 $U_c$ 组合起来可以遍历 $d$ 维空间中的任意一个 permutation, 这样就避免来的信息损失&lt;/li>
&lt;li>利用率高，FPBA 获取了 $x_i$ 与 $x_j$ 之间 $d$ 维空间可能的表示&lt;/li>
&lt;/ol>
&lt;p>基于以上这三个特点，作者认为 FPBA 是 GMHA 框架的一个 capacity upper bound. 此时每个 token 的 KV cache 占用为 $2d^2$ (key and value).&lt;/p>
&lt;p>然后，作者分析了 MHA 及其变体与 GMHA 的关系，MHA 可以写作如下形式&lt;/p>
$$
\begin{aligned}
O &amp;= \sum_{c=1}^h\left(\sum_{j=1}^N\phi\left(\frac{xQ_c(x_jK_c)^T}{\sqrt{d}}\right)x_jV_c\right)O_c^T\\
&amp;= \sum_{c=1}^h\left(\sum_{j=1}^N\phi\left(\frac{x(Q_cK_c^T)x_j^T}{\sqrt{d}}\right)x_jV_cO_c^T\right)
\end{aligned}
$$&lt;p>其中 $Q_c,K_c,V_c\in\mathbb{R}^{d\times h_d}$, $O_c\in\mathbb{R}^{d\times h_d}$ 分别是 query, key, value, output projection layer 对应的权重矩阵，$n$ 是 attention head 的个数，令 $h_d$ 为每个 attention 的 head dimension，则我们有 $nh_d=d$.&lt;/p>
&lt;p>可以看到，MHA 实际上是一个特殊的 FPBA, 其中，$W_c$ 和 $U_c$ 分别由秩为 $h_d$ 的低秩分解 $Q_cK_c^T$ 以及 $V_cO_c^T$ 近似。此时每个 token 的 KV cache 占用为 $2d$ (key and value).&lt;/p>
&lt;p>MQA 可以看作是 GQA 的一个特殊情况。对于 GQA 来说，我们有一个 group size $g\in[1, h]$, 当 $g=1$ 时，GQA 就是 MHA. 当 $g=h$ 时，GQA 就是 MQA, 通常 $g$ 满足 $h\ %\ g=0$. GQA 的表达式与 MHA 基本相同，只是多个 head 会共享一个 $K_c$ 以及 $V_c$. 此时，每个 token 的 KV cache 占用为 $2gh_d$. 对于 MQA，其每个 token 的 KV cache 占用为 $2h_d$.&lt;/p>
&lt;p>对于 MLA, 其表达式如下所示&lt;/p>
$$
\begin{aligned}
O &amp;= \sum_{c=1}^m\left(\sum_{j=1}^N\phi\left(\frac{xS_QQ_c(x_jS_KK_c)^T}{\sqrt{d}}\right)x_jS_VV_c\right)O_c^T\\
&amp;= \sum_{c=1}^m\left(\sum_{j=1}^N\phi\left(\frac{x(S_QQ_cK_c^TS_K^T)x_j^T}{\sqrt{d}}\right)x_jS_VV_cO_c^T\right)
\end{aligned}
$$&lt;p>其中，$S_Q,S_K,S_V\in\mathbb{R}^{d\times C}$ 在所有的 heads 中是共享的，$Q_c,K_c,V_c\in\mathbb{R}^{C\times h_d}$ 是每个 head 的 query, key, value projection layer 的参数， 是 latent factorization 的维度。与 FPBA 相比，我们可以看到，MLA 实际上是在 $d/m$ 个 head 上共享了参数，其中，$W_c$ 和 $U_c$ 分别由秩为 的低秩分解 $S_QQ_cK_c^TS_K^T$ 以及 $S_VV_cO_c^T$ 近似。尽管模型中 $C&amp;gt;h_d$, 但是最终的 rank 仍然是 $h_d$, 因此模型的表现也就受到了限制。&lt;/p>
&lt;h2 id="method">Method
&lt;/h2>&lt;p>对已有的 attention 分析之后，作者认为，要提高模型的表现，attention 需要做到亮点：&lt;/p>
&lt;ol>
&lt;li>最小化 KV cache 占用和参数量&lt;/li>
&lt;li>attention 的 capacity 尽可能接近 FPBA&lt;/li>
&lt;/ol>
&lt;p>基于这两个原则，作者提出了 MFA, MFA 主要依赖三个策略：&lt;/p>
&lt;ol>
&lt;li>提升 attention heads 的 head dimension, 通过提高 head dimension, 我们可以有效提高 attention head 的表达能力&lt;/li>
&lt;li>使用矩阵分解来降低参数量&lt;/li>
&lt;li>使用单一的 KV head 来降低 KV cache 内存占用&lt;/li>
&lt;/ol>
&lt;p>最终，MFA 的表达式如下所示&lt;/p>
$$
\begin{aligned}
O &amp;= \sum_{c=1}^m\left(\sum_{j=1}^N\phi\left(\frac{xS_QQ_c(x_jS_K)^T}{\sqrt{d}}\right)x_jS_V\right)O_c^T\\
&amp;= \sum_{c=1}^m\left(\sum_{j=1}^N\left(\frac{x(S_QQ_cS_K^T)x_j^T}{\sqrt{d}}\right)x_jS_VO_c^T\right)
\end{aligned}
$$&lt;p>其中 $S_Q,S_K,S_V\in\mathbb{R}^{d\times C}$ 是所有的 attention head 所共享的，$Q_c,O_c\in\mathbb{R}^{C\times C}$ 是每个 head 的 query up projection 和 output projection, $C$ 是 latent factorization 的维度。&lt;/p>
&lt;p>在 inference 的时候，由于我们只需要保存 $x_jS_K$ 和 $x_jS_V$, 因此所需要的 KV cache size 为 $2C$. 与 FPBA 相比，MFA 分别使用 $S_QQ_cS_K^T$ 和 $S_VO_c^T$ 来近似 $W_c$ 和 $U_c$, 近似矩阵的 rank 为 $C$. 由于 $C&amp;gt;d$, 因此其表达能力也更强，MFA 有如下优势：&lt;/p>
&lt;ol>
&lt;li>scalable head count: MFA 可以支持使用更多的 attention heads, 每增加一个 heads, 所需要的额外参数为 $2C^2$. 并且，增加 attention heads 个数不会增加 KV cache 占用&lt;/li>
&lt;li>enhanced head expressiveness: MFA 近似矩阵的 rank 为 $C&amp;gt;d$, 因此表达能力更强&lt;/li>
&lt;li>Compatibility with position encodings: MFA 可以无缝集成 position encoding.&lt;/li>
&lt;/ol>
&lt;p>为了进一步降低 MFA 的 KV cache 占用，作者提出了 MFA-Key-Reuse (MFA-KA). 核心思想是使用 $S_K$ 来表示 $S_V$, 这样可以额外降低 $50%$ 的 KV cache 占用，表示方法如下所示&lt;/p>
$$
S_V = S_K + \alpha\odot NS_K = (I +\mathrm{diag}(\alpha)N)S_K
$$&lt;p>其中 $N\in\mathbb{R}^{N\times N}$, $\alpha\in\mathbb{R}^C$.&lt;/p>
&lt;p>最终，MFA, MFA-KR 与 GQA 的对比如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-mfa/MFA-illustration.png"
width="1352"
height="632"
srcset="https://maosong2022.github.io/p/notes-on-mfa/MFA-illustration_hu7660720248060629286.png 480w, https://maosong2022.github.io/p/notes-on-mfa/MFA-illustration_hu8920417942560612274.png 1024w"
loading="lazy"
alt="Comparison of MFA with GQA"
class="gallery-image"
data-flex-grow="213"
data-flex-basis="513px"
>&lt;/p>
&lt;p>不同 attention 的量化对比如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Method&lt;/th>
&lt;th>KV Cache&lt;/th>
&lt;th>Parameter&lt;/th>
&lt;th>Heads&lt;/th>
&lt;th>Factor. rank per head&lt;/th>
&lt;th>Shared latent subspace Dim.&lt;/th>
&lt;th>Total effec. rank&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>FPBA&lt;/td>
&lt;td>$2d^2$&lt;/td>
&lt;td>$2d^3$&lt;/td>
&lt;td>$d$&lt;/td>
&lt;td>$d$&lt;/td>
&lt;td>$d$&lt;/td>
&lt;td>$d^2$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MHA&lt;/td>
&lt;td>$2d$&lt;/td>
&lt;td>$4d^2$&lt;/td>
&lt;td>$n$&lt;/td>
&lt;td>$h_d$&lt;/td>
&lt;td>$d$&lt;/td>
&lt;td>$nh_d$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MQA&lt;/td>
&lt;td>$2h_d$&lt;/td>
&lt;td>$(2 + 2/n)d^2$&lt;/td>
&lt;td>$n$&lt;/td>
&lt;td>$h_d$&lt;/td>
&lt;td>$h_d$&lt;/td>
&lt;td>$nh_d$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GQA&lt;/td>
&lt;td>$2gh_d$&lt;/td>
&lt;td>$(2 + 2g/n)d^2$&lt;/td>
&lt;td>$n$&lt;/td>
&lt;td>$h_d$&lt;/td>
&lt;td>$gh_d$&lt;/td>
&lt;td>$nh_d$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MLA&lt;/td>
&lt;td>$2C$&lt;/td>
&lt;td>$5dC + d^2$&lt;/td>
&lt;td>$m$&lt;/td>
&lt;td>$h_d$&lt;/td>
&lt;td>$C$&lt;/td>
&lt;td>$mh_d$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MFA&lt;/td>
&lt;td>$2C$&lt;/td>
&lt;td>$3Cd + 2mC^2$&lt;/td>
&lt;td>$m$&lt;/td>
&lt;td>$C$&lt;/td>
&lt;td>$C$&lt;/td>
&lt;td>$mC$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="code">Code
&lt;/h2>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Step3vAttention&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Step3VLConfig&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">layer_idx&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="fm">__init__&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">config&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">config&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">layer_idx&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">layer_idx&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">getattr&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;head_dim&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_attention_heads&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_key_value_heads&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">total_num_kv_heads&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_key_value_heads&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_attention_heads&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_attention_heads&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_key_value_groups&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_attention_heads&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_key_value_heads&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">getattr&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;share_q_dim&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">kv_size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_key_value_heads&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">scaling&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="o">**-&lt;/span>&lt;span class="mf">0.5&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">is_causal&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">True&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_size&lt;/span> &lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">k_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">v_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">o_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_attention_heads&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># query down projection normalization&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">inter_norm&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Step3vRMSNorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">eps&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">rms_norm_eps&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># query up projection&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">wq&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_attention_heads&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">position_embeddings&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Tuple&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attention_mask&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">past_key_value&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">Cache&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">cache_position&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">LongTensor&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">**&lt;/span>&lt;span class="n">kwargs&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Unpack&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">FlashAttentionKwargs&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">Tuple&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">Tuple&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">]]]:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">input_shape&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[:&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">query_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">key_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">k_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">input_shape&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">))&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transpose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">value_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">v_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">input_shape&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">))&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transpose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">query_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">inter_norm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">query_states&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">query_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">wq&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">query_states&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">input_shape&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">))&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transpose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">cos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sin&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">position_embeddings&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">query_states&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">key_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">apply_rotary_pos_emb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">query_states&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">key_states&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">cos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sin&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">...&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 MFA 以及 MFA-KR, 一个在 KV cache 有限的条件下最大限度提高 attention 表达能力的 attention 机制。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2412.19255" target="_blank" rel="noopener"
>Multi-matrix Factorization Attention&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://huggingface.co/stepfun-ai/step3/blob/main/modeling_step3.py" target="_blank" rel="noopener"
>Step v3 code&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on MX-format</title><link>https://maosong2022.github.io/p/notes-on-mx-format/</link><pubDate>Thu, 21 Aug 2025 18:23:03 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-mx-format/</guid><description>&lt;p>MX format 是一个表示数据的数据格式，在 LLM 中主要用于量化。相比于直接对整个张量进行量化，MX format 可以在更细粒度的层面控制量化，从而提高模型的表现&lt;/p>
&lt;h2 id="microscaling">Microscaling
&lt;/h2>&lt;p>Microscaling (MS) format 如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-mx-format/MX-format-illustration.png"
width="1048"
height="490"
srcset="https://maosong2022.github.io/p/notes-on-mx-format/MX-format-illustration_hu5964344889815972436.png 480w, https://maosong2022.github.io/p/notes-on-mx-format/MX-format-illustration_hu10722800666061163847.png 1024w"
loading="lazy"
alt="illustration of MX format"
class="gallery-image"
data-flex-grow="213"
data-flex-basis="513px"
>&lt;/p>
&lt;p>MX format 包括三个部分：&lt;/p>
&lt;ol>
&lt;li>elements $P_1,\dots,P_k$ 未 scale 的数据，要求 $P_1,\dots,P_k$ 的数据类型相同&lt;/li>
&lt;li>shared scale $X$, 对 element 进行的 scale 参数，所有的 $k$ 个 bits 共享一个 $X$&lt;/li>
&lt;li>block size, 决定 element block 的大小&lt;/li>
&lt;/ol>
&lt;p>在存储时，我们只需要存储 $X$ 以及 $P_1,\dots,P_k$, 我们假设 $X$ 需要 $w$ bits 来表示，$P_i$ 需要 $d$ bits 来表示，则我们一共需要 $w+kd$ bits 来表示这 $k$ 个元素。&lt;/p>
&lt;h2 id="concrete-mx-compliant-formats">Concrete MX-compliant Formats
&lt;/h2>&lt;p>MX-format 包含了一下几种数据格式：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Format Name&lt;/th>
&lt;th>Element Data Type&lt;/th>
&lt;th>Element Bits(d)&lt;/th>
&lt;th>Scaling Block Size(k)&lt;/th>
&lt;th>Scale Data Type&lt;/th>
&lt;th>Scale Bits(w)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>MXFP8&lt;/td>
&lt;td>FP8 (E5M2)&lt;/td>
&lt;td>8&lt;/td>
&lt;td>32&lt;/td>
&lt;td>E8M0&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MXFP8&lt;/td>
&lt;td>FP8 (E4M3)&lt;/td>
&lt;td>8&lt;/td>
&lt;td>32&lt;/td>
&lt;td>E8M0&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MXFP6&lt;/td>
&lt;td>FP6 (E3M2)&lt;/td>
&lt;td>6&lt;/td>
&lt;td>32&lt;/td>
&lt;td>E8M0&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MXFP6&lt;/td>
&lt;td>FP6 (E2M3)&lt;/td>
&lt;td>6&lt;/td>
&lt;td>32&lt;/td>
&lt;td>E8M0&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MXFP4&lt;/td>
&lt;td>FP4 (E2M1)&lt;/td>
&lt;td>4&lt;/td>
&lt;td>32&lt;/td>
&lt;td>E8M0&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MXINT8&lt;/td>
&lt;td>INT8&lt;/td>
&lt;td>8&lt;/td>
&lt;td>32&lt;/td>
&lt;td>E8M0&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="gpt-oss-quantization">GPT-oss Quantization
&lt;/h2>&lt;p>&lt;a class="link" href="https://maosong.website/p/notes-on-gpt-oss/" target="_blank" rel="noopener"
>gpt-oss&lt;/a> 中使用了 MXFP4 来表示 MoE 中的 down projection 以及 up projection weight matrix 的权重。&lt;/p>
&lt;p>其具体操作过程如下：&lt;/p>
&lt;ol>
&lt;li>我们将参数分为大小为 32 的 block&lt;/li>
&lt;li>每个 block 由一个 scale $X$ 来表示，其精度为 E8M0, 即 8bits, 表示范围为 $[-127,127]$, 以及 $32$ 个元素 $P_i$ 来表示，每个元素的精度为 E2M1, 即 4bits, 表示范围为 $[-6.0,6.0]$.&lt;/li>
&lt;li>由于每个元素由 4bits 来表示，因此我们将两个元素合并在一起来表示&lt;/li>
&lt;/ol>
&lt;p>在加载时，我们可以用如下代码来恢复 $P_i$ 的值到 FP8&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">FP4_VALUES&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mf">0.0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mf">0.5&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mf">1.0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mf">1.5&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mf">2.0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mf">3.0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mf">4.0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mf">6.0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mf">0.0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mf">0.5&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mf">1.0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mf">1.5&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mf">2.0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mf">3.0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mf">4.0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mf">6.0&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">convert_moe_packed_tensors&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">blocks&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">scales&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">*&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dtype&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dtype&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">bfloat16&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">rows_per_chunk&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">32768&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">1024&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kn">import&lt;/span> &lt;span class="nn">math&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># scales are represented with uini8&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">scales&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">scales&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">int32&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">127&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">assert&lt;/span> &lt;span class="n">blocks&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[:&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">scales&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">blocks&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="si">=}&lt;/span>&lt;span class="s2"> does not match &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">scales&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="si">=}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">lut&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">FP4_VALUES&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">dtype&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">device&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">blocks&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">device&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">*&lt;/span>&lt;span class="n">prefix_shape&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">G&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">B&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">blocks&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">rows_total&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">math&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">prod&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">prefix_shape&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">G&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">blocks&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">blocks&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reshape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">rows_total&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">B&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">scales&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">scales&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reshape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">rows_total&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># each byte representing 2 elements and represented with unit8&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">out&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">empty&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">rows_total&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">B&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">dtype&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">device&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">blocks&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">device&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">r0&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">rows_total&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">rows_per_chunk&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">r1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">r0&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">rows_per_chunk&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">rows_total&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">blk&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">blocks&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">r0&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">r1&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">exp&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">scales&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">r0&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">r1&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># nibble indices -&amp;gt; int64&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">idx_lo&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">blk&lt;/span> &lt;span class="o">&amp;amp;&lt;/span> &lt;span class="mh">0x0F&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">long&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">idx_hi&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">blk&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="mi">4&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">long&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sub&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">out&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">r0&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">r1&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sub&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">::&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">lut&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">idx_lo&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sub&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">::&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">lut&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">idx_hi&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ldexp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">sub&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">exp&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">out&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">sub&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">del&lt;/span> &lt;span class="n">idx_lo&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">idx_hi&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">blk&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">exp&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">out&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">out&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reshape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">prefix_shape&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">G&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">B&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">prefix_shape&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">G&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">B&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># to match for now existing implementation&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">out&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float8_e5m2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://github.com/huggingface/transformers/blob/242bb2cafccec9f90479f5f688bca9d240b1031f/src/transformers/models/gpt_oss/convert_gpt_oss_weights_to_hf.py#L78" target="_blank" rel="noopener"
>gpt-oss code&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf#page=4.11" target="_blank" rel="noopener"
>report&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on flashattention</title><link>https://maosong2022.github.io/p/notes-on-flashattention/</link><pubDate>Thu, 21 Aug 2025 11:32:53 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-flashattention/</guid><description>&lt;p>作者提出了 flashattention, 一个通过降低 multi head attention 内存访问开销来提高 attention 计算效率的方法&lt;/p>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>Transformer 的 attention 是一个平方度复杂度的算法，这个平方复杂度既体现在时间复杂度上（矩阵乘法），也体现在空间复杂度上（需要存储中间结果）。因此，要降低 attention 的复杂度，我们有两种思路：&lt;/p>
&lt;ol>
&lt;li>从时间复杂度上入手，比如使用稀疏 attention 机制或者线性注意力机制&lt;/li>
&lt;li>从空间复杂度上入手，比如使用 &lt;a class="link" href="https://maosong.website/p/notes-on-gqa/" target="_blank" rel="noopener"
>GQA&lt;/a>, &lt;a class="link" href="https://maosong.website/p/notes-on-mqa/" target="_blank" rel="noopener"
>MQA&lt;/a> 等减少内存的访问开销&lt;/li>
&lt;/ol>
&lt;p>本文提出的 flashattention 就属于降低空间复杂度的一种做法。作者认为，我们应该设计一种 &lt;strong>IO-aware&lt;/strong> 的 attention 算法，来减少 attention 计算式的内存访问开销，进而提高 attention 的计算效率。&lt;/p>
&lt;p>作者首先提到，一个未解决的问题就是：&lt;/p>
&lt;blockquote>
&lt;p>降低 attention 的内存访问开销是否可以提高 attention 的计算效率？&lt;/p>
&lt;/blockquote>
&lt;p>作者发现，已有的一些工作虽然在理论上降低了 attention 的计算效率，但是在实际中，他们的效果并没有提升太多。作者分析原因认为，已有工作主要关注于降低 FLOPs, 但是忽略了内存访问开销。&lt;/p>
&lt;p>因此，作者在本文中就提出了 flashattention, 一个 IO-aware 的 attention 算法，作者通过尽可能降低内存访问开销来提高模型的计算效率。具体做法就是，避免从内存中读写 attention matrix, 作者认为这个目标有两个挑战：&lt;/p>
&lt;ol>
&lt;li>计算 softmax 的时候不访问所有的输入&lt;/li>
&lt;li>在反向传播时不存储中间的 attention matrix&lt;/li>
&lt;/ol>
&lt;p>作者提出了两个方法来分别解决这两个问题：&lt;/p>
&lt;ol>
&lt;li>作者使用了 &lt;strong>tiling&lt;/strong> 技巧，将 input 分成多个 block, 然后分别进行处理，进而降低 softmax 的内存访问开销&lt;/li>
&lt;li>作者使用了 &lt;strong>recompute&lt;/strong> 技巧，在反向传播时，重新计算 softmax normalization factor&lt;/li>
&lt;/ol>
&lt;p>通过这些改进，我们可以让 attention 运行更快，并且降低内存访问开销。&lt;/p>
&lt;p>作者还从理论上分析了 flashattention 的复杂度，提供了理论基础。&lt;/p>
&lt;p>作者通过实验验证了 flashattention 的有效性，主要是三点：&lt;/p>
&lt;ol>
&lt;li>训练效率更高：相比于 Huggingface 和 Megatron, flashattention 的训练效率提升了 2-3 倍&lt;/li>
&lt;li>模型的表现更好：相比于 GPT-2, 模型的 perplexity 提升了 0.7 个点左右&lt;/li>
&lt;li>速度更快：flashattention 比标准的 attention 实现快 3 倍以上&lt;/li>
&lt;/ol>
&lt;h2 id="background">Background
&lt;/h2>&lt;h3 id="hardware-performance">Hardware Performance
&lt;/h3>&lt;p>作者首先介绍了以下 GPU 的内存架构，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-flashattention/flashattention-GPU-hierarchy.png"
width="430"
height="383"
srcset="https://maosong2022.github.io/p/notes-on-flashattention/flashattention-GPU-hierarchy_hu12057169839763299901.png 480w, https://maosong2022.github.io/p/notes-on-flashattention/flashattention-GPU-hierarchy_hu4456192002702243811.png 1024w"
loading="lazy"
alt="Memory Hierarchy of GPU"
class="gallery-image"
data-flex-grow="112"
data-flex-basis="269px"
>&lt;/p>
&lt;p>可以看到，GPU 内存可以分为三个层级：&lt;/p>
&lt;ol>
&lt;li>SRAM: GPU 的寄存器，容量小，但是访问速度极快&lt;/li>
&lt;li>High bandwith memory (HBM): GPU 的高速内存，访问速度较快，容量中等&lt;/li>
&lt;li>DRAM: CPU 内存，容量最大，但是访问速度较慢&lt;/li>
&lt;/ol>
&lt;p>接下来作者介绍了 Execution model 的概念，GPU 有多个线程来执行同一个操作（SPMD），这个操作也被称为 kernel, kernel 会从 HBM 中加载输入到 SRAM 中进行计算，然后写回 HBM.&lt;/p>
&lt;p>对一个算法，我们可以将其归类为 compute-bound 和 memory-bound 两类， 我们可以用 arithmetic intensity 来进行区分，arithmetic intensity 定义为 arithmetic operations 与 memory access 的比率。&lt;/p>
&lt;ol>
&lt;li>compute bound: 算法的瓶颈在于算力，由于算力不足导致运行时间慢，比如矩阵乘法&lt;/li>
&lt;li>memory-bound: 算法的瓶颈在于内存访问效率，比如 element-wise 操作或者是 reduction&lt;/li>
&lt;/ol>
&lt;p>为了提高 memory-bound 类型算法的效率，我们进行 kernel fusion, 即把多个访问同一片内存的操作放在一起处理，避免多次读写内存&lt;/p>
&lt;h3 id="standard-attention-implementation">Standard Attention Implementation
&lt;/h3>&lt;p>作者还回顾了一下标准化的 attention 实现。&lt;/p>
&lt;h4 id="forward-pass">Forward Pass
&lt;/h4>&lt;p>给定 $Q,K,V\in\mathbb{R}^{N\times d}$, 其中 $N$ 是序列长度, $d$ 是 head dimension, attention 的定义如下&lt;/p>
$$
S = QK^T\in\mathbb{R}^{N\times N},\quad P = \mathrm{softmax}(S)\in\mathbb{R}^{N\times N},\quad O = PV\in\mathbb{R}^{N\times d}
$$&lt;p>这里 $\mathrm{softmax}$ 是逐行计算的。&lt;/p>
&lt;p>算法的执行过程如下&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-flashattention/flashattention-standard-attention-implementation.png"
width="1022"
height="251"
srcset="https://maosong2022.github.io/p/notes-on-flashattention/flashattention-standard-attention-implementation_hu6197068896169413721.png 480w, https://maosong2022.github.io/p/notes-on-flashattention/flashattention-standard-attention-implementation_hu2691770135753182065.png 1024w"
loading="lazy"
alt="Algorithm 0: Standard Attention Implementation"
class="gallery-image"
data-flex-grow="407"
data-flex-basis="977px"
>&lt;/p>
&lt;p>我们有第一个结论&lt;/p>
&lt;blockquote>
&lt;p>Proposition 1
标准化 attention 前向传播时访问 HBM 的内存访问开销为 $\mathcal{O}(Nd+N^2)$.&lt;/p>
&lt;/blockquote>
&lt;p>证明：对于 attention, 我们需要从 HBM 中加载 $Q,K,V\in\mathbb{R}^{N\times d}$, 然后输出 $O\in\mathbb{R}^{N\times d}$ 并保存到内存中。&lt;/p>
&lt;p>首先我们需要计算 $S = QK^T$, 这一步需要加载 $Q,K$ 并将 $S$ 保存到 HBM 中，内存访问量为 $\mathcal{O}(Nd+N^2)$.&lt;/p>
&lt;p>接下来，我们需要计算 $P = \mathrm{softmax}(S)$, 这一步需要加载 $S$ 然后将 $P$ 保存到 HBM 中，内存访问量为 $\mathcal{O}(N^2)$.&lt;/p>
&lt;p>最后，我们需要计算 $O = PV$, 这一步需要加载 $P$ 和 $V$ 然后将 $O$ 保存到 HBM 中，内存访问量为 $\mathcal{O}(Nd+N^2)$.&lt;/p>
&lt;p>总的来说，标准化 attention 的内存访问开销为 $\mathcal{O}(Nd+N^2)$.&lt;/p>
&lt;h4 id="backward-pass">Backward Pass
&lt;/h4>&lt;p>标准 attention 反向传播过程如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-flashattention/flashattention-standard-attention-backward-pass.png"
width="1187"
height="281"
srcset="https://maosong2022.github.io/p/notes-on-flashattention/flashattention-standard-attention-backward-pass_hu9714355826535941759.png 480w, https://maosong2022.github.io/p/notes-on-flashattention/flashattention-standard-attention-backward-pass_hu3333822996667130018.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="422"
data-flex-basis="1013px"
>&lt;/p>
&lt;blockquote>
&lt;p>Proposition 2
标准化 attention 反向传播时访问 HBM 的内存访问开销为 $\mathcal{O}(Nd+N^2)$.&lt;/p>
&lt;/blockquote>
&lt;p>证明：对于标准化 attention 的反向传播，我们需要从 HBM 中加载 $Q,K,V,dO\in\mathbb{R}^{N\times d}$ , 然后输出 $dQ,dK,dV$ 并保存到 HBM 中。&lt;/p>
&lt;p>首先我们计算 $dV=P^TdO$, 这一步需要加载 $P,dO$ 并将 $dV$ 保存到 HBM 中，内存访问开销为 $\mathcal{O}(Nd+N^2)$.&lt;/p>
&lt;p>接下来我们计算 $dP=dOV^T$, 这一步需要加载 $dO, V$ 并将 $dP$ 保存到 HBM 中，内存访问开销为 $\mathcal{O}(Nd)$.&lt;/p>
&lt;p>然后我们计算 $dS$, 这一步需要加载 $P$ 并将 $dS$ 保存到 HBM 中，内存访问开销为 $\mathcal{O}(N^2)$.&lt;/p>
&lt;p>对于 $dQ$ 和 $dK$ 的计算，内存访问开销都是 $\mathcal{O}(Nd+N^2)$.&lt;/p>
&lt;p>因此，标准化 attention 的内存访问开销为 $\mathcal{O}(Nd+N^2)$.&lt;/p>
&lt;h2 id="method">Method
&lt;/h2>&lt;p>作者在本节首先介绍了 flashattention 算法，然后作者证明了 flashattention 的正确性以及分析了复杂度。最后作者对 flashattention 进行扩展得到了 Block-sparse Flashattention.&lt;/p>
&lt;h3 id="flashattention">Flashattention
&lt;/h3>&lt;p>attention 模块的输入是 $Q,K,V\in\mathbb{R}^{N\times d}$, 输出是 $O\in\mathbb{R}^{N\times d}$, 作者的目标是减少计算过程中的 HBM 访问次数&lt;/p>
&lt;p>作者分别使用了 tiling 和 recomputation 来解决 attention 前向传播和反向传播中的内存访问开销。flashattention 的核心思想是，我们将 $Q,K,V$ 分割成 block, 然后在 block 层面进行加载和计算。&lt;/p>
&lt;h4 id="tiling">Tiling
&lt;/h4>&lt;p>首先作者介绍了一下如何使用 tiling 来计算 softmax.&lt;/p>
&lt;p>给定一个向量 $x\in\mathbb{R}^{B}$, 其 softmax 计算方式如下&lt;/p>
$$
m(x) = \max_i x_i,\ f(x) = [e^{x_1-m(x)},\dots,e^{x_B-m(x)}], \ \ell(x)=\sum_if(x)_i, \ \mathrm{softmax}(x) = \frac{f(x)}{\ell(x)}
$$&lt;p>如果我们现在有两个向量 $x^{(1)}, x^{(2)}\in\mathbb{R}^{B}$, 记 $x=[x^{(1)}, x^{(2)}]^T\in\mathbb{R}^{2B}$, 我们可以将 $\mathrm{softmax}(x)$ 的计算分解为&lt;/p>
$$
\begin{aligned}
m(x) &amp;= \max(m(x^{(1)}), m(x^{(2)}))， f(x) = [e^{m(x^{(1)})-m(x)}f(x^{(1)}),e^{m(x^{(2)})-m(x)}f(x^{(2)})]\\
\ell(x) &amp;= e^{m(x^{(1)})-m(x)}\ell(x^{(1)}) + e^{m(x^{(2)})-m(x)}\ell(x^{(2)}), \mathrm{softmax}(x) = \frac{f(x)}{\ell(x)}
\end{aligned}
$$&lt;p>因此，如果我们额外记录 $m(x)$ 以及 $\ell(x)$ 这两个量，那么我们可以每次仅计算 softmax 的一个 block&lt;/p>
&lt;h4 id="recomputation">Recomputation
&lt;/h4>&lt;p>在反向传播过程中，一般我们需要存储 $S,P\in\mathbb{R}^{N\times N}$, 需要的空间复杂度为 $\mathcal{O}(N^2)$. 但是，通过存储 $O\in\mathbb{R}^{N\times d}$ 以及 $(m,\ell)$, 我们可以避免重新计算 $S,P$,这可以看做是 gradient checkpointing. 但是与 checkpointing 相比，因为 flashattention 减少了内存访问开销，因此其反向过程并没有变得更慢。&lt;/p>
&lt;h4 id="algorithm">Algorithm
&lt;/h4>&lt;p>最终，flashattention 的算法如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-flashattention/flashattention-algorithm.png"
width="1207"
height="700"
srcset="https://maosong2022.github.io/p/notes-on-flashattention/flashattention-algorithm_hu14953009943583888973.png 480w, https://maosong2022.github.io/p/notes-on-flashattention/flashattention-algorithm_hu411171794513571213.png 1024w"
loading="lazy"
alt="Flashattention algorithm"
class="gallery-image"
data-flex-grow="172"
data-flex-basis="413px"
>&lt;/p>
&lt;h3 id="analysis">Analysis
&lt;/h3>&lt;h4 id="correctness">Correctness
&lt;/h4>&lt;p>算法的正确性由定理 1 给出&lt;/p>
&lt;blockquote>
&lt;p>Theorem 1
flashattention (即算法 1) 输出 $O=\mathrm{softmax}(QK^T)V$, 其时间复杂度为 $\mathcal{O}(N^2d)$, 空间复杂度为 $\mathcal{O}(N)$.&lt;/p>
&lt;/blockquote>
&lt;p>证明：时间复杂度主要由矩阵乘法决定。在计算 $S_{ij}=Q_iK_j^T$ 时，所花费的 FLOPS 为 $\mathcal{O}(B_rB_cd)$. 在计算 $\tilde{P}_{ij}V_j$ 时，所花费的 FLOPS 为 $\mathcal{O}(B_rB_cd)$. 循环一共执行了&lt;/p>
$$
T_cT_r = \left\lceil\frac{N}{B_c}\right\rceil\left\lceil\frac{N}{B_r}\right\rceil
$$&lt;p>从而总的 FLOPS 为&lt;/p>
$$
\mathcal{O}\left(\frac{N^2}{B_rB_c}B_rB_cd\right) = \mathcal{O}(N^2d)
$$&lt;p>在 flashattention 的计算过程中，我们只需要保存 $(\ell, m)$ 即可，因此需要的额外内存空间为 $\mathcal{O}(N)$.&lt;/p>
&lt;p>接下来，我们可以证明 flashattention 的正确性，我们使用归纳法来证明。令 $j$ 满足 $0\leq j\leq T_c$, $K_{:j}\in\mathbb{R}^{jB_c\times d}$, $V_{:j}\in\mathbb{R}^{jB_c\times d}$ 分别为 $K$ 和 $V$ 的前 $jB_c$ 行。 $S_{:, :j}=QK_{:j}^T\in\mathbb{R}^{N\times jB_c}$, $P_{:,:j}=\mathrm{softmax}(S_{:,:j})\in\mathbb{R}^{N\times jB_c}$, $m^{(j)}, \ell^{(j)}, O^{(j)}$ 分别为 $m,\ell, O$ 的第 $j$ 个元素。我们证明经过第 $j$ 次迭代后，HBM 中保存的是&lt;/p>
$$
m^{(j)}=\mathrm{rowmax}(S_{:,:j})\in\mathbb{R}^N, \ell^{(j)}=\mathrm{rowsum}(\exp(S_{:,:j}-m^{(j)}))\in\mathbb{R}^N, O^{(j)} = P_{:,:j}V_{:j}\in\mathbb{R}^{N\times d}
$$&lt;p>当 $j=0$ 时，上面的结果显然成立。现在我们假设对某个 $j=0,\dots, T_c-1$ 上面的结果成立，我们需要证明对 $j+1$ 也成立。&lt;/p>
&lt;p>首先&lt;/p>
$$
m^{(j+1)}=\max(m^{(j)}， \tilde{m}) = \max(\mathrm{rowmax}(S_{:,:j}), \mathrm{rowmax}(S_{:,j:j+1}))=\mathrm{rowmax}(S_{:,:j+1})
$$&lt;p>接下来&lt;/p>
$$
\begin{aligned}
\ell^{(j+1)} &amp;= \exp(m^{(j)}-m^{(j+1)})\ell^{(j)} + \exp(\tilde{m}-m^{(j+1)})\tilde{\ell}\\
&amp;=\exp(m^{(j)}-m^{(j+1)})\mathrm{rowsum}(\exp(S_{:,:j}-m^{(j)})) + \exp(\tilde{m}-m^{(j+1)})\mathrm{rowsum}(\exp(S_{:,j:j+1}-\tilde{m}))\\
&amp;= \mathrm{rowsum}(\exp(S_{:,:j}-m^{(j+1)})) + \mathrm{rowsum}(\exp(S_{:,j:j+1}-m^{(j+1)}))\\
&amp;= \mathrm{rowsum}(\exp(S_{:,:j+1}-m^{(j+1)}))
\end{aligned}
$$&lt;p>最后，我们计算 $O^{(j+1)}$ 得到：&lt;/p>
$$
\begin{aligned}
O^{(j+1)} &amp;= \mathrm{diag}(\ell^{(j+1)})^{-1}(\mathrm{diag}(\ell^{(j)})\exp(m^{(j)}-m^{(j+1)})O^{(j)}+\exp(\tilde{m}-m^{(j+1)})\exp(S_{:,j:j+1}-\tilde{m})V_{:,j:j+1})\\
&amp;= \mathrm{diag}(\ell^{(j+1)})^{-1}(\mathrm{diag}(\ell^{(j)})\exp(m^{(j)}-m^{(j+1)})P_{:,:j}V_{:,:j}+\exp(-m^{(j+1)})\exp(S_{:,j:j+1})V_{:,j:j+1})\\
&amp;= \mathrm{diag}(\ell^{(j+1)})^{-1}(\mathrm{diag}(\ell^{(j)})\exp(m^{(j)}-m^{(j+1)})\mathrm{diag}(\ell^{(j)})^{-1}\exp(S_{:,:j}-m^{(j)})V_{:,:j}+\exp(-m^{(j+1)})\exp(S_{:,j:j+1})V_{:,j:j+1})\\
&amp;= \mathrm{diag}(\ell^{(j+1)})^{-1}(\exp(-m^{(j+1)})\exp(S_{:,:j}))V_{:,:j}+\exp(-m^{(j+1)})\exp(S_{:,j:j+1})V_{:,j:j+1})\\
&amp;= \mathrm{diag}(\ell^{(j+1)})^{-1}(
\begin{bmatrix}
\exp(S_{:,:j}-m^{(j+1)}) &amp; \exp(S_{:,:j}-m^{(j+1)})
\end{bmatrix}\begin{bmatrix}
V_{:,:j} \\
V_{:,j:j+1}
\end{bmatrix}\\
&amp;= \mathrm{softmax}(S_{:,:j+1})V_{:,:j+1}
\end{aligned}
$$&lt;p>因此上面的结果对 $j+1$ 也成立，从而 flashattention 的结果对 $j=0,\dots,T_c$ 都成立。&lt;/p>
&lt;h4 id="forward-pass-1">Forward Pass
&lt;/h4>&lt;p>第一个问题是如何提高 softmax 计算的效率，作者的做法先先计算 normalization constant 然后再分别计算不同的 column.&lt;/p>
&lt;p>给定 $Q,K,V\in\mathbb{R}^{N\times d}$, 其中 $N$ 是序列长度, $d$ 是 head dimension, attention 的定义如下&lt;/p>
$$
S = QK^T\in\mathbb{R}^{N\times N},\quad P = \mathrm{softmax}(S)\in\mathbb{R}^{N\times N},\quad O = PV\in\mathbb{R}^{N\times d}
$$&lt;p>我们有 $S_{ij}=q_i^Tk_j$, 这里 $q_i$ 和 $k_j$ 分别是 $Q$ 和 $K$ 的第 $i$ 列以及第 $j$ 列， normalization constant 定义为：&lt;/p>
$$
L_i = \sum_{j=1}^N \exp\left(q_i^Tk_j\right)
$$&lt;p>对任意 $i$, 计算 $L_i$ 只需要 $\mathcal{O}(N)$ 的空间复杂度。&lt;/p>
&lt;p>令 $v_j$ 是 $V$ 的第 $i$ 列，则输出 $O$ 的第 $i$ 列 $o_i$ 为&lt;/p>
$$
o_i = P_{i:}V = \sum_{j=1}^N P_{ij}v_j = \sum_{j=1}^N\frac{\exp(q_i^Tk_j)}{L_i}v_j
$$&lt;p>这个过程中，对任意 $i$, 计算 $o_i$ 也只需要 $\mathcal{O}(N)$ 的空间复杂度。&lt;/p>
&lt;p>因此，在 $L_i$ 已经计算好的情况下，我们可以在 $\mathcal{O}(N)$ 的空间复杂度下计算 $o_i$.&lt;/p>
&lt;p>最终，flashattention 的 forward pass 过程如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-flashattention/flashattention-forward-pass-algorithm.png"
width="1207"
height="847"
srcset="https://maosong2022.github.io/p/notes-on-flashattention/flashattention-forward-pass-algorithm_hu4926938026953834442.png 480w, https://maosong2022.github.io/p/notes-on-flashattention/flashattention-forward-pass-algorithm_hu9357899049073594485.png 1024w"
loading="lazy"
alt="flashattention forward pass"
class="gallery-image"
data-flex-grow="142"
data-flex-basis="342px"
>&lt;/p>
&lt;p>接下来，作者分析了 flashattention 的内存访问开销。结论如下&lt;/p>
&lt;blockquote>
&lt;p>Theorem 2
令 $N$ 为 sequence length, $d$ 为 head dimension, $M$ 是 SRAM 的 size, 且满足 $d\leq M\leq Nd$. 则 flashattention 前向传播的内存访问开销为 $\Theta(N^2d^2M^{-1})$.&lt;/p>
&lt;/blockquote>
&lt;p>证明：由 Algorithm 1（或者 Algorithm 2）可以知道，$K$ 和 $V$ 的每一个元素都只需要从 HBM 中加载一次，而每一次外层循环都会从 HBM 中加载一次 $O$ 和 $Q$, 因此总的 HBM 访问次数为 $\mathcal{O}(Nd+NdT_c)=\mathcal{O}(NdT_c)$.&lt;/p>
&lt;p>接下来，我们给出每一次内层循环的内存访问开销，这是由 SRAM 的大小决定的。由于我们需要 SRAM 可以存储 $K_j\in\mathbb{R}^{B_c\times d}$ 以及 $V_j\in\mathbb{R}^{B_c\times d}$ ，我们的 block size 需要满足&lt;/p>
$$
B_cd = \mathcal{O}(M) \Rightarrow B_c = \mathcal{O}\left(\frac{M}{d}\right)
$$&lt;p>同理，对于 $O$ 和 $Q$, 我们有&lt;/p>
$$
B_rd = \mathcal{O}(M) \Rightarrow B_r = \mathcal{O}\left(\frac{M}{d}\right)
$$&lt;p>最后，我们还需要 SRAM 可以存储 $S_{ij}\in\mathbb{R}^{B_r\times B_c}$, 因此&lt;/p>
$$
B_rB_c=\mathcal{O}(M)
$$&lt;p>这样，&lt;/p>
$$
B_c = \mathcal{O}\left(\frac{M}{d}\right), B_r=\mathcal{O}\left(\min\left(\frac{M}{d},\frac{M}{B_c}\right)\right)=\mathcal{O}\left(\min\left(\frac{M}{d},d\right)\right)
$$&lt;p>从而&lt;/p>
$$
T_c = \frac{N}{B_c} = \mathcal{O}\left(\frac{Nd}{M}\right)
$$&lt;p>最终，总的内存访问开销为&lt;/p>
$$
\mathcal{O}(NdT_c) = \mathcal{O}\left(\frac{N^2d^2}{M}\right)
$$&lt;p>一般来说, $d$ 的大小为 $64-128$ （见 &lt;a class="link" href="MoE%20overview.md" >MoE overview&lt;/a>）, $M$ 的大小为 $100 KB$ 左右, $d^2&amp;laquo; M, 因此 flashattention 的内存访问开销远小于标准化 attention 的内存访问开销。&lt;/p>
&lt;p>作者还证明 flashattention 的内存访问开销是一个下界，即&lt;/p>
&lt;blockquote>
&lt;p>Proposition 3
令 $N$ 为 sequence length, $d$ 为 head dimension, $M$ 是 SRAM 的 size, 且满足 $d\leq M\leq Nd$. 则不存在一个对任意 $M\in[d,Nd]$ 都可以在 内存访问开销为 $\Theta(N^2d^2M^{-1})$ 的条件下完成 attention 计算的算法。&lt;/p>
&lt;/blockquote>
&lt;p>证明可以用反证法，基本思想是加载 $Q,K,V$ 的 HBM 访问次数至少为 $\mathcal{O}(Nd)$.&lt;/p>
&lt;h4 id="backward-pass-1">Backward Pass
&lt;/h4>&lt;p>第二个问题是能否在线性空间复杂度下计算 attention 的反向传播过程。&lt;/p>
&lt;p>首先我们记损失函数为 $\phi$, 然后令 $\phi$ 对 $O,Q,K,V$ 的梯度分别为 $dO,dQ,dK, dV\in\mathbb{R}^{N\times d}$, 我们的目标是计算 $dQ, dK, dV$.&lt;/p>
&lt;p>$dV$ 的计算是最容易的，我们有 $dV=P^TdO$, 因此&lt;/p>
$$
dv_j = \sum_{i=1}^N P_{ij}do_i = \sum_{i=1}^N\frac{\exp(q_i^Tk_j)}{L_i}do_i
$$&lt;p>由于我们已经计算了 $L_i$, 因此，$dv_j$ 只需要 $\mathcal{O}(d)$ 的空间复杂度。&lt;/p>
&lt;p>接下来，注意到 $dP=dOV^T$, 因此我们有&lt;/p>
$$
dP_{ij} = do_i^Tv_j
$$&lt;p>计算的空间复杂度也是要 $\mathcal{O}(N)$ 的&lt;/p>
&lt;p>注意到 $P_{i:}=\mathrm{softmax}(s_{i:})$, 且 $y=\mathrm{softmax}(x)$ 的 Jacobian 是 $\mathrm{diag}(y)-yy^T$ (推导过程见 &lt;a class="link" href="softmax.md" >softmax&lt;/a>), 我们有&lt;/p>
$$
dS_{i:} = (\mathrm{diag}(P_{i:})-P_{i:}P_{i:}^T)dP_{i:} = P_{i:} \odot dP_{i:} - (P_{i:}^TdP_{i:})P_{i:}
$$&lt;p>我们定义&lt;/p>
$$
D_i = P_{i:}^TdP_{i:}= \sum_{j=1}^N\frac{\exp(q_i^Tk_j)}{L_i}do_i^Tv_j = do_i^T\sum_{j=1}^N\frac{\exp(q_i^Tk_j)}{L_i}v_j = do_i^To_i
$$&lt;p>$D_i$ 的空间复杂度也只需要 $\mathcal{O}(N)$.&lt;/p>
&lt;p>则&lt;/p>
$$
dS_{i:} =P_{i:} \odot dP_{i:} - D_iP_{i:}
$$&lt;p>我们有&lt;/p>
$$
dS_{ij} = P_{ij}dP_{ij} - D_iP_{ij} = P_{ij}(dP_{ij}-D_i)
$$&lt;p>注意到 $S_{ij}=q_i^Tk_j$, 我们有&lt;/p>
$$
dq_i = \sum_{j=1}^N dS_{ij}k_j = \sum_{j=1}^NP_{ij}(dP_{ij}-D_i)k_j = \sum_{j=1}^N\frac{\exp(q_i^Tk_j)}{L_i}(do_i^Tv_j-D_i)k_j
$$&lt;p>因此计算 $dq_i$ 的空间复杂度为 $\mathcal{O}(d)$.&lt;/p>
&lt;p>同样的，&lt;/p>
$$
dk_j = \sum_{j=1}^N dS_{ij}q_i = \sum_{j=1}^NP_{ij}(dP_{ij}-D_i)q_i = \sum_{j=1}^N\frac{\exp(q_i^Tk_j)}{L_i}(do_i^Tv_j-D_i)q_i
$$&lt;p>其空间复杂度为 $\mathcal{O}(N)$.&lt;/p>
&lt;p>总之，attention 的反向传播过程所需要的空间复杂度为 $\mathcal{O}(N)$.&lt;/p>
&lt;p>作者发现有两点可以改进：&lt;/p>
&lt;ol>
&lt;li>attention mask 不需要存储，我们只需要保存 forward pass 时的输入，然后在 backward pass 时重新生成即可，这样只需要 $\mathcal{O}(N)$ 的空间复杂度。&lt;/li>
&lt;li>计算 softmax 的梯度是，如果使用公式 $D_i=P_{i:}^TdP_{i:}$ 来计算的话，由于 $P_{i:}\in\mathbb{R}^N$, 可能会导致超过 SRAM 的内存使用限制，因此，我们可以使用 $D_i=do_i^To_i$ 来避免这个问题，其中 $o_i\in\mathbb{R}^d$.&lt;/li>
&lt;/ol>
&lt;p>最终，flashattention 的 backward pass 过程如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-flashattention/flashattention-backward-pass.png"
width="1206"
height="1203"
srcset="https://maosong2022.github.io/p/notes-on-flashattention/flashattention-backward-pass_hu11493104955663032119.png 480w, https://maosong2022.github.io/p/notes-on-flashattention/flashattention-backward-pass_hu17037096936506419817.png 1024w"
loading="lazy"
alt="flashattention backward pass"
class="gallery-image"
data-flex-grow="100"
data-flex-basis="240px"
>&lt;/p>
&lt;p>经过前面的分析，flashattention 的反向传播的时间复杂度为 $\mathcal{O}(N^2)$, 空间复杂度为 $\mathcal{O}(N)$.&lt;/p>
&lt;blockquote>
&lt;p>Theorem 5
令 $N$ 为 sequence length, $d$ 为 head dimension, $M$ 是 SRAM 的 size, 且满足 $d\leq M\leq Nd$. 则 flashattention 反向传播的内存访问开销为 $\Theta(N^2d^2M^{-1})$.&lt;/p>
&lt;/blockquote>
&lt;p>定理的证明与 Theorem 2 基本一致，我们此处不再赘述。&lt;/p>
&lt;h3 id="block-sparse-flashattention">Block-sparse Flashattention
&lt;/h3>&lt;p>当 attention 具有 block sparsity 的性质时，作者提出了 blck-sparse flashattention 来进一步提高 attention 的计算效率。&lt;/p>
&lt;p>给定 $Q,K,V\in\mathbb{R}^{N\times d}$, 以及一个 mask $M\in{0,1}^{N\times N}$, 我们需要计算&lt;/p>
$$
S = QK^T\in\mathbb{R}^{N\times N},\quad P = \mathrm{softmax}(S\odot \mathbb{1}_{M})\in\mathbb{R}^{N\times N},\quad O = PV\in\mathbb{R}^{N\times d}
$$&lt;p>其中当 $M_{kl}=1$ 时， $(S\odot \mathbb{1}_ {M})_ {kl}=S_ {kl}$, 否则 $(S\odot \mathbb{1}_ {M})_{kl}=0$.&lt;/p>
&lt;p>Block-sparse attention 的算法如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-flashattention/flashattention-block-sparse-algorithm.png"
width="1205"
height="905"
srcset="https://maosong2022.github.io/p/notes-on-flashattention/flashattention-block-sparse-algorithm_hu13462824161635296969.png 480w, https://maosong2022.github.io/p/notes-on-flashattention/flashattention-block-sparse-algorithm_hu9658140667513876646.png 1024w"
loading="lazy"
alt="Block-sparse flashattention forward pass"
class="gallery-image"
data-flex-grow="133"
data-flex-basis="319px"
>&lt;/p>
&lt;blockquote>
&lt;p>Proposition 4
令 $N$ 为 sequence length, $d$ 为 head dimension, $M$ 是 SRAM 的 size, 且满足 $d\leq M\leq Nd$. 则 block-sparse attention 的内存访问开销为 $\Theta(Nd+N^2d^2M^{-1}s)$, 其中 $s$ 是 block-sparse mask 中的非零 block 的比例&lt;/p>
&lt;/blockquote>
&lt;p>证明与 Theorem 2 的证明是类似的，总的内存访问开销为 $\mathcal{O}(Nd+NdT_c)$, 但是在计算的过程中，由于 mask 矩阵的 block-sparsity, 我们实际上只需要计算一小部分 $M_{ij}\neq0$ 的情况，因此最终的内存访问开销为&lt;/p>
$$
\mathcal{O}\left(Nd+\frac{N^2d^2}{M}s\right)
$$&lt;p>可以看到，attention mask 的 sparsity 越高，block-sparse flashattention 的效率也就越高。当 $N$ 非常大时，$s 通常为 $1/\sqrt{N}$ 或者 $N^{-1}\log N$, 从而最终的内存访问开销为 $\mathcal{O}(N\sqrt{N})$ 或者 $\mathcal{O}(N\log N)$.&lt;/p>
&lt;p>作者对比了以下 block-sparse flashattention 和 flashattention 的效率对比，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-flashattention/flashattention-block-sparse-efficiency.png"
width="287"
height="214"
srcset="https://maosong2022.github.io/p/notes-on-flashattention/flashattention-block-sparse-efficiency_hu13029486997688254715.png 480w, https://maosong2022.github.io/p/notes-on-flashattention/flashattention-block-sparse-efficiency_hu11418939862626082814.png 1024w"
loading="lazy"
alt="Efficiency of block-sparse flashattention"
class="gallery-image"
data-flex-grow="134"
data-flex-basis="321px"
>&lt;/p>
&lt;h2 id="experiment">Experiment
&lt;/h2>&lt;p>作者通过实验验证了 flashattention 的有效性，如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Attention&lt;/th>
&lt;th>Standard&lt;/th>
&lt;th>FlashAttention&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>GFLOPs&lt;/td>
&lt;td>66.6&lt;/td>
&lt;td>75.2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>HBM R/W (GB)&lt;/td>
&lt;td>40.3&lt;/td>
&lt;td>4.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Runtime (ms)&lt;/td>
&lt;td>41.7&lt;/td>
&lt;td>7.3&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，尽管 flashattention 相比于标准化 attention 需要更多的算力，但是由于其内存访问开销更少，所以最终的运行时间大有了大幅度降低&lt;/p>
&lt;p>作者还探究了 block size 对 flashattention 性能对的影响，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-flashattention/flashattention-ablation-block-size.png"
width="315"
height="216"
srcset="https://maosong2022.github.io/p/notes-on-flashattention/flashattention-ablation-block-size_hu8791717821977031005.png 480w, https://maosong2022.github.io/p/notes-on-flashattention/flashattention-ablation-block-size_hu12796013879068543841.png 1024w"
loading="lazy"
alt="Ablation on block size"
class="gallery-image"
data-flex-grow="145"
data-flex-basis="350px"
>&lt;/p>
&lt;p>可以看到，随着 block size 增加，循环次数降低，内存访问开销也逐渐降低。但是当 block size 充分大 ( $&amp;gt; 256$) 之后，运行时间就会被别的因素所限制，并且过大的 block size 可能会导致 SRAM 的内存溢出&lt;/p>
&lt;p>作者首先在 BERT 和 GPT-2 上验证了 flashattention 的表现，BERT 的实验结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>BERT Implementation&lt;/th>
&lt;th>Training time (minutes)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Nvidia MLPerf 1.1&lt;/td>
&lt;td>$20.0\pm1.5$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FlashAttention (ours)&lt;/td>
&lt;td>$17.4\pm1.4$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>GPT-2 的实验结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model implementations&lt;/th>
&lt;th>OpenWebText (ppl)&lt;/th>
&lt;th>Training time (speedup)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>GPT-2 small - Huggingface&lt;/td>
&lt;td>18.2&lt;/td>
&lt;td>9.5 days (1.0 )&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GPT-2 small - Megatron-LM&lt;/td>
&lt;td>18.2&lt;/td>
&lt;td>4.7 days (2.0 )&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GPT-2 small - FlashAttention&lt;/td>
&lt;td>18.2&lt;/td>
&lt;td>2.7 days (3.5 )&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GPT-2 medium - Huggingface&lt;/td>
&lt;td>14.2&lt;/td>
&lt;td>21.0 days (1.0 )&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GPT-2 medium - Megatron-LM&lt;/td>
&lt;td>14.3&lt;/td>
&lt;td>11.5 days (1.8 )&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GPT-2 medium - FlashAttention&lt;/td>
&lt;td>14.3&lt;/td>
&lt;td>6.9 days (3.0 )&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>实验结果显示，flashattention 比 Huggingface 快 3 倍左右，比 Megatron 快 1.7 倍左右&lt;/p>
&lt;ol>
&lt;li>训练速度：实验显示，flashattention 在 BERT 上，比 MLPerf 1.1 快 $15%$, 在 GPT-2 上比 HuggingFace 快 3 倍，比 Megatron 快 1.8 倍&lt;/li>
&lt;li>准确率：flashattention 是第一个在 Path-X 上比随机表现更好的 transformer 模型；block-sparse flashattention 是第一个在 Path-256 上比随机表现更好的的 sequence model&lt;/li>
&lt;/ol>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者提出了 flashattention, 一个通过优化标准 attention 内存访问效率来提高 attention 计算效率的方法，作者详细介绍了算法设计的原理与证明，并通过实验证明了结果的有效性。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2205.14135" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on StreamingLLM</title><link>https://maosong2022.github.io/p/notes-on-streamingllm/</link><pubDate>Wed, 20 Aug 2025 10:16:35 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-streamingllm/</guid><description>&lt;p>作者提出了 StreamingLLM, 一个基于 attention sink 来提高 sliding window attention 在超长上下文场景下表现的方法。&lt;/p>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>已有的基于 softmax attention 的架构的问题在于很难扩展到长上下文的场景，主要原因有两点：&lt;/p>
&lt;ol>
&lt;li>KV cache 会随着序列长度增加而商城，从而提高 decoding 的 latency&lt;/li>
&lt;li>序列长度超过预训练的 context length 之后，模型表现会急剧下降&lt;/li>
&lt;/ol>
&lt;p>为了解决这个问题，已有的方法可以分为三类：&lt;/p>
&lt;ol>
&lt;li>length extrapolation: 使用 RoPE 或者 AliBi 等方法来扩展 LLM 的 context length, 这类方法的问题是扩展的上下文长度仍然有限，对于 streaming 的场景作用有限&lt;/li>
&lt;li>context window attention: 扩展 LLM 的上下文长度，如 &lt;a class="link" href="https://maosong.website/p/notes-on-flashattention/" target="_blank" rel="noopener"
>flash attention&lt;/a> 等来降低 attention 的计算和内存开销。这类方法也是只在有限的上下文场景下 work&lt;/li>
&lt;li>Improving LLMs’ Utilization of Long Text: 更好利用长上下文的数据&lt;/li>
&lt;/ol>
&lt;p>基于已有的工作的发现，作者提出了本文研究的核心问题：&lt;/p>
&lt;blockquote>
&lt;p>如何在不损失模型表现和效率的情况下，提高模型在无限长上下文场景下的表现。&lt;/p>
&lt;/blockquote>
&lt;p>为了解决这个问题，作者首先分析了 sliding window attention 的不足，作者发现，sliding window attention 在超过 KV cache size 之后，表现也会急剧下降。作者通过实验发现，sliding window attention 表现急剧下降的原因在于 &lt;strong>attention sink&lt;/strong>, 也就是模型损失了对于初始 token 的关注，从而导致模型表现下降。&lt;/p>
&lt;p>基于 attention sink, 作者设计了 StreamingLLM, 用于提高 sliding window attention 在长上下文场景下的表现，结果发现，模型的表现有了大幅度的提升。&lt;/p>
&lt;p>作者还进一步在预训练阶段加入了 sink token 充当初始 token, 进一步提高模型的表现。&lt;/p>
&lt;h2 id="method">Method
&lt;/h2>&lt;h3 id="attention-sink">Attention Sink
&lt;/h3>&lt;p>作者首先探究了一下 softmax attention 以及 sliding window attention 性能下降的节点，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink_perplexity.png"
width="1155"
height="216"
srcset="https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink_perplexity_hu17107578125132293124.png 480w, https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink_perplexity_hu13497598808075063968.png 1024w"
loading="lazy"
alt="Perplexity of different attention with 20K tokens"
class="gallery-image"
data-flex-grow="534"
data-flex-basis="1283px"
>&lt;/p>
&lt;p>可以看到，softmax attention 性能急剧下降的节点为 pre-training 的 context length; 而 sliding window attention 性能急剧下降的节点为 KV cache size.&lt;/p>
&lt;p>接下来，作者分析了一下不同 layer 的 attention 分布情况，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-attention-logits-visualization.png"
width="1099"
height="256"
srcset="https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-attention-logits-visualization_hu10873929585851139529.png 480w, https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-attention-logits-visualization_hu18075330575893701465.png 1024w"
loading="lazy"
alt="Visualization of attention logits"
class="gallery-image"
data-flex-grow="429"
data-flex-basis="1030px"
>&lt;/p>
&lt;p>可以看到，初始的 2 层 layer 里 attention logits 的分布比较均匀。但是在后续的 layer 里，第一个 token 的权重都大幅度上升。&lt;/p>
&lt;p>作者分析原因认为，sliding window attention 在超过 KV cache size 之后性能急剧下降的主要原因是初始 token 不再参与 softmax 的计算，这导致了 softmax 的计算出现了比较大的变化，从而模型的表现开始下降。&lt;/p>
&lt;p>为了探究初始 token 对最终模型表现的影响因素是语义层面还是位置层面的，作者将初始的 token 替换为 &lt;code>\n&lt;/code>, 并比较了模型的表现，结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Llama-2-13B&lt;/th>
&lt;th>PPL (↓)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>0 + 1024(Window)&lt;/td>
&lt;td>5158.07&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4 + 1020&lt;/td>
&lt;td>5.40&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4&amp;quot;\n&amp;quot;+1020&lt;/td>
&lt;td>5.60&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，把初始的四个 token 替换为 &lt;code>\n&lt;/code>, 并不影响模型最终的表现，这说明是初始 token 的位置信息在发挥作用。&lt;/p>
&lt;p>作者接下来探究了一下模型架构的影响，实验结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Cache Config&lt;/th>
&lt;th>0+2048&lt;/th>
&lt;th>1+2047&lt;/th>
&lt;th>2+2046&lt;/th>
&lt;th>4+2044&lt;/th>
&lt;th>8+2040&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Falcon-7B&lt;/td>
&lt;td>17.90&lt;/td>
&lt;td>12.12&lt;/td>
&lt;td>12.12&lt;/td>
&lt;td>12.12&lt;/td>
&lt;td>12.12&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MPT-7B&lt;/td>
&lt;td>460.29&lt;/td>
&lt;td>14.99&lt;/td>
&lt;td>15.00&lt;/td>
&lt;td>14.99&lt;/td>
&lt;td>14.98&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Pythia-12B&lt;/td>
&lt;td>21.62&lt;/td>
&lt;td>11.95&lt;/td>
&lt;td>12.09&lt;/td>
&lt;td>12.09&lt;/td>
&lt;td>12.02&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Cache Config&lt;/td>
&lt;td>0+4096&lt;/td>
&lt;td>1+4095&lt;/td>
&lt;td>2+4094&lt;/td>
&lt;td>4+4092&lt;/td>
&lt;td>8+4088&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Llama-2-7B&lt;/td>
&lt;td>3359.95&lt;/td>
&lt;td>11.88&lt;/td>
&lt;td>10.51&lt;/td>
&lt;td>9.59&lt;/td>
&lt;td>9.54&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，不同的模型架构都存在这个问题，这说明 sliding window attention 的影响与架构无关。并且，作者认为，使用初始 4 个 token 就可以有效的避免模型的性能下降，进一步增加初始 token 的数量不会有进一步提升。&lt;/p>
&lt;p>作者分析 attention sink 出现的原因在于，&lt;/p>
&lt;ol>
&lt;li>初始的 token 对于后续所有的 token 都是可见的，因此其会携带一些信息&lt;/li>
&lt;li>在预训练阶段，模型并没有一个一致的初始 token 来标注起始信息，这导致模型会默认使用第一个 token 来储存一些信息。&lt;/li>
&lt;/ol>
&lt;p>为了解决这个问题，作者就提出了缓存初始 token 的方法，具体做法就是，在 sliding window attention 的基础上，我们还会加上初始 token 的信息，作者展示示意图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-KV_cache-rolling.png"
width="481"
height="190"
srcset="https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-KV_cache-rolling_hu14539682788052812184.png 480w, https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-KV_cache-rolling_hu8473910721752708950.png 1024w"
loading="lazy"
alt="Rolling KV cache of StramingLLM"
class="gallery-image"
data-flex-grow="253"
data-flex-basis="607px"
>&lt;/p>
&lt;p>也就是说，我们初始 token 始终会参与计算（论文中初始 token 数量为 4），然后我们会维持一个大小为 3 的 KV cache 队列来进行最终 sliding window attention 的计算，这样，每次计算 attention 的时候，我们就会使用 $# \text{iniital token} + # \text{sliding window token}$ 这么多的 token 来计算 attention. 作者对比了不同 attention 的计算方式，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-illustration-streamingLLM.png"
width="1148"
height="389"
srcset="https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-illustration-streamingLLM_hu10367217091090354661.png 480w, https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-illustration-streamingLLM_hu8009958211234850987.png 1024w"
loading="lazy"
alt="Illustration of StreamingLLM"
class="gallery-image"
data-flex-grow="295"
data-flex-basis="708px"
>&lt;/p>
&lt;p>前面是在 inference 阶段进行优化的，作者现在进一步探究在 pre-training 阶段加入 attention sink 参与训练对模型表现的影响。&lt;/p>
&lt;p>[[softmax-off-by-one]] 提出了我们应该加入一个 zero sink token, 其计算公式如下&lt;/p>
$$
\mathrm{softmax}_1(x)_i = \frac{\exp(x_i)}{1 + \sum_{j=1}^N \exp(x_j)}
$$&lt;p>这里 $x\in\mathbb{R}^N$ 是输入的序列。我们可以将 sink token 视为一个 key 以及 value 都是 0 向量的特殊 token.&lt;/p>
&lt;p>在本文中，作者使用了一个可学习的 sink token. 作者对比了原始 softmax attention, 使用 zero sink attention, learnable sink attention 三种方法的表现，结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Cache Config&lt;/th>
&lt;th>0+1024&lt;/th>
&lt;th>1+1023&lt;/th>
&lt;th>2+1022&lt;/th>
&lt;th>4+1020&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Vanilla&lt;/td>
&lt;td>27.87&lt;/td>
&lt;td>18.49&lt;/td>
&lt;td>18.05&lt;/td>
&lt;td>18.05&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Zero Sink&lt;/td>
&lt;td>29214&lt;/td>
&lt;td>19.90&lt;/td>
&lt;td>18.27&lt;/td>
&lt;td>18.01&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Learnable Sink&lt;/td>
&lt;td>1235&lt;/td>
&lt;td>18.01&lt;/td>
&lt;td>18.01&lt;/td>
&lt;td>18.02&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到 zero sink 仍然需要一部分初始 token 来维持模型的表现。作者在论文中推荐使用 learnable sink.&lt;/p>
&lt;h2 id="experiments">Experiments
&lt;/h2>&lt;p>作者首先验证了 StreamlingLLM 在不同架构上的表现，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-preplexsity-4M-tokens.png"
width="1151"
height="172"
srcset="https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-preplexsity-4M-tokens_hu10222675238153024805.png 480w, https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-preplexsity-4M-tokens_hu4302749122445536560.png 1024w"
loading="lazy"
alt="Perplexity of StreamingLLM on 4M tokens"
class="gallery-image"
data-flex-grow="669"
data-flex-basis="1606px"
>&lt;/p>
&lt;p>实验结果显示，StreamingLLM 可以扩展到 4M 的上下文&lt;/p>
&lt;p>接下来，作者探究了以下在 Pretraining 阶段加入 learnable sink token 对模型表现的影响，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-pre-training-sink-token.png"
width="358"
height="208"
srcset="https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-pre-training-sink-token_hu10575160091401072951.png 480w, https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-pre-training-sink-token_hu1459256266174936863.png 1024w"
loading="lazy"
alt="Pre-training loss curves of models w/ sink tokens"
class="gallery-image"
data-flex-grow="172"
data-flex-basis="413px"
>&lt;/p>
&lt;p>可以看到，加入 sink token 之后对模型的表现没有显著影响。并且，模型在下游任务上的表现与标准的 softmax attention 表现差不多。&lt;/p>
&lt;p>作者还对 StreamlingLLM 进行了可视化，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-visualization-attention-logits.png"
width="1153"
height="214"
srcset="https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-visualization-attention-logits_hu17917977003625206939.png 480w, https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-visualization-attention-logits_hu10512833175537702855.png 1024w"
loading="lazy"
alt="Visualization of attention logits with StreamingLLM"
class="gallery-image"
data-flex-grow="538"
data-flex-basis="1293px"
>&lt;/p>
&lt;p>作者进一步评估了 StreamingLLM 在下游任务上的表现，我们主要关注一下 ARC 上的表现，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-ARC-performance.png"
width="808"
height="255"
srcset="https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-ARC-performance_hu21210541812592330.png 480w, https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-ARC-performance_hu7689106914913301718.png 1024w"
loading="lazy"
alt="Accuracy on the ARC"
class="gallery-image"
data-flex-grow="316"
data-flex-basis="760px"
>&lt;/p>
&lt;p>可以看到，full attention 出现了 OOM error, 而 sliding window attention 虽然避免了 OOM 的问题，但是其表现非常差。而 StreamingLLM 则进一步提高了 Sliding Window attention 的表现。&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 StreamingLLM, 一个在 Sliding window attention 中加入 sink token 来避免超过 cache size 之后模型表现急剧下降的问题。作者详细介绍了 attention sink 现象以及解决方法。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://openreview.net/forum?id=NG7sS51zVF" target="_blank" rel="noopener"
>Efficient Streaming Language Models with Attention Sinks&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on gpt-oss</title><link>https://maosong2022.github.io/p/notes-on-gpt-oss/</link><pubDate>Tue, 19 Aug 2025 16:14:56 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-gpt-oss/</guid><description>&lt;p>openAI 发布了 gpt-oss 大语言模型，包含 120B-A5.1B 以及 20.9B-A3.6B 两个 size, 作者强调了模型的 instruction following, tool use, 以及 adaptive thinking 能力&lt;/p>
&lt;h2 id="method">Method
&lt;/h2>&lt;h3 id="architecture">Architecture
&lt;/h3>&lt;p>gpt-oss 系列是一个基于 MoE transformer 架构的 LLM. 架构中交替使用 sliding window attention 和 full attention, sliding window size 为 128 token, 架构图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-gpt-oss/gpt-oss-architecture.png"
width="876"
height="789"
srcset="https://maosong2022.github.io/p/notes-on-gpt-oss/gpt-oss-architecture_hu6988446586431083952.png 480w, https://maosong2022.github.io/p/notes-on-gpt-oss/gpt-oss-architecture_hu8173092701626198973.png 1024w"
loading="lazy"
alt="gpt-oss-architecture"
class="gallery-image"
data-flex-grow="111"
data-flex-basis="266px"
>&lt;/p>
&lt;p>模型的配置如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>120B&lt;/th>
&lt;th>20B&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Date&lt;/td>
&lt;td>2025/8/5&lt;/td>
&lt;td>2025/8/5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Total Parameters&lt;/td>
&lt;td>116B&lt;/td>
&lt;td>20B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Activated Parameters&lt;/td>
&lt;td>5.13B&lt;/td>
&lt;td>3.61B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># MoE Layers&lt;/td>
&lt;td>36&lt;/td>
&lt;td>24&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Hidden Dim&lt;/td>
&lt;td>2880&lt;/td>
&lt;td>2880&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MoE Intermediate Dim&lt;/td>
&lt;td>2880&lt;/td>
&lt;td>2880&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Attention&lt;/td>
&lt;td>GQA+sliding window&lt;/td>
&lt;td>GQA+sliding window&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Attention Head Dim&lt;/td>
&lt;td>64&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Attention bias&lt;/td>
&lt;td>True&lt;/td>
&lt;td>True&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Attention Heads&lt;/td>
&lt;td>64&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Key-Value Heads&lt;/td>
&lt;td>8&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Experts (total)&lt;/td>
&lt;td>128&lt;/td>
&lt;td>32&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Experts Active Per Token&lt;/td>
&lt;td>4&lt;/td>
&lt;td>4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Shared Experts&lt;/td>
&lt;td>0&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>QK-Norm&lt;/td>
&lt;td>No&lt;/td>
&lt;td>No&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>在架构上，gpt-oss 做的主要改变有：&lt;/p>
&lt;ol>
&lt;li>Q, K, V projection layer, expert layer, routing layer 都使用了 bias&lt;/li>
&lt;li>修改了 expert layer 中 SwiGLU 的定义&lt;/li>
&lt;li>attention 中额外使用了一个 attention sink&lt;/li>
&lt;/ol>
&lt;h4 id="swiglu">SwiGLU
&lt;/h4>&lt;p>大多数模型使用的基于 SwiGLU 的 MLP 定义如下&lt;/p>
$$
y = W_2(W_3x \odot \mathrm{SwiGLU}(W_1x))
$$&lt;p>其中 $\mathrm{SwiGLU}(x)=x\odot\mathrm{sigmoid}(x)$, 在 gpt-oss 模型中，作者首先定义了两个常数 $\alpha=1.702$, $\mathrm{limit}=7.0$, 然后 SwiGLU MLP 的定义如下&lt;/p>
$$
\begin{aligned}
o_1&amp;=W_1x+b_1,\\
o_3&amp;=W_3x+b_3\\
o_1&amp;=\mathrm{clamp}(o_1,\max=\mathrm{limit})\\
o_3&amp;=\mathrm{clamp}(o_3,\min=-\mathrm{limit},\max=\mathrm{limit})\\
o_3&amp;= o_3\odot \mathrm{sigmoid}(\alpha\cdot o_3)\\
o_3&amp;= (o_1+1)\odot o_3\\
y &amp;= W_2o_3
\end{aligned}
$$&lt;h4 id="attention-sink">Attention Sink
&lt;/h4>&lt;p>作者使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-streamingllm/" target="_blank" rel="noopener"
>attention sink&lt;/a> 来避免 window attention 在超过 kv cache size 之后，表现大幅度下降的问题。&lt;/p>
&lt;h3 id="quantization">Quantization
&lt;/h3>&lt;p>为了降低模型的内存占用量，作者使用了 PTQ 来训练 MoE 的权重，使用的精度为 MXFP4, 这样每个参数由 4.25 bits 来表示。最终，模型的参数存储格式如下：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-gpt-oss/gpt-oss-precision-format.png"
width="1152"
height="1235"
srcset="https://maosong2022.github.io/p/notes-on-gpt-oss/gpt-oss-precision-format_hu14293680188010143134.png 480w, https://maosong2022.github.io/p/notes-on-gpt-oss/gpt-oss-precision-format_hu9442745571382269718.png 1024w"
loading="lazy"
alt="precision format of gpt-oss"
class="gallery-image"
data-flex-grow="93"
data-flex-basis="223px"
>&lt;/p>
&lt;p>通过这个流程，gpt-oss-120B 可以部署在 80GB 内存的 GPU 上，gpt-oss-20B 可以部署在 16GB 内存的 GPU 上。模型各部分参数量如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Component&lt;/th>
&lt;th>120b&lt;/th>
&lt;th>20b&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>MLP&lt;/td>
&lt;td>114.71B&lt;/td>
&lt;td>19.12B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Attention&lt;/td>
&lt;td>0.96B&lt;/td>
&lt;td>0.64B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Embed + Unembed&lt;/td>
&lt;td>1.16B&lt;/td>
&lt;td>1.16B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Active Parameters&lt;/td>
&lt;td>5.13B&lt;/td>
&lt;td>3.61B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Total Parameters&lt;/td>
&lt;td>116.83B&lt;/td>
&lt;td>20.91B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Checkpoint Size&lt;/td>
&lt;td>60.8GiB&lt;/td>
&lt;td>12.8GiB&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>这里在计算激活参数的时候，没有没有考虑 embedding 的参数量。&lt;/p>
&lt;h3 id="pre-training">Pre-training
&lt;/h3>&lt;p>预训练细节不多，主要是使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-flashattention/" target="_blank" rel="noopener"
>flash attention&lt;/a> 进行加速计算，使用了 Triton 进行了 kernel 的优化，gpt-oss-120b 训练了 120M H100-hours, gpt-oss-20B 的训练时间是 gpt-oss-120Bd 的十分之一左右&lt;/p>
&lt;h3 id="post-training">Post-training
&lt;/h3>&lt;p>post-training 的数据包括 coding, math 以及 science 等，主要使用 RL 进行训练&lt;/p>
&lt;p>作者介绍了以下 post-training 使用的格式，即 &lt;code>harmony chat format&lt;/code>. 角色的优先级如下所示&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">System &amp;gt; Developer &amp;gt; User &amp;gt; Assistant &amp;gt; Tool
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>作者还加入了 channels 来限制可以使用的信息，比如使用 &lt;code>analysis&lt;/code> 来表示 CoT tokens, 使用 &lt;code>commentary&lt;/code> 来表示 function calling 等，一个具体的例子如下：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-gpt-oss/gpt-oss-chat-template.png"
width="1351"
height="821"
srcset="https://maosong2022.github.io/p/notes-on-gpt-oss/gpt-oss-chat-template_hu10679804726557176179.png 480w, https://maosong2022.github.io/p/notes-on-gpt-oss/gpt-oss-chat-template_hu9263536855199565913.png 1024w"
loading="lazy"
alt="gpt-oss-chat-template"
class="gallery-image"
data-flex-grow="164"
data-flex-basis="394px"
>&lt;/p>
&lt;h2 id="evaluation">Evaluation
&lt;/h2>&lt;p>gpt-oss 系列的表现如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-gpt-oss/gpt-oss-performance.png"
width="1179"
height="1106"
srcset="https://maosong2022.github.io/p/notes-on-gpt-oss/gpt-oss-performance_hu13944137467359153204.png 480w, https://maosong2022.github.io/p/notes-on-gpt-oss/gpt-oss-performance_hu10206985392795617313.png 1024w"
loading="lazy"
alt="performance of gpt-oss"
class="gallery-image"
data-flex-grow="106"
data-flex-basis="255px"
>&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者提出了 gpt-oss 系列大语言模型，gpt-oss 在架构上与已有的主流模型架构如 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a>, &lt;a class="link" href="DeepSeek-V3.md" >DeepSeek-V3&lt;/a> 等都有一定区别&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://openai.com/index/introducing-gpt-oss/" target="_blank" rel="noopener"
>technical report&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on QK-Norm</title><link>https://maosong2022.github.io/p/notes-on-qk-norm/</link><pubDate>Wed, 13 Aug 2025 16:12:11 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-qk-norm/</guid><description>&lt;p>作者提出了 QK norm, 一个解决 softmax 注意力权重不稳定的 scaling 算法。&lt;/p>
&lt;h2 id="problem-definition">Problem Definition
&lt;/h2>&lt;p>Softmax 可以用于将 logits 转化为一个概率分布，但是 softmax 问题是输入的微小差别会对输出产生巨大影响，甚至会 mask 掉其他信号。因此我们需要选取合适的缩放因子，来解决 softmax 的极端值问题。SDPA 的可视化结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-qk-norm/QK-Norm-SDPA-attention.png"
width="1351"
height="398"
srcset="https://maosong2022.github.io/p/notes-on-qk-norm/QK-Norm-SDPA-attention_hu17671495108868641229.png 480w, https://maosong2022.github.io/p/notes-on-qk-norm/QK-Norm-SDPA-attention_hu16468425081775612855.png 1024w"
loading="lazy"
alt="SDPA attention visualization"
class="gallery-image"
data-flex-grow="339"
data-flex-basis="814px"
>&lt;/p>
&lt;h2 id="method">Method
&lt;/h2>&lt;p>作者首先提出了 QKNorm, 其使用了一个可学习的 scaling 参数来控制 $QK^T$ 的范围，进而让 attention 的 pattern 更加分散。&lt;/p>
&lt;p>作者首先回顾了以下已有的进展，主要是三点：&lt;/p>
&lt;ol>
&lt;li>FixNorm: 将 word embedding 限制为单位长度&lt;/li>
&lt;li>PerNorm: 使用 Pre-norm 替换 Post-norm&lt;/li>
&lt;li>ScaleNorm: 使用 $\ell_2$ normalization 替换 LayerNorm, 并乘以一个可学习的 scaling 参数。&lt;/li>
&lt;/ol>
&lt;p>作者基于这三点进行了改进，改进后的 attention 定义如下&lt;/p>
$$
\mathrm{softmax}\left(g\cdot \hat{Q}\hat{K}^T\right)V
$$&lt;p>其中,&lt;/p>
$$
\hat{Q} = [\frac{q_1}{\|q_1\|_2},\dots,\frac{q_m}{\|q_m\|_2}], \hat{K} = [\frac{k_1}{\|k_1\|_2},\dots,\frac{k_n}{\|k_n\|_2}]
$$&lt;p>是对原始的 $Q, K$ 按列进行 $\ell_2$ normalization 得到的结果， $g$ 是一个可学习的参数，其初始化值为&lt;/p>
$$
g_0 = \log_2(L^2-L)
$$&lt;p>这里 $L$ 是训练数据 $97.5$ 分位。&lt;/p>
&lt;p>使用这种动态缩放之后，attention 的分布变得更加分散了，结果如下图所示：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-qk-norm/QK-norm-normalized-attention.png"
width="1366"
height="406"
srcset="https://maosong2022.github.io/p/notes-on-qk-norm/QK-norm-normalized-attention_hu487579007854365109.png 480w, https://maosong2022.github.io/p/notes-on-qk-norm/QK-norm-normalized-attention_hu13488020570320115229.png 1024w"
loading="lazy"
alt="QK normalized attention"
class="gallery-image"
data-flex-grow="336"
data-flex-basis="807px"
>&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://aclanthology.org/2020.findings-emnlp.379.pdf" target="_blank" rel="noopener"
>Query-Key Normalization for Transformers&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on GLM-4.5</title><link>https://maosong2022.github.io/p/notes-on-glm-4.5/</link><pubDate>Wed, 13 Aug 2025 12:27:48 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-glm-4.5/</guid><description>&lt;p>智谱 AI 提出了 GLM4.5, 包含 GLM4.5 和 GLM-4.5-Air,两个 MoE LLM. 模型大小分别为 355B-A22B 和 106B-A12B, GLM4.5 主要关注 agentic, reasoning 以及 coding 三个领域。&lt;/p>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>作者认为，通用模型有三个关键能力，即 ARC：&lt;/p>
&lt;ol>
&lt;li>Agent: 与外部工具以及真实世界进行交互&lt;/li>
&lt;li>Reasoning: 解决数学和科学领域的复杂问题&lt;/li>
&lt;li>Coding: 解决真实世界软件工程相关问题&lt;/li>
&lt;/ol>
&lt;p>已有的商业模型如 o1/o3, Claude Sonnet 4 已经在 ARC 上达到了非常好的表现，但是开源模型仍然比较稀缺&lt;/p>
&lt;p>基于这个目标，作者就提出了 GLM4.5 和 GLM-4.5-Air, 来统一完成三个不同的目标。&lt;/p>
&lt;h2 id="method">Method
&lt;/h2>&lt;h2 id="pre-training">Pre-training
&lt;/h2>&lt;h3 id="architecture">Architecture
&lt;/h3>&lt;p>GLM-4.5 是一个基于 MoE 架构的 LLM, 架构与 DeepSeek-MoE 相似，作者做了如下几点改变：&lt;/p>
&lt;ol>
&lt;li>在 MoE layer 中，使用了 loss-free balance routing, 然后使用了 sigmoid function 作为 routing score 的 normalization.&lt;/li>
&lt;li>与 &lt;a class="link" href="https://maosong.website/p/notes-on-kimi-k2/" target="_blank" rel="noopener"
>Kimi-k2&lt;/a> 和 DeepSeek-V3 相比，作者降低了 head dimension, 提升了 number of layers. 作者认为更深的模型更有利于提高模型的 Reasoning 表现&lt;/li>
&lt;li>attention 上，作者使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-gqa/" target="_blank" rel="noopener"
>GQA&lt;/a>, 对于 #RoPE, 作者使用了 partial RoPE, 只旋转每个 token 的前半部分， 作者还将 attention heads 的个数增加到了 2.5 倍，作者发现增加 attention heads 可以提高模型的 Reasoning 表现&lt;/li>
&lt;li>作者还使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-qk-norm/" target="_blank" rel="noopener"
>QK-Norm&lt;/a> 来防止 attention logits 爆炸&lt;/li>
&lt;li>作者还使用了一个 MoE layer 作为 MTP layer 来支持 speculative decoding.&lt;/li>
&lt;/ol>
&lt;p>模型与 DeepSeek-V3 和 Kimi-k2 的对比如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>GLM-4.5&lt;/th>
&lt;th>GLM-4.5-Air&lt;/th>
&lt;th>Step 3&lt;/th>
&lt;th>Kimi K2&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Date&lt;/td>
&lt;td>2025/8/8&lt;/td>
&lt;td>2025/8/8&lt;/td>
&lt;td>2025/7/25&lt;/td>
&lt;td>2025/7/28&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Total Parameters&lt;/td>
&lt;td>355B&lt;/td>
&lt;td>106B&lt;/td>
&lt;td>316B&lt;/td>
&lt;td>1043B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Activated Parameters&lt;/td>
&lt;td>32B&lt;/td>
&lt;td>12B&lt;/td>
&lt;td>38B&lt;/td>
&lt;td>32B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Dense Layers&lt;/td>
&lt;td>3&lt;/td>
&lt;td>1&lt;/td>
&lt;td>5&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># MoE Layers&lt;/td>
&lt;td>89&lt;/td>
&lt;td>45&lt;/td>
&lt;td>56&lt;/td>
&lt;td>60&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># MTP Layers&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;td>0&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Hidden Dim&lt;/td>
&lt;td>5120&lt;/td>
&lt;td>4096&lt;/td>
&lt;td>7168&lt;/td>
&lt;td>7168&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Dense Intermediate Dim&lt;/td>
&lt;td>12288&lt;/td>
&lt;td>10944&lt;/td>
&lt;td>18432&lt;/td>
&lt;td>18432&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MoE Intermediate Dim&lt;/td>
&lt;td>1536&lt;/td>
&lt;td>1408&lt;/td>
&lt;td>5120&lt;/td>
&lt;td>2048&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Attention&lt;/td>
&lt;td>GQA&lt;/td>
&lt;td>GQA&lt;/td>
&lt;td>MFA&lt;/td>
&lt;td>MLA&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Attention Head Dim&lt;/td>
&lt;td>128&lt;/td>
&lt;td>128&lt;/td>
&lt;td>256&lt;/td>
&lt;td>192&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Attention Heads&lt;/td>
&lt;td>96&lt;/td>
&lt;td>96&lt;/td>
&lt;td>64&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Key-Value Heads&lt;/td>
&lt;td>8&lt;/td>
&lt;td>8&lt;/td>
&lt;td>1&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>scoring&lt;/td>
&lt;td>sigmoid&lt;/td>
&lt;td>sigmoid&lt;/td>
&lt;td>softmax&lt;/td>
&lt;td>softmax&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Experts (total)&lt;/td>
&lt;td>160&lt;/td>
&lt;td>128&lt;/td>
&lt;td>48&lt;/td>
&lt;td>384&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Experts Active Per Token&lt;/td>
&lt;td>8&lt;/td>
&lt;td>8&lt;/td>
&lt;td>3&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Shared Experts&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>QK-Norm&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>No&lt;/td>
&lt;td>No&lt;/td>
&lt;td>No&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="pre-training-data">Pre-training Data
&lt;/h3>&lt;p>预训练数据包括四个方面&lt;/p>
&lt;ol>
&lt;li>Web: 过滤低质量数据和使用模版产生的数据&lt;/li>
&lt;li>Multilingual: 基于 webpages 和 Fineweb-2&lt;/li>
&lt;li>Code: 基于 GitHub 和其他代码平台，作者使用了 [[Fill in the middle]] 来训练模型。&lt;/li>
&lt;li>Math &amp;amp; Scirence: 训练一个 classifier 来给数据进行打分。&lt;/li>
&lt;/ol>
&lt;p>最终，预训练数据一共包括 &lt;strong>23T token&lt;/strong>.&lt;/p>
&lt;h3 id="pre-training-recipe">Pre-training Recipe
&lt;/h3>&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4_5-pre-training.png"
width="1173"
height="406"
srcset="https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4_5-pre-training_hu3166061419089618432.png 480w, https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4_5-pre-training_hu7846610875110494798.png 1024w"
loading="lazy"
alt="Pre-training recipe of GLM4.5"
class="gallery-image"
data-flex-grow="288"
data-flex-basis="693px"
>&lt;/p>
&lt;p>预训练包括 2 个阶段:&lt;/p>
&lt;ol>
&lt;li>Pre-training: 使用网页数据进行训练&lt;/li>
&lt;li>Mid-training: 加入 code, math, science 数据进行训练，在这个阶段，作者使用了 repo-level 的 code 数据，合成的 reasoning 数据以及长上下文数据。作者将模型上下文从 4K 扩展到 32K，然后在扩展到 128K.&lt;/li>
&lt;/ol>
&lt;p>作者在 pre-training 的时候使用了 random truncation, 在 mid-training 的时候使用了 best-fit packing 技巧&lt;/p>
&lt;p>训练时，与 Kimi-k2 一样，作者使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-moonlight/" target="_blank" rel="noopener"
>Muon&lt;/a> 作为优化器。作者使用了 cosine decay schedule. batch size 从 16M token 到 64M token.&lt;/p>
&lt;h2 id="post-training">Post-training
&lt;/h2>&lt;p>Post-training 分为两个阶段：&lt;/p>
&lt;ul>
&lt;li>Stage 1, Expert Training. 构建 agent, reasoning, General chat 三个 domain 的专家模型&lt;/li>
&lt;li>Stage 2, Unified Training. 使用 self-distillation 来汇总多个模型的能力&lt;/li>
&lt;/ul>
&lt;p>训练框架如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-post-training-framework.png"
width="1337"
height="1001"
srcset="https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-post-training-framework_hu6853955760118926496.png 480w, https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-post-training-framework_hu15055473525730948009.png 1024w"
loading="lazy"
alt="Post-training of GLM4.5"
class="gallery-image"
data-flex-grow="133"
data-flex-basis="320px"
>&lt;/p>
&lt;h3 id="sft">SFT
&lt;/h3>&lt;p>两个 stage 都由 SFT 开始，&lt;/p>
&lt;ul>
&lt;li>在 Stage 1 里，SFT 的目标是让 expert model 掌握初步的 chat, reasoning 以及 tool-use 的能力。作者使用了一小部分包含 CoT 的 SFT 数据进行训练&lt;/li>
&lt;li>在 Stage 2 中，SFT 的目标是将不同的 expert model 蒸馏到一个模型中，作者使用了百万级的数据，包含 reasoning 任务和通用的 chat 数据，来训练模型的 hybrid reasoning 能力&lt;/li>
&lt;/ul>
&lt;p>在训练模型的 tool-use 能力是，作者发现，function call 在 code 场景下会出现混淆，提高了模型的学习成本。因此，作者的解决方法是使用了类似 XML 的 special token tags&lt;/p>
&lt;blockquote>
&lt;p>Recall
与之相反，Kimi-K2 认为模板应该尽可能简洁，因此 Kimi 采取了 TypeScript 作为 function call 的语言&lt;/p>
&lt;/blockquote>
&lt;p>从专家模型进行采样是，作者进行了数据过滤。还对数据进行了分级结果发现，使用难题进行训练可以提升模型 $2%\sim4%$ 的表现，多次采样也可以提高模型的表现&lt;/p>
&lt;p>Agentic SFT 数据的构建包括四个步骤：&lt;/p>
&lt;ol>
&lt;li>Agentic Framework and Tool Collection: 收集 MCP 和 tool API&lt;/li>
&lt;li>Task Synthesis: 合成不同的 agentic 任务&lt;/li>
&lt;li>Trajectory Generation: 采样生成的 rollout&lt;/li>
&lt;li>Quality Filtering: 过滤低质量的数据&lt;/li>
&lt;/ol>
&lt;h3 id="rl">RL
&lt;/h3>&lt;h4 id="reasoning-rl">Reasoning RL
&lt;/h4>&lt;p>这个阶段使用了 GRPO 算法进行训练，与 &lt;a class="link" href="https://maosong.website/p/notes-on-dapo/" target="_blank" rel="noopener"
>DAPO&lt;/a> 一样，作者去除了损失函数中的 KL divergence。&lt;/p>
&lt;p>首先，作者探究了课程学习对模型表现的影响，结果发现，课程学习可以有效提高模型的性能。因此，作者构建了一个 2 阶段的课程学习框架。实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-curriculum-RL.png"
width="995"
height="504"
srcset="https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-curriculum-RL_hu15212439347355957989.png 480w, https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-curriculum-RL_hu10130863368084009164.png 1024w"
loading="lazy"
alt="Performance of curriculum RL"
class="gallery-image"
data-flex-grow="197"
data-flex-basis="473px"
>&lt;/p>
&lt;p>可以看到，在第二个阶段，模型可以进一步通过更难的题目获得提升。&lt;/p>
&lt;p>其次，作者探究了以下渐进式扩展模型上下文对模型表现的影响。DeepScaleR 认为，逐步提高模型的上下文长度，可以有效提高模型的表现。但是，本文确认为这种方法会损害模型的性能，原因在于，模型在 SFT 阶段的上下文长度就是 64K, 如果我们降低模型的上下文长度，这会导致训练数据分布不一致，从而影响模型的长上下文表现。因此作者直接在 64K 的上下文上进行训练。&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-multi-stage-context-extension.png"
width="1159"
height="453"
srcset="https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-multi-stage-context-extension_hu8500596761537484178.png 480w, https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-multi-stage-context-extension_hu4453680687905474818.png 1024w"
loading="lazy"
alt="Ablation on progressive context extension"
class="gallery-image"
data-flex-grow="255"
data-flex-basis="614px"
>&lt;/p>
&lt;p>接下来，作者探究了以下采样温度对模型表现的影响，温度太低会导致模型探索能力下降，太高的话会导致输出质量下降。因此作者动态调整采样温度来平衡模型的性能以及探索能力。&lt;/p>
&lt;blockquote>
&lt;p>[!tip]
Kimi-K2 认为随着 RL 训练的进行，我们应该逐步降低采样温度来稳定模型的表现&lt;/p>
&lt;/blockquote>
&lt;p>最后，作者分析了以下 code 以及 Science RL 中的一些问题。对于 code RL, 作者发现，我们应该在 sequence 层面而不是 token 层面进行平均。对于 Science RL, 作者强调了高质量数据的重要性。实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-code-science-RL.png"
width="1146"
height="450"
srcset="https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-code-science-RL_hu12963305608509671776.png 480w, https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-code-science-RL_hu15562279798747579185.png 1024w"
loading="lazy"
alt="Ablation on Code and Science RL"
class="gallery-image"
data-flex-grow="254"
data-flex-basis="611px"
>&lt;/p>
&lt;h4 id="agent-rl">Agent RL
&lt;/h4>&lt;p>作者主要关注 web-search 以及 code generation 两个任务。对于 web-search, 作者构建了一个数据合成 pipeline, 用于生成 multi-step reasoning 的 QA 数据。构建过程包括基于知识图谱的 multi-hop reasoning 和 human-in-the-loop 的内容提取。对于 code generation, 作者基于 GitHub 的 PR 以及 issues 构建了 benchmark&lt;/p>
&lt;p>RL 的训练目标如下&lt;/p>
$$
\mathcal{L}(\theta) = \mathbb{E}_{x\sim\mathcal{D}}\left[\frac1K\sum_{i=1}^K(r(x,y_i) - \bar{r}(x))\right]
$$&lt;p>其中 $(x,y_i)$ 是基于 $\pi_{\mathrm{old}}$ 采样的 trace, $\bar{r}(x) = 1/k\sum_{i=1}^Kr(x,y_i)$ 是平均的 reward. 计算损失时，只有模型的回答参与计算。&lt;/p>
&lt;p>作者发现，通过训练模型的 web-search 以及 code generation 能分，模型在 tool-use 以及 coding 任务上的表现也有了提升。作者还是用了 format penalty 来保证模型输出格式的正确性。如果格式不对的话，模型获得的奖励是 0&lt;/p>
&lt;blockquote>
&lt;p>Recall
在 &lt;a class="link" href="https://maosong.website/p/notes-on-glm-4.1v-thinking/" target="_blank" rel="noopener"
>GLM-4.V-Thinking&lt;/a> 中，作者认为应该在 cold-start SFT 阶段让模型学会格式要求，而不是在 RL 阶段加入 format reward&lt;/p>
&lt;/blockquote>
&lt;p>由于 agent RL 的训练比较耗时，为了提高训练效率。作者首先基于 SFT 模型进行 agent RL 训练，训练到一定步数之后，作者使用 self-distillation 来将能力蒸馏回 SFT model, 接下来再基于 Self-distillation 后的 SFT 模型来进行 agent RL 训练&lt;/p>
&lt;blockquote>
&lt;p>Recall
&lt;a class="link" href="https://maosong.website/p/notes-on-seed1.5-vl/" target="_blank" rel="noopener"
>Seed1.5-VL&lt;/a> 和 LlaMA3.2 都提到了使用 multi-round SFT-RL 的形式来提高模型的表现&lt;/p>
&lt;/blockquote>
&lt;p>作者还发现，随着交互轮数的提升，模型的表现也有相应提升。实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-interaction-turns.png"
width="696"
height="428"
srcset="https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-interaction-turns_hu12097979109763063333.png 480w, https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-interaction-turns_hu17021798138852578449.png 1024w"
loading="lazy"
alt="Interaction turn scaling"
class="gallery-image"
data-flex-grow="162"
data-flex-basis="390px"
>&lt;/p>
&lt;h4 id="general-rl">General RL
&lt;/h4>&lt;p>General RL 用于提高模型的整体表现，解决潜在的问题以及提升关键能力，作者主要使用了 RLHF 和 RLAIF 两种方法&lt;/p>
&lt;p>对于 Holistic RL, 作者收集了 5000 条 prompt, reward 基于人类反馈和 AI 反馈。人类反馈用于训练一个 reward model, 对于 AI 反馈，作者构建了 scoring rubrics. 然后作者将两种反馈结合在一起&lt;/p>
&lt;p>对于 Instruction following RL, 作者构建了基于规则的奖励，reward model 的奖励以及 critical model 的奖励。实验结果显示，这种奖励方式可以有效降低模型的 reward hacking&lt;/p>
&lt;p>对于 function calling RL, 作者使用了 step-wise rule-based RL 来提高模型表现。对于 end-to-end multi-turn RL, 作者训练了一个 expert model 来蒸馏专家到模型。&lt;/p>
&lt;p>最后，对于 Pathology RL, 作者希望通过 RL 来解决潜在的问题，比如语言混合输出，重复输出以及格式错误等。作者构建了一批模型容易出错的数据，然后来训练模型。&lt;/p>
&lt;h4 id="infra">Infra
&lt;/h4>&lt;p>作者针对不同任务分别构建了不同的 scheduling 模式：&lt;/p>
&lt;ul>
&lt;li>对于通用 RL 任务，作者将 training engine 和 inference engine 放在一个 worker 来提高效率&lt;/li>
&lt;li>对于 agentic RL 任务，作者将 training 和 inference engine 分开，来提高 data throughput&lt;/li>
&lt;/ul>
&lt;p>在训练时，作者使用了 BF16 精度，在推理时，作者使用了 FP8 精度来提高推理效率。&lt;/p>
&lt;p>针对 agentic RL 任务，作者还进行了优化。与 Kimi-k2 类似，作者让 inference engine 持续产出 rollout, 然后让 training engine 来更新模型权重，最后同步到 inference engine 上&lt;/p>
&lt;h2 id="experiments">Experiments
&lt;/h2>&lt;p>整体表现如下图所示，GLm4.5 在 ARC benchmark 上的平均表现达到了第三名。&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-average_performance.png"
width="1071"
height="776"
srcset="https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-average_performance_hu7451238329626406108.png 480w, https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-average_performance_hu1953441247019446580.png 1024w"
loading="lazy"
alt="Average performance on ARC benchmarks"
class="gallery-image"
data-flex-grow="138"
data-flex-basis="331px"
>&lt;/p>
&lt;p>具体来看，&lt;/p>
&lt;ol>
&lt;li>在 agentic benchmark 上, GLM4.5 仅次于 o3 的表现&lt;/li>
&lt;li>在 coding benchmark 上，GLM4.5 次于 Claude Opus 4 和 Claude Sonnet 4, 排第三名&lt;/li>
&lt;li>在通用能力上，GLM&lt;/li>
&lt;/ol>
&lt;p>人工对比 coding agent 能力的结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-agent-coding-performance.png"
width="1143"
height="514"
srcset="https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-agent-coding-performance_hu12062937037509217689.png 480w, https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-agent-coding-performance_hu10453388556220715451.png 1024w"
loading="lazy"
alt="Comparison of GLM4.5 aginst other models"
class="gallery-image"
data-flex-grow="222"
data-flex-basis="533px"
>&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者提出了 GLM4.5， 一个基于 MoE 架构的大语言模型系列，包含 GLM4.5(355B-A22B) 和 GLM4.5-Air(106B-A12B) 两个模型，作者详细介绍了模型的架构，训练，数据和评估。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2508.06471" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on ARC-Hunyuan-Video-7B</title><link>https://maosong2022.github.io/p/notes-on-arc-hunyuan-video-7b/</link><pubDate>Tue, 12 Aug 2025 10:57:57 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-arc-hunyuan-video-7b/</guid><description>&lt;p>腾讯 ARC LAB 提出了 ARC-Hunyuan-Video-7B, 一个针对短视频理解和推理的视频多模态大模型。&lt;/p>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>作者首先提出了 Structured video comprehension 的概念&lt;/p>
&lt;blockquote>
&lt;p>the ability to decompose a video into its constituent events and narrative elements with temporal precision.&lt;/p>
&lt;/blockquote>
&lt;p>视频信息包含了 dense visual elements, 比如文字等信息，还有 rich audio streams, 比如音效，音乐，再就是 rapid narrative pacing, 用于强调情感。&lt;/p>
&lt;p>已有的多模态大模型对于视频的细粒度理解和推理能力存在不足。尽管 &lt;a class="link" href="https://maosong.website/p/notes-on-keye-vl/" target="_blank" rel="noopener"
>Keye-VL&lt;/a> 也是一个短视频理解多模态大模型，但是其并没有利用 audio 模态的信息。其他的 audio-visual LLM 虽然可以处理 audio 模态的信息，但是他们主要集中于通用场景的视频，这类视频的特点是：叙述节奏慢，信息密度低。因此，对于叙述节奏快，信息丰富的短视频，这些模型表现就比较差。&lt;/p>
&lt;p>基于以上这些问题，作者提出了 ARC-Hunyuan-Video-7B, 一个基于 Hunyuan-7B vision-language model 的短视频多模态大模型，其主要做了两点改进：&lt;/p>
&lt;ol>
&lt;li>加入了一个 audio encoder 用于提取 audio 信息，然后对 audio 信息与视觉信息进行同步&lt;/li>
&lt;li>使用了一个 timestamp overlap 机制，将时间戳印在视频的帧上，帮助模型理解事件的时序信息&lt;/li>
&lt;/ol>
&lt;p>作者还涉及了多阶段的训练策略，来提高模型的表现。最后在测试时，对于一个 1 分钟的视频，模型在 H20 GPU 上进需要 10 秒就可以处理完毕。&lt;/p>
&lt;h2 id="method">Method
&lt;/h2>&lt;h3 id="architecture">Architecture
&lt;/h3>&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-arc-hunyuan-video-7b/ARC-Hunyuan-Video-architecture.png"
width="682"
height="467"
srcset="https://maosong2022.github.io/p/notes-on-arc-hunyuan-video-7b/ARC-Hunyuan-Video-architecture_hu7049036770913836449.png 480w, https://maosong2022.github.io/p/notes-on-arc-hunyuan-video-7b/ARC-Hunyuan-Video-architecture_hu10155841825363121766.png 1024w"
loading="lazy"
alt="Architecture of ARC-Hunyuan-Video-7B"
class="gallery-image"
data-flex-grow="146"
data-flex-basis="350px"
>&lt;/p>
&lt;p>模型基于 Hunyuan-7B-VLM 开发得到，&lt;/p>
&lt;ul>
&lt;li>Visual Encoding: 作者将时间戳以 &lt;code>HH:MM:SS&lt;/code> 的形式印在视频的帧上。视频的采样率为 1FPS, 超过 150s 的视频会进行均匀采样降低至 150s. 每一帧会 resize 到 $640\times 640$, 最后每一帧输出 112 token&lt;/li>
&lt;li>Audio Encoding: 使用 Whisper 作为编码器。小于 300s 的视频，会按照 30s 为单位切分为不同的 chunk, 大于 300s 的视频，会分割为 150 个 chunk, 然后每个 chunk 只保留开头 2s 的内容。最后，使用 Whisper 进行特征提取，最后再通过 MLP 与 LLM 的特征空间进行对齐&lt;/li>
&lt;li>Visual-audio Synchronization: 先对 audio token 进行 zero padding 与 Visual token 的个数进行对齐，然后将两者相加&lt;/li>
&lt;/ul>
&lt;h3 id="training">Training
&lt;/h3>&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-arc-hunyuan-video-7b/ARC-Hunyuan-Video-training-overall.png"
width="682"
height="467"
srcset="https://maosong2022.github.io/p/notes-on-arc-hunyuan-video-7b/ARC-Hunyuan-Video-training-overall_hu11903493230483800583.png 480w, https://maosong2022.github.io/p/notes-on-arc-hunyuan-video-7b/ARC-Hunyuan-Video-training-overall_hu12166673853554469610.png 1024w"
loading="lazy"
alt="Training pipeline of ARC-Hunyuan-Video-7B"
class="gallery-image"
data-flex-grow="146"
data-flex-basis="350px"
>&lt;/p>
&lt;h4 id="pre-training">Pre-training
&lt;/h4>&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-arc-hunyuan-video-7b/ARC-Hunyuan-Video-annotation.png"
width="1362"
height="289"
srcset="https://maosong2022.github.io/p/notes-on-arc-hunyuan-video-7b/ARC-Hunyuan-Video-annotation_hu16957196791930419461.png 480w, https://maosong2022.github.io/p/notes-on-arc-hunyuan-video-7b/ARC-Hunyuan-Video-annotation_hu5574767225043336818.png 1024w"
loading="lazy"
alt="Boostrapped annotation pipeline of ARC-Hunyuan-Video-7B"
class="gallery-image"
data-flex-grow="471"
data-flex-basis="1131px"
>&lt;/p>
&lt;p>作者首先保住了一些数据。具体流程就是，使用 Whisper-v3 来转录带时间戳的音频，然后使用 InternVL-2.5-8B [[InternVL-2.5]] 来生成细粒度的 caption. 作者还使用 CoT prompt 策略，让模型一步步生成时间的描述，创作者的想法以及如何吸引用户的 tag 等。&lt;/p>
&lt;p>预训练数据如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Category&lt;/th>
&lt;th>data&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Video description and summary&lt;/td>
&lt;td>4.5M short-form video&lt;br>0.2M public video&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Image caption and OCR&lt;/td>
&lt;td>4.7M image-text pairs&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ASR&lt;/td>
&lt;td>3.2M audio-text pairs&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Video temporal grounding&lt;/td>
&lt;td>0.5M temporally grounding instances&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Video multi-granular caption&lt;/td>
&lt;td>50K high-quality samples&lt;br>80K in-house videos&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>训练包含两个 stage:&lt;/p>
&lt;ol>
&lt;li>Stage 1: 使用 ASR 数据训练模型接受音频输入的能力，为了避免视觉模态能力下降，作者还加入了 Image-text pair data 来训练，缺失的模态用 zero padding 来补齐&lt;/li>
&lt;li>Stage 2: 使用全部数据进行训练，仅训练 MLP 和 LLM&lt;/li>
&lt;/ol>
&lt;h4 id="post-training">Post-training
&lt;/h4>&lt;p>作者首先进行了消融实验，探究 human-annotated 数据对模型表现的影响。作者收集了 140 条人类标注的数据，然后基于这些数据进行消融实验：&lt;/p>
&lt;ol>
&lt;li>直接使用人类标注数据进行 SFT, 模型表现变化不大&lt;/li>
&lt;li>直接使用人类标注数据作为正样本，合成数据作为负样本进行 DPO, 模型表现变化也不大&lt;/li>
&lt;/ol>
&lt;p>作者分析原因认为，&lt;strong>人类标注数据和合成数据之间存在 distribution shift&lt;/strong>.&lt;/p>
&lt;p>受 DeepSeek-R1 启发，作者决定使用 GRPO 算法来提高模型的表现，训练任务包含两个：&lt;/p>
&lt;ol>
&lt;li>multi-dimensional Multi-choide QA: 提高模型的视频理解能力&lt;/li>
&lt;li>Temporal video grounding: 提高模型的时序感知能力&lt;/li>
&lt;/ol>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Stage&lt;/th>
&lt;th>Data&lt;/th>
&lt;th>Module&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>SFT&lt;/td>
&lt;td>MCQ:&lt;br> - 460K open-ended QA&lt;br> - 70K MCQA&lt;br> - 20K QA&lt;br>Grounding:&lt;br> - 10K academic&lt;br> - 5K real-world&lt;br>General:&lt;br> - 45K description&lt;br> - 12K caption&lt;/td>
&lt;td>MLP+LLM&lt;/td>
&lt;td>提高指令跟随能力&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Cold Start SFT&lt;/td>
&lt;td>- 90K MCQA&lt;br>- 18K temporal grounding&lt;br>- 20K open-ended QA&lt;br>- 15K summarization&lt;br>- 3K chapter-level captioning&lt;/td>
&lt;td>MLP+LLM&lt;/td>
&lt;td>初步激活模型的 reas 能力&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RL&lt;/td>
&lt;td>- 100K MCQ&lt;br>- 35K temporal grounding&lt;/td>
&lt;td>LLM&lt;/td>
&lt;td>提升模型的 reasoning 能力&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SFT&lt;/td>
&lt;td>- 25K human-annotated subjective question&lt;br>- 100K MCQ with CoT&lt;br>- 50K temporal grounding with reasoning traces&lt;/td>
&lt;td>-&lt;/td>
&lt;td>使用人类标注数据进一步提高模型的能力&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="experiments">Experiments
&lt;/h2>&lt;p>作者构建了一个评估短视频理解能力的 benchmark: ShortVid-Bench, 评估以下方面：&lt;/p>
&lt;ol>
&lt;li>Temporal Reasoning and Localization&lt;/li>
&lt;li>Affective Intent Classification&lt;/li>
&lt;li>Creator Intent Taxonomy&lt;/li>
&lt;li>Narrative Comprehension&lt;/li>
&lt;li>Humor &amp;amp; Meme Deconstruction&lt;/li>
&lt;li>Creative Innovation Analysis&lt;/li>
&lt;/ol>
&lt;p>对比的模型包括 Qwen2.5-VL-7B, Qwen2.5-omni-7B 和 Keye-VL-8B&lt;/p>
&lt;p>评估结果如下：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-arc-hunyuan-video-7b/ARC-Hunyuan-Video-performance.png"
width="1367"
height="244"
srcset="https://maosong2022.github.io/p/notes-on-arc-hunyuan-video-7b/ARC-Hunyuan-Video-performance_hu14059164927016574712.png 480w, https://maosong2022.github.io/p/notes-on-arc-hunyuan-video-7b/ARC-Hunyuan-Video-performance_hu131946593681889770.png 1024w"
loading="lazy"
alt="Performance of ARC-Hunyuan-Video-7B"
class="gallery-image"
data-flex-grow="560"
data-flex-basis="1344px"
>&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者提出了 ARC-Hunyuan-Video-7B, 一个针对短视频的视频理解多模态大模型。作者详细介绍了模型的架构，训练数据以及评估。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2507.20939" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/TencentARC/ARC-Hunyuan-Video-7B" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on GQA</title><link>https://maosong2022.github.io/p/notes-on-gqa/</link><pubDate>Thu, 07 Aug 2025 18:08:36 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-gqa/</guid><description>&lt;p>Google Research 在 23 年 12 月份提出了 Group Query Attention (GQA), 一个提升 multi-head attention 效率的方法。GQA 自 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2/" target="_blank" rel="noopener"
>Qwen2&lt;/a> 系列开始被应用。&lt;/p>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>Multi-head attention (MHA) 的问题在于 inference 阶段，每次 decoding，都需要重新加载 attention 模块中 query layer, key layer 和 value layer 的权重，而加载权重会受带宽限制。&lt;/p>
&lt;p>已有的工作有 &lt;a class="link" href="https://maosong.website/p/notes-on-mqa/" target="_blank" rel="noopener"
>MQA&lt;/a>, 也就是我们把多个 head 的 key layer 以及 value layer 压缩成一个，这样对于 $h$ 个 head 的 attention，我们有 $h$ 个 query layer，$1$ 个 key layer 以及 1 个 value layer. 但是 MQA 的问题在于其会导致性能下降，而且训练过程会不稳定。&lt;/p>
&lt;p>因此，在本文中作者就作出了两点贡献：&lt;/p>
&lt;ol>
&lt;li>如何将一个 MHA 模型转化为一个一个 MQA 模型&lt;/li>
&lt;li>提出了 Group Query Attention (GQA)，在保持模型性能的同时，提高计算效率&lt;/li>
&lt;/ol>
&lt;h2 id="method">Method
&lt;/h2>&lt;h3 id="uptraining">Uptraining
&lt;/h3>&lt;p>将 MHA 模型转化为 MQA 模型分为两步：&lt;/p>
&lt;ol>
&lt;li>将 MHA 权重转化为 MQA 权重&lt;/li>
&lt;li>额外的预训练&lt;/li>
&lt;/ol>
&lt;p>具体来讲，作者使用了一个 mean pooling 的方法，来将不同 head 的 query layer 以及 key layer 的权重转化为 MQA 对应 layer 的权重。然后作者 pre-training 若干步来让模型适应新的结构。&lt;/p>
&lt;h3 id="gqa">GQA
&lt;/h3>&lt;p>GQA 的思路在于在 MHA 和 MQA 之间达到一个平衡，也就是说我们将 key layer 和 value layer 进行分组，每个组内共享一个 key layer 和 value layer, 我们假设有 $h$ 个 head，$G$ 个 group，那么&lt;/p>
&lt;ol>
&lt;li>$G=1$ 时，所有的 head 共享一个 key layer 和一个 value layer, 此时 GQA 等价于 MQA&lt;/li>
&lt;li>$G=H$ 时，每个 head 都有一个 key layer 和一个 value layer, 此时 GQA 等价于 MHA&lt;/li>
&lt;li>$1&amp;lt;G&amp;lt;H$ 时，GQA 时 MQA 和 MHA 的一个 trade-off，兼顾两者的性能与效率&lt;/li>
&lt;/ol>
&lt;p>三者的示意图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-gqa/MQA_comparison_group_query.png"
width="2080"
height="784"
srcset="https://maosong2022.github.io/p/notes-on-gqa/MQA_comparison_group_query_hu16749365881659510466.png 480w, https://maosong2022.github.io/p/notes-on-gqa/MQA_comparison_group_query_hu11954291867796797564.png 1024w"
loading="lazy"
alt="Overview of grouped-query methods"
class="gallery-image"
data-flex-grow="265"
data-flex-basis="636px"
>&lt;/p>
&lt;h2 id="code">Code
&lt;/h2>&lt;p>MQA 的代码也比较好理解，我们首先定义 group size，即 &lt;code>num_key_value_heads&lt;/code>, 然后基于 group size 定义对应的 key layer &lt;code>self.k_proj&lt;/code> 和 value layer &lt;code>self.v_proj&lt;/code>.&lt;/p>
&lt;p>计算得到 &lt;code>key_states&lt;/code> 和 &lt;code>value_states&lt;/code> 之后，在计算 attention，即 &lt;code>eager_attention_forward&lt;/code> 的时候，我们对 &lt;code>key_states&lt;/code> 和 &lt;code>value_states&lt;/code> 进行复制，即 &lt;code>repeat_kv&lt;/code>&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;span class="lnt">49
&lt;/span>&lt;span class="lnt">50
&lt;/span>&lt;span class="lnt">51
&lt;/span>&lt;span class="lnt">52
&lt;/span>&lt;span class="lnt">53
&lt;/span>&lt;span class="lnt">54
&lt;/span>&lt;span class="lnt">55
&lt;/span>&lt;span class="lnt">56
&lt;/span>&lt;span class="lnt">57
&lt;/span>&lt;span class="lnt">58
&lt;/span>&lt;span class="lnt">59
&lt;/span>&lt;span class="lnt">60
&lt;/span>&lt;span class="lnt">61
&lt;/span>&lt;span class="lnt">62
&lt;/span>&lt;span class="lnt">63
&lt;/span>&lt;span class="lnt">64
&lt;/span>&lt;span class="lnt">65
&lt;/span>&lt;span class="lnt">66
&lt;/span>&lt;span class="lnt">67
&lt;/span>&lt;span class="lnt">68
&lt;/span>&lt;span class="lnt">69
&lt;/span>&lt;span class="lnt">70
&lt;/span>&lt;span class="lnt">71
&lt;/span>&lt;span class="lnt">72
&lt;/span>&lt;span class="lnt">73
&lt;/span>&lt;span class="lnt">74
&lt;/span>&lt;span class="lnt">75
&lt;/span>&lt;span class="lnt">76
&lt;/span>&lt;span class="lnt">77
&lt;/span>&lt;span class="lnt">78
&lt;/span>&lt;span class="lnt">79
&lt;/span>&lt;span class="lnt">80
&lt;/span>&lt;span class="lnt">81
&lt;/span>&lt;span class="lnt">82
&lt;/span>&lt;span class="lnt">83
&lt;/span>&lt;span class="lnt">84
&lt;/span>&lt;span class="lnt">85
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">repeat_kv&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n_rep&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">batch&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_key_value_heads&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">slen&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">head_dim&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">n_rep&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">hidden_states&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="p">:]&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">expand&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">batch&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_key_value_heads&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n_rep&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">slen&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">head_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reshape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">batch&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_key_value_heads&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">n_rep&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">slen&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">head_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">eager_attention_forward&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">module&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">query&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">key&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">value&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attention_mask&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">scaling&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">float&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dropout&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">float&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">0.0&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">**&lt;/span>&lt;span class="n">kwargs&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Unpack&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">TransformersKwargs&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">key_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">repeat_kv&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">key&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">module&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_key_value_groups&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">value_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">repeat_kv&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">value&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">module&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_key_value_groups&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attn_weights&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">matmul&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">query&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">key_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transpose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">scaling&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">attention_mask&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">causal_mask&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">attention_mask&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="p">:&lt;/span> &lt;span class="n">key_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">]]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attn_weights&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">attn_weights&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">causal_mask&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attn_weights&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">functional&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">softmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">attn_weights&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">query&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dtype&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attn_weights&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">functional&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dropout&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">attn_weights&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">dropout&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">training&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">module&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">training&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attn_output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">matmul&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">attn_weights&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">value_states&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attn_output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">attn_output&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transpose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">contiguous&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">attn_output&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">attn_weights&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Qwen3Attention&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Qwen3Config&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">layer_idx&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="fm">__init__&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">getattr&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;head_dim&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_attention_heads&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">scaling&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="o">**-&lt;/span>&lt;span class="mf">0.5&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_attention_heads&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">attention_bias&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">k_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_key_value_heads&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">attention_bias&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">v_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_key_value_heads&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">attention_bias&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">o_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_attention_heads&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">attention_bias&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">position_embeddings&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">tuple&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attention_mask&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">past_key_value&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">Cache&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">cache_position&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">LongTensor&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">**&lt;/span>&lt;span class="n">kwargs&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Unpack&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">FlashAttentionKwargs&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">tuple&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">tuple&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">]]]:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">input_shape&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[:&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_shape&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">input_shape&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">query_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_norm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_shape&lt;/span>&lt;span class="p">))&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transpose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">key_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">k_norm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">k_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_shape&lt;/span>&lt;span class="p">))&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transpose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">value_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">v_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_shape&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transpose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">cos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sin&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">position_embeddings&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">query_states&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">key_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">apply_rotary_pos_emb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">query_states&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">key_states&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">cos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sin&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attn_output&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">attn_weights&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">eager_attention_forward&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">query_states&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">key_states&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">value_states&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attention_mask&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dropout&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.0&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">scaling&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">scaling&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sliding_window&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sliding_window&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="c1"># diff with Llama&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">**&lt;/span>&lt;span class="n">kwargs&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attn_output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">attn_output&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reshape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">input_shape&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">contiguous&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attn_output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">o_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">attn_output&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">attn_output&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">attn_weights&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>本文中，作者提出了一个解决 multi-query attention 的 uptraining 方法，以及提出了 GQA，一个结合 MHA 表现和 MQA 效率的新型注意力机制。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2305.13245" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on MQA</title><link>https://maosong2022.github.io/p/notes-on-mqa/</link><pubDate>Thu, 07 Aug 2025 18:06:37 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-mqa/</guid><description>&lt;p>Google 在 2019 年提出了 multi-query attention (MQA), 用于解决 MQA 内存带宽瓶颈问题。&lt;/p>
&lt;h2 id="method">Method
&lt;/h2>&lt;h3 id="background">Background
&lt;/h3>&lt;p>对于 multi-head attention, 我们假设其 hidden size 为 $d$, 有 $h$ 个 heads, 每个 head 的 size 为 $d_h=d/h$, 输入 sequence 长度为 $n$, batch size 为 $d$. 则总的 arithmetic operations 为 $O(bnd^2)$. 总的内存访问量为 $O(bnd + bhn^2+d^2)$, 第一项是 $Q,K,V$ 的内存占用（$Q,K,V$ 分别是 query, key 和 value layer 的输出），第二项是 attention score 的占用，第三项是 query, key 和 value layer 的权重。&lt;/p>
&lt;p>因此，其 &lt;strong>Memory Access Ratio&lt;/strong> (MAR), 也就是内存访问量 与 arithmetic operations 之比为&lt;/p>
$$
O\left(\frac1k + \frac{1}{bn}\right)
$$&lt;p>对于现代的 GPU 来说，其一般算力比较强，但是内存访问带宽相对较慢，因此我们希望 MAR 越低越好，以充分发挥 GPU 的算力。&lt;/p>
&lt;h3 id="mha-analysis">MHA Analysis
&lt;/h3>&lt;p>在训练的时候，由于我们知道 ground truth sequence, 因此我们可以并行计算。但是在 inference 的时候，我们只能 token-by-token 进行计算，因此我们分析一下 token-by-token 场景下的 MAR&lt;/p>
&lt;p>我们整体的 arithmetic operations 还是 $O(bnd^2)$.&lt;/p>
&lt;p>但是，现在我们要调用 $n$ 次 multi-head attention, 因此我们总的内存访问量为 $O(bn^2d + nd^2)$, 第一项是 $K$ 和 $V$ , 第二项是 query, key 和 value layer 的权重。&lt;/p>
&lt;p>这种情况下，MAR 就变成了&lt;/p>
$$
O\left(\frac{n}{d} + \frac{1}{b}\right)
$$&lt;p>当 $n\approx d$ 或者 $b\approx 1$ 时，MAR 就非常接近于 1，意味着内存带宽成了一个主要的瓶颈。为了解决这个问题，我们有两种做法：&lt;/p>
&lt;ol>
&lt;li>提升 batch size $b$, 也就是同时 inference 多次&lt;/li>
&lt;li>降低 $K$ 和 $V$ 的大小&lt;/li>
&lt;/ol>
&lt;h3 id="mqa">MQA
&lt;/h3>&lt;p>MQA 的做法就是第二种，也就是降低 $K$ 和 $V$ 的大小，但是 $K,V$ 分别是 key 和 value layer 的输出，要降低输出大小，我们就必须改变 key 和 value layer 的 size。基于这个考虑，作者在所有的 head 上共享了一个 key 和 value layer，也就是说，原来&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">k_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_heads&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">head_dim&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># (d, n*d_h)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">v_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_heads&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">head_dim&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># (d, n*d_h)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>现在在 MQA 里，其变成了&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">k_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">head_dim&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># (d, n*d_h)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">v_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">head_dim&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># (d, n*d_h)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="mqa-analysis">MQA Analysis
&lt;/h3>&lt;p>我们还是在 token-by-token 的场景下进行分析。&lt;/p>
&lt;p>我们整体的 arithmetic operations 还是 $O(bnd^2)$.&lt;/p>
&lt;p>调用 $n$ 次 multi-query attention 的总的内存访问量为 $O(bnd +bn^2d_h+ nd^2)$, 第一项是 $q$ , 第二项是 $K$ 和 $V$ , 第三项是是 query, key 和 value layer 的权重。&lt;/p>
&lt;p>此时，MAR 变成了&lt;/p>
$$
O\left(\frac{1}{d} + \frac{n}{dh}+\frac{1}{b}\right)
$$&lt;p>现在，我们就将 $n/d$ 这一项给降低了 $h$ 倍。如果我们的 batch size 足够大的话，理论上 MQA 应该能极大提高整体的计算效率。&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>MQA 为了追求极致的内存带宽占用，选择使用单一的 key 和 value, 来极大提高 inference 的 decoding 效率，但是后来在 GQA 中验证发现，MQA 虽然非常高效，但是其表现比较差，这也是后来没有得以应用的原因。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/1911.02150" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Moonlight</title><link>https://maosong2022.github.io/p/notes-on-moonlight/</link><pubDate>Thu, 07 Aug 2025 10:49:32 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-moonlight/</guid><description>&lt;p>Kimi 提出了 Moonlight, 一个基于 Muon optimizer 训练得到的 16B-A3B MoE LLM. 作者详细介绍了如何 scale up muon optimizer.&lt;/p>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>&lt;a class="link" href="https://maosong.website/p/notes-on-muon-blog/" target="_blank" rel="noopener"
>Muon&lt;/a> 验证了 Muon optimizer 在小语言模型 nanoGPT 上的表现，但是对于更大规模 LLM 的表现，尚未有人探究。因此 Kimi 就希望在大规模 LLM 上验证 Muon optimizer 的表现。作者主要进行了两点改进：&lt;/p>
&lt;ol>
&lt;li>加入 weight decay&lt;/li>
&lt;li>调整了不同参数更新的 scale&lt;/li>
&lt;/ol>
&lt;p>基于改进后的 Muon optimizer, 其训练效率相比于 AdamW 提升了 2 倍。作者基于 Muon Optimizer 训练得到了 Moonlight, 一个 16B-A3B 的 MoE LLM.&lt;/p>
&lt;p>作者主要作出了三点贡献：&lt;/p>
&lt;ol>
&lt;li>探究了 weight decay 在 scaling Muon 时的作用&lt;/li>
&lt;li>分布式 Muon optimizer 的实现&lt;/li>
&lt;li>验证了 Muon optimizer 的 scaling law&lt;/li>
&lt;/ol>
&lt;h2 id="method">Method
&lt;/h2>&lt;h3 id="background">Background
&lt;/h3>&lt;p>作者首先介绍了一下 Muon optimizer, 给定步数 $t$, 参数矩阵 $W_{t-1}$, momentum $\mu$, 学习率 $\eta_t$ 以及目标函数 $\mathcal{L}_t$, Muon optimizer 的更新方式如下：&lt;/p>
$$
\begin{aligned}
M_t &amp;= \mu M_{t-1} + \nabla\mathcal{L}_t(W_{t-1})\\
O_t &amp;= \mathrm{Newton-Schulz}(M_t)\\
W_t &amp;= W_{t-1} - \eta_t O_t
\end{aligned}
$$&lt;p>这里 $M_t$ 是 gradient 的 momentum, 初始化为 $M_0=0$. 在上面的更新公式中，Newton-Schulz 的作用是求解 $(M_tM_t^T)^{-1/2}M_t$. 令 $M_t=U\Sigma V^T$ 为 SVD 分解， 我们有&lt;/p>
$$
(M_tM_t^T)^{-1/2}M_t = UV^T
$$&lt;p>这是一个半正交矩阵，即 $(UV^T)^T(UV^T)=I$.&lt;/p>
&lt;p>Newton-Schulz 迭代的具体公式如下：&lt;/p>
$$
X_0 = \frac{M_t}{\|M_t\|_F},\quad X_k = aX_{k-1} + b(X_{k-1}X_{k-1}^T)X_{k-1} + c(X_{k-1}X_{k-1}^T)^2X_{k-1}
$$&lt;p>其中，normalization 是为了保证 Newton-Schulz 的收敛性。 $a,b,c$ 是三个超参数，在 Muon 中设置为 $(a,b,c)=(3.4445, 4.7750, 2.0315)$.&lt;/p>
&lt;h3 id="scaling-up-muon">Scaling up Muon
&lt;/h3>&lt;p>作者发现，尽管 Muon 在小规模场景下 work 的很好，但是大规模性场景下的收益就非常有限了。作者发现，这是因为模型的参数以及每一层输出的 RMS 变得很大，这可能会影响模型的性能。因此，作者就和 AdamW 一样使用 weight dacay 来避免这个问题，即&lt;/p>
$$
W_t =W_{t-1} - \eta_t(O_t + \lambda W_{t-1})
$$&lt;p>作者通过实验对比了 AdamW, vanilla Muon 和 Muon w/ weigth decay 三者的表现，实验结果如下图所示&lt;/p>
&lt;p>实验结果显示，尽管 vanilla Muon 手链最快，但是由于其权重增长很快，因此最后模型的表现不如 AdamW 和 Muon w/ weigth decay.&lt;/p>
&lt;p>接下来，作者分析了以下更新矩阵的 Root Mean Square (RMS), 结论是 Muon optimizer 的 RMS 与参数矩阵的形状相关：&lt;/p>
&lt;blockquote>
&lt;p>Lemma
For a full-rank matrix parameter of shape $[A, B]$, its theoretical Muon update RMS is $\sqrt{1/\max(A, B)}$.&lt;/p>
&lt;/blockquote>
&lt;p>证明如下：通过 Newton-Schulz 迭代，我们得到 $O_t=UV^T$, 其中 $M_t=U\Sigma V^T$ 是 SVD 分解，我们有&lt;/p>
$$
\mathrm{RMS}(O_t) = \sqrt{\frac{\sum_{i=1}^A\sum_{j=1}^BO_{t,i,j}^2}{AB}}=\sqrt{\frac{r}{AB}}
$$&lt;p>其中, $r=\mathrm{rank}(M_t)$ , 这样就完成了证明。&lt;/p>
&lt;p>而 Adam 和 AdamW 的 RMS 都在 $1$ 附近。作者认为 RMS 也会影响模型表现：&lt;/p>
&lt;ol>
&lt;li>当 $\max(A,B)$ 过大时，如 dense MLP matrix, 其更新就会变得很小，限制了模型的表现&lt;/li>
&lt;li>当 $\max(A,B)$ 过小时，如 &lt;a class="link" href="https://maosong.website/p/notes-on-gqa/" target="_blank" rel="noopener"
>GQA&lt;/a> 中的 KV head 或者 DeepSeek-V3 中的 MLA, 更新又会变得很大，导致训练不稳定。&lt;/li>
&lt;/ol>
&lt;p>因此，作者就提出了一个 rescaling 的技巧，来消除 Muon optimizer 的影响。&lt;/p>
&lt;p>作者通过实验发现，AdamW 的 RMS 通常在 $0.2\sim0.4$ 左右，因此，作者将 Muon optimizer 的更新设置如下&lt;/p>
$$
W_t = W_{t-1} - \eta_t(0.2\cdot O_t\cdot \sqrt{\max(A,B)} + \lambda W_{t-1})
$$&lt;p>基于这个改变， Muon 和 AdamW 可以共享学习率以及 weight decay 参数。&lt;/p>
&lt;h3 id="distributed-muon">Distributed Muon
&lt;/h3>&lt;p>ZeRO-1 天然适合 AdamW, 因为 AdamW 都是 element-wise 进行计算的。但是 Muon 则需要梯度矩阵的全部信息。因此，作者就针对 ZeRO-1 进行适配， 提出了 &lt;strong>Distributed Muon&lt;/strong>, 分布式版本将优化器的状态进行切分，然后加入了两个额外的操作：&lt;/p>
&lt;ol>
&lt;li>DP gather: 将 ZeRO-1 切分的梯度矩阵 gather 为一个完整的矩阵&lt;/li>
&lt;li>Calculate Full Update: 对完整的梯度矩阵执行 Newton-Schulz 迭代&lt;/li>
&lt;/ol>
&lt;p>最终，Distributed Muon 的算法如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-moonlight/Moonlight-Distributed-muon.png"
width="1365"
height="553"
srcset="https://maosong2022.github.io/p/notes-on-moonlight/Moonlight-Distributed-muon_hu5196607620242879402.png 480w, https://maosong2022.github.io/p/notes-on-moonlight/Moonlight-Distributed-muon_hu16903668799533242919.png 1024w"
loading="lazy"
alt="Distributed Muon"
class="gallery-image"
data-flex-grow="246"
data-flex-basis="592px"
>&lt;/p>
&lt;p>最后，作者分析了一下 distributed Muon 和 distributed AdamW 的内存和算力占用：&lt;/p>
&lt;ol>
&lt;li>内存开销：Muon 只有一阶矩，而 AdamW 有二阶矩，因此 Muon 的额外内存开销为 AdamW 的一半。&lt;/li>
&lt;li>通信开销：对于 ZeRO-1，通信开销来源于三个过程：All-Gather 参数 $P$ 用于前向传播, Reduce-Scatter 梯度 $G$ 用于反向传播, All-Gather 更新后的参数 $P$ 用于下一轮的前向传播。AdamW 不引入额外通信，所以其每个参数的通信量为 $4+4=8$, 分别代表 $G$ 和 $P$ 的通信量。而 Muon 则需要额外的一次通信来得到 full matrix, 因此每个参数通信量为 $4+4+2=10$, 分别代表 $P, G$ 和 full matrix. 也就是说，分布式 Muon 的通信量最高为 AdamW 的 $1.25$ 倍。实际上由于我们使用 multiple DP, 这个比例会更接近于 $1.0$.&lt;/li>
&lt;li>latency：Distributed Muon 相比于 AdamW latency 更高，这是因为 Muon 需要进行 DP gather 以及计算 Newton-Schulz 迭代。但实际上，latency 很小，因为 Newton-Schulz 迭代只需要迭代 5 次，并且 optimizer 的 end-to-end latency 相比于 forward-backward 过程是可以忽略的。一些额外的技巧也可以降低 latency.&lt;/li>
&lt;/ol>
&lt;p>实际在训练的过程中，作者发现 Distributed Muon 相比于 AdamW 并没有太明显的 latency.&lt;/p>
&lt;h2 id="experiments">Experiments
&lt;/h2>&lt;h3 id="scaling-law-of-muon">Scaling Law of Muon
&lt;/h3>&lt;p>作者分析了一下 Muon Optimizer 的 scaling law, 实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-moonlight/Moonlight-scaling-law.png"
width="825"
height="729"
srcset="https://maosong2022.github.io/p/notes-on-moonlight/Moonlight-scaling-law_hu792594416422527289.png 480w, https://maosong2022.github.io/p/notes-on-moonlight/Moonlight-scaling-law_hu10928916953722435004.png 1024w"
loading="lazy"
alt="Scaling law for Muon and AdamW"
class="gallery-image"
data-flex-grow="113"
data-flex-basis="271px"
>&lt;/p>
&lt;p>实验结果表明，在最优设置下，Muon Optimizer 只需要 $52%$ 的 FLOPs 就可以达到 AdamW 的表现&lt;/p>
&lt;h3 id="pretraining-with-muon">Pretraining with Muon
&lt;/h3>&lt;p>作者分贝使用 AdamW 和 Muon 训练模型，然后评测了以下模型在不同 benchmark 上的表现，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-moonlight/Moonlight-pre-training-performance.png"
width="970"
height="577"
srcset="https://maosong2022.github.io/p/notes-on-moonlight/Moonlight-pre-training-performance_hu15788088666298477644.png 480w, https://maosong2022.github.io/p/notes-on-moonlight/Moonlight-pre-training-performance_hu8873686752845461476.png 1024w"
loading="lazy"
alt="Pretraining performance of different optimizer"
class="gallery-image"
data-flex-grow="168"
data-flex-basis="403px"
>&lt;/p>
&lt;p>可以看到，在相同的设置下，Muon optimizer 的表现更好。&lt;/p>
&lt;h3 id="dynamics-of-singular-spectrum">Dynamics of Singular Spectrum
&lt;/h3>&lt;p>Muon optimizer 的核心思想就是让比较难更新的方向也能被更新到，本节作者就探究了 Muon 是否满足这个性质，作者对参数矩阵进行 SVD 分解，然后定义 SVD entropy 如下&lt;/p>
$$
H(\sigma) = -\frac{1}{\log n}\sum_{i=1}^n\frac{\sigma_i^2}{\sum_{j=1}^n\sigma_j^2}\log\frac{\sigma_i^2}{\sum_{j=1}^n\sigma_j^2}
$$&lt;p>作者对 SVD entropy 可视化如下&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/Muonlight-SVD-entropy.png"
loading="lazy"
alt="Visualization of SVD entropy"
>&lt;/p>
&lt;p>可以看到，Muon optimizer 的 SVD entropy 比 AdamW 更大，这说明 AdamW 的更新方向更多更广，验证了 Muon optimizer 的核心思想&lt;/p>
&lt;h3 id="sft-with-muon">SFT with Muon
&lt;/h3>&lt;p>作者还在 SFT 阶段验证了 Muon optimizer 的有效性。实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-moonlight/Moonlight-SFT-performance.png"
width="922"
height="262"
srcset="https://maosong2022.github.io/p/notes-on-moonlight/Moonlight-SFT-performance_hu10176444289637991591.png 480w, https://maosong2022.github.io/p/notes-on-moonlight/Moonlight-SFT-performance_hu1757576398384477888.png 1024w"
loading="lazy"
alt="Performance of Muon on SFT stage"
class="gallery-image"
data-flex-grow="351"
data-flex-basis="844px"
>&lt;/p>
&lt;p>结论主要有两个：&lt;/p>
&lt;ol>
&lt;li>预训练阶段与 SFT 阶段使用不同的优化器时，模型表现没有明显区别&lt;/li>
&lt;li>SFT 阶段使用 Muon 可以达到与 AdamW 差不多的表现，但是最好还是在 pre-training 阶段使用 Muon&lt;/li>
&lt;/ol>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者探究了如何 scale up Muon Optimizer. 通过改进，作者在 16B-A3B 的 MoE LLM 上验证了 Muon Optimizer 的性能。实验结果发现，Muon Optimizer 的训练效率比 AdamW 提升了 2 倍左右。&lt;/p>
&lt;p>作者提出了三个未来可行的研究方向：&lt;/p>
&lt;ol>
&lt;li>目前 Muon 只能针对 2D 参数进行优化，其他参数仍然依赖于 AdamW 优化器，是否可以使用 Muon 优化所有参数？&lt;/li>
&lt;li>Muon optimizer 可以理解是 spectral norm 下的 steepest descent 方法，如何将其扩展到 Schatten norm 是一个可以研究的方向&lt;/li>
&lt;li>实验里提到，预训练和 SFT 阶段使用不同的 optimizer, 表现不是最优的，如何解决这个因为不同 optimizer 导致的性能差距是一个需要解决的问题。&lt;/li>
&lt;/ol>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2502.16982" target="_blank" rel="noopener"
>Muon is Scalable for LLM Training&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Hunyuan-Large</title><link>https://maosong2022.github.io/p/notes-on-hunyuan-large/</link><pubDate>Wed, 06 Aug 2025 16:46:32 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-hunyuan-large/</guid><description>&lt;p>腾讯混元提出了 Hunyuan-Large, 一个 389B-A52B 的 MoE LLM, 上下文长度为 256K.&lt;/p>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>Hunyuan-Large 主要在三个方向进行了改进：&lt;/p>
&lt;ol>
&lt;li>使用了更高质量的合成数据：模型使用了 7T 的预训练数据，其中包含了 1.5T 的合成数据&lt;/li>
&lt;li>优化了模型的架构：作者提出了 KV cache compression, recycle routing, expert-specific learning rate scaling 策略来提高模型的表现&lt;/li>
&lt;li>探究了 MoE 模型的 scaling law: 作者探究了 MoE 模型的 scaling law&lt;/li>
&lt;/ol>
&lt;h2 id="pre-training">Pre-training
&lt;/h2>&lt;h3 id="architecture">Architecture
&lt;/h3>&lt;p>Hunyuan-Large 是一个基于 MoE 的 transformer 架构，attention 部分使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-gqa/" target="_blank" rel="noopener"
>GQA&lt;/a>, position encoding 使用了 RoPE, MLP 的激活函数为 SwiGLU. 在 MoE layer 中，Hunyuan-Large 使用了 shared experts. 最终，模型的配置如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-hunyuan-large/hunyuan-large-architecture-config.png"
width="654"
height="399"
srcset="https://maosong2022.github.io/p/notes-on-hunyuan-large/hunyuan-large-architecture-config_hu7926587958527572246.png 480w, https://maosong2022.github.io/p/notes-on-hunyuan-large/hunyuan-large-architecture-config_hu112456650109780077.png 1024w"
loading="lazy"
alt="Configuration of Hunyuan-Large"
class="gallery-image"
data-flex-grow="163"
data-flex-basis="393px"
>&lt;/p>
&lt;h4 id="kv-cache-compression">KV Cache Compression
&lt;/h4>&lt;p>为了减少 KV cache 的内存开销，作者使用了两个技巧：&lt;/p>
&lt;ol>
&lt;li>GQA: 通过共享 KV projection 的参数，来减少内存访问次数&lt;/li>
&lt;li>[[CLA]]: 在相邻的 layer 中共享 KV cache, 来进一步压缩 KV cache&lt;/li>
&lt;/ol>
&lt;p>在 Hunyuan-Large 中，作者将 GQA 的 group size 设置为 8, 然后相邻的 2 层 layer 共享 KV cache.&lt;/p>
&lt;p>假设输入的 batch size 为 $B$, sequence 长度为 $L$, layers 个数为 $\ell$, attention heads 个数为 $h$, KV heads 个数为 $h_{kv}$, 每个 head 的 hidden size 为 $d_h$, 则每一层的 GQA 需要缓存 $K,V\in\mathbb{R}^{B\times _{kv}\times L\times d_h}$， KV cache 的总占用为&lt;/p>
$$
2\times B\times h_{kv}\times L\times d_h \times \ell \times 2=4BLh_{kv}d_h\ell
$$&lt;p>第一个 $2$ 是因为同时缓存 K 和 V, 第二个 $2$ 是因为一般使用 &lt;code>bfloat16&lt;/code> 数据格式。&lt;/p>
&lt;p>对于 CLA, 因为连续两层共享相同的 KV cache，因此结果除以 2; 对于 MHA, $h_{kv}=h$; 对于 &lt;a class="link" href="https://maosong.website/p/notes-on-mqa/" target="_blank" rel="noopener"
>MQA&lt;/a>, $h_{kv}=1$. 最后，KV cache 的内存占用如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Attention Mechanism&lt;/th>
&lt;th>KV Cache Memory&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>MHA&lt;/td>
&lt;td>$4BLhd_h\ell$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GQA&lt;/td>
&lt;td>$4BLh_{kv}d_h\ell$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MQA&lt;/td>
&lt;td>$4BLd_h\ell$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CLA&lt;/td>
&lt;td>$2BLhd_h\ell$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GQA+CLA&lt;/td>
&lt;td>$2BLh_{kv}d_h\ell$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，使用 GQA+CLA 之后，模型的 kv cache 占用相比于 MHA 变成了&lt;/p>
$$
\frac{2BLh_{kv}d_h\ell}{4BLhd_h\ell}=\frac{1}{16}
$$&lt;p>也就是说，Hunyuan-Large 的 KV cache 内存占用下降到了 MHA 的 1/16.&lt;/p>
&lt;h4 id="expert-routing-strategy">Expert Routing Strategy
&lt;/h4>&lt;p>作者采用了 shared expert + activated expert 的形式，其中包含 1 个 shared expert, 然后从 16 个专家里激活 1 个专家。&lt;/p>
&lt;p>为了解决 MoE 中 expert capacity 难以设定的问题，作者提出了一个 recycle routing 的策略，基本思想就是，当 activated expert 的容量超出限制时，会从其他没有超出容量限制的专家里重新进行激活。&lt;/p>
&lt;h4 id="expert-specific-learning-rate-scaling">Expert Specific Learning Rate Scaling
&lt;/h4>&lt;p>作者使用 AdamW 作为优化器，作者探讨了如何设定学习率。基于之前的工作，最优的学习率与 batch size 相关：&lt;/p>
$$
\epsilon_{\mathrm{opt}}(B) = \frac{2\epsilon_{\max}}{\sqrt{\frac{\mathcal{B}_{\mathrm{noise}}}{B}}+\sqrt{\frac{B}{\mathcal{B}_{\mathrm{noise}}}}}
$$&lt;p>这里 $\epsilon_{\max}$ 是 AdamW 的学习率, $\mathcal{B}_{\mathrm{noise}}$ 是训练速度与数据使用效率的一个平衡因子。&lt;/p>
&lt;p>但是，在 MoE 模型中，不同专家处理的 token 是不一样的。基于 load balancing loss, shared expert 和 activated expert 处理的 token 个数比例大概是 $n :1$, 其中 $n=16$ 是总的专家个数。因此，对于 shared expert, 作者使用 $\epsilon_{\mathrm{opt}}(B)$ 作为学习率，然后对于 activated expert, 作者使用 $\epsilon_{\mathrm{opt}}(B/n)$ 作为学习率。&lt;/p>
&lt;h3 id="data">Data
&lt;/h3>&lt;p>预训练数据包括收集和合成。收集的数据主要来自互联网，覆盖中英文两种语言。&lt;/p>
&lt;p>合成数据包括 4 个步骤：&lt;/p>
&lt;ol>
&lt;li>instruction generation: 作者使用高质量的语料作为 seed, 然后生成多样的 instruction 覆盖不同的 domain&lt;/li>
&lt;li>Instruction evolution: refine 上一步生成的 instruction&lt;/li>
&lt;li>Response generation: 使用 specialized model 来生成回答&lt;/li>
&lt;li>response filtering: 对生成的回答进行过滤&lt;/li>
&lt;/ol>
&lt;p>数据合成的流程如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-hunyuan-large/Hunyuan-large-data-synthesis.png"
width="1126"
height="565"
srcset="https://maosong2022.github.io/p/notes-on-hunyuan-large/Hunyuan-large-data-synthesis_hu18416123704675275786.png 480w, https://maosong2022.github.io/p/notes-on-hunyuan-large/Hunyuan-large-data-synthesis_hu18395332435503133072.png 1024w"
loading="lazy"
alt="Data synthesis pipeline"
class="gallery-image"
data-flex-grow="199"
data-flex-basis="478px"
>&lt;/p>
&lt;p>tokenizer 大小为 128K, 由 tittoken tokenizer 和额外的 28K token 组成。&lt;/p>
&lt;h3 id="pre-training-recipe">Pre-training Recipe
&lt;/h3>&lt;p>作者首先探究了一个针对 MoE 模型的 scaling law. 结果发现，最优的激活参数量为 58.1B, training token 个数为 5.6T. 经过平滑之后，作者最终将模型的激活参数两定为 &lt;strong>52B&lt;/strong>, 训练 token 数定为 $7T$.&lt;/p>
&lt;p>在训练时，作者将学习率分为了 3 个 stage:&lt;/p>
&lt;ol>
&lt;li>warmup phase&lt;/li>
&lt;li>gradual decay phase&lt;/li>
&lt;li>concise annealing phase&lt;/li>
&lt;/ol>
&lt;p>上面的三个 stage 结束之后，作者加入了两个 stage 来扩展模型的上下文长度从 32K 扩展到 256K. 训练的数据包括 75% 的短文本和 25% 的长文本。两个 stage 训练的 token 数均为 $10B$ 左右。&lt;/p>
&lt;h2 id="post-training">Post-training
&lt;/h2>&lt;p>post-training 分为 SFT 和 RLHF 两个阶段。&lt;/p>
&lt;h3 id="sft">SFT
&lt;/h3>&lt;p>SFT 数据副高 math, coding, logical reasoning 等 domain, 包含超过 1M 的数据。&lt;/p>
&lt;p>SFT 训练了 3 个 epoch, 学习率从 2e-5 降低到 2e-6, 为了避免 overfitting, 作者使用了 0.1 的 attention dropout 和 0.2 的 hidden dropout.&lt;/p>
&lt;blockquote>
&lt;p>[!tip]
作者发现，MoE 模型可以从 dropout 中学习到更多&lt;/p>
&lt;/blockquote>
&lt;h3 id="rlhf">RLHF
&lt;/h3>&lt;p>作者使用 DPO 来进行 RLHF, 作者同时使用了 offline 和 online 的数据来进行训练，前者是收集的数据，后者是当前 policy 生成的数据。与 LLaMA 3 和 Nemotron-4 一样，为了提高训练稳定性，对于 chosen reponse, 作者使用了 SFT loss.&lt;/p>
&lt;p>作者还是用了 exponential moving average 策略来减少 reward hacking 现象，以及降低 alignment tax.&lt;/p>
&lt;h2 id="experiment">Experiment
&lt;/h2>&lt;p>对于 base 版本，作者对比了 LLaMA 3, Mixtral, DeepSeek-V2, 实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-hunyuan-large/Hunyuan-large-base-performance.png"
width="1135"
height="761"
srcset="https://maosong2022.github.io/p/notes-on-hunyuan-large/Hunyuan-large-base-performance_hu9250287511197580218.png 480w, https://maosong2022.github.io/p/notes-on-hunyuan-large/Hunyuan-large-base-performance_hu13135794748576120680.png 1024w"
loading="lazy"
alt="Performance of Hunyuan-Large-Base"
class="gallery-image"
data-flex-grow="149"
data-flex-basis="357px"
>&lt;/p>
&lt;p>Instruction 版本的表现如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-hunyuan-large/Hunyuan-large-intruction-performance.png"
width="1178"
height="538"
srcset="https://maosong2022.github.io/p/notes-on-hunyuan-large/Hunyuan-large-intruction-performance_hu132719345509594511.png 480w, https://maosong2022.github.io/p/notes-on-hunyuan-large/Hunyuan-large-intruction-performance_hu9830594855642638810.png 1024w"
loading="lazy"
alt="Performance of Hunyuan-Large Instuct"
class="gallery-image"
data-flex-grow="218"
data-flex-basis="525px"
>&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者提出了 Hunyuan-Large, 一个 389B-A52B 的 LLM, 上下文长度为 256K. 作者详细介绍了模型的架构，数据和训练方式。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2411.02265" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on GSPO</title><link>https://maosong2022.github.io/p/notes-on-gspo/</link><pubDate>Wed, 06 Aug 2025 11:26:26 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-gspo/</guid><description>&lt;p>Qwen 提出了 Group Sequence Policy Optimization (GSPO), 一个针对 GRPO 进行改进的 RL 算法。GSPO 在 sequence 层面计算 importance ratio, 避免了 token-level 计算带来的训练不稳定性。&lt;/p>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>GRPO 的问题在于训练超大规模的 LLM 时，会出现 model collapse, &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a> 和 MiniMax-01 均对 GRPO 算法进行了改进。&lt;/p>
&lt;p>在本文中，作者认为 GRPO 算法的问题在于 Importance sampling weight 的计算是有问题的，这导致了误差随生成回答长度累积，最后被 clipping 机制放大，从而导致模型崩溃。&lt;/p>
&lt;p>为了解决这个问题，作者提出了 GSPO, 一个在 sequence 层面进行 Importance sampling 的 RL 算法，GSPO 还对 reward 进行 normalization 来保持训练的稳定性&lt;/p>
&lt;h2 id="method">Method
&lt;/h2>&lt;h3 id="preliminary">Preliminary
&lt;/h3>&lt;p>对于一个大语言模型 $\pi_{\theta}$, 我们将 $x$ 记为 query, 将 $y$ 记为 $\pi_{\theta}$ 针对 $x$ 的 response, 即&lt;/p>
$$
\pi_{\theta}(y\mid x) = \prod_{t=1}^{|y|}\pi_{\theta}(y_t\mid x, y_{&lt;t})
$$&lt;p>其中 $|y|$ 代表 response $y$ 的长度, 一般我们还有一个 reward model $r$, 用于生成奖励 $r(x,y)\in[0, 1]$.&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Note
本文中没有提及 KL divergence, 作者认为这不是重点，所以没有在公式中标出。&lt;/p>
&lt;/blockquote>
&lt;p>PPO 算法包含四个模型：reference model, 也就是 $\pi_{old}$, policy model, 也就是 $\pi_{\theta}$, value model, 用于计算 advantage, 以及 reward model, 用于计算奖励。 PPO 算法如下面公式所示&lt;/p>
$$
\mathcal{J}_{\mathrm{PPO}}(\theta) = \mathbb{E}_{(x,y)\sim\mathcal{D},y_{\leq t}\sim \pi_{\theta_{old}}(\cdot\mid q)}\left[ \min\left(r_t(\theta)\hat{A}_t,\mathrm{clip}\left(r_t(\theta), 1-\epsilon, 1+\epsilon\right)\hat{A}_t\right) \right]
$$&lt;p>其中&lt;/p>
$$
r_t(\theta) = \frac{\pi_{\theta}(y_t\mid x, y_{&lt; t})}{\pi_{\theta_{old}}(y_t\mid x, y_{&lt; t})}
$$&lt;p>是 token $y_t$ 的 importance ratio, $\hat{A}_t$ 是 $y_t$ 的 advantage estimation.&lt;/p>
&lt;p>&lt;strong>PPO&lt;/strong> 算法的问题在于其严重依赖 value model, 一般 value model 与 policy model 的大小相当，以保持内存和算力的负载均衡。&lt;/p>
&lt;p>为了解决 PPO 的这个问题，GRPO 通过多次采样然后计算 relative advantage 来避免使用 value model. 其目标函数为&lt;/p>
$$
\mathcal{J}_{\mathrm{GRPO}}(\theta) = \mathbb{E}_{(x, y)\sim\mathcal{D},\{y_i\}_{i=1}^G\sim \pi_{\theta_{old}}(\cdot\mid q)}\left[ \frac{1}{G}\sum_{i=1}^G\frac{1}{|y_i|}\sum_{t=1}^{|y_i|}\min\left(r_{i,t}(\theta)\hat{A}_{i,t},\mathrm{clip}\left(r_{i,t}(\theta), 1-\epsilon, 1+\epsilon\right)\hat{A}_{i,t}\right) \right]
$$&lt;p>其中，$G$ 是针对 query $x$ 的多次采样 response,&lt;/p>
$$
r_{i,t}(\theta) = \frac{\pi_{\theta}(y_{i,t}\mid x, y_{i,&lt; t})}{\pi_{\theta_{old}}(y_{i,t}\mid x, y_{i,&lt; t})}
$$&lt;p>是 importance ratio,&lt;/p>
$$
\hat{A}_{i,t} = \hat{A}_{i} = \frac{r(x,y_i) - \mathrm{mean}(\{r(x,y_i)\}_{i=1}^G)}{\mathrm{std}(\{r(x,y_i)\}_{i=1}^G)}
$$&lt;p>是使用 group response 估计得到的 advantage.&lt;/p>
&lt;h3 id="motivation">Motivation
&lt;/h3>&lt;p>在 Qwen3 中已经提到，对于稀疏的 MoE 模型，我们必须使用更大的 batch size 来最大化内存和算力使用效率。但是，更大的 batch 意味着有一些数据必须是 off-policy 的，因此 PPO 和 GRPO 就需要使用 clipping 来降低 off-policy sample 对模型表现产生过大影响。&lt;/p>
&lt;p>基于 clipping 机制，作者认为 GRPO 的目标函数是 &amp;ldquo;ill-pose&amp;rdquo; 的，其原因在于 GRPO 计算的 Importance ratio 是与 RL 训练目标不匹配的。&lt;/p>
&lt;p>通常，importance sampling 的公式如下：&lt;/p>
$$
\mathbb{E}_{z\sim\pi_{\mathrm{tar}}}[f(z)] = \mathbb{E}_{z\sim\pi_{\mathrm{beh}}}\left[\frac{\pi_{\mathrm{tar}}(z)}{\pi_{\mathrm{beh}}(z)}f(z)\right]
$$&lt;p>其中 $\pi_{\mathrm{tar}}$ 是目标分布， $\pi_{\mathrm{beh}}$ 是采样分布, importance ratio $\pi_{\mathrm{tar}}(z)/\pi_{\mathrm{beh}}(z)$ 负责对采样进行修正。&lt;/p>
&lt;p>对于 GRPO, 其 importance ratio 是在 token 层面定义的，而一次采样是在 sequence 层面定义的，因此这种区别就导致了 GRPO 存在 high-variance noise.&lt;/p>
&lt;p>因此，作者认为，&lt;strong>Importance ratio 的关键在于优化目标应该与 reward 的粒度是一致的&lt;/strong>。&lt;/p>
&lt;h3 id="gspo">GSPO
&lt;/h3>&lt;p>基于上面的想法，作者就提出了 GSPO, 作者首先针对 LLM 改写了上面的 importance sampling 的公式&lt;/p>
$$
\mathbb{E}_{x\sim \mathcal{D}, y\sim\pi_{\theta}(\cdot\mid x)}[r(x,y)] = \mathbb{E}_{x\sim\mathcal{D}, y\sim\pi_{\theta_{old}}(\cdot\mid x)}\left[\frac{\pi_{\theta}(y\mid x)}{\pi_{\theta_{old}}(y\mid x)}r(x,y)\right]
$$&lt;p>这里的 Importance ratio 表现了采样的回答 $\pi_{old}(y\mid x)$ 与目标回答 $\pi_{\theta}(y\mid x)$ 之间的差距，这是从 sequence 层面上体现的。&lt;/p>
&lt;p>因此，作者基于上面的式子，构建出了 GSPO 的目标函数&lt;/p>
$$
\mathcal{J}_{\mathrm{GSPO}}(\theta) = \mathbb{E}_{(x, y)\sim\mathcal{D},\{y_i\}_{i=1}^G\sim \pi_{\theta_{old}}(\cdot\mid q)}\left[ \frac{1}{G}\sum_{i=1}^G\min\left(s_{i}(\theta)\hat{A}_{i},\mathrm{clip}\left(s_{i}(\theta), 1-\epsilon, 1+\epsilon\right)\hat{A}_{i}\right) \right]
$$&lt;p>这里 $\hat{A}_{i}$ 与 GRPO 一致， $s_i(\theta)$ 是 normalize 之后的 importance ratio, 定义如下&lt;/p>
$$
s_i(\theta) = \left(\frac{\pi_{\theta}(y_{i}\mid x)}{\pi_{\theta_{old}}(y_{i}\mid x)}\right)^{\frac{1}{|y_i|}} = \exp\left(\frac{1}{|y_i|}\sum_{i=1}^{|y_i|}\frac{\pi_{\theta}(y_{i,t}\mid x, y_{i,&lt; t})}{\pi_{\theta_{old}}(y_{i,t}\mid x, y_{i,&lt; t})}\right)
$$&lt;p>这里作者使用了 length normalization 来控制 variance.&lt;/p>
&lt;p>作者对比了以下 GSPO 和 GRPO 两者的梯度，我们忽略期望的计算，直接写出内部的梯度，有&lt;/p>
$$
\begin{aligned}
\nabla_\theta \mathcal{J}_{\mathrm{GSPO}}(\theta) &amp;= \frac{1}{G}\sum_{i=1}^G\left(\frac{\pi_{\theta}(y_{i}\mid x)}{\pi_{\theta_{old}}(y_{i}\mid x)}\right)^{\frac{1}{|y_i|}}\hat{A}_i\cdot \frac{1}{|y_i|}\sum_{t=1}^{|y_i|} \nabla_{\theta} \log\pi_{\theta}(y_{i,t}\mid x, y_{&lt;t})\\
\nabla_\theta \mathcal{J}_{\mathrm{GRPO}}(\theta) &amp;= \frac{1}{G}\sum_{i=1}^G\hat{A}_i\cdot \frac{1}{|y_i|}\sum_{t=1}^{|y_i|} \frac{\pi_{\theta}(y_{i,t}\mid x, y_{i,&lt; t})}{\pi_{\theta_{old}}(y_{i,t}\mid x, y_{i,&lt; t})}\nabla_{\theta} \log\pi_{\theta}(y_{i,t}\mid x, y_{&lt;t})\\
\end{aligned}
$$&lt;p>可以看到，两者不同的地方在于 token 的 reweight 方式，GRPO 中先加权再求和；而 GSPO 中则是先求和，然后在 sequence 层面进行加权&lt;/p>
&lt;h3 id="gspo-token">GSPO-token
&lt;/h3>&lt;p>为了支持 multi-turn RL 等需要细粒度 advantage 的场景，作者对 GSPO 的目标函数进行了改进，得到了 GSPO-token, 其目标函数为&lt;/p>
$$
\mathcal{J}_{\mathrm{GSPO-token}}(\theta) = \mathbb{E}_{(x, y)\sim\mathcal{D},\{y_i\}_{i=1}^G\sim \pi_{\theta_{old}}(\cdot\mid q)}\left[ \frac{1}{G}\sum_{i=1}^G\frac{1}{|y_i|}\sum_{t=1}^{|y_i|}\min\left(s_{i,t}(\theta)\hat{A}_{i},\mathrm{clip}\left(s_{i,t}(\theta), 1-\epsilon, 1+\epsilon\right)\hat{A}_{i}\right) \right]
$$&lt;p>其中，&lt;/p>
$$
s_{i,t}(\theta) = \mathrm{sg}[s_i(\theta)]\frac{\pi_{\theta_{old}}(y_{i,t}\mid x, y_{i,&lt; t})}{\mathrm{sg}[\pi_{\theta_{old}}(y_{i,t}\mid x, y_{i,&lt; t})]}
$$&lt;p>这里 $\mathrm{sg}$ 是 stop gradient operation. 通过这种改写方式，GSPO-token 与 GSPO 的优化目标一致，但是可以在更细粒度的 token 层面进行优化。&lt;/p>
&lt;h2 id="evaluation">Evaluation
&lt;/h2>&lt;p>作者使用 Qwen3-30B-A3B 进行测试。对于 GRPO，作者发现必须使用 Routing Replay training strategy 才能提升 MoE RL 的收敛性， Routing Replay 的具体做法就是保留 $\pi_{\theta_{old}}$ 的激活专家，在 $\pi_{\theta}$ 中使用相同的专家来保持稳定性。但是，GSPO 则不需要这个技巧。&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-gspo/GSPO-GRPO-routing-replay.png"
width="1218"
height="388"
srcset="https://maosong2022.github.io/p/notes-on-gspo/GSPO-GRPO-routing-replay_hu13102765534603069096.png 480w, https://maosong2022.github.io/p/notes-on-gspo/GSPO-GRPO-routing-replay_hu5031302780883302860.png 1024w"
loading="lazy"
alt="Routing Replay strategy for GRPO"
class="gallery-image"
data-flex-grow="313"
data-flex-basis="753px"
>&lt;/p>
&lt;p>实验结果如下图所示。&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-gspo/GSPO-performance.png"
width="1214"
height="701"
srcset="https://maosong2022.github.io/p/notes-on-gspo/GSPO-performance_hu14450057870262816776.png 480w, https://maosong2022.github.io/p/notes-on-gspo/GSPO-performance_hu5553482751355135556.png 1024w"
loading="lazy"
alt="Comparison of GSPO and GRPO"
class="gallery-image"
data-flex-grow="173"
data-flex-basis="415px"
>&lt;/p>
&lt;p>从实验结果可以看到，GSPO 比 GRPO 更加高效，效果也更好。&lt;/p>
&lt;p>作者带对比了 GSPO 与 GRPO 的 clipping 比例，结果发现，GSPO clipping 的 token 比例比 GRPO 高几个数量级，作者认为，尽管 GSPO clip 了更多 token, 但是 GSPO 更加高效，这也说明 GRPOtoken 层面的梯度估计是存在噪声的。&lt;/p>
&lt;p>作者还介绍了以下 MoE 模型中 RL 训练的 expert-activation volatility 现象，也就是说，MoE 模型参数更新之后，对于同一个 response, 激活的专家可能飞铲个不同。作者举例说明，对于 Qwen3-30B-A3B, 更新一次梯度之后，有 $10%$ 左右的激活专家变得不一样了。&lt;/p>
&lt;p>作者最后介绍了以下 GSPO 的两个优势，一个是解决了 GRPO 依赖于 Routing Replay 的问题；第二个是支持 SGLang 和 vLLM 等推理框架。&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者提出了 GSPO，一个在 sequence 层面计算 importance ratio 的 RL 算法，解决了 GRPO 在训练大规模 MoE LLM 时出现的训练不稳定性以及 mode collapse 现象。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2507.18071" target="_blank" rel="noopener"
>Group Sequence Policy Optimization&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Muon blog</title><link>https://maosong2022.github.io/p/notes-on-muon-blog/</link><pubDate>Tue, 05 Aug 2025 11:10:51 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-muon-blog/</guid><description>&lt;p>Muon (MomentUm Orthogonalized by Newton-Schulz) 是一个针对二维神经网络的优化器，它基于 SGD-momentum 改进，增加了一个 Newton-Schulz 的后处理步骤&lt;/p>
&lt;h2 id="method">Method
&lt;/h2>&lt;p>Newton-Schulz (NS) 的目的是用一个正交矩阵近似一个给定矩阵，即&lt;/p>
$$
\mathrm{Ortho}(G) = \arg\min_{O} \{\|O-G\|_F: \text{either } O^TO=I\text{ or } OO^T=I\}
$$&lt;p>也就是说，NS iteration 将 SDG-moment 的更新矩阵替换为了“最近的” semi-orthogonal matrix. 这等价于将更新矩阵替换为 $UV^T$, 其中 $USV^T$ 是更新矩阵的 SVD 分解。&lt;/p>
&lt;blockquote>
&lt;p>[!tip]
作者观察到，对于 SGD-momentum 和 Adam 来说，其在基于 transformer 的神经网络里有非常高的 condition number, 也就是 optimizer 仅在少数几个方向上进行优化。作者认为，通过正交化，可以有效提高模型在其他方向上的更新速度，进而提高模型表现&lt;/p>
&lt;/blockquote>
&lt;h3 id="newton-schulz">Newton-Schulz
&lt;/h3>&lt;p>作者提到，正交化矩阵的方法有很多，比如 SVD 分解，但是其问题是非常慢，还有 Coupled Newton iteration, 但是其精度要求非常高，必须要在 &lt;code>float32&lt;/code> 以上。&lt;/p>
&lt;p>作者因此使用了 Newton-Schulz iteration.&lt;/p>
&lt;p>令 $G=USV^T$ 是 SGD-momentum 更新矩阵的 SVD 分解，则基于系数 $(a,b,c)$ 的 NS iteration 定义如下：&lt;/p>
$$
\begin{aligned}
G' &amp;= aG + b(GG^T)G + c(GG^T)^2G\\
&amp;= (aI+b(GG^T)+c(GG^T)^2)G\\
&amp;= (aI+bUS^2U^T+cUS^4U^T)USV^T\\
&amp;= U(aS+bS^3+cS^5)V^T
\end{aligned}
$$&lt;p>也就是说，如果我们定义五次多项式函数 $\phi(x)=ax+bx^3+cx^5$, 然后执行 $N$ 次 NS iteration, 则我们得到 $U\phi^N(S)V^T$, 其中 $\phi^N$ 代表 $\phi$ 复合 $N$ 次。&lt;/p>
&lt;p>为了保证 NS iteration 收敛到 $\mathrm{Ortho}(G) = UV^T$, 我们必须保证两点：&lt;/p>
&lt;ol>
&lt;li>$S$ 的值，也就是 $G$ 的奇异值必须在区间 $[0,1]$ 上&lt;/li>
&lt;li>$\phi$ 必须满足 $\phi^N\to 1$, $N\to\infty$, $\forall x\in[0,1]$.&lt;/li>
&lt;/ol>
&lt;p>为了满足第一个条件，我们可以对 $G$ 进行 rescale, 即 $G\gets G/|G|_F$, rescale 不影响最终的结果，即 $\mathrm{Ortho}(G) = \mathrm{Ortho}(cG)$.&lt;/p>
&lt;p>对于 $\phi(x)$, 我们有很多选择，比如我们定义 $(a,b,c):=(2,-1.5,0.5)$ 就得到如下结果&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-muon-blog/Muon_naive_phi_x.png"
width="1080"
height="660"
srcset="https://maosong2022.github.io/p/notes-on-muon-blog/Muon_naive_phi_x_hu8143931100452006854.png 480w, https://maosong2022.github.io/p/notes-on-muon-blog/Muon_naive_phi_x_hu4887157011477254809.png 1024w"
loading="lazy"
alt="plot of $f(x)=2x-1.5x^3&amp;#43;0.5x^5$"
class="gallery-image"
data-flex-grow="163"
data-flex-basis="392px"
>&lt;/p>
&lt;h3 id="coefficient-optimization">Coefficient Optimization
&lt;/h3>&lt;p>尽管 $(a,b,c):=(2,-1.5,0.5)$ 已经满足了第二个条件，但是我们还是想进一步优化，优化的方向主要有两个：&lt;/p>
&lt;ol>
&lt;li>让 $a$ 尽可能大，这是因为 $\phi&amp;rsquo;(0)=a$ 控制了较小奇异值的收敛速率。&lt;/li>
&lt;li>对于所有的 $x\in[0,1]$, 我们希望 $\phi^N(x)\in[1-\epsilon, 1+\epsilon]$, $N\to\infty$. 这样 NS iteration 的结果与 $\mathrm{Ortho}(G)$ 不会相差太远。&lt;/li>
&lt;/ol>
&lt;p>作者发现， $\epsilon$ 可以设置为 $0.3$ 而不影响 Muon optimizer 的收敛性。因此，作者的目标现在是&lt;/p>
$$
\begin{aligned}
\max\quad &amp;a\\
\mathrm{s.t.}\quad &amp;\lim_{N\to\infty}\phi^N(x)\in[0.7, 1.3]
\end{aligned}
$$&lt;p>作者通过 ad-hoc gradient 方法求解得到一组数值解为 $(a,b,c)=(3.4445, 4.7750, 2.0315)$, 作者将这组数值应用于 Muon optimizer 中。迭代结果如下图，可以看到，当 $x\approx0$ 时，函数变得更加陡峭。&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-muon-blog/Muon-optimized-phi.png"
width="1067"
height="648"
srcset="https://maosong2022.github.io/p/notes-on-muon-blog/Muon-optimized-phi_hu15921126282054750677.png 480w, https://maosong2022.github.io/p/notes-on-muon-blog/Muon-optimized-phi_hu14550454722052517792.png 1024w"
loading="lazy"
alt="Plot of $f(x)=3.4445x-4.7750x^3&amp;#43;2.0315x^5$"
class="gallery-image"
data-flex-grow="164"
data-flex-basis="395px"
>&lt;/p>
&lt;p>实验中，作者发现，仅需迭代五次，最终的结果就 work 的很好。作者还尝试了不同的多项式，结果发现并没有太大的提升。&lt;/p>
&lt;h3 id="algorithm">Algorithm
&lt;/h3>&lt;p>最终，Muon Optimizer 的算法如下&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-muon-blog/Muon-Algorithm.png"
width="1242"
height="840"
srcset="https://maosong2022.github.io/p/notes-on-muon-blog/Muon-Algorithm_hu9916644918783305429.png 480w, https://maosong2022.github.io/p/notes-on-muon-blog/Muon-Algorithm_hu18151449566632123377.png 1024w"
loading="lazy"
alt="Muon Algorithm"
class="gallery-image"
data-flex-grow="147"
data-flex-basis="354px"
>&lt;/p>
&lt;p>其中, &lt;code>NewtonSchulz5&lt;/code> 算法伪代码定义如下&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">newtonschulz5&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">G&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">steps&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">eps&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">1e-7&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">assert&lt;/span> &lt;span class="n">G&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ndim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">2&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">a&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">c&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mf">3.4445&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mf">4.7750&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">2.0315&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">X&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">G&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">bfloat16&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">X&lt;/span> &lt;span class="o">/=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">norm&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">eps&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">G&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">G&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">X&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">X&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">T&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">steps&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">A&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">X&lt;/span> &lt;span class="o">@&lt;/span> &lt;span class="n">X&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">T&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">B&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">b&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">A&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">c&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">A&lt;/span> &lt;span class="o">@&lt;/span> &lt;span class="n">A&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">X&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">a&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">X&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">B&lt;/span> &lt;span class="o">@&lt;/span> &lt;span class="n">X&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">G&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">G&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">X&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">X&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">T&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">X&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="analysis">Analysis
&lt;/h2>&lt;p>本节作者分析了以下 Muon 的内存占用和算力开销。&lt;/p>
&lt;p>在 NS iteration 之前，Muon optimizer 和 SGD-moment 是一样的。&lt;/p>
&lt;p>对于 $n\times m$ 的矩阵（假设 $m\leq n$）， 首先 NS iteration 会进行转置，NS iteration 的每一步需要 $2(2nm^2+m^3)$ FLOPs, 其中括号前面的系数 $2$ 代表精度。因此，Muon 相比于 SGD momentum 需要的额外 FLOPs 为 $2T(2nm^2+m^3)$, 其中 $T$ 是迭代次数。&lt;/p>
&lt;p>使用 baseline 进行一次训练（前向 + 后向），所需要的 FLOPS 为 $6nmB$, 其中 $B$ 是 batch size. 因此，Muon 的 FLOP 开销至多为 $Tm/B$, 其中 $m$ 是模型的 hidden size, $B$ 是 batch size, $T$ 是 NS iteration 的步数。&lt;/p>
&lt;p>作者分别基于 nanoGPT 和 LLaMA-405B 进行验证，结果发现，Muon optimizer 带来的额外开销不足 $1%$.&lt;/p>
&lt;p>作者发信啊，使用 Nesterov-style momentum 可以比普通的 SGD-momentum 效果更好，因此作者在 muon 中使用了前者。&lt;/p>
&lt;p>作者还发现，对于 QKV layer，分别进行优化效果会更好。&lt;/p>
&lt;h2 id="experiments">Experiments
&lt;/h2>&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-muon-blog/Muon-nanoGPT-loss-vs-training-tokens.png"
width="1400"
height="970"
srcset="https://maosong2022.github.io/p/notes-on-muon-blog/Muon-nanoGPT-loss-vs-training-tokens_hu12203099100559668716.png 480w, https://maosong2022.github.io/p/notes-on-muon-blog/Muon-nanoGPT-loss-vs-training-tokens_hu14425504501733015997.png 1024w"
loading="lazy"
alt="Optimizer comparison by tokens"
class="gallery-image"
data-flex-grow="144"
data-flex-basis="346px"
>&lt;/p>
&lt;h2 id="limitation-and-future-work">Limitation and Future Work
&lt;/h2>&lt;p>Muon 仅被设计用于优化 2D 参数（因为涉及矩阵计算），其余的参数仍然需要 AdamW 等优化器参与。&lt;/p>
&lt;p>作者认为未来的工作有：&lt;/p>
&lt;ol>
&lt;li>能否 scale up Muon Optimizer&lt;/li>
&lt;li>分布式优化&lt;/li>
&lt;li>在 fine-tuning 和 RL 阶段使用 Muon Optimizer&lt;/li>
&lt;/ol>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者提出了 Muon optimizer，该优化器在 nanoGPT speedrun 上取得了 SOTA 的结果，作者详细介绍了优化器的工作原理。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://kellerjordan.github.io/posts/muon/" target="_blank" rel="noopener"
>Muon: An optimizer for hidden layers in neural networks&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on AFM2025</title><link>https://maosong2022.github.io/p/notes-on-afm2025/</link><pubDate>Tue, 29 Jul 2025 12:36:28 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-afm2025/</guid><description>&lt;p>Apple 在 7 月份发布了 AFM 技术报告，包括两个多语种多模态大模型，分别为 3B 和 xB, 一个面向 device, 另一个面向 server， 前者主要集中于效率，后者集中于表现。&lt;/p>
&lt;h2 id="pre-training">Pre-training
&lt;/h2>&lt;h3 id="architecture">Architecture
&lt;/h3>&lt;h4 id="on-device-model">On-Device Model
&lt;/h4>&lt;p>对于 on-device model, 作者将模型分为两个 block, Block1 占 $62.5%$ 的 transformer layers, Block2 占 $37.5%$ 的 transformer layers. 但是，对于 Block2, 作者移除了 key, value projection, 对应的 KV cache 则直接从 Block1 中获取。通过这种方式，作者将 KV cache memory usage 减少了 $37.5%$. 并且，由于 Block2 不产生任何 key values, prefill stage 可以跳过这些计算，这样 TTFT 也可以减少 $37.5%$.&lt;/p>
&lt;h4 id="server-model">Server Model
&lt;/h4>&lt;p>对于 server model, 作者对架构进行了改进，来提高效率。架构如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-afm2025/AFM-2025-PT-MoE-architecture.png"
width="1271"
height="525"
srcset="https://maosong2022.github.io/p/notes-on-afm2025/AFM-2025-PT-MoE-architecture_hu6894308848522232101.png 480w, https://maosong2022.github.io/p/notes-on-afm2025/AFM-2025-PT-MoE-architecture_hu7460912542276442605.png 1024w"
loading="lazy"
alt="Diagram of the PT-MoE architecture"
class="gallery-image"
data-flex-grow="242"
data-flex-basis="581px"
>&lt;/p>
&lt;p>&lt;strong>Parallel Track Transformer&lt;/strong>
作者提出了 Parallel Track (PT) Transformer 架构，PT-Transformer 将 transformer 模型分割多个小的 transformer, 作者将这些小的 transformer 称之为 &lt;em>track&lt;/em>. 每个 track 包含多个 transformer block. 不同的 track 只会在输入和输出的时候进行交互，这样就能够减少同步的开销。作者讲这种模式称为 &lt;strong>track parallelism&lt;/strong>.&lt;/p>
&lt;p>&lt;strong>PT-MoE&lt;/strong>
为了进一步提高 server model 的效率，作者将 MoE 和 PT-transformer 结合在一起。具体的做法就是，每两个 transformer block 为一组，每组里包含一个 dense layer 和一个 MoE layer.&lt;/p>
&lt;p>&lt;strong>Interleaving Global and Local Attention Layers&lt;/strong>
作者还设计额 interleaved attention 机制，也就是，将 transformer block 按照四个为 1 组，前面 3 个 block 使用 window attention, window size 为 4096 和 RoPE. 最后一个 block 使用 global attention layer 以及 &lt;a class="link" href="NoPE.md" >NoPE&lt;/a>. 作者认为，使用 NoPE 可以提高模型对长上下文的泛化性。&lt;/p>
&lt;blockquote>
&lt;p>Recall
Qwen2.5-VL 的 ViT 使用的类似的做法，即 8 个 block 为一组，前面 7 个 block 使用 window attention, 最后一个 block 使用 full self attention.&lt;/p>
&lt;/blockquote>
&lt;h4 id="vision-encoder">Vision Encoder
&lt;/h4>&lt;p>Vision encoder 包含 ViT 和 adapter 两个模块&lt;/p>
&lt;p>对于 ViT 来说，作者使用了 ViT 架构：&lt;/p>
&lt;ul>
&lt;li>server model 使用了 1B 参数的 ViT-g&lt;/li>
&lt;li>on-device model 使用了 300M 参数的 ViTDet-L backbone&lt;/li>
&lt;/ul>
&lt;p>作者在 ViTDet 的基础上加入了 Register-Window 机制，这个机制用于编码一个 global register token 来与不同的 loca windows 进行交互。&lt;/p>
&lt;p>对于 adapter 来说，其包含了一个 transformer layer, 一个 linear projection layer, 一个 $3\times 3$ 的 convolutional layer. 其中， linear projection 用于将 visual token 映射到 LLM 的特征空间，pooling layer 用于压缩 visual token 个数。&lt;/p>
&lt;h3 id="data">Data
&lt;/h3>&lt;p>主要包括 web data 和 image data 两部分&lt;/p>
&lt;p>image data 部分：&lt;/p>
&lt;ol>
&lt;li>Image-Text Crawl Data: 包含 &lt;strong>175M&lt;/strong> 图文交错数据，包含 &lt;strong>550M&lt;/strong> images&lt;/li>
&lt;li>Synthetic Image Caption data: &lt;strong>5B&lt;/strong> image caption 数据&lt;/li>
&lt;li>Text-Rich Image Data&lt;/li>
&lt;li>High-quality Domain-Specific Image-text Data: 包括 caption 数据， grounding 数据，table, chart, plots 数据以及 knowledge-required domains 的数据&lt;/li>
&lt;/ol>
&lt;h3 id="training-recipe">Training Recipe
&lt;/h3>&lt;p>text tokenizer 大小为 150K.&lt;/p>
&lt;p>Vision encoder 的训练包含两个 stage:&lt;/p>
&lt;ol>
&lt;li>基于 CLIP 的方法，使用 &lt;strong>6B&lt;/strong>的 image-text pair 数据进行训练，图片精度为 448, 作者还使用了 FLIP 来提高训练效率&lt;/li>
&lt;li>使用一个 compact LLM, 同时训练 vsion encoder, adapter 和 compact LLM. 加入了更高质量的数据，图片精度为 672.&lt;/li>
&lt;/ol>
&lt;p>LLM 的训练使用了 &lt;strong>13.4T&lt;/strong> token&lt;/p>
&lt;h2 id="post-training">Post-training
&lt;/h2>&lt;h3 id="sft">SFT
&lt;/h3>&lt;p>SFT 数据包括：&lt;/p>
&lt;ol>
&lt;li>General knowledge&lt;/li>
&lt;li>Reasoning: 纯文本包括 math 和 reasoning, 多模态包括 STEM, math, CoT 数据&lt;/li>
&lt;li>Text-Rich Image understanding: chart, table 数据&lt;/li>
&lt;li>Multilingual OCR: OCR 相关数据&lt;/li>
&lt;li>Text and visual grounding: grounding 数据&lt;/li>
&lt;li>Multi-image reasoning: 多图推理数据&lt;/li>
&lt;/ol>
&lt;p>作者还基于 retrieval-based 方法来收集数据，具体做法就是给定一些 prompt, 然后通过一个 Image search pipeline 来进行检索。&lt;/p>
&lt;p>训练的时候，作者将图片精度从 672 提升到 1344, 处理方式就是将图片切分为四个子图，然后作者还加入了一个总蓝图。这样，vision encoder 的输入包括四个子图和一个 thumbnail 图.&lt;/p>
&lt;p>为了提高 on-device model 的效率，作者设置了三种模式：&lt;/p>
&lt;ul>
&lt;li>rapid mode: 图片精度为 224&lt;/li>
&lt;li>balanced mode: 只有 thumbnail 图&lt;/li>
&lt;li>high-resolution mode: 四个子图和一个 thumbnail 图&lt;/li>
&lt;/ul>
&lt;p>对于不同的 mode, 如果输入的是低精度图片，则 $50%$ 概率为 rapid mode; 如果输入的是高精度图片，则 $1%$ 的概率为 rapid mode. 对于其他数据，作者将 $20%$ 的数据设置为 balanced mode.&lt;/p>
&lt;h3 id="rlhf">RLHF
&lt;/h3>&lt;p>作者使用 &lt;a class="link" href="RLOO.md" >RLOO&lt;/a> 作为 RLHF 的算法。&lt;/p>
&lt;p>RL 的 infra 如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-afm2025/AFM-2025-RL-infra.png"
width="1277"
height="350"
srcset="https://maosong2022.github.io/p/notes-on-afm2025/AFM-2025-RL-infra_hu11778082663676811747.png 480w, https://maosong2022.github.io/p/notes-on-afm2025/AFM-2025-RL-infra_hu2100111379656708107.png 1024w"
loading="lazy"
alt="AFM2025 RL Infra"
class="gallery-image"
data-flex-grow="364"
data-flex-basis="875px"
>&lt;/p>
&lt;p>infra 主要由两个部分组成：&lt;/p>
&lt;ol>
&lt;li>Trajectory Generators: 生成轨迹并提供反馈&lt;/li>
&lt;li>Policy updater: 更新 policy&lt;/li>
&lt;/ol>
&lt;p>训练时，作者首先训练了一个 reward model, 与 &lt;a class="link" href="AFM-2024.md" >AFM-2024&lt;/a> 相似，作者使用了一个 preference loss function 以及一个 single-sided grading 作为 regularization.&lt;/p>
&lt;p>数据包括以下类别:&lt;/p>
&lt;ul>
&lt;li>text-only prompts&lt;/li>
&lt;li>Image-text prompts&lt;/li>
&lt;li>Math prompts&lt;/li>
&lt;li>Image-text STEM reasoning prompts&lt;/li>
&lt;/ul>
&lt;p>其中，前面两个使用 reward function 进行打分，后面两个基于 ruled-based verifier 进行打分&lt;/p>
&lt;p>作者还发现，人类的打分和 reward model 的发奋可能会出现 $20%\sim30%$ 的偏差。为了解决这个问题，作者训练了一个单独的 reward model, 专门用于 prompt selection.&lt;/p>
&lt;h2 id="tool-use">Tool Use
&lt;/h2>&lt;p>工具调用数据由于 Multi-turn 和依赖软件工具，比较难以收集。为了解决这个问题，作者设计了一个交互式标注平台，包括一个 agent 和一个工具执行环境。环境包括了工具和数据库，能执行工具调用并反馈。&lt;/p>
&lt;p>标注时，用户发起一个请求，然后 agent 自动执行工具调用，最后平台返回反正的轨迹。&lt;/p>
&lt;h2 id="multilingual">Multilingual
&lt;/h2>&lt;p>作者逐步增加模型对于新语言的理解能力。默认情形下，输入和输出的语种一致，但是包含 $0.4%$ 的跨语种数据。在 SFT 和 RLHF 阶段，英语和多语种数据的比例为 $80%:20%$.&lt;/p>
&lt;h2 id="optimization">Optimization
&lt;/h2>&lt;p>作者使用了 QAT 来将 on-device model 压缩到 2 bits-per-weight, 使用 Adaptive Scalable Texture Compression (ASTC) 来 post-training 3.56 bits-per-weight 版本的 server model.&lt;/p>
&lt;h3 id="qat">QAT
&lt;/h3>&lt;p>QAT 是一个在模型训练过程中模拟量化误差，从而提升模型量化后表现的方法。它解决了传统后量化方法精度损失较大的问题，是平衡模型性能用户效率的关键手段。&lt;/p>
&lt;p>训练时，作者通过修改权重 $W$ 来模仿量化：&lt;/p>
$$
\tilde{W} = s\left(\mathrm{clamp}(\lfloor \frac{W}{s}+z\rceil, q_{\min}, q_{\max}) - z\right)
$$&lt;p>其中, $s$ 是 scaling factor, $z$ 是 zero point, $q_{\min}$, $q_{\max}$ 是 quantization 的 range. 为了解决 rounding operation 不可微的问题，作者使用了 straight-through estimator 的方法来近似梯度。&lt;/p>
&lt;p>作者还提出了一个可学习的 scaling factor $f$ 用于计算 quantization scale, 计算方法如下所示&lt;/p>
$$
s = \frac{f\cdot \max(|W|)}{q_{\max}}
$$&lt;p>作者通过精细设计 $f$ 的初始化来保证模型训练的 robust.&lt;/p>
&lt;h3 id="astc">ASTC
&lt;/h3>&lt;p>对于 server model, 作者使用了 ASTC, 一个针对 GPU 图形纹理压缩的技术，来压缩模型权重。具体做法就是，模型训练好之后，作者对模型权重应用 ASTC, 然后对每个块进行预处理。存储时，每个块用 ASTC-HAR-ch 模式压缩为 128 位。最小值单独存储为 float16.&lt;/p>
&lt;p>推理时，GPU 硬件自动解压缩 ASTC 块，然后解压的权重最小值相加参与矩阵计算&lt;/p>
&lt;h3 id="quality-recovery-adapters">Quality Recovery Adapters
&lt;/h3>&lt;p>作者还是用 LoRA 来恢复量化模型的精度，并通过选择性压缩策略优化 ASTC 过程，在极小的算力开销下实现了接近全量微调的性能。&lt;/p>
&lt;h2 id="evaluation">Evaluation
&lt;/h2>&lt;p>On-device model 表现如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>MMLU&lt;/th>
&lt;th>MMMLU&lt;/th>
&lt;th>MGSM&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>AFM On-Device&lt;/td>
&lt;td>67.85&lt;/td>
&lt;td>60.60&lt;/td>
&lt;td>74.91&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen-2.5-3B&lt;/td>
&lt;td>66.37&lt;/td>
&lt;td>56.53&lt;/td>
&lt;td>64.80&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen-3-4B&lt;/td>
&lt;td>&lt;strong>75.10&lt;/strong>&lt;/td>
&lt;td>&lt;strong>66.52&lt;/strong>&lt;/td>
&lt;td>&lt;strong>82.97&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Gemma-3-4B&lt;/td>
&lt;td>62.81&lt;/td>
&lt;td>56.71&lt;/td>
&lt;td>74.74&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Gemma-3n-E4B&lt;/td>
&lt;td>57.84&lt;/td>
&lt;td>50.93&lt;/td>
&lt;td>77.77&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>server model 表现如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>MMLU&lt;/th>
&lt;th>MMMLU&lt;/th>
&lt;th>MGSM&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>AFM Server&lt;/td>
&lt;td>80.20&lt;/td>
&lt;td>74.60&lt;/td>
&lt;td>87.09&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LLaMA 4 Scout&lt;/td>
&lt;td>84.88&lt;/td>
&lt;td>80.24&lt;/td>
&lt;td>90.34&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen-3-235B&lt;/td>
&lt;td>&lt;strong>87.52&lt;/strong>&lt;/td>
&lt;td>&lt;strong>82.95&lt;/strong>&lt;/td>
&lt;td>&lt;strong>92.00&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GPT-4o&lt;/td>
&lt;td>85.70&lt;/td>
&lt;td>84.00&lt;/td>
&lt;td>90.30&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者提出了 AFM-2025 多模态多语种大语言模型系列，包括 on-device 和 server 两个版本，作者介绍了模型的架构，训练数据和训练方式。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://machinelearning.apple.com/papers/apple_intelligence_foundation_language_models_tech_report_2025.pdf" target="_blank" rel="noopener"
>Publication&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://machinelearning.apple.com/research/apple-foundation-models-tech-report-2025" target="_blank" rel="noopener"
>blog&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Kimi-k2</title><link>https://maosong2022.github.io/p/notes-on-kimi-k2/</link><pubDate>Thu, 24 Jul 2025 10:56:50 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-kimi-k2/</guid><description>&lt;p>Kimi-k2 是一个总参数为 1T, 激活参数为 32B 的 MoE 大语言模型，模型使用 15.5T token 进行训练，optimizer 使用了 MuonClip. 作者主要关注模型的 agent 能力&lt;/p>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>作者首先强调现在 LLM 发展主要是 Agentic Intelligence, 也就是让模型自主感知，规划，思考和与环境交互。&lt;/p>
&lt;p>基于这个目标，作者就提出了 Kimi K2, 一个 1.04T 总参数，32B 激活参数的 MoE 模型，用于解决实现 agent Intelligence 中遇到的问题。作者主要进行了三点改进：&lt;/p>
&lt;ol>
&lt;li>MuonClip, 一个基于 &lt;a class="link" href="https://maosong.website/p/notes-on-moonlight/" target="_blank" rel="noopener"
>Muon&lt;/a> 的优化算法，来提高 Kimi K2 对 token 的利用效率以及提高训练的稳定性&lt;/li>
&lt;li>大规模的 agentic 数据合成 pipeline: 作者构建了一个用于合成工具调用，agent 数据的 pipeline&lt;/li>
&lt;li>通用的 RL 框架，作者将 RLVR 和 self-critic rubric reward mechanism 结合起来，用于提升模型的表现&lt;/li>
&lt;/ol>
&lt;h2 id="pre-training">Pre-training
&lt;/h2>&lt;h3 id="architecture">Architecture
&lt;/h3>&lt;p>Kimi-K2 的架构与 DeepSeek-V3 相似，配置如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>指标&lt;/th>
&lt;th>DeepSeek-V3&lt;/th>
&lt;th>Kimi K2&lt;/th>
&lt;th>$\Delta$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td># Layers&lt;/td>
&lt;td>61&lt;/td>
&lt;td>61&lt;/td>
&lt;td>=&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Total Parameters&lt;/td>
&lt;td>$671\text{B}$&lt;/td>
&lt;td>$1.04\text{T}$&lt;/td>
&lt;td>$\uparrow 54%$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Activated Parameters&lt;/td>
&lt;td>$37\text{B}$&lt;/td>
&lt;td>$32.6\text{B}$&lt;/td>
&lt;td>$\downarrow 13%$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Experts (total)&lt;/td>
&lt;td>256&lt;/td>
&lt;td>384&lt;/td>
&lt;td>$\uparrow 50%$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Experts Active per Token&lt;/td>
&lt;td>8&lt;/td>
&lt;td>8&lt;/td>
&lt;td>=&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Shared Experts&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;td>=&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Attention Heads&lt;/td>
&lt;td>128&lt;/td>
&lt;td>64&lt;/td>
&lt;td>$\downarrow 50%$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Number of Dense Layers&lt;/td>
&lt;td>3&lt;/td>
&lt;td>1&lt;/td>
&lt;td>$\downarrow 67%$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Expert Grouping&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>No&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>与 DeepSeek-V3 相比，模型主要进行了以下改动：&lt;/p>
&lt;ol>
&lt;li>作者认为提高专家的稀疏性，可以有效提高模型表现。因此作者将专家个数提升了 50%.&lt;/li>
&lt;li>为了降低模型在 inference 阶段的算力开销，作者将 attention heads 的个数降低了 50%.&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Sparsity Scaling Law&lt;/strong>
作者首先构建了基于 Muon 的 sparsity law, 这里 sparsity 定义为激活专家个数与总专家个数之比，即：&lt;/p>
$$
\mathrm{sparsity} = \frac{\# \mathrm{activated\ experts}}{\# \mathrm{total\ experts}}
$$&lt;p>作者在小规模上的实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-K2-sparsity-scaling-law.png"
width="673"
height="678"
srcset="https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-K2-sparsity-scaling-law_hu12013290483963777320.png 480w, https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-K2-sparsity-scaling-law_hu15866147703512200734.png 1024w"
loading="lazy"
alt="Sparsity Scaling Law"
class="gallery-image"
data-flex-grow="99"
data-flex-basis="238px"
>&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Observation&lt;/strong>
实验结果表明，在相同算力（激活专家个数）下，模型的表现随 sparsity 提高而增加。&lt;/p>
&lt;/blockquote>
&lt;p>但是，进一步提高 sparsity, 会让 infra 变的难以优化，因此在 Kimi-K2 里，将 sparsity 定义为 $48$.&lt;/p>
&lt;p>&lt;strong>Number of attention heads&lt;/strong>
作者还分析了探究了 attention heads 的最优配置。DeepSeek-V3 将 attention heads 的个数设置为 layers 层数的 2 倍，来提高带宽利用率以及提高计算效率。但是，当上下文长度增加之后，attention head 变成了 computational bound. 作者对比了不同配置模型的表现，实验结果如下：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-K2-attention-heads.png"
width="671"
height="675"
srcset="https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-K2-attention-heads_hu8958375214150279472.png 480w, https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-K2-attention-heads_hu10448179446271388634.png 1024w"
loading="lazy"
alt="Scaling curves for attention heads"
class="gallery-image"
data-flex-grow="99"
data-flex-basis="238px"
>&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Observation&lt;/strong>
实验结果表明，使用双倍 attention head 带来的收益是比较小的，只有 $0.5%$ 到 $1.2%$ 左右&lt;/p>
&lt;/blockquote>
&lt;p>因此，作者在 Kimi-K2 中，奖 attention head 的个数设置为 $64$.&lt;/p>
&lt;h3 id="data">Data
&lt;/h3>&lt;p>Kimi-K2 主要强调了 token efficiency, 即每个 token 对模型表现提升的贡献。token efficiency 越高，说明每个 token 对最终模型的贡献也就越高&lt;/p>
&lt;p>相比于 &lt;a class="link" href="https://maosong.website/p/notes-on-kimi-k1.5/" target="_blank" rel="noopener"
>Kimi-k1.5&lt;/a>, Kimi-K2 使用了一个 rephrasing pipeline, 来提高高质量 token 的利用率，作者在 knowledge 和 mathematics domain 上进行了实验。&lt;/p>
&lt;p>最终，Kimi-K2 的预训练数据包括了 &lt;strong>15.5T&lt;/strong> token, 主要覆盖 Web text, code, mathmatics 以及 Knowledge 四个 domain. 大部分的数据处理与 Kimi-k1.5 相同。&lt;/p>
&lt;p>&lt;strong>Knowledge Data Rephrasing&lt;/strong>
作者构建了一个 synthetic rephrasing 框架来提高 knowledge token 的利用率，框架主要包含以下几个模块：&lt;/p>
&lt;ol>
&lt;li>Style and perspective-diverse prompting: 作者构建了一系列 prompt, 来让 LLM 从多个角度和风格来重新表述原始文本&lt;/li>
&lt;li>Chunk-wise auto-regressive generation: 为了保持模型在长文档中的 coherence 以及避免信息损失，作者使用了一个 chunk-based rewriting 策略，也就是对每个 chunk 分别进行改写，然后将它门汇总在一起&lt;/li>
&lt;li>Fidelity verification: 作者对原始文本和改写文本进行了 fidelity 检查来保证语义的一致性。&lt;/li>
&lt;/ol>
&lt;p>作者对比了以下三个设置对模型在 SimpleQA benchmark 上表现的影响，这三个设置分别是：&lt;/p>
&lt;ol>
&lt;li>原始数据集训练 10 epoch&lt;/li>
&lt;li>改写数据一次，然后训练 10 epoch&lt;/li>
&lt;li>改写数据一次，训练 1 epoch&lt;/li>
&lt;/ol>
&lt;p>实验结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th># Rephrasings&lt;/th>
&lt;th># Epochs SimpleQA&lt;/th>
&lt;th>Accuracy&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>0 (raw wiki-text)&lt;/td>
&lt;td>10&lt;/td>
&lt;td>23.76&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>10&lt;/td>
&lt;td>27.39&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>1&lt;/td>
&lt;td>28.94&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，改写的策略可以有效提高模型在 SimpleQA benchmark 上的表现&lt;/p>
&lt;p>&lt;strong>Math Data Rephrasing&lt;/strong>
对于数学相关数据，作者基于 SwallowMath, 将数据改写成了学习笔记 (learning-note) 的风格。然后，作者还将其他语言的高质量文本翻译为英文。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Recall&lt;/strong>
个人认为，数据改写在一定程度上也算是合成数据，对于合成数据这一块，微软提出的 phi 系列是一个比较好的参考&lt;/p>
&lt;/blockquote>
&lt;h3 id="muonclip-optimizer">MuonClip Optimizer
&lt;/h3>&lt;p>Kimi-K2 使用了 Muon optimizer, 之前的实验结果说明了在相同的 compute budget 下，Muon optimizer 的表现超过了 AdamW. 也就是说，Muon optimizer 更加高效。&lt;/p>
&lt;p>但是，Muon optimizer 的问题是训练不稳定。为了解决这个问题，作者提出了 QK-clip, 一个 weight clipping 机制，来显示约束 attention logits. QK-clip 的机制是&lt;strong>当输出的 logits 超过某一个阈值之后，就对齐进行截断&lt;/strong>。&lt;/p>
&lt;p>每个 head 的 attention 的计算公式如下&lt;/p>
$$
O = \mathrm{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V
$$&lt;p>其中, $d$ 是 hidden size, $Q, K,V$ 分别是 query, key, value, 定义如下：&lt;/p>
$$
Q = XW_Q, K=XW_K, V=XW_V
$$&lt;p>这里 $W_Q,W_K,W_V$ 是模型可学习的参数。&lt;/p>
&lt;p>作者定义每个 head 的 max logit 如下：&lt;/p>
$$
S_{\max}^h = \frac{1}{\sqrt{d}} \max_{X\in\mathcal{B}}\max_{i,j} [QK^T]_{ij}
$$&lt;p>最简单的做法就是直接进行截断，也就是&lt;/p>
$$
W_Q\gets \gamma^\alpha W_q, W_K\gets \gamma^{1-\alpha}W_K
$$&lt;p>其中 $\gamma=\min(1, \tau S_{\max})$, 这里 $S_{\max}=\max_h S_{\max}^h$ 是所有 head 对应 $S_{\max}^h$ 的最大值。&lt;/p>
&lt;p>但是，实际中，作者发现只有少部分 head 会出现 logits 爆炸现象。为了提高计算效率，作者针对每个 head 单独进行 scaling, 也就是 $\gamma_{h}=\min(1, \tau S_{\max}^h)$. 对于 MLA 架构，作者仅在 unshared 模块使用 clipping:&lt;/p>
&lt;ul>
&lt;li>$q^c$ 以及 $k^c$, scaling factor 为 $\sqrt{\gamma_h}$&lt;/li>
&lt;li>$q^R$, scaling factor 为 $\gamma_h$&lt;/li>
&lt;/ul>
&lt;p>最后，作者将 Muon, weight decay, RMS matching 和 QK-clip 汇总在一起，得到 Kimi-k2 使用的 MuonClip optimizer, 算法如下所示：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-muonclip.png"
width="1385"
height="668"
srcset="https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-muonclip_hu15126210376126559992.png 480w, https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-muonclip_hu12041395682263694561.png 1024w"
loading="lazy"
alt="MuonClip Optimizer"
class="gallery-image"
data-flex-grow="207"
data-flex-basis="497px"
>&lt;/p>
&lt;p>接下来，作者对比了以下 Muon 和 MuonClip 两个优化器的表现，作者分别使用这两个优化器训练 53B-A9B 的 MoE 模型，实验结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-muonclip-performance.png"
width="1365"
height="551"
srcset="https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-muonclip-performance_hu6393944337521741241.png 480w, https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-muonclip-performance_hu16210125810578172496.png 1024w"
loading="lazy"
alt="Comparison between Muon and MuonClip"
class="gallery-image"
data-flex-grow="247"
data-flex-basis="594px"
>&lt;/p>
&lt;p>实验结果表明，Muon 优化器在 1000 步左右就出现了 logits 爆炸的现象，但是 MuonClip 通过优化可以避免这个问题。作者还发现，MuonClip 可以减少 loss spike 的产生，整体的 loss 变化情况如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-model-training-dynamics.png"
width="977"
height="561"
srcset="https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-model-training-dynamics_hu4565157971618681387.png 480w, https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-model-training-dynamics_hu4579571635122099452.png 1024w"
loading="lazy"
alt="Training loss curve"
class="gallery-image"
data-flex-grow="174"
data-flex-basis="417px"
>&lt;/p>
&lt;p>作者还在附录中说明 QK-clip 并不影响最终的收敛性，并且，为了尽可能减少对模型训练的干扰，作者发现，仅在训练初期 QK-clip 被激活:&lt;/p>
&lt;ol>
&lt;li>在初始的 70, 000 步里，有 12.7% 的 attention heads 至少触发了一次 QK-clip, 并且将它们的 $S_\max$ 降到了 100 以下&lt;/li>
&lt;li>接下来的 70, 000 步里，QK-clip 就不再被激活&lt;/li>
&lt;/ol>
&lt;h3 id="infra">Infra
&lt;/h3>&lt;p>Kimi-k2 使用了 16-way 的 PP, 16-way 的 EP 以及 ZeRO-1 Data Parallelism. 具体流程如下&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-communication-computation.png"
width="1362"
height="318"
srcset="https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-communication-computation_hu9768363041919262943.png 480w, https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-communication-computation_hu17009654877161129053.png 1024w"
loading="lazy"
alt="Kimi-K2 parallelism"
class="gallery-image"
data-flex-grow="428"
data-flex-basis="1027px"
>&lt;/p>
&lt;p>作者发现通过增加 warm-up mircro-batches, 我们可以有效重叠 EP 的 all-to-all communication 以及 computation. 但是，对于 [[DualPipe]], 其要求双倍的存储来保存参数和梯度，并且提高 PP 粒度会引入更多的 bubble, 因此 Kimi-K2 没有使用 DualPipe.&lt;/p>
&lt;p>为了减少 [[1F1B]] 的 PP 通信开销，作者在反向传播的过程中同时进行 PP 通信，来进一步重合通信与计算。&lt;/p>
&lt;p>作者还发现，使用更小的 EP group 可以提高整体的训练速度，因此作者将 EP 设置为 16.&lt;/p>
&lt;p>作者发现，剩余的 GPU 内存不足以存放 MoE 的 activation, 因此作者采取了三个办法解决这个问题：&lt;/p>
&lt;ol>
&lt;li>Selective recomputation: 作者对 LayerNorm, SwiGLU, MLA 的 up-projection 和 MoE 的 down-projections 等进行重新计算。&lt;/li>
&lt;li>FP8 storage for insentive activations: 作者使用了 FP8 精度来存储 MoE up-projections 以及 SwiGLU. 作者发现使用 FP8 进行计算可能会导致性能下降，因此作者并没有使用 FP8 进行计算。&lt;/li>
&lt;li>Activation CPU offload: 作者将其余的 activation 放在 CPU RAM 上，在 1F1B 的过程中，作者再进行加载&lt;/li>
&lt;/ol>
&lt;h3 id="training-recipe">Training Recipe
&lt;/h3>&lt;p>模型上下文长度为 4096, 使用 MuonClip 进行优化，使用 WSD lr Scheduler.weight decay 为 0.1, global batch size 为 67M tokens.&lt;/p>
&lt;p>预训练结束之后，作者加入了一个 long-context activation stage. 这个阶段，作者使用了 400B 的 4K 上下文的 token 和 60B 的 32K 上下文的 60B token. 最后作者使用 &lt;a class="link" href="https://maosong.website/p/notes-on-yarn/" target="_blank" rel="noopener"
>YARN&lt;/a> 将模型上下文长度扩展到 128K.&lt;/p>
&lt;h2 id="post-training">Post-training
&lt;/h2>&lt;h3 id="sft">SFT
&lt;/h3>&lt;p>数据的构建主要是基于：&lt;/p>
&lt;ol>
&lt;li>prompt 的多样性&lt;/li>
&lt;li>Response 的质量&lt;/li>
&lt;/ol>
&lt;p>作者构建了一系列的数据生成 pipeline, 然后使用 Kimi-k1.5 来生成多样化的回答，最后再进行质量评估和过滤。这里，作者主要介绍了一下 tool-use 数据的构建&lt;/p>
&lt;p>受 ACEBench 启发，作者构建了一个模仿真实世界 tool-use 的数据合成 pipeline. pipeline 如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-tool-use-synthetic-pipeline.png"
width="1366"
height="439"
srcset="https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-tool-use-synthetic-pipeline_hu8910712181245528206.png 480w, https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-tool-use-synthetic-pipeline_hu2723101591411375395.png 1024w"
loading="lazy"
alt="Data synthesis pipeline for tool use"
class="gallery-image"
data-flex-grow="311"
data-flex-basis="746px"
>&lt;/p>
&lt;p>pipeline 主要包含三个阶段：&lt;/p>
&lt;ul>
&lt;li>tool spec generation: 作者基于 MCP tools 和 self-evolved 的数据合成策略来构建 tool repository. MCP tools 包括 3000+ 的 tools,合成了 20，000 个 tools&lt;/li>
&lt;li>Agent and task generation: 作者还用不同的 system prompt 以及不同的 tools combination 来构建对应的 agent. 然后对不同的 agent, 作者构建了对应的成功标准均，工具调用模式以及评估方式&lt;/li>
&lt;li>Trajectory generation: 作者首先构建不同风格的 LLM, 然后作者使用一个 simulator 来执行 tool call 并给予反馈。&lt;/li>
&lt;/ul>
&lt;p>最后，作者对数据进行了过滤。&lt;/p>
&lt;p>作者还加入了 coding 以及软件工程等任务相关数据来进一步提高模型的 agent 能力&lt;/p>
&lt;h3 id="rl">RL
&lt;/h3>&lt;p>RL 与 Kimi-K1.5 差不多。作者进一步提高了 RL 阶段的算力并做出了亮点改进：&lt;/p>
&lt;ol>
&lt;li>作者构建了类似 Gym 的框架，用于扩展 RL 的能力&lt;/li>
&lt;li>作者加入了更多 RLVR 的任务&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Data&lt;/strong>
数据主要包括以下几类：&lt;/p>
&lt;ol>
&lt;li>Math, STEM and logical tasks: 数据构建的原则为多样化和中等难度&lt;/li>
&lt;li>Instruction following: 作者基于 hybrid rule verification 和 multi-source instruction generation 来合成复杂的指令跟随数据&lt;/li>
&lt;li>Faithfulness: 作者训练了一个 judge model 来提供 reward&lt;/li>
&lt;li>Coding &amp;amp; Software Engineering: 作者从开源数据收集并合成了代码相关数据&lt;/li>
&lt;li>Safety. 提高模型的安全性，防止 jailbreak&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Reward&lt;/strong>
作者使用了 self-critique rubric reward 的奖励机制。&lt;/p>
&lt;p>首先，对于 Kimi-k2 的回答，作者会使用另一个 kimi-k2 作为 critic，基于三个方面进行排序：&lt;/p>
&lt;ul>
&lt;li>core rubric: AI 的核心价值观&lt;/li>
&lt;li>prescriptive rubric: 避免 reward hacking&lt;/li>
&lt;li>human-annotated rubric: 特定的上下文&lt;/li>
&lt;/ul>
&lt;p>在训练的过程中，critic 也会基于 verifiable signals 进行 refine&lt;/p>
&lt;p>&lt;strong>RL training&lt;/strong>
RL 的训练目标与 Kimi-k1.5 相同&lt;/p>
$$
\mathcal{L}(\pi_\theta) = \mathbb{E}_{x\sim \mathcal{D}}\left[\frac1K\sum_{i=1}^K \left(r(x,y_i)-\bar{r}(x) -\tau\log\frac{\pi_{\mathrm{\theta}}(y_i\mid x)}{\pi_{\mathrm{old}}(y_i\mid x)}\right)^2\right]
$$&lt;p>其中 $\bar{r}(x)=1/K\sum_{i=1}^Kr(x,y_i)$ 是 sample response 的平均奖励。&lt;/p>
&lt;p>作者做了以下几点改进来提高模型在不同 domain 上的表现：&lt;/p>
&lt;ol>
&lt;li>Budget control: 作者针对不同任务分别设置了最大 token 限制，模型超过这个限制会受到惩罚。猜测应该是 length penalty 或者类似 Qwen3 一样，直接中断思考过程输出最终答案&lt;/li>
&lt;li>PTX loss: 作者使用了 PTX loss 来提高模型对于高质量数据的利用率以及降低模型的过拟合&lt;/li>
&lt;li>Temperature Decay: 作者发现，训练后期保持较高的采样温度会影响模型的表现，因此作者设置了一个 schedule, 来逐步降低采样温度。&lt;/li>
&lt;/ol>
&lt;h3 id="rl-infra">RL Infra
&lt;/h3>&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-RL-infra.png"
width="734"
height="477"
srcset="https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-RL-infra_hu7836952951472259143.png 480w, https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-RL-infra_hu929938601854115055.png 1024w"
loading="lazy"
alt="Kimi-K2 RL infra"
class="gallery-image"
data-flex-grow="153"
data-flex-basis="369px"
>&lt;/p>
&lt;p>RL infra 与 Kimi-k1.5 类似。主要包含三个模块。其中 train engine 和 inference engine 两者互相切换，一个 engine 进行 training 的时候，另一个 engine 就进行 inference, 为了提高 engine 切换的效率，作者使用了 checkpoint engine 来传输更新后的参数。&lt;/p>
&lt;h2 id="evaluation">Evaluation
&lt;/h2>&lt;p>模型评估结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-performance.png"
width="696"
height="979"
srcset="https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-performance_hu17285728062120188885.png 480w, https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-performance_hu8344921060842393810.png 1024w"
loading="lazy"
alt="Performance of Kimi-k2"
class="gallery-image"
data-flex-grow="71"
data-flex-basis="170px"
>&lt;/p>
&lt;p>评估结果显示，模型在 coding 和通用任务上的表现仅次于 Claude 4, 大部分 benchmark 上都是第二名甚至是 sota.&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 Kimi-K2, 一个总参数为 1B 的 MoE 大语言模型。作者在架构，数据和优化器上进行了创新。并且通过 post-training 显著提高了模型的 agent 以及 tool-use 能力&lt;/p>
&lt;p>作者发现模型主要存在的问题有：&lt;/p>
&lt;ol>
&lt;li>reasoning 任务过难或者 tool 的定义不清晰的时候，模型会使用很多 token&lt;/li>
&lt;li>有时候工具调用可能会降低模型的表现&lt;/li>
&lt;li>模型在 agentic coding 任务上的能力需要进一步提升&lt;/li>
&lt;/ol>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://github.com/MoonshotAI/Kimi-K2" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Keye-VL</title><link>https://maosong2022.github.io/p/notes-on-keye-vl/</link><pubDate>Wed, 23 Jul 2025 11:11:43 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-keye-vl/</guid><description>&lt;p>Keye-VL 是快手在 25 年 7 月份提出的一个 8B 的多模态大模型，其亮点为短视频理解能力。预训练包括 4 个 stage，使用了 600B token，后训练包括 2 个 stage，用于提升模型的 reasoning 和 non-reasoning 能力。&lt;/p>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>作者首先回顾了已有的 MLLM 工作，然后强调理解短视频仍然是一个很难的任务，特别是要求模型基于 video 和 audio 来理解视频。因此，在本文中，作者提出了 Kwai Keye-VL，一个 8B 的多模态大模型，主要用于短视频理解任务。&lt;/p>
&lt;h2 id="method">Method
&lt;/h2>&lt;h3 id="architecture">Architecture
&lt;/h3>&lt;p>Keye-VL 是一个标准的 ViT-MLP-LLM 的架构，其中，ViT 是 SigLIP-400M-384-14, MLP 是一个基于 SwiGLU 的 2 层 MLP，使用了和 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5-vl/" target="_blank" rel="noopener"
>Qwen2.5-VL&lt;/a> 一样的 patch merge 方法，LLM 使用的是 Qwen3-8B，模型架构示意图如下&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-keye-vl/Keye-VL-architecture.png"
width="1371"
height="1000"
srcset="https://maosong2022.github.io/p/notes-on-keye-vl/Keye-VL-architecture_hu8715583027172451106.png 480w, https://maosong2022.github.io/p/notes-on-keye-vl/Keye-VL-architecture_hu3357172034037663162.png 1024w"
loading="lazy"
alt="Keye-VL model architecture"
class="gallery-image"
data-flex-grow="137"
data-flex-basis="329px"
>&lt;/p>
&lt;p>作者针对 ViT 和 visual encoding 分别做了如下改进&lt;/p>
&lt;h4 id="navit">NaViT
&lt;/h4>&lt;p>作者实现了 native resolution ViT，来处理不同分辨率的图片。&lt;/p>
&lt;p>具体做法为，作者基于 SigLIP-400M-384-14 来初始化 ViT。&lt;/p>
&lt;p>然后，作者首先采用了 interpolation 来将 ViT 的 position encoding 扩展到不同的图片精度下面去。&lt;/p>
&lt;p>接下来，作者提出了 2D RoPE 来进一步提升 position encoding 的表现。&lt;/p>
&lt;p>最后，作者加入了 NaViT 的 packing 技巧来继续预训练 ViT.&lt;/p>
&lt;p>在 ViT 预训练的过程中，作者使用了 &lt;strong>500B&lt;/strong> 的 token&lt;/p>
&lt;h4 id="visual-encoding">Visual Encoding
&lt;/h4>&lt;p>为了提升模型理解图片和视频的能力，作者针对图片和视频进行了进一步的处理。&lt;/p>
&lt;p>对于不同精度的图片，作者将最大 token 个数设置为 16384。&lt;/p>
&lt;p>对于视频，作者将每帧的 token 数限制在 $[128,768]$, 每个视频的最大 token 个数设置为 24576&lt;/p>
&lt;p>对于提取的 frames，作者重新计算了 FPS, 然后在 3D RoPE 中让时间维度与真实时间严格对齐。&lt;/p>
&lt;h3 id="pre-training">Pre-training
&lt;/h3>&lt;h4 id="data">Data
&lt;/h4>&lt;p>预训练数据一共包括 600B token，覆盖了 6 个类别：&lt;/p>
&lt;ul>
&lt;li>Image caption: 包括中英文数据，来源为 LAION, DataComp 以及 Coyo. 作者基于 CLIP 来计算相速度，然后过滤掉相似度比较低的数据，作者还对数据进行了 re-caption，实验发现 re-caption 可以提高模型的细粒度图片理解能力&lt;/li>
&lt;li>OCR &amp;amp; VQA: 数据包括开源数据和合成数据。合成数据包括 Synthesis 和 Rendering 两个方法，第一是基于 text-dense image 构建 OCR 数据以及基于 image-caption pair 数据使用 MLLM 构建 VQA 数据。第二是使用字体渲染工具合成高质量的 OCR 数据&lt;/li>
&lt;li>Grounding &amp;amp; Counting: Grounding 数据主要包括 RefCoCo, VisualGenome, TolokaVQA, Counting 数据包括 PixMo. 作者仍然使用 CLIP 对数据进行过滤&lt;/li>
&lt;li>Interleaved text-image data: 作者发现图文交错数据可以提供通用知识，并且还可以提高模型的视觉语言对齐能力，第三就是提升模型的泛化能力。作者主要从 academic PDF 以及结构化知识中提取对应的数据。作者基于 Garbled character recognition, low-resolution/broken image filtering 以及 text-image similarity validation 来保证数据的质量&lt;/li>
&lt;li>Video understanding: 作者使用 Qwen2.5-omni 从 interleaved video-asr 来将视频数据转化图文交错数据， 然后基于 ASR 的结果进行 recaption，最后对每一帧进行 OCR. 作者还构建了 Frame-level re-ordering 以及 multiple video matching 两个任务来提高模型的上下文理解能力&lt;/li>
&lt;li>Pure Text: 未提及&lt;/li>
&lt;/ul>
&lt;p>对于源数据，作者进行了数据清洗：&lt;/p>
&lt;ol>
&lt;li>使用 CLIP 对数据进行打分，然后过滤掉低质量的数据&lt;/li>
&lt;li>使用开源的 MLLM 作为 discriminator 来选择高质量的数据&lt;/li>
&lt;li>去重&lt;/li>
&lt;/ol>
&lt;h4 id="training-recipe">Training Recipe
&lt;/h4>&lt;p>预训练包括 4 个 stage：&lt;/p>
&lt;ul>
&lt;li>Stage 0: 使用 SigLIP 损失函数来继续训练 ViT&lt;/li>
&lt;li>Stage 1: cross-modal Alignment，仅训练 MLP&lt;/li>
&lt;li>Stage 2: multi-task pre-training, 解冻所有参数，使用 Multi-task 数据来训练模型&lt;/li>
&lt;li>Stage 3: annealing, 在高质量数据集上进行 fine-tune，进一步提升模型的能力&lt;/li>
&lt;/ul>
&lt;p>作者发现，预训练后的模型在下游任务上的表现对训练数据配比非常敏感。为了解决这个问题，在最后一个训练阶段，作者使用了一个 merging 的技巧，来保持模型的能力。&lt;/p>
&lt;h3 id="post-training">Post-training
&lt;/h3>&lt;p>post-training 阶段一共包含了 2 个 step, 5 个 stage, 第一个 step 包含 2 个 stage，用于提升模型的 non-reasoning 能力。第二个 step 包含 3 个 stage, 用于提升模型的 reasoning 能力&lt;/p>
&lt;h4 id="no-reasoning-training">No-reasoning Training
&lt;/h4>&lt;p>第一个 step 是 non-reasoning training, 包含了 SFT 和 &lt;a class="link" href="MPO.md" >MPO&lt;/a> 两个 stage, 训练 pipeline 如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-keye-vl/Keye-VL-non-reasoning-training.png"
width="1340"
height="472"
srcset="https://maosong2022.github.io/p/notes-on-keye-vl/Keye-VL-non-reasoning-training_hu4483349681294792353.png 480w, https://maosong2022.github.io/p/notes-on-keye-vl/Keye-VL-non-reasoning-training_hu13267003824404863093.png 1024w"
loading="lazy"
alt="Non reasoning training pipeline"
class="gallery-image"
data-flex-grow="283"
data-flex-basis="681px"
>&lt;/p>
&lt;p>&lt;strong>SFT&lt;/strong>
SFT 阶段一共使用了 &lt;strong>5M&lt;/strong> 的多模态 QA 样本，为了提升数据的多样性，作者使用 TaskGalaxy 来将数据分类为 70,000 种任务类型，然后作者对每条数据，使用 MLLM 评估问题的难度，来过滤掉过于简单的问题。最后，作者请人类来进行标注，保证数据的可靠性。&lt;/p>
&lt;p>&lt;strong>MPO&lt;/strong>
训练方面，作者使用了 MPO 进行训练。
数据方面，作者使用了：&lt;/p>
&lt;ol>
&lt;li>400,000 开源的数据，作者主要进行了去重以及过滤低质量的数据&lt;/li>
&lt;li>50,000 偏好数据，基于 MM-RLHF 和 MMPR 等数据构建，然后构建高质量的 negative examples&lt;/li>
&lt;li>10,000 条 self-improvement 样本：基于 benchmark 和人类反馈，使用 SFT model 的回答作为 chosen samples, 然后基于 reward model 或者 rule-based rewards 评估模型输出，选择分数最低的座位 rejected samples&lt;/li>
&lt;li>90,000 纯文本样本： in-house data&lt;/li>
&lt;li>30,000 人类标注样本：使用开源和闭源模型进行回答，然后请人类进行排序&lt;/li>
&lt;/ol>
&lt;h4 id="reasoning-training">Reasoning Training
&lt;/h4>&lt;p>第二个 step 是 reasoning training, 包含了 CoT Cold-Start, Mix-Mode RL 和 Iterative Alignment 三个 stage, 训练 pipeline 如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-keye-vl/Keye-VL-reasoning-training.png"
width="1358"
height="596"
srcset="https://maosong2022.github.io/p/notes-on-keye-vl/Keye-VL-reasoning-training_hu3508552858551465121.png 480w, https://maosong2022.github.io/p/notes-on-keye-vl/Keye-VL-reasoning-training_hu15300135858013059770.png 1024w"
loading="lazy"
alt="Non reasoning training pipeline"
class="gallery-image"
data-flex-grow="227"
data-flex-basis="546px"
>&lt;/p>
&lt;p>&lt;strong>CoT cold-start&lt;/strong>
作者收集了如下数据：&lt;/p>
&lt;ul>
&lt;li>330, 000 条 non-reasoning 样本：和第一个 step 的数据分布类似，但是不重合&lt;/li>
&lt;li>230,000 reasoning 样本：作者构建了一个 data construction pipeline,用于保证 long-CoT 的正确性&lt;/li>
&lt;li>20,000 automatic reasoning 样本：作者基于 MMPR, MM-Eureka 以及 OpenR1-math 来收集数据，基于这些模型来训练模型自动化决定是否要进行 reasoning&lt;/li>
&lt;li>100,000 agentic reasoning 样本：训练模型的 &amp;ldquo;think with image&amp;rdquo; 能力。基于 3M QA pairs，作者使用 Qwen2.5-72B 来识别需要进行 manipulating 的问题，然后生成对应的代码。接下来作者构建了一部分 OCR 数据，让模型学会 constrast enhancement 或者 rotation 操作。最后对于数学问题，作者让 Gemini-2.5-Pro 生成对应的思考过程，再让 GPT-4o 将对应的计算转换为可执行的代码。&lt;/li>
&lt;li>32, 000 Video data: 包含了 24,000 条 thinking samples 和 80,000 条 non-thinking samples&lt;/li>
&lt;/ul>
&lt;p>训练时，所有样本混在一起进行训练。作者认为，将日常使用的 SFT 数据和 reasoning 数据放在一起训练，可以保持模型在通用场景下的能力。&lt;/p>
&lt;p>&lt;strong>Mix-Mode RL&lt;/strong>
训练数据主要包括 4 个任务：&lt;/p>
&lt;ol>
&lt;li>Multimodal perception: 复杂文本识别和 counting 任务&lt;/li>
&lt;li>Multimodal reasoning: MMPR 和 MM-Eureka&lt;/li>
&lt;li>Text-based mathematical reasoning: 数学推理问题&lt;/li>
&lt;li>Agentic reasoning: 从 DeepEyes 中获取的 47,000 条样本&lt;/li>
&lt;/ol>
&lt;p>作者使用了 GRPO 来训练，reward 基于 MLLM 进行，包括最终结果和思考过程。&lt;/p>
&lt;p>作者还使用 RL 来提高模型的短视频理解能力。作者发现 RL 训练之后，模型的短视频理解能力有了大幅度的提升。&lt;/p>
&lt;p>&lt;strong>Iterative Alignment&lt;/strong>
这一步主要解决模型的重复性输出，或者 reasoning logic 不对的问题。作者使用了 rejection-sampling 数据，包括 instruction following, OCR, mathematics, charts, counting 等。&lt;/p>
&lt;p>作者基于 Rule-based score 和 model-based score 来进行打分，最后使用 MPO 算法进行训练。通过这个过程，模型的输出格式和动态思考能力都有了提升。&lt;/p>
&lt;h2 id="evaluation">Evaluation
&lt;/h2>&lt;p>作者首先评估了 ViT 的表现，主要有两点：&lt;/p>
&lt;ol>
&lt;li>在 SigLIP 的基础上加入 1D interpolation 之后，模型的表现所有下降，作者认为这是由于 1D 的 position encoding 无法识别 2D 的 patch 排列导致的&lt;/li>
&lt;li>加入 2D RoPE 之后，ViT 与 SigLIP 的表现持平&lt;/li>
&lt;/ol>
&lt;p>接下来是 Keye-VL 在公开 benchmark 上的表现，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-keye-vl/Keye-VL-performance.png"
width="1068"
height="1161"
srcset="https://maosong2022.github.io/p/notes-on-keye-vl/Keye-VL-performance_hu2244647248615998183.png 480w, https://maosong2022.github.io/p/notes-on-keye-vl/Keye-VL-performance_hu12384064493261148122.png 1024w"
loading="lazy"
alt="Performance of Keye-VL"
class="gallery-image"
data-flex-grow="91"
data-flex-basis="220px"
>&lt;/p>
&lt;p>作者还构建了一个内部的 benchmark, 用于进一步评估模型的能力。&lt;/p>
&lt;p>已有 benchmark 的问题：&lt;/p>
&lt;ol>
&lt;li>contamination&lt;/li>
&lt;li>多语种覆盖不足：大部分 benchmark 都是英文的&lt;/li>
&lt;li>任务和 domain 覆盖不足：大部分 benchmark 只考虑基本的 perception 和 reasoning 能力&lt;/li>
&lt;li>任务难度和评估格式单调&lt;/li>
&lt;/ol>
&lt;p>构建 benchmark 的原则：&lt;/p>
&lt;ol>
&lt;li>在中文场景下的真实用户需求，open ended QA, 包括短视频理解能力&lt;/li>
&lt;li>细粒度的评估&lt;/li>
&lt;li>多样性高&lt;/li>
&lt;li>没有 contamination&lt;/li>
&lt;li>多角度评估策略: 正确性，相关性，理解性，流畅性和创造性&lt;/li>
&lt;/ol>
&lt;p>结果如下：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-keye-vl/Keye-VL-performance-internal.png"
width="1339"
height="559"
srcset="https://maosong2022.github.io/p/notes-on-keye-vl/Keye-VL-performance-internal_hu13798451161994777213.png 480w, https://maosong2022.github.io/p/notes-on-keye-vl/Keye-VL-performance-internal_hu3362434457835284134.png 1024w"
loading="lazy"
alt="Performance of Keye-VL on the internal benchmark"
class="gallery-image"
data-flex-grow="239"
data-flex-basis="574px"
>&lt;/p>
&lt;p>分析：&lt;/p>
&lt;ol>
&lt;li>Keye-VL 在 OCR 等任务上的表现有所不足，其细粒度的识别能力也有所不足，会认错人。有时候还会忽略掉一些信息&lt;/li>
&lt;li>描述 temporal action 时会出现不稳定性，模型对镜头的感知能力不足。需要准确定位时间等&lt;/li>
&lt;li>在需要逻辑链条和数学计算时，模型能力不足，对于特定 domain 上的任务会出现事实性错误。写作时，模型倾向于输出通用的回答，而不是定制化的回答。&lt;/li>
&lt;/ol>
&lt;h2 id="discussion">Discussion
&lt;/h2>&lt;p>作者讨论了两点关键发现：&lt;/p>
&lt;ol>
&lt;li>reasoning 和 non-reasoning 的数据可以互相促进彼此的表现，这与 ERNIE 4.5 的发现一致。&lt;/li>
&lt;li>作者认为通过 mix-mode 的训练，模型在简单和复杂任务上的表现都可以提升，因此作者使用了混合数据来进行训练，结果发现效果很好。&lt;/li>
&lt;/ol>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>本文中，作者提出了 Keye-VL 8B，一个短视频理解能力出色的多模态大模型，作者详细介绍了 pre-training 和 post-training. 其中，mix-mode training 可以有效提高模型的表现。&lt;/p>
&lt;p>作者认为 Keye-VL 有如下改进的地方：&lt;/p>
&lt;ol>
&lt;li>并没有优化 video encoder 或者是改进 video encoding 的策略&lt;/li>
&lt;li>Keye-VL 的视觉感知能力有进一步的提升空间，其 &amp;ldquo;reasoning with image&amp;rdquo; 能力依然落后于领先的 reasoning model&lt;/li>
&lt;li>使用一个额外的 MLLM 作为 reward model 会极大消耗算力，如何构建一个更可靠更高效的 reward model 需要进一步探索。&lt;/li>
&lt;/ol>
&lt;h2 id="reference">Reference
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2507.01949" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/Kwai-Keye/Keye/tree/main" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>LLM Parameter Computation</title><link>https://maosong2022.github.io/p/llm-parameter-computation/</link><pubDate>Tue, 22 Jul 2025 10:50:47 +0800</pubDate><guid>https://maosong2022.github.io/p/llm-parameter-computation/</guid><description>&lt;p>本文中，我们介绍一下如何计算 LLM 的参数量。我们将基于 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a> 模型架构出发，对模型架构进行拆解，然后给出 LLM 参数量计算公式。&lt;/p>
&lt;h2 id="dense-model">Dense Model
&lt;/h2>&lt;p>我们首先来看一下 Qwen3 的架构，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/llm-parameter-computation/transformer_architecture.png"
width="1210"
height="1364"
srcset="https://maosong2022.github.io/p/llm-parameter-computation/transformer_architecture_hu2202173807577829293.png 480w, https://maosong2022.github.io/p/llm-parameter-computation/transformer_architecture_hu9835691261630527796.png 1024w"
loading="lazy"
alt="Architecture of Qwen3"
class="gallery-image"
data-flex-grow="88"
data-flex-basis="212px"
>&lt;/p>
&lt;p>这里，&lt;code>Qwen3ForCausalLM&lt;/code> 就是我们的的 LLM, 其关键代码如下&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Qwen3ForCausalLM&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">model&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Qwen3Model&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">lm_head&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">vocab_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>我们假设 &lt;code>vocab_size&lt;/code>, 也就是词表大小为 $|V|$ (我们用 $V$ 表示词表), &lt;code>hidden_size&lt;/code> 为 $d$, 则总参数量为&lt;/p>
$$
\mathrm{parameter}(\texttt{Qwen3ForCausalLM}) =d|V| + \mathrm{parameter}(\texttt{Qwen3Model})
$$&lt;p>&lt;code>Qwen3Model&lt;/code> 包含三个（含参数的）模块，分别是 &lt;code>nn.Embedding&lt;/code>, &lt;code>Qwen3DecodeLayer&lt;/code> 以及 &lt;code>Qwen3RMSNorm&lt;/code>, 分别代表了输入 token 的 embedding layer, Transformer block 和对输出的 normalization. 其关键代码如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Qwen3Model&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Qwen3Config&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">embed_tokens&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Embedding&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">vocab_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">padding_idx&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">layers&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ModuleList&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">[&lt;/span>&lt;span class="n">Qwen3DecoderLayer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">layer_idx&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">layer_idx&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_hidden_layers&lt;/span>&lt;span class="p">)]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">norm&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Qwen3RMSNorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">eps&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">rms_norm_eps&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>其中，&lt;code>nn.Embedding&lt;/code> 参数量与 &lt;code>lm_head&lt;/code> 一样，都是 $d|V|$.&lt;/p>
&lt;p>对于 normalization, 现在大部分 LLM 用的都是 &lt;code>RMSNorm&lt;/code>, 其定义如下：&lt;/p>
$$
\mathrm{RMSNorm}(x) = \frac{x}{\sqrt{\|x\|_2^2+\epsilon}}\odot \gamma
$$&lt;p>其参数量为：$d$.&lt;/p>
&lt;p>如果说我们使用的是 &lt;code>LayerNorm&lt;/code>, 则其定义如下：&lt;/p>
$$
\mathrm{LayerNorm}(x) = \frac{x-\mathbb{E}[x]}{\sqrt{\mathrm{var}[x]+\epsilon}}\odot \beta + \gamma
$$&lt;p>其参数量为： $2d$.&lt;/p>
&lt;p>因此，&lt;code>Qwen3Model&lt;/code> 的参数量为&lt;/p>
$$
\mathrm{parameter}(\texttt{Qwen3Model})=d|V| + N*\mathrm{parameter}(\texttt{Qwen3DecoderLayer}) + d
$$&lt;p>这里第一项为 &lt;code>nn.Embedding&lt;/code>, 第三项为 &lt;code>Qwen3RMSNorm&lt;/code>， 第二项里，$N$ 代表 decode layer 的个数，也就是 &lt;code>config.num_hidden_layers&lt;/code>.&lt;/p>
&lt;p>&lt;code>Qwen3DecoderLayer&lt;/code> 包含了四个模块，其关键代码如下&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Qwen3DecoderLayer&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Qwen3Config&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">self_attn&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Qwen3Attention&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">layer_idx&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">layer_idx&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mlp&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Qwen3MLP&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">input_layernorm&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Qwen3RMSNorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">eps&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">rms_norm_eps&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">post_attention_layernorm&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Qwen3RMSNorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">eps&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">rms_norm_eps&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>因此，&lt;code>Qwen3DecoderLayer&lt;/code> 的参数量为&lt;/p>
$$
\mathrm{parameter}(\texttt{Qwen3DecoderLayer}) = 2d + \mathrm{parameter}(\texttt{Qwen3MLP}) + \mathrm{parameter}(\texttt{Qwen3Attention})
$$&lt;p>其中，第一项是两个 &lt;code>Qwen3RMSNorm&lt;/code> 的参数。&lt;/p>
&lt;p>对于 &lt;code>Qwen3MLP&lt;/code>, 其定义如下：&lt;/p>
$$
y = W_2(W_3x\odot \mathrm{SwiGLU}(W_1x))
$$&lt;p>这里 $W_3,W_1\in\mathbb{R}^{d_{ff}\times d}$, $W_2\in\mathbb{R}^{d\times d_{ff}}$, $d_{ff}$ 是 MLP 的 hidden size, 代码中用 &lt;code>intermediate_size&lt;/code> 来表示，因此&lt;/p>
$$
\mathrm{parameter}(\texttt{Qwen3MLP}) = dd_{ff} + dd_{ff} + d_{ff}d=3dd_{ff}
$$&lt;p>这里三项分别代表 $W_1,W_3,W_2$ 的参数量。&lt;/p>
&lt;p>如果说，我们使用原始 transformer 的 MLP, 也就是&lt;/p>
$$
y = W_2\max(0,W_1x+b_1)+b_2
$$&lt;p>其中 $W_1\in\mathbb{R}^{d_{ff}\times d}$, $W_2\in\mathbb{R}^{d\times d_{ff}}$, $b_1\in\mathbb{R}^{d_{ff}}$, $b_2\in\mathbb{R}^d$, 则总参数为&lt;/p>
$$
\mathrm{parameter}(\texttt{TransformerMLP}) = d_{ff}d + dd_{ff} + d_{ff} +d = 2d_{ff}d + d_{ff} + d
$$&lt;p>这里的四项分别代表了 $W_1,W_2,b_1,b_2$.&lt;/p>
&lt;h3 id="qwen3attention">Qwen3Attention
&lt;/h3>&lt;p>接下来，就是 Attention 部分的参数，&lt;code>Qwen3Attention&lt;/code> 的关键代码如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Qwen3Attention&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Qwen3Config&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">layer_idx&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_attention_heads&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">attention_bias&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">k_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_key_value_heads&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">attention_bias&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">v_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_key_value_heads&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">attention_bias&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">o_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_attention_heads&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">attention_bias&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_norm&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Qwen3RMSNorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">eps&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">rms_norm_eps&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">k_norm&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Qwen3RMSNorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">eps&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">rms_norm_eps&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>这里，我们先定义几个量：&lt;/p>
&lt;ul>
&lt;li>我们将 head 的个数记为 $h$, 即 &lt;code>num_attention_heads&lt;/code>&lt;/li>
&lt;li>我们将每个 head 的 hidden size 记为 $h_d$, 即 &lt;code>head_dim&lt;/code>&lt;/li>
&lt;li>我们将 key 和 value head 的个数记为 $h_{kv}$ , 即 &lt;code>num_key_value_heads&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>&lt;code>Qwen3Attention&lt;/code> 的参数由以下几个部分组成：&lt;/p>
&lt;ul>
&lt;li>Query projection: $W_{Q}\in\mathbb{R}^{hh_d\times d}$, $b_Q\in\mathbb{R}^{hh_d}$&lt;/li>
&lt;li>Key projection: $W_K\in\mathbb{R}^{h_{kv}h_d\times d}$, $b_K\in\mathbb{R}^{h_{kv}h_d}$&lt;/li>
&lt;li>Value projection: $W_V\in\mathbb{R}^{h_{kv}h_d\times d}$, $b_V\in\mathbb{R}^{h_{kv}h_d}$&lt;/li>
&lt;li>output projection: $W_O\in\mathbb{R}^{d\times hh_d}$&lt;/li>
&lt;li>RMSNorm：前文已经提到过，两个 normalization (query norm 以及 key norm) 的总参数量为 $2h_d$.&lt;/li>
&lt;/ul>
&lt;p>因此， &lt;code>Qwen3Attention&lt;/code> 部分的总参数量为&lt;/p>
$$
\mathrm{parameter}(\texttt{Qwen3Attention}) = hh_{d}d + 2h_{kv}h_dd +dhh_d + 2h_d = 2hh_dd + 2h_{kv}h_dd + 2h_d
$$&lt;p>分别代表 $W_Q, W_O, W_K,W_V$ 和 两个 normalization layer 的参数量。&lt;/p>
&lt;p>注意，这里我们没有加入 bias, 这是因为 QKV bias 在 Qwen3 中被取消，取而代之的是两个 normalization.&lt;/p>
&lt;p>如果我们查看 &lt;code>Qwen2Attention&lt;/code> 的代码，我们可以得到 &lt;code>Qwen2Attention&lt;/code> 的总参数量为&lt;/p>
$$
\mathrm{parameter}(\texttt{Qwen2Attention}) = 2hh_{d}d + 2h_{kv}h_dd +h_{kv}h_d+hh_d+h_{kv}h_d
$$&lt;p>分别代表 $W_Q, W_K, W_V,W_O$ 和 $b_Q,b_K, b_V$ 的参数量。&lt;/p>
&lt;p>我们将计算结果汇总在一起就得到：&lt;/p>
$$
\begin{aligned}
\mathrm{parameter}(\texttt{Qwen3ForCausalLM}) &amp;=d|V| + \mathrm{parameter}(\texttt{Qwen3Model})\\
&amp;=2d|V| + N*\mathrm{parameter}(\texttt{Qwen3DecoderLayer}) + d\\
&amp;= N*(2d + \mathrm{parameter}(\texttt{Qwen3MLP}) + \mathrm{parameter}(\texttt{Qwen3Attention})) + d(2|V|+1)\\
&amp;= N*(2d+3dd_{ff}+2hh_{d}d + 2h_{kv}h_dd + 2h_d) + d(2|V|+1)\\
\end{aligned}
$$&lt;p>这是针对 &lt;code>Qwen3ForCausalLM&lt;/code> 的参数量计算。这里的变量定义如下：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Math Variable&lt;/th>
&lt;th>Code Variable&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$N$&lt;/td>
&lt;td>&lt;code>num_hidden_layers&lt;/code>&lt;/td>
&lt;td>Transformer block 个数&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\vert V\vert$&lt;/td>
&lt;td>&lt;code>vocab_size&lt;/code>&lt;/td>
&lt;td>词表大小&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$d$&lt;/td>
&lt;td>&lt;code>hidden_size&lt;/code>&lt;/td>
&lt;td>token embedding 的维度&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$d_{ff}$&lt;/td>
&lt;td>&lt;code>intermediate_size&lt;/code>&lt;/td>
&lt;td>MLP 的中间层的维度&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$h_d$&lt;/td>
&lt;td>&lt;code>head_dim&lt;/code>&lt;/td>
&lt;td>每个 head 的维度&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$h$&lt;/td>
&lt;td>&lt;code>num_attention_heads&lt;/code>&lt;/td>
&lt;td>query head 的个数&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$h_{kv}$&lt;/td>
&lt;td>&lt;code>num_key_value_heads&lt;/code>&lt;/td>
&lt;td>key 和 value head 的个数&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="verification">Verification
&lt;/h3>&lt;p>接下来，我们就可以基于 Qwen3 的模型来验证了，比如，&lt;code>Qwen3-32B&lt;/code> 的配置如下:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Value&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$N$&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\vert V\vert$&lt;/td>
&lt;td>151936&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$d$&lt;/td>
&lt;td>5120&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$d_{ff}$&lt;/td>
&lt;td>25600&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$h_d$&lt;/td>
&lt;td>128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$h$&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$h_{kv}$&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>根据上式，最终的参数量为&lt;/p>
$$
\mathrm{parameter}(\texttt{Qwen3-32B}) = 32.762123264*10^9\approx 32.8B
$$&lt;p>我们使用 &lt;code>Qwen3-32B&lt;/code> 的 &lt;code>index.json&lt;/code> 可以得到其真实的参数量为&lt;/p>
$$
\texttt{total\_size}/\texttt{precision} = 65524246528/2 = 32762123264
$$&lt;p>与我们计算的结果一致（这里除以 2 的原因是其表示模型权重文件的总大小，以 bytes 为单位，一般模型都是 &lt;code>bfloat16&lt;/code>, 大小为 2 个 bytes, 因此总参数量为总大小除以权重的精度）&lt;/p>
&lt;h2 id="moe-model">MoE Model
&lt;/h2>&lt;p>MoE model 与 Dense model 不同的地方在于每一层的 FFN, 因此，其总参数计算方式为：&lt;/p>
$$
\mathrm{parameter}(\texttt{Qwen3MoeForCausalLM})=N*(2d+\mathrm{parameter}(\texttt{Qwen3MoE})+hh_{d}d + 2h_{kv}h_dd +dhh_d + 2d) + d(2|V|+1)
$$&lt;p>对于 MoE layer, 其关键代码为：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Qwen3MoeSparseMoeBlock&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_experts&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_experts&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">top_k&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_experts_per_tok&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">gate&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_experts&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">experts&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ModuleList&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">Qwen3MoeMLP&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">intermediate_size&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">moe_intermediate_size&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_experts&lt;/span>&lt;span class="p">)])&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>我们记 $n$ 为总专家个数，即 &lt;code>num_experts&lt;/code>， 记 $k$ 为激活专家个数，即 &lt;code>num_experts_per_tok&lt;/code> 或者 &lt;code>top_k&lt;/code>,&lt;/p>
&lt;p>首先 &lt;code>gate&lt;/code> 的参数量为 $dn$, 接下来每个 expert 都是一个 &lt;code>Qwen3MLP&lt;/code>, 因此 &lt;code>experts&lt;/code> 总参数量为 $n * 3dd_{ff}$. 这样 MoE layer 的总参数量为&lt;/p>
$$
\mathrm{parameter}(\texttt{Qwen3MoE}) = nd + 3ndd_{ff}
$$&lt;p>在推理时，只有一部分专家，也就是 $k$ 个专家会参与计算，此时激活参数量为&lt;/p>
$$
\mathrm{parameter\_activated}(\texttt{Qwen3MoE}) = nd + 3kdd_{ff}
$$&lt;p>我们带入到 &lt;code>Qwen3MoeForCausalLM&lt;/code> 中就得到：&lt;/p>
&lt;p>模型总参数量为：&lt;/p>
$$
\mathrm{parameter}(\texttt{Qwen3MoeForCausalLM})=N*(2d+nd + 3ndd_{ff}+hh_{d}d + 2h_{kv}h_dd +dhh_d + 2h_d) + d(2|V|+1)
$$&lt;p>模型激活参数量为：&lt;/p>
$$
\mathrm{parameter\_activated}(\texttt{Qwen3MoeForCausalLM})=N*(2d+nd + 3kdd_{ff} + 2h_{kv}h_dd +dhh_d + 2h_d) + d(2|V|+1)
$$&lt;p>这里的变量定义如下：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Math Variable&lt;/th>
&lt;th>Code Variable&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$N$&lt;/td>
&lt;td>&lt;code>num_hidden_layers&lt;/code>&lt;/td>
&lt;td>Transformer block 个数&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\vert V\vert$&lt;/td>
&lt;td>&lt;code>vocab_size&lt;/code>&lt;/td>
&lt;td>词表大小&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$d$&lt;/td>
&lt;td>&lt;code>hidden_size&lt;/code>&lt;/td>
&lt;td>token embedding 的维度&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$d_{ff}$&lt;/td>
&lt;td>&lt;code>intermediate_size&lt;/code>&lt;/td>
&lt;td>MLP 的中间层的维度&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$h_d$&lt;/td>
&lt;td>&lt;code>head_dim&lt;/code>&lt;/td>
&lt;td>每个 head 的维度&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$h$&lt;/td>
&lt;td>&lt;code>num_attention_heads&lt;/code>&lt;/td>
&lt;td>query head 的个数&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$h_{kv}$&lt;/td>
&lt;td>&lt;code>num_key_value_heads&lt;/code>&lt;/td>
&lt;td>key 和 value head 的个数&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$n$&lt;/td>
&lt;td>&lt;code>num_experts&lt;/code>&lt;/td>
&lt;td>总专家个数&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$k$&lt;/td>
&lt;td>&lt;code>top_k&lt;/code> (&lt;code>num_experts_per_tok&lt;/code>)&lt;/td>
&lt;td>激活专家个数&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="moe-verification">MoE Verification
&lt;/h3>&lt;p>我们用 &lt;code>Qwen3-235B-A22B&lt;/code> 来验证，其配置如下：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Value&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$N$&lt;/td>
&lt;td>94&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\vert V\vert$&lt;/td>
&lt;td>151936&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$d$&lt;/td>
&lt;td>4096&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$d_{ff}$&lt;/td>
&lt;td>1536&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$h_d$&lt;/td>
&lt;td>128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$h$&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$h_{kv}$&lt;/td>
&lt;td>4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$n$&lt;/td>
&lt;td>128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$k$&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>计算得到总参数量为&lt;/p>
$$
\mathrm{parameter}(\texttt{Qwen3-235B-A22B}) = 235.09363456*10^9\approx 235B
$$&lt;p>计算得到激活参数量为&lt;/p>
$$
\mathrm{parameter\_activated}(\texttt{Qwen3-235B-A22B}) = 22.14456064 * 10^9
$$&lt;p>实际总参数量为&lt;/p>
$$
470187269120/2 = 235093634560.0\approx 235B
$$&lt;p>可以看到，计算结果与实际相符。&lt;/p>
&lt;h2 id="extension">Extension
&lt;/h2>&lt;p>通过以上计算过程，我们可以很轻松将上述公式扩展到混合架构或者是 MLA 上，对于混合架构，我们分别计算不同 attention 的 layer 个数，然后分别计算。对于 MLA, 我们可以替换 Attention 的计算逻辑。&lt;/p>
&lt;p>对于小语言模型系列，比如 0.6B 和 1.7B, 模型的大部分参数集中在 embedding 上，因此 Qwen3 采取了 tie embedding 的方式来减少参数量，具体做法就是 &lt;code>nn.Embedding&lt;/code> 和 &lt;code>lm_head&lt;/code> 共享参数。&lt;/p>
&lt;h2 id="visualization">Visualization
&lt;/h2>&lt;p>接下来，我们来可视化一下不同大小模型不同模块的参数量占比。计算的代码如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">compute_param_distribution&lt;/span>&lt;span class="p">(&lt;/span> &lt;span class="n">N&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">V&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d_ff&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">h_d&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">h&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">h_kv&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">tie_word_embeddings&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">bool&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">dict&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> Compute parameter distribution across different components of the model.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> Args:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> N: Number of layers
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> V: Vocabulary size
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> d: Hidden dimension
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> d_ff: Feed-forward dimension
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> h_d: Head dimension
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> h: Number of attention heads
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> h_kv: Number of key/value heads
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> tie_word_embeddings: Whether input and output embeddings are tied
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> Returns:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> dict: Parameter counts for each component
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Embedding parameters&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">embedding&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">V&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="n">tie_word_embeddings&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">embedding&lt;/span> &lt;span class="o">*=&lt;/span> &lt;span class="mi">2&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Attention parameters per layer&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attention_per_layer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">h&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">h_d&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">h_kv&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">h_d&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">h&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">h_d&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">h_d&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attention_total&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">attention_per_layer&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">N&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># FFN parameters per layer&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ffn_per_layer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">3&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d_ff&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ffn_total&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ffn_per_layer&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">N&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># RMSNorm (2d), QK-Norm (2d), output normalization (d)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">others&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">N&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">4&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">d&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;Embedding&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">embedding&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;Attention&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">attention_total&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;FFN&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">ffn_total&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;Others&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">others&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>对于 dense 模型，每部分的参数量可视化如下：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/llm-parameter-computation/Qwen3_param_distribution.png"
width="1200"
height="600"
srcset="https://maosong2022.github.io/p/llm-parameter-computation/Qwen3_param_distribution_hu16139450686441288163.png 480w, https://maosong2022.github.io/p/llm-parameter-computation/Qwen3_param_distribution_hu3481494963200819479.png 1024w"
loading="lazy"
alt="Parameter distribution across model components"
class="gallery-image"
data-flex-grow="200"
data-flex-basis="480px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/llm-parameter-computation/Qwen3_param_percentage.png"
width="1200"
height="600"
srcset="https://maosong2022.github.io/p/llm-parameter-computation/Qwen3_param_percentage_hu5308572400207334525.png 480w, https://maosong2022.github.io/p/llm-parameter-computation/Qwen3_param_percentage_hu465813338534858304.png 1024w"
loading="lazy"
alt="Parameter percentage across model components"
class="gallery-image"
data-flex-grow="200"
data-flex-basis="480px"
>&lt;/p>
&lt;p>可以看到，随着模型 size 增加，模型大部分参数量都集中在 FFN 上。&lt;/p>
&lt;p>接下来，我们可视化一下 MoE 模型的参数分布，核心计算代码如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">compute_moe_param_distribution&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">N&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">V&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d_ff&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">h_d&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">h&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">h_kv&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">activate&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">dict&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> Compute parameter distribution across different components of the MoE model.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> Args:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> N: Number of layers
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> V: Vocabulary size
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> d: Hidden dimension
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> d_ff: Feed-forward dimension
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> h_d: Head dimension
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> h: Number of attention heads
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> h_kv: Number of key/value heads
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> n: Number of experts
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> k: Number of experts per token
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> activate: Whether to compute activated parameters
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> Returns:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> dict: Parameter counts for each component
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Embedding parameters&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">embedding&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">V&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Attention parameters per layer&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attention_per_layer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">h&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">h_d&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">h_kv&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">h_d&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">h&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">h_d&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">h_d&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attention_total&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">attention_per_layer&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">N&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># FFN parameters per layer&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">activate&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">moe_per_layer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">3&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">k&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d_ff&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">moe_per_layer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">3&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d_ff&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ffn_total&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">moe_per_layer&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">N&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># RMSNorm (2d), QK-Norm (2d), output normalization (d)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">others&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">N&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">h_d&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">d&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;Embedding&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">embedding&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;Attention&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">attention_total&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;MoE&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">ffn_total&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;Others&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">others&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/llm-parameter-computation/Qwen3_moe_param_distribution.png"
width="1189"
height="589"
srcset="https://maosong2022.github.io/p/llm-parameter-computation/Qwen3_moe_param_distribution_hu1131540867004967356.png 480w, https://maosong2022.github.io/p/llm-parameter-computation/Qwen3_moe_param_distribution_hu5782957262142391997.png 1024w"
loading="lazy"
alt="Parameter distribution across MoE model components"
class="gallery-image"
data-flex-grow="201"
data-flex-basis="484px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/llm-parameter-computation/Qwen3_moe_param_percentage.png"
width="1188"
height="589"
srcset="https://maosong2022.github.io/p/llm-parameter-computation/Qwen3_moe_param_percentage_hu14879019433354223254.png 480w, https://maosong2022.github.io/p/llm-parameter-computation/Qwen3_moe_param_percentage_hu12678685186241888711.png 1024w"
loading="lazy"
alt="Parameter percentage across MoE model components"
class="gallery-image"
data-flex-grow="201"
data-flex-basis="484px"
>&lt;/p>
&lt;p>可以看到，MoE 模型的大部分参数还是集中在 MoE 模块上，但是由于其稀疏机制，在激活的参数里，MoE 占比从 95% 以上降低到了 60% 左右。&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>在本文中，我们基于 Qwen3 大语言模型系列，介绍了如何计算 dense 模型和 MoE 模型的参数量。模型的参数量计算为后面的显存占用以及优化提供了基础。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f" target="_blank" rel="noopener"
>Qwen3 Collection&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Seed1.6</title><link>https://maosong2022.github.io/p/notes-on-seed1.6/</link><pubDate>Fri, 18 Jul 2025 14:59:35 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-seed1.6/</guid><description>&lt;p>Seed 在 2025 年 6 月 25 号发布了 Seed 1.6，支持 adaptive deep thinking, multimodal understanding, GUI-based interaction, and deep reasoning with a 256K context window&lt;/p>
&lt;p>Seed 1.6 基于 Seed 1.5 开发，是一个 MoE 模型, 总参数为 230B, 激活参数为 23B.&lt;/p>
&lt;h2 id="pre-training">Pre-training
&lt;/h2>&lt;p>pre-training 分为 3 个 stage:&lt;/p>
&lt;ol>
&lt;li>Text-only pre-training: 使用 web pages, books, papers, code 以及其他数据来进行训练，作者使用了 rule-based 以及 model-based 策略来提高数据的质量。&lt;/li>
&lt;li>Multimodal mixed continue training: 提高学科，代码以及 reasoning 相关的数据，来进一步提高知识密度以及 reasoning 的复杂度。同时，加入高质量的视觉数据。&lt;/li>
&lt;li>Long-context continual training: 将模型的上下文长度从 32K 扩展到 256K&lt;/li>
&lt;/ol>
&lt;p>Seed 1.6 Base 的表现&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Seed1.6 Base&lt;/th>
&lt;th>LLaMA-4 -Maverick Base&lt;/th>
&lt;th>DeepSeek-V3Base&lt;/th>
&lt;th>Qwen3-235B-A22B Base&lt;/th>
&lt;th>Seed1.5 Base&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>MMLU&lt;/td>
&lt;td>&lt;strong>88.83&lt;/strong>&lt;/td>
&lt;td>85.16&lt;/td>
&lt;td>87.19&lt;/td>
&lt;td>87.81&lt;/td>
&lt;td>88.35&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MMLU-Pro&lt;/td>
&lt;td>&lt;strong>69.98&lt;/strong>&lt;/td>
&lt;td>63.91&lt;/td>
&lt;td>59.84&lt;/td>
&lt;td>68.18&lt;/td>
&lt;td>66.47&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SuperGPQA&lt;/td>
&lt;td>&lt;strong>45.08&lt;/strong>&lt;/td>
&lt;td>40.85&lt;/td>
&lt;td>41.53&lt;/td>
&lt;td>44.06&lt;/td>
&lt;td>36.81&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>BBH&lt;/td>
&lt;td>&lt;strong>92.08&lt;/strong>&lt;/td>
&lt;td>83.62&lt;/td>
&lt;td>86.22&lt;/td>
&lt;td>88.87&lt;/td>
&lt;td>88.36&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GPQA-Diamond&lt;/td>
&lt;td>43.43&lt;/td>
&lt;td>43.94&lt;/td>
&lt;td>41.92&lt;/td>
&lt;td>&lt;strong>47.47&lt;/strong>&lt;/td>
&lt;td>45.25&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GSM8k&lt;/td>
&lt;td>93.10&lt;/td>
&lt;td>87.72&lt;/td>
&lt;td>87.57&lt;/td>
&lt;td>&lt;strong>94.39&lt;/strong>&lt;/td>
&lt;td>89.99&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MATH&lt;/td>
&lt;td>&lt;strong>72.86&lt;/strong>&lt;/td>
&lt;td>63.32&lt;/td>
&lt;td>62.62&lt;/td>
&lt;td>71.84&lt;/td>
&lt;td>66.18&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MBPP&lt;/td>
&lt;td>&lt;strong>83.60&lt;/strong>&lt;/td>
&lt;td>75.40&lt;/td>
&lt;td>74.20&lt;/td>
&lt;td>81.40&lt;/td>
&lt;td>81.40&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="post-training">Post-training
&lt;/h2>&lt;p>作者在 post-training 构建了两个变体：&lt;/p>
&lt;ol>
&lt;li>Seed1.6-Thinking: 先进的多模态 reasoning 能力&lt;/li>
&lt;li>Seed1.5(Adaptive CoT): 使用 AdaCoT 来压缩 CoT 的长度，提高效率&lt;/li>
&lt;/ol>
&lt;h3 id="seed16-thinking">Seed1.6-Thinking
&lt;/h3>&lt;p>Seed1.6-Thinking 与 Seed-Thinking-v1.5 的训练方式差不多，作者使用了 multi-stage [[rejection fine-tuning]] (RFT) 和 RL 来提高模型的表现。在每一轮中，先进行 RL 的训练，然后进行 RFT 的训练。相比于 Seed-Thinking-v1.5, Seed1.6-Thinking 使用了更多的算力和更高质量的数据，包括数学问题，代码，谜题以及 non-reasoning 数据。&lt;/p>
&lt;p>表现如下：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-seed1.6/Seed1_6_thinking_LLM_performance.png"
width="5436"
height="2498"
srcset="https://maosong2022.github.io/p/notes-on-seed1.6/Seed1_6_thinking_LLM_performance_hu6736598818851231143.png 480w, https://maosong2022.github.io/p/notes-on-seed1.6/Seed1_6_thinking_LLM_performance_hu16256607489974395991.png 1024w"
loading="lazy"
alt="Seed1.6-Thinking performance (LLM)"
class="gallery-image"
data-flex-grow="217"
data-flex-basis="522px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-seed1.6/Seed1_6_thinking_VLM_performance.png"
width="5255"
height="2514"
srcset="https://maosong2022.github.io/p/notes-on-seed1.6/Seed1_6_thinking_VLM_performance_hu6201545990707352627.png 480w, https://maosong2022.github.io/p/notes-on-seed1.6/Seed1_6_thinking_VLM_performance_hu11298797869590931751.png 1024w"
loading="lazy"
alt="Seed1.6-Thinking performance (VLM)"
class="gallery-image"
data-flex-grow="209"
data-flex-basis="501px"
>&lt;/p>
&lt;p>为了进一步提高模型的表现，作者在推理时使用了 parallel decoding, 来让模型在回答之前使用更多的 token, 作者发现 parallel decoding 可以显著提高模型在困难任务上的表现。&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-seed1.6/Seed1_6_thinking_parallel_decoding_performance.png"
width="4468"
height="1468"
srcset="https://maosong2022.github.io/p/notes-on-seed1.6/Seed1_6_thinking_parallel_decoding_performance_hu14877516256314024226.png 480w, https://maosong2022.github.io/p/notes-on-seed1.6/Seed1_6_thinking_parallel_decoding_performance_hu9088461374692967399.png 1024w"
loading="lazy"
alt="Performance of Seed1.6 Thinking parallel decoding"
class="gallery-image"
data-flex-grow="304"
data-flex-basis="730px"
>&lt;/p>
&lt;h3 id="seed15adaptive-cot">Seed1.5(Adaptive CoT)
&lt;/h3>&lt;p>reasoning model 的问题是会出现 overthinking,导致输出的过程中花费很多不必要的 token.&lt;/p>
&lt;p>为了解决这个问题，Seed1.6 提出了 AdaCoT, 来训练得到三个模型：FullCoT, NoCoT 和 AdaCoT&lt;/p>
&lt;p>通过 AdaCoT, 我们可以显著降低 CoT 的长度。为了完成这个目标，作者在 RL 训练阶段构建了一个 reward, 来惩罚模型的 overthinking. 用户可以基于 prompt 来选择不同的模型：&lt;/p>
&lt;ul>
&lt;li>FullCoT: 对每个 prompt，都进行 reasoning, 模型表现与 Seed1.6-Thinking 差不多，但是 CoT length 更小&lt;/li>
&lt;li>NoCoT: 直接回答，不进行 reasoning&lt;/li>
&lt;li>AdaCoT: 由模型自主决定是否需要进行 reasoning, 模型是 FullCoT 和 NoCoT 的一个 fusion 版本&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-seed1.6/Seed1_6_AdaCoT_performance.png"
width="6798"
height="3416"
srcset="https://maosong2022.github.io/p/notes-on-seed1.6/Seed1_6_AdaCoT_performance_hu4571070263511632519.png 480w, https://maosong2022.github.io/p/notes-on-seed1.6/Seed1_6_AdaCoT_performance_hu17493154294080386988.png 1024w"
loading="lazy"
alt="Seed1.6-AdaCoT performance"
class="gallery-image"
data-flex-grow="199"
data-flex-basis="477px"
>&lt;/p>
&lt;p>结果发现，对于不同难度的任务，AdaCoT 的出发概率是不同的，说明模型会基于任务难度调整思考方式。&lt;/p>
&lt;p>作者还评测了以下模型在高考中的表现，结果如下图：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-seed1.6/Seed1_6_gaokao_performance.png"
width="5195"
height="4645"
srcset="https://maosong2022.github.io/p/notes-on-seed1.6/Seed1_6_gaokao_performance_hu13567853550537972132.png 480w, https://maosong2022.github.io/p/notes-on-seed1.6/Seed1_6_gaokao_performance_hu5316658797452294396.png 1024w"
loading="lazy"
alt="Seed1.6 performance on gaokao"
class="gallery-image"
data-flex-grow="111"
data-flex-basis="268px"
>&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者提出了 Seed1.6 系列多模态大模型，模型在多个 benchmark 上表现优异。作者还提出了 AdaCoT, 让模型基于问题难度动态选择 CoT length&lt;/p>
&lt;p>作者认为下一步工作是：&lt;/p>
&lt;ol>
&lt;li>构建更加高效的架构，来进一步提高 reasoning 的有效性&lt;/li>
&lt;li>扩展模型的多模态能力&lt;/li>
&lt;li>提升模型的 agent 能力，特鄙视 end-to-end task execution&lt;/li>
&lt;/ol>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://seed.bytedance.com/en/blog/introduction-to-techniques-used-in-seed1-6" target="_blank" rel="noopener"
>blog&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on V-Triune</title><link>https://maosong2022.github.io/p/notes-on-v-triune/</link><pubDate>Thu, 17 Jul 2025 09:37:36 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-v-triune/</guid><description>&lt;p>V-Triune 探究了如何使用一套统一的 RL 训练框架，来同时提高模型的 perception 和 reasoning 能力。&lt;/p>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>已有的 RL 训练方法主要提升模型在特定 domain 上的能力，比如 MM-Eureka, Vision-R1 等专注于提升 MLLM 的 reasoning 能力，而 R1-V, DeepPerception 等专注于提升模型的 perception 能力。&lt;/p>
&lt;p>因此，本文的 motivation 就是，如何用一套统一的 RL 训练框架，来同时提高模型的 perception 和 reasoning 能力。&lt;/p>
&lt;p>V-Triune 包含 3 个关键模块：&lt;/p>
&lt;ul>
&lt;li>Sample-Level data formatting: 即在 sample 的层面定义 reward 等信息&lt;/li>
&lt;li>Verifier-Level Computation: 即分离 verifier 和 generator, 提高整体效率&lt;/li>
&lt;li>Source-Level Metric Monitoring: 基于 data source 来监控模型的表现&lt;/li>
&lt;/ul>
&lt;p>作者还提出了 Dynamic IoU reward, 来提高训练的稳定性。&lt;/p>
&lt;p>基于 V-Triune 和 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5-vl/" target="_blank" rel="noopener"
>Qwen2.5-VL&lt;/a>, 作者提出了 Orsta,一个在 reasoning 和 perception 任务上均有提升的多模态大模型。&lt;/p>
&lt;p>论文的主要贡献为：&lt;/p>
&lt;ol>
&lt;li>提出了 V-Triune, 一个统一的 RL 训练框架，来同时提高模型的 reasoning 和 perception 能力&lt;/li>
&lt;li>在 infra 上进行了改进，提高整体的训练效率和稳定性&lt;/li>
&lt;li>基于 V-Triune, 构建了 Orsta, 相比于 Baseline, 模型的 perception 和 reasoning 能力均有了提升。&lt;/li>
&lt;/ol>
&lt;h2 id="preliminary">Preliminary
&lt;/h2>&lt;p>作者首先回顾了一下 GRPO 和 &lt;a class="link" href="https://maosong.website/p/notes-on-dapo/" target="_blank" rel="noopener"
>DAPO&lt;/a> , 我们这里就不再赘述。&lt;/p>
&lt;p>然后，作者介绍了一下 reward function, reward function 由两部分组成， 第一部分是 format reward, 第二部分是 accuracy reward,.&lt;/p>
&lt;p>对于 format reward, 作者采取了和 OpenR1 相同的做法，也就是要求输出仅包含 1 个给定的 token, 这里特定的 token 为 $s_1=\texttt{&lt;think>}$, $s_2=\texttt{&lt;/think>}$, $s_3=\texttt{&lt;answer>}$, $s_4=\texttt{&lt;/answer>}$, reward 的定义如下：&lt;/p>
$$
R_{\mathrm{format}}(o_q) = 0.25\sum_{i=1}^4 \mathbb{1}(\mathrm{count}(o_q,s_i)=1)
$$&lt;p>这里 $\mathbb{1}(\cdot)$ 是示性函数。&lt;/p>
&lt;p>对于 accuracy reward, 作者根据不同的任务分别进行处理。&lt;/p>
&lt;p>对于 reasoning 任务，reward function 的定义如下&lt;/p>
$$
R_{\mathrm{acc}}(\hat{a},a) = \begin{cases}
1, &amp;\text{ if }\texttt{verify(parse($\hat{a}$), parse($a$))}\text{ is True}\\
0, &amp;\text{ else}
\end{cases}
$$&lt;p>这里 $\hat{a}$, $a$ 分别是 prediction 和 ground truth.&lt;/p>
&lt;p>对于 perception 任务，reward function 与我们要处理的子任务相关。如果是 OCR 和 counting 任务，则我们采取和 reasoning 一样的 reward function. 如果我们要处理的是 grounding 和 detection 任务，则我们可以使用 IoU 以及 mAP 两种 reward 形式。&lt;/p>
&lt;p>IoU reward 的定义如下：&lt;/p>
$$
R_{\mathrm{acc}}(\hat{a},a) = \begin{cases}
\mathrm{IoU}(\hat{a}, a), &amp;\text{ if }\mathrm{IoU}(\hat{a},a)\geq \epsilon\\
0, &amp;\text{ else}
\end{cases}
$$&lt;p>这里 $\epsilon&amp;gt;0$ 为超参数， IoU 定义如下&lt;/p>
$$
\mathrm{IoU}(\hat{a},a) = \frac{\mathrm{Area}(\hat{a}\cap a)}{\mathrm{Area}(\hat{a}\cup a)}
$$&lt;p>mAP 的定义如下&lt;/p>
$$
\mathrm{AP}_c = \int_0^1 \max_{\tilde{r}\geq r}P_c(\tilde{r}) dr,\quad mAP=\frac1C\sum_{c=1}^C \mathrm{AP}_c
$$&lt;p>最终的 reward 是 format reward 和 accuracy reward 的加权求和：&lt;/p>
$$
R = \alpha_{\mathrm{acc}}R_{\mathrm{acc}} + \alpha_{\mathrm{format}}R_{\mathrm{format}}
$$&lt;h2 id="v-triune">V-Triune
&lt;/h2>&lt;p>V-Triune 框架如下图所示，其包含三个模块&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-v-triune/V-Triune_framework.png"
width="1351"
height="562"
srcset="https://maosong2022.github.io/p/notes-on-v-triune/V-Triune_framework_hu10953460521504204366.png 480w, https://maosong2022.github.io/p/notes-on-v-triune/V-Triune_framework_hu16889825761822693623.png 1024w"
loading="lazy"
alt="V-Triune System"
class="gallery-image"
data-flex-grow="240"
data-flex-basis="576px"
>&lt;/p>
&lt;h3 id="sample-level-data-formatting">Sample-Level Data Formatting
&lt;/h3>&lt;p>核心思想就是不同的任务的 reward function 可能是不一样的，如果在 task 层面设计 reward function 的话，RL 训练就没有了 flexibility, 因此，在本文中，作者在 sample 层面设计 reward function. 具体做法就是，每个样本除了 QA 相关信息，还有 reward model 的相关参数，这样就可以更加灵活地调整不同 sample 的损失了。&lt;/p>
&lt;h3 id="verifier-level-reward-computation">Verifier-Level Reward Computation
&lt;/h3>&lt;p>核心思想就是构建多个不同的 Verifier, 来分别处理不同的 sample, 因为我们 reward model 也是在 sample 层面构建的，因此我们可以基于 sample 的 metadata 来动态分配对应的 verifier, 这样就提升了 veriier 的可扩展性。本文作者使用了 MathVerifier 和 DetectionVerifier 两种。&lt;/p>
&lt;h3 id="source-level-metric-monitoring">Source-Level Metric Monitoring
&lt;/h3>&lt;p>核心思想就是根据 sample 的 metadata 信息来监控不同的 metric, 这样可以防止训练不稳定。&lt;/p>
&lt;h3 id="dynamic-iou-reward">Dynamic IoU Reward
&lt;/h3>&lt;p>核心思想就是根据训练进程，动态调整 IoU 的阈值。&lt;/p>
&lt;p>作者发现，如果将 IoU 阈值设置的比较低的话，模型很容易拿到比较高的 reward; 如果将 IoU 阈值设置的比较高的话，模型又很难拿到奖励。&lt;/p>
&lt;p>因此，作者的解决方案就是，使用课程学习，随着训练的进行，逐步提高 IoU 的阈值。&lt;/p>
&lt;h2 id="data">Data
&lt;/h2>&lt;p>数据集列举如下：&lt;/p>
&lt;ul>
&lt;li>Math: mm_math, geometry3K, mmk12&lt;/li>
&lt;li>Puzzle: PuzzleVQA, AlgoPuzzleQA, VisualPuzzles&lt;/li>
&lt;li>Science: ScienceQA, SciVQA, ViRL39K (Broader STEM topics, (GradeSchool) Science)&lt;/li>
&lt;li>Chart: ChartQAPro, ChartX, Table-VQA, ViRL39K (Tables/Diagrams/Charts)&lt;/li>
&lt;li>Detection: V3Det, Object365&lt;/li>
&lt;li>Grounding: D3&lt;/li>
&lt;li>Counting: CLEVR&lt;/li>
&lt;li>OCR: LLaVA-OV (OCR questions), EST-VQA&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Rule-based filtering&lt;/strong>
对于 visual reasoning 数据，作者的过滤如下：&lt;/p>
&lt;ul>
&lt;li>多项选择题以及判断题，防止 reward hacking&lt;/li>
&lt;li>答案包含特殊符号如 &amp;ldquo;=&amp;rdquo;, &amp;ldquo;[&amp;rdquo; 等的数据&lt;/li>
&lt;li>答案字符数超过 20 个，防止答案太复杂&lt;/li>
&lt;/ul>
&lt;p>对于 visual perception 数据，作者过滤流程如下：&lt;/p>
&lt;ul>
&lt;li>Detection: 超过 10 个 bounding box 的，或者 box 占 50% 以上面积的数据，保持 single BB 和 multi BB 的比例为 1:2&lt;/li>
&lt;li>Grounding: box 占 50% 以上面积的数据, label 过于复杂的数据&lt;/li>
&lt;li>Counting: 保持类别平衡，仅使用英文数据&lt;/li>
&lt;li>OCR: 保留英文数据，对 labels 进行 verify&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Difficulty-based filtering&lt;/strong>
主要是移除比较简单的问题。&lt;/p>
&lt;p>经过这两个阶段的过滤之后，得到了 &lt;strong>20.6K&lt;/strong> perception samples 以及 &lt;strong>27.1K&lt;/strong> reasoning samples. 数据保存在 Parquet 格式&lt;/p>
&lt;h2 id="training-recipe">Training Recipe
&lt;/h2>&lt;h3 id="disable-vit-training">Disable ViT Training
&lt;/h3>&lt;p>作者首先发现，全量微调会导致训练崩溃。作者分析原因发现，主要是 ViT 在反向传播过程中，存在 gradient amplify 的问题，第一层与最后一层的梯度的 norm 相差 5-10 倍。作者认为有两点原因：&lt;/p>
&lt;ol>
&lt;li>RL 会强制要求模型的不同模态之间进行对齐&lt;/li>
&lt;li>对比学习训练得到的 VIT 可能使得其不适合 RL 的训练。&lt;/li>
&lt;/ol>
&lt;p>基于这两点考虑，作者再进行训练的时候，冻结了 ViT 模块，结果如下图&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-v-triune/V-Triune-frozen-ViT.png"
width="1383"
height="705"
srcset="https://maosong2022.github.io/p/notes-on-v-triune/V-Triune-frozen-ViT_hu7988052896170322526.png 480w, https://maosong2022.github.io/p/notes-on-v-triune/V-Triune-frozen-ViT_hu8856288206821860516.png 1024w"
loading="lazy"
alt="Analysis of ViT training instability"
class="gallery-image"
data-flex-grow="196"
data-flex-basis="470px"
>&lt;/p>
&lt;h3 id="spurious-image-special-tokens">Spurious Image Special Tokens
&lt;/h3>&lt;p>作者发现，模型的输出可能会包含一些 &lt;code>&amp;lt;image_pad&amp;gt;&lt;/code> 等 token, 因此，作者对输出进行了过滤，然后再进行计算。&lt;/p>
&lt;h3 id="cot-prompt-pool">CoT Prompt Pool
&lt;/h3>&lt;p>作者还发现，多样化的 CoT prompt 会损害模型的表现。因此，作者构建了 20 个 prompt 模版，每次随机挑选其中一个加入到 instruction 中。&lt;/p>
&lt;h3 id="training-configuration">Training Configuration
&lt;/h3>&lt;p>模型基于 Qwen2.5-7B-instruct 和 Qwen2.5VL-32B 来进行开发&lt;/p>
&lt;p>作者探究了 on-policy 和 off-policy 两种配置。roll-out batch size 设置为 1024, 使用 GRPO 进行训练，GRPO 的 group size 设置为 8&lt;/p>
&lt;p>作者还设置 $\epsilon_{high}=0.28$ 以及 $\epsilon_{low}=0.2$ 来提高 token 的采样率。作者去除了 KL divergence loss, 并且在 token 层面上进行平均&lt;/p>
&lt;h2 id="evaluation">Evaluation
&lt;/h2>&lt;h3 id="performance">Performance
&lt;/h3>&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-v-triune/V-Triune-performance.png"
width="1159"
height="1051"
srcset="https://maosong2022.github.io/p/notes-on-v-triune/V-Triune-performance_hu6593213700954064222.png 480w, https://maosong2022.github.io/p/notes-on-v-triune/V-Triune-performance_hu15819943646912053752.png 1024w"
loading="lazy"
alt="Performance of Orsta on MEGA-Bench core"
class="gallery-image"
data-flex-grow="110"
data-flex-basis="264px"
>&lt;/p>
&lt;h3 id="analysis">Analysis
&lt;/h3>&lt;p>&lt;strong>on-policy v.s. off-policy&lt;/strong>
作者首先对比了以下 on-policy RL 和 off-policy RL 两种训练方式的表现， 结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-v-triune/V-triune-on-policy.png"
width="1137"
height="526"
srcset="https://maosong2022.github.io/p/notes-on-v-triune/V-triune-on-policy_hu17883304061716859651.png 480w, https://maosong2022.github.io/p/notes-on-v-triune/V-triune-on-policy_hu14295860107519637628.png 1024w"
loading="lazy"
alt="Ablation on on-policy v.s. off-policy"
class="gallery-image"
data-flex-grow="216"
data-flex-basis="518px"
>&lt;/p>
&lt;p>结果有两点发现：&lt;/p>
&lt;ol>
&lt;li>on-policy 的表现基本上都是比 off-policy 要好的。&lt;/li>
&lt;li>小模型通过 RL 带来的表现提升更明显，大模型的表现则更慢&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>generalization&lt;/strong>
作者还评估了以下模型的泛化性，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-v-triune/V-Triune-generalization.png"
width="422"
height="673"
srcset="https://maosong2022.github.io/p/notes-on-v-triune/V-Triune-generalization_hu7830263504092978837.png 480w, https://maosong2022.github.io/p/notes-on-v-triune/V-Triune-generalization_hu5722992839965848528.png 1024w"
loading="lazy"
alt="Generalization of V-Triune"
class="gallery-image"
data-flex-grow="62"
data-flex-basis="150px"
>&lt;/p>
&lt;p>结果发现，RL 的训练更像是对齐，而不是泛化。也就是说，模型在 in-domain 的任务上表现较好，在 OOD 的任务上表现就一般。这与已有的结论是不一样的&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Recall
&lt;a class="link" href="https://maosong.website/p/notes-on-magistral/" target="_blank" rel="noopener"
>Magistral&lt;/a> 发现模型在 math domain 上进行训练，在 code domain 上的表现也会提升&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Training Dynamics&lt;/strong>
作者还分析了不同任务不同指标随训练进行的变化情况，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-v-triune/V-Triune-training-dynamics.png"
width="1150"
height="900"
srcset="https://maosong2022.github.io/p/notes-on-v-triune/V-Triune-training-dynamics_hu15276947352141942304.png 480w, https://maosong2022.github.io/p/notes-on-v-triune/V-Triune-training-dynamics_hu12555432383874143707.png 1024w"
loading="lazy"
alt="V-Triune training dynamics"
class="gallery-image"
data-flex-grow="127"
data-flex-basis="306px"
>&lt;/p>
&lt;p>结果显示，reasoning 和 OCR 任务岁训练进行其输出长度和正确率都有所提升。但是 detection 相关任务则变化不是很明显。&lt;/p>
&lt;h3 id="ablation-study">Ablation Study
&lt;/h3>&lt;p>作者进行了三个消融实验：&lt;/p>
&lt;ol>
&lt;li>训练策略：冻结 ViT, 冻结 LLM, 两者都不冻结&lt;/li>
&lt;li>任务分解策略：仅使用 reasoning 数据进行训练，仅使用 perception 数据进行训练，同时使用两种数据进行训练。&lt;/li>
&lt;li>learning rate: 不同的学习率&lt;/li>
&lt;/ol>
&lt;p>实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-v-triune/V-Triune-ablation-study.png"
width="1146"
height="585"
srcset="https://maosong2022.github.io/p/notes-on-v-triune/V-Triune-ablation-study_hu17402617248809186274.png 480w, https://maosong2022.github.io/p/notes-on-v-triune/V-Triune-ablation-study_hu10212683979436721237.png 1024w"
loading="lazy"
alt="Ablation study"
class="gallery-image"
data-flex-grow="195"
data-flex-basis="470px"
>&lt;/p>
&lt;p>结果发现，&lt;/p>
&lt;ol>
&lt;li>仅训练 ViT 对表现没有任何帮助，只有训练 LLM 才会带来性能的提升。&lt;/li>
&lt;li>同时使用两种数据进行训练的效果是最好的，其次是仅使用 reasoning 数据进行训练，最后是仅使用 perception 数据进行训练&lt;/li>
&lt;li>较大的学习率会损害模型的表现。&lt;/li>
&lt;/ol>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>本文中，作者提出了 V-Triune, 一个使用 RL 来同时提高 MLLM 的 reasoning 和 perception 能力的 RL 训练框架。&lt;/p>
&lt;p>作者发现 RL 对于 VLM 来说更像是一种 Alignment 策略，而不是一种 Generalization 策略，也就是模型并没有引入新的能力，而是提升模型本身的 utility 以及 robustness.&lt;/p>
&lt;p>作者认为本文有以下 Limitation:&lt;/p>
&lt;ol>
&lt;li>在 perception 任务上没有看到明显的 test-time scaling 现象。作者认为，探究使用 o3 类似的方法或许能提高模型的表现&lt;/li>
&lt;li>探究 RL-zero 在 VLM 中的应用，也就是直接在 base model 上进行 RL 的训练。&lt;/li>
&lt;/ol>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2505.18129" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Magistral</title><link>https://maosong2022.github.io/p/notes-on-magistral/</link><pubDate>Wed, 16 Jul 2025 11:04:04 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-magistral/</guid><description>&lt;p>Magistral 是 Mistral 提出的一个 reasoning model 系列，主要针对 math 和 code 两个 domain&lt;/p>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>Mistral 在 2025 年 6 月 12 日发布了 Magistral ，一个 reasoning model, 包含两个模型，一个是由纯 RL 训练得到的 &lt;em>Magistral Medium&lt;/em>, 另一个是由 SFT 和蒸馏 Magistral Medium 得到的 &lt;strong>Magistral Small&lt;/strong>&lt;/p>
&lt;p>作者首先介绍了一下本文的贡献：&lt;/p>
&lt;ol>
&lt;li>介绍了如何仅使用 RL (而不是用蒸馏) 来训练 Magistral Medium&lt;/li>
&lt;li>infra 上的改进，主要使用最新的权重来更新 generator&lt;/li>
&lt;li>多语种能力，支持多种语言&lt;/li>
&lt;li>系统性探究了 RLVR 的能力边界&lt;/li>
&lt;li>开源了 Magistral small (24B)&lt;/li>
&lt;/ol>
&lt;h2 id="method">Method
&lt;/h2>&lt;h3 id="rl">RL
&lt;/h3>&lt;p>RL 算法基于 GRPO 改进，主要有以下几点：&lt;/p>
&lt;ol>
&lt;li>去掉了 KL Divergence loss, 这一点跟 &lt;a class="link" href="https://maosong.website/p/notes-on-dapo/" target="_blank" rel="noopener"
>DAPO&lt;/a> 是一致的，提升模型的探索能力&lt;/li>
&lt;li>Loss Normalization，在 sample 层面做平均，还是跟 DAPO 一致，减少不同长度输出对训练造成的影响&lt;/li>
&lt;li>Advantage Normalization, 作者首先将每个 token 的 Advantage 定义为 $\hat{A}&lt;em>i=R_i-\mu$, 其中 $\mu$ 是每个 group 的 reward 均值， 然后在每个 mini-batch 里对 advantage 进行 Normalization (这里 $\hat{A}&lt;/em>{mean}$ 和 $\hat{A}_{std}$ 分别为 mini-batch advantage 的均值和方差)：&lt;/li>
&lt;/ol>
$$
\hat{A}_{i,t}^{norm}=\frac{\hat{A}_i-\hat{A}_{mean}}{\hat{A}_{std}}
$$&lt;ol start="4">
&lt;li>CLIP-Higher, 跟 DAPO 一致，提高稀有 token 的被采样概率&lt;/li>
&lt;li>Eliminating non-divsere groups, 跟 DAPO 一致，去掉过于简单和过于难的题目&lt;/li>
&lt;/ol>
&lt;p>最终 RL 阶段训练的损失函数为：&lt;/p>
$$
\mathcal{L}(\theta) = \mathbb{E}_{q\sim P(Q),\{o_i\}_{i=1}^G\sim \pi_{old}(\cdot\mid q)}\frac{1}{\sum_{i=1}^G|o_i|}\sum_{i=1}^G\sum_{t=1}^{|o_i|}\min\left[r_{i,t}(\theta)\hat{A}_{i,t}^{norm}, \mathrm{clip}(r_{i,t}(\theta), 1-\epsilon_{low}, 1+\epsilon_{high})\hat{A}_{i,t}^{norm}\right], \mathrm{s.t.}, \exists 1\leq m &lt; n \leq G, r_m\neq r_n.
$$&lt;p>其中&lt;/p>
$$
r_{i,t}(\theta) = \frac{\pi_{\theta}(o_{i,t}\mid q, o_{i,&lt;t})}{\pi_{old}(o_{i,t}\mid q, o_{i,&lt;t})}
$$&lt;h3 id="reward-shaping">Reward Shaping
&lt;/h3>&lt;p>作者还基于四个维度来构建 reward: formatting, correctness, length, 以及 language consistency&lt;/p>
&lt;p>&lt;strong>Formatting&lt;/strong>
针对数学和代码问题，作者要求模型输出符合特定的格式&lt;/p>
&lt;ul>
&lt;li>Tag requirements: 思考过程用 &lt;code>&amp;lt;think&amp;gt;&amp;lt;/think&amp;gt;&lt;/code> 包含，且只能包含一个 tag&lt;/li>
&lt;li>mathematical responses: 对于数学问题，结果用 &lt;code>\boxed{}&lt;/code> 包含&lt;/li>
&lt;li>code response：包含一个 markdown block&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>[!tip] Future
最新的 &lt;a class="link" href="https://maosong.website/p/notes-on-glm-4.1v-thinking/" target="_blank" rel="noopener"
>GLM-4.1V-Thinking&lt;/a> 认为，不应该在 RL 训练阶段加入 format reward&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>correctness&lt;/strong>
基于答案的正确性分配奖励&lt;/p>
&lt;ul>
&lt;li>math correctness：使用 rule-based verifier 进行打分，使用 parser 和 Sympy 来比较模型输出以及 ground truth&lt;/li>
&lt;li>code correctness: 构建单元测试，评估输出代码是否能通过所有的单元测试&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Length penalty&lt;/strong>
与 DAPO 一致，使用 Length penalty 来惩罚过长的回答。&lt;/p>
&lt;p>&lt;strong>Language consistency&lt;/strong>
减少模型混合语言输出的问题。作者的做法是将 10% 的问题从英语转化为其他语种，然后使用 fastText 进行分类，确保内容都是一个语言。作者发现，通过这个简单的修改，就提高了模型的语言跟随能力。&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Recall
MiMo 中也遇到了混合语言输出的问题，但是其并没有给出解决办法。&lt;/p>
&lt;/blockquote>
&lt;p>作者还在 system prompt 中规定了输出的格式以及语言。&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Observation
作者发现，RL 的训练对于 system prompt 非常的敏感，因为 system prompt 会提高 model 的 entropy, 然后提高模型的探索能力。&lt;/p>
&lt;/blockquote>
&lt;h2 id="data">Data
&lt;/h2>&lt;p>数据包括 math problems 以及 code problems&lt;/p>
&lt;h3 id="math">Math
&lt;/h3>&lt;p>作者首先收集了 700K 样本，然后作者通过预处理以及过滤来保证数据的质量：&lt;/p>
&lt;p>&lt;strong>format filtering&lt;/strong>
要求问题完整，答案准确且可验证；去掉证明题和 multi-part 的问题；改写多项选择题为解答题，提高难度防止 reward hacking&lt;/p>
&lt;p>&lt;strong>difficulty filtering&lt;/strong>
使用两阶段的过滤策略。&lt;/p>
&lt;ol>
&lt;li>第一阶段使用 LLM 进行多次采样，然后去除掉比较简单或者比较复杂的&lt;/li>
&lt;li>第二阶段使用 RL checkpoint 进行多次采样，去除掉标准答案可能会存在问题的题目&lt;/li>
&lt;/ol>
&lt;p>最终一共得到 &lt;strong>38K&lt;/strong> 的样本&lt;/p>
&lt;h3 id="code">Code
&lt;/h3>&lt;p>首先去掉没有 solution 和 test 的问题；然后去除掉标准答案不能通过所有 test 的问题。最终一共得到 &lt;strong>35K&lt;/strong> 的样本，包含 Python 和 C++ 两种语言&lt;/p>
&lt;h2 id="training">Training
&lt;/h2>&lt;p>训练框架如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-magistral/Magistral_data_training.png"
width="1515"
height="523"
srcset="https://maosong2022.github.io/p/notes-on-magistral/Magistral_data_training_hu7985413339294341630.png 480w, https://maosong2022.github.io/p/notes-on-magistral/Magistral_data_training_hu12253124277348638876.png 1024w"
loading="lazy"
alt="Magistral_data_training"
class="gallery-image"
data-flex-grow="289"
data-flex-basis="695px"
>&lt;/p>
&lt;p>对于两个模型，作者采用了不同的方式进行训练&lt;/p>
&lt;ul>
&lt;li>Magistral Medium: 使用 pure RL 进行训练&lt;/li>
&lt;li>Magistral small: 使用 SFT + RL 进行训练&lt;/li>
&lt;/ul>
&lt;p>Magistral Medium 训练时满足的要求：&lt;/p>
&lt;ol>
&lt;li>dataset is not too easy: 太简单的题目对模型提升没有帮助&lt;/li>
&lt;li>Generation length does not stop growing: 逐步提升模型的最大输出长度&lt;/li>
&lt;li>KV-cache memory burden is not too large: 降低 batch size 来减少 KV-cache 的内存占用&lt;/li>
&lt;/ol>
&lt;p>Magistral small 训练&lt;/p>
&lt;p>&lt;strong>SFT&lt;/strong>
数据集包括两部分，一部分是 Magistral Medium 回答正确的这部分数据，第二部分是公开数据集，包括 [[OpenThoughts]] 和 [[OpenR1]] 两个数据集，作者使用 Magistral Medium 来生成回答。作者还加入了 10% 的 instruction tuning 数据来保持模型的通用能力。&lt;/p>
&lt;p>&lt;strong>RL&lt;/strong>
RL 的训练与 Magistral Medium 一致。&lt;/p>
&lt;h2 id="infra">Infra
&lt;/h2>&lt;p>作者首先介绍了一下 RL 训练的 infra, infra 主要包括三个模块：&lt;/p>
&lt;ul>
&lt;li>Trainers: 用于更新模型的权重&lt;/li>
&lt;li>Generators: 用于采样，生成 roll-out&lt;/li>
&lt;li>Verifiers: 对模型输出的结果进行打分&lt;/li>
&lt;/ul>
&lt;p>分布式 RL 训练的主要问题在于，不同长度的 roll-out 花费的时间不一致，作者发现，最长的 roll-out 和最短的 roll-out 的时间相差超过 5 倍以上。&lt;/p>
&lt;p>因此，作者就提出了异步生成这个方法。具体的做法就是&lt;/p>
&lt;ol>
&lt;li>首先由 Generator 生成多条 roll-out&lt;/li>
&lt;li>当 roll-out 完成之后，立马用 Verifiers 对轨迹进行打分&lt;/li>
&lt;li>收集 roll-out 以及对应的 reward, 直到达到给定的 batch 大小&lt;/li>
&lt;li>使用 Trainer 更新 Generator 的权重, 将更新后的权重同步给 Generator，这样其他 generator 在生成新的 token 时用的就是新的权重&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-magistral/Magistral_infra.png"
width="1169"
height="968"
srcset="https://maosong2022.github.io/p/notes-on-magistral/Magistral_infra_hu16434872165372945647.png 480w, https://maosong2022.github.io/p/notes-on-magistral/Magistral_infra_hu16801221237754484869.png 1024w"
loading="lazy"
alt="Magistral_infra"
class="gallery-image"
data-flex-grow="120"
data-flex-basis="289px"
>&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Recall
MiMo 里的做法是，当我们收集到给定数量的 roll-out 之后，我们就基于这些 roll-out 更新权重，然后进行下一次采样&lt;/p>
&lt;/blockquote>
&lt;p>训练时，对于每个 rank, 只要其收集到足够的 roll-out, 就会进行一次梯度更新。&lt;/p>
&lt;h2 id="evaluation">Evaluation
&lt;/h2>&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-magistral/Magistral_performance_AIME.png"
width="1810"
height="1002"
srcset="https://maosong2022.github.io/p/notes-on-magistral/Magistral_performance_AIME_hu6201950020122303348.png 480w, https://maosong2022.github.io/p/notes-on-magistral/Magistral_performance_AIME_hu17050491895656832894.png 1024w"
loading="lazy"
alt="Performance of Magistral Medium"
class="gallery-image"
data-flex-grow="180"
data-flex-basis="433px"
>&lt;/p>
&lt;h2 id="ablation">Ablation
&lt;/h2>&lt;p>&lt;strong>RL 的泛化性&lt;/strong>
作者探究了 RL 的 cross-domain generalization 能力，实验结果如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>AIME’24&lt;/th>
&lt;th>LiveCodeBench v5&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Starting Checkpoint&lt;/td>
&lt;td>32.2&lt;/td>
&lt;td>22.7&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RL (Math only)&lt;/td>
&lt;td>62.5&lt;/td>
&lt;td>38.3 (+15.6)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RL (Code only)&lt;/td>
&lt;td>49.7 (+17.5)&lt;/td>
&lt;td>42.7&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，不管是使用 math 还是 code 数据单独进行训练，模型在另一个 domain 上的表现都有所提升。&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Future
这个结论与最新的 GLM-4.1V-Thinking 结论一致&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Distillation v.s. RL for small models&lt;/strong>
作者探究了对于小语言模型，使用 RL 进行训练的效果更好，还是使用蒸馏的效果更好。&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-magistral/Magistral_distillation_vs_rl_performance.png"
width="1822"
height="934"
srcset="https://maosong2022.github.io/p/notes-on-magistral/Magistral_distillation_vs_rl_performance_hu12804151669788107874.png 480w, https://maosong2022.github.io/p/notes-on-magistral/Magistral_distillation_vs_rl_performance_hu14645921621896337014.png 1024w"
loading="lazy"
alt="Performance of RL v.s. distillation"
class="gallery-image"
data-flex-grow="195"
data-flex-basis="468px"
>&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Observation
实验结果发现，仅使用 RL 的效果与蒸馏差不多，甚至更好&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Batch size&lt;/strong>
作者还探究了 batch size $n_{batch}$ 以及 mini-batch size $n_{mini}$ 的影响。这里 batch size 指的是用于更新梯度的 roll-out 数量，mini-batch size 指的是计算梯度的 roll-out 数量，作者还定义了并行生成 roll-out 的数量 $n_{async}$. 当 $n_{async}&amp;raquo; n_{batch}$ 时，生成的 sequence 就很可能是 off-policy 的。作者固定 $n_{async}=4096$, 然后对比了不同的 $n_{batch}$ 和 $n_{mini}$ 对模型表现的影响，结果如下：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-magistral/Magistral_batch_ablation.png"
width="1602"
height="698"
srcset="https://maosong2022.github.io/p/notes-on-magistral/Magistral_batch_ablation_hu16454842133673153190.png 480w, https://maosong2022.github.io/p/notes-on-magistral/Magistral_batch_ablation_hu3588199010302775526.png 1024w"
loading="lazy"
alt="Ablation study on batch size"
class="gallery-image"
data-flex-grow="229"
data-flex-basis="550px"
>&lt;/p>
&lt;blockquote>
&lt;p>![tip] Observation
当 $n_{batch}=n_{mini}$ 时，模型表现差不太多（左图）；当 $n_{batch}$ 为常数，而 $n_{mini}$ 逐渐减小时，模型表现会逐渐变差。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Advantage normalization&lt;/strong>
作者对比了三种针对 advantage 的 normalization 方式：&lt;/p>
&lt;ul>
&lt;li>mini-batch: 在每个 mini-batch 里对模型进行 normalization&lt;/li>
&lt;li>Group normalization: 在每个 group 里进行 normalization&lt;/li>
&lt;li>no normalization: 没有 normalization&lt;/li>
&lt;/ul>
&lt;p>实验结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-magistral/Magistral_normalization.png"
width="1820"
height="500"
srcset="https://maosong2022.github.io/p/notes-on-magistral/Magistral_normalization_hu658871494775987334.png 480w, https://maosong2022.github.io/p/notes-on-magistral/Magistral_normalization_hu1935902630730719135.png 1024w"
loading="lazy"
alt="Ablation study on normalization"
class="gallery-image"
data-flex-grow="364"
data-flex-basis="873px"
>&lt;/p>
&lt;p>作者发现，三种方式的区别并不是很大。&lt;/p>
&lt;h2 id="analysis">Analysis
&lt;/h2>&lt;p>&lt;strong>length dimension&lt;/strong>
作者首先保存模型的 weight, 然后使用 PCA 进行降维，并在 2 维上进行可视化，作者对权重进行扰动，然后记录模型的 reward 以及输出长度。结果发现，模型存在一个 length dimension. 可视化结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-magistral/Magistral_length_dimension.png"
width="1546"
height="602"
srcset="https://maosong2022.github.io/p/notes-on-magistral/Magistral_length_dimension_hu660748011264665621.png 480w, https://maosong2022.github.io/p/notes-on-magistral/Magistral_length_dimension_hu16323640694693724369.png 1024w"
loading="lazy"
alt="Magistral length dimension visualization"
class="gallery-image"
data-flex-grow="256"
data-flex-basis="616px"
>&lt;/p>
&lt;p>&lt;strong>multimodal extension&lt;/strong>
由于 Magistral medium 和 Magistral small 都是基于 MLLM 中的 LLM 开发得到的，作者还探究了更换 LLM 的 checkpoint 之后，原始 MLLM 的表现，结果发现，模型在多模态 reasoning 相关任务上的表现也得到了提升，结果如下图所示：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-magistral/Magistral_multimodal_extention.png"
width="1782"
height="822"
srcset="https://maosong2022.github.io/p/notes-on-magistral/Magistral_multimodal_extention_hu14193739507602157915.png 480w, https://maosong2022.github.io/p/notes-on-magistral/Magistral_multimodal_extention_hu12191812944879346854.png 1024w"
loading="lazy"
alt="Magistral multimodal performance"
class="gallery-image"
data-flex-grow="216"
data-flex-basis="520px"
>&lt;/p>
&lt;p>&lt;strong>Impact on other capabilities&lt;/strong>
作者还探究了 RL 训练对模型其他表现的影响，结果发现，RL 训练可以提高模型的 tool calling 和指令跟随能力，实验结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Category&lt;/th>
&lt;th>Benchmark&lt;/th>
&lt;th>Mistral Medium 3&lt;/th>
&lt;th>Magistral Medium&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Function calling&lt;/td>
&lt;td>Internal bench&lt;/td>
&lt;td>87.2&lt;/td>
&lt;td>87.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Instruction following&lt;/td>
&lt;td>IFEval&lt;/td>
&lt;td>86.8&lt;/td>
&lt;td>87.4&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>failed approaches&lt;/strong>
作者还介绍了一些尝试失败的做法：&lt;/p>
&lt;ol>
&lt;li>partial Reward: 对于 coding 任务，作者使用 test 的通过率作为奖励，结果发现效果并不好，这是因为一些错误的解法的 test 通过率也很高&lt;/li>
&lt;li>entropy bonus loss: 作者发现在损失函数中加入 entropy bonus loss 之后，模型的训练变得不稳定，而且效果不如使用更高的 $\epsilon_{high}$&lt;/li>
&lt;li>作者还进一步验证在 PPO loss 中加入 KL divergence loss, 结果发现效果并不好，这与 DAPO 的结论一致&lt;/li>
&lt;li>作者还尝试先 SFT Magistral Medium, 再进行 RL, 结果发现 RL 可以大幅度提高 SFT checkpoint 的表现。&lt;/li>
&lt;/ol>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>本文中，作者提出了 Magistral, 一个针对 math 和 code 的 reasoning model, 作者介绍了训练细节。但是，从方法层面来看，和 DAPO 区别不是很大。关键点应该是作者详细介绍了各种消融实验，为后来相关探索提供了经验。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2506.10910v1" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on SmolLM3</title><link>https://maosong2022.github.io/p/notes-on-smollm3/</link><pubDate>Tue, 15 Jul 2025 11:01:13 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-smollm3/</guid><description>&lt;p>Hugging Face 在 2025 年 7 月 8 号发布了 SmolLM3, 一个 3B 的，128K 上下文，支持 6 种语言，支持 dual mode reasoning 的小语言模型。&lt;/p>
&lt;h2 id="pre-training">Pre-training
&lt;/h2>&lt;h3 id="architecture">Architecture
&lt;/h3>&lt;p>SmolLM3 是一个基于 transformer 的小语言模型，其模型架构与 SmolLM2 相似，SmolLM3 做了以下改进：&lt;/p>
&lt;ol>
&lt;li>使用 GQA 代替 multi-head attention. 作者通过消融实验发现 &lt;a class="link" href="https://maosong.website/p/notes-on-gqa/" target="_blank" rel="noopener"
>GQA&lt;/a> 和 MHA 的效果差不多，并且还可以减少 KV cache 的 size&lt;/li>
&lt;li>NoPE: 作者使用了NoPE, 来选择性（每 4 个 layer）移除 position embedding. 这个方法可以在不损害模型短文本能力的同时，提高模型长上下文的表现&lt;/li>
&lt;li>Intra-Document Masking: 不同的文档之间使用 attention masking 隔开&lt;/li>
&lt;li>Training stability: 与 olmo 2 一样，作者移除了 embedding layer 的 weight decay, 来提升训练的稳定性。&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_model_atanomy.png"
width="2554"
height="1470"
srcset="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_model_atanomy_hu13998036533088262709.png 480w, https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_model_atanomy_hu8563067276937063516.png 1024w"
loading="lazy"
alt="SmolLM3 model atanomy"
class="gallery-image"
data-flex-grow="173"
data-flex-basis="416px"
>&lt;/p>
&lt;h3 id="data">Data
&lt;/h3>&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_pre-training_recipe.png"
width="1932"
height="1196"
srcset="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_pre-training_recipe_hu14656950727837741103.png 480w, https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_pre-training_recipe_hu1794311974944927812.png 1024w"
loading="lazy"
alt="SmolLM3 pre-training recipe"
class="gallery-image"
data-flex-grow="161"
data-flex-basis="387px"
>&lt;/p>
&lt;p>与 SmolLM2 一样，作者使用了&lt;strong>11.2T&lt;/strong> token 进行训练，训练包括 3 个 stage。作者针对数据混合策略进行了消融实验，实验配置如下：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Stage&lt;/th>
&lt;th>Stable phase&lt;/th>
&lt;th>Stable phase&lt;/th>
&lt;th>Decay phase&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Description&lt;/td>
&lt;td>Base training&lt;/td>
&lt;td>High quality injection&lt;/td>
&lt;td>LR Decay&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Tokens&lt;/td>
&lt;td>8T&lt;/td>
&lt;td>2T&lt;/td>
&lt;td>1.1T&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Web&lt;/td>
&lt;td>85%&lt;/td>
&lt;td>75%&lt;/td>
&lt;td>63%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Code&lt;/td>
&lt;td>12%&lt;/td>
&lt;td>15%&lt;/td>
&lt;td>24%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Math&lt;/td>
&lt;td>3%&lt;/td>
&lt;td>10%&lt;/td>
&lt;td>13%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>datasets&lt;/td>
&lt;td>&lt;strong>web&lt;/strong>: FineWeb-Edu, DCLM, FineWeb2, Fineweb2-HQ&lt;br>&lt;strong>code&lt;/strong>: The Stack v2, StarCoder2 PRS, Jupyter and Kaggle notebooks, Github issues, StackExchange&lt;br>&lt;strong>math&lt;/strong>: FineMath3, InfiWebMath3+&lt;/td>
&lt;td>Adding Stack-Edu, FineMath4+, InfiWebMath4+, MegaMath&lt;/td>
&lt;td>upsampling of high-quality code data&lt;br>upsampling math data and introducing instruction and reasoning datasets such as OpenMathReasoning&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>其中，Web data 包含 12% 的多语种数据。&lt;/p>
&lt;h3 id="training-recipe">Training Recipe
&lt;/h3>&lt;p>训练过程中，作者使用了 2.36M tokens 的 batch size, 上下文长度为 4096. 优化器为 AdamW&lt;/p>
&lt;p>作者使用了 nanotron 进行训练， datatrove 来处理数据， lighteval 来评估模型的表现。&lt;/p>
&lt;p>模型在 384 张 H100 上训练了 24 天。分布式训练的配置如下&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_distributed_training.png"
width="1672"
height="1024"
srcset="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_distributed_training_hu7285902030689819858.png 480w, https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_distributed_training_hu8235147299335409429.png 1024w"
loading="lazy"
alt="SmolLM3 distributed training"
class="gallery-image"
data-flex-grow="163"
data-flex-basis="391px"
>&lt;/p>
&lt;h2 id="mid-training">Mid-training
&lt;/h2>&lt;p>Mid-training 的主要目标为扩展模型的上下文以及提升模型的 reasoning 能力。&lt;/p>
&lt;h3 id="long-context-extension">Long Context Extension
&lt;/h3>&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_long_context_training.png"
width="1942"
height="1128"
srcset="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_long_context_training_hu16383025728698209251.png 480w, https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_long_context_training_hu13112308143836602144.png 1024w"
loading="lazy"
alt="SmolLM3 long context training"
class="gallery-image"
data-flex-grow="172"
data-flex-basis="413px"
>&lt;/p>
&lt;p>在预训练阶段结束之后，作者使用了额外的 &lt;strong>100B&lt;/strong> tokens 来扩展模型的上下文。作者将扩展过程分为两个 stage:&lt;/p>
&lt;ol>
&lt;li>Stage 1: 将模型的上下文从 4K 提升到 32K. 具体做法是将 RoPE 的 base frequency 提升到 1.5M&lt;/li>
&lt;li>Stage 2: 将，模型的上下文从 32K 提升到 64K. 具体做法是将 RoPE 的 base frequency 提升到 5M&lt;/li>
&lt;/ol>
&lt;p>训练过程中，作者对 math, code 和 reasoning data 进行了上采样。作者发现，对长文本数据进行上采样并不会提高模型在 RULER 和 HELMET 上的表现。&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Recall
&lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5-1m/" target="_blank" rel="noopener"
>Qwen2.5-1M&lt;/a> 里也分析了长文本数据的问题，也就是大部分长文本数据依然是局部相关性强，而全局相关性弱&lt;/p>
&lt;/blockquote>
&lt;p>与 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5/" target="_blank" rel="noopener"
>Qwen2.5&lt;/a> 一样，作者还是用了 &lt;a class="link" href="https://maosong.website/p/notes-on-yarn/" target="_blank" rel="noopener"
>YARN&lt;/a> 来在推理阶段进一步提高模型的上下文长度，作者发现模型可以处理 128K 上下文长度的文本。&lt;/p>
&lt;h3 id="reasoning-mid-training">Reasoning Mid-training
&lt;/h3>&lt;p>扩展模型上下文长度之后，作者还额外增加了一个 mid-training stage 来提高模型的 reasoning 能力。这个阶段的目标在于提升模型的通用能力。作者希望模型不针对特定的 domain, 如 math 或者 code 等。&lt;/p>
&lt;p>训练过程中，作者使用了 &lt;strong>35B&lt;/strong> 的 token. 数据来源包括 Open-Thoughts3-1.2M 以及 NVIDIA 的 Llama-Nemetron-Post-Training-Dataset-v1.1. Reasoning trace 由 DeepSeek-R1 生成。作者使用了 ChatML 的格式，还使用了 Packing 来提升训练效率。训练持续了 4 个 epoch.&lt;/p>
&lt;h2 id="post-training">Post-training
&lt;/h2>&lt;p>如何构建 dual instruction model 来同时支持 reasoning 和 non-reasoning 任务并没有一个共识，大部分模型的数据都是非公开的。因此，作者就构建了一个 training pipeline, 用于提升模型在两种模式下的能力。&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_post-training_recipe.png"
width="1796"
height="1208"
srcset="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_post-training_recipe_hu5481082171162702903.png 480w, https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_post-training_recipe_hu243045709497717267.png 1024w"
loading="lazy"
alt="SmolLM3 post-training recipe"
class="gallery-image"
data-flex-grow="148"
data-flex-basis="356px"
>&lt;/p>
&lt;h3 id="chat-template">Chat Template
&lt;/h3>&lt;p>作者首先构建了一个 chat template, 用于支持 reasoning 和 non-reasoning 两种模式。该 chat template 支持用户使用 &lt;code>/think&lt;/code> 和 &lt;code>/no_think&lt;/code> flag 来控制模型的思考模式。在 non-reasoning 模式下，作者还在模型输出中 prefill 了 &lt;code>&amp;lt;think&amp;gt;\n\n&amp;lt;/think&amp;gt;&lt;/code>, 这一点与 Qwen3 一致。&lt;/p>
&lt;p>SmolLM3 还支持工具调用，其在 chat template 中加入了两种描述方式：XLM tools 和 Python tools.&lt;/p>
&lt;p>SmolLM3 还在 system prompt 中加入了 metadata, 如知识的截止时间，当前的 reasoning mode 等。&lt;/p>
&lt;h3 id="sft">SFT
&lt;/h3>&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_synthetic_SFT_data.png"
width="1562"
height="940"
srcset="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_synthetic_SFT_data_hu11130185069272692077.png 480w, https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_synthetic_SFT_data_hu3647788789608535616.png 1024w"
loading="lazy"
alt="SmolLM3 Synthetic SFT data"
class="gallery-image"
data-flex-grow="166"
data-flex-basis="398px"
>&lt;/p>
&lt;p>作者针对 math, code, general reasoning, instruction following, 以及 multilinguality 这几个领域来提升模型的表现。&lt;/p>
&lt;p>作者首先使用 reasoning mode 的 Qwen3-32B 来合成数据，合成数据的过程如上图所示。&lt;/p>
&lt;p>最终，SFT 阶段的数据包括 &lt;strong>1.8B&lt;/strong> token, 其中 &lt;strong>1B&lt;/strong> 为 non-reasoning mode 的 token, 覆盖 12 个数据集， &lt;strong>0.8B&lt;/strong> 为 reasoning token, 覆盖 10 个数据集。作者训练了 4 个 epoch, 使用了 Packing 的技巧。&lt;/p>
&lt;h3 id="apo">APO
&lt;/h3>&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_synthetic_preference_data.png"
width="1542"
height="760"
srcset="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_synthetic_preference_data_hu9363143912475736506.png 480w, https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_synthetic_preference_data_hu12538931937351185121.png 1024w"
loading="lazy"
alt="SmolLM3 synthetic preference data"
class="gallery-image"
data-flex-grow="202"
data-flex-basis="486px"
>&lt;/p>
&lt;p>SFT 阶段之后，作者使用了 Tulu3 的 preference dataset 用于 non-reasoning mode 的训练，然后合成了一批数据用于 reasoning mode 的训练，这批合成数据使用 Qwen3 32B 和 Qwen3 0.6B 生成得到，具体做法就是 Qwen3 32B 输出的结果定义为正样本，Qwen3 0.6B 输出的结果定义为负样本。&lt;/p>
&lt;p>作者使用了 APO 算法来进行训练，APO 是 DPO 的一个变体， DPO 的目标函数为&lt;/p>
$$
\mathcal{L}_{DPO}(x,y_w,y_{l}; \theta) = -\log \sigma(r_\theta(x,y_w) - r_\theta(x, y_l))
$$&lt;p>其中&lt;/p>
$$
r_\theta(x,y) = \beta\log \frac{\pi_{\theta}(y\mid x)}{\pi_{ref}(y\mid x)}
$$&lt;p>$\beta&amp;gt;0$ 是一个超参数。APO 的目标函数如下&lt;/p>
$$
\mathcal{L}_{APO}(x,y_w,y_l;\theta) = -\sigma(r_\theta(x,y_w)) + \sigma(r_\theta(x,y_l))
$$&lt;p>作者发现，模型的 reasoning 能力提升之后，其长上下文能力反而下降了。作者认为这是因为在 reasoning mid-training stage 的训练中，提升 reasoning 能力损害了模型的 long context 能力。并且，APO 的训练数据上下文长度大多都在 24K 左右。为了解决这个问题，作者提出了 Model merging 的方法&lt;/p>
&lt;h3 id="model-merging">Model Merging
&lt;/h3>&lt;p>作者使用 MergeKit 来完成 model merging 的任务。merge 的过程包括两步：&lt;/p>
&lt;ol>
&lt;li>构造一个 model soup, 包括 APO 的每个 checkpoint&lt;/li>
&lt;li>将 model soup 与 mid-training 的一个拥有强上下文能力的 checkpoint 结合起来，作者使用了 linear merge, APO model soup 和 mid-training checkpoint 的权重分别为 0.9 和 0.1.&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>[!tip] Recall
&lt;a class="link" href="https://maosong.website/p/notes-on-kimi-k1.5/" target="_blank" rel="noopener"
>Kimi-k1.5&lt;/a> 也使用了 model merging 的方法，来将 long CoT 模型的能力迁移到 short-CoT 模型上去&lt;/p>
&lt;/blockquote>
&lt;h2 id="evaluation">Evaluation
&lt;/h2>&lt;p>作者首先评估了一下 base model 的表现，评估使用了 12 个 benchmark, 对比的模型包括 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a>, Gemma3, LLaMA 3.2. 结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_base_model_performance.png"
width="2992"
height="2059"
srcset="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_base_model_performance_hu6476501870165868626.png 480w, https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_base_model_performance_hu9200080401812923224.png 1024w"
loading="lazy"
alt="SmolLM3 base model performance"
class="gallery-image"
data-flex-grow="145"
data-flex-basis="348px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_base_model_extact_performance.png"
width="2194"
height="954"
srcset="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_base_model_extact_performance_hu7414033305597534731.png 480w, https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_base_model_extact_performance_hu18133849881670103547.png 1024w"
loading="lazy"
alt="SmolLM3 base model performance"
class="gallery-image"
data-flex-grow="229"
data-flex-basis="551px"
>&lt;/p>
&lt;p>模型的多语种表现如下&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_multilingual_performance.png"
width="1600"
height="1062"
srcset="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_multilingual_performance_hu8100262855846916136.png 480w, https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_multilingual_performance_hu256869903175579211.png 1024w"
loading="lazy"
alt="SmolLM3 multilingual performance"
class="gallery-image"
data-flex-grow="150"
data-flex-basis="361px"
>&lt;/p>
&lt;p>Instruction (non-reasoning) 版本的评估结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_instruction_performance.png"
width="2992"
height="2064"
srcset="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_instruction_performance_hu6052638395361747764.png 480w, https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_instruction_performance_hu11311995837787702253.png 1024w"
loading="lazy"
alt="SmolLM3 Instruct models performance (w/o reasoning)"
class="gallery-image"
data-flex-grow="144"
data-flex-basis="347px"
>&lt;/p>
&lt;p>Reasoning 版本的评估结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_reasoning_performance.png"
width="2158"
height="1208"
srcset="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_reasoning_performance_hu1378326931387372470.png 480w, https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_reasoning_performance_hu8158417547839893481.png 1024w"
loading="lazy"
alt="SmolLM3 reasoning performance"
class="gallery-image"
data-flex-grow="178"
data-flex-basis="428px"
>&lt;/p>
&lt;p>可以看到，Qwen3-4B 的表现是最好的。而 SmolLM3 的表现在 3B 左右也是非常强劲的&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者提出了 SmolLM3, 一个拥有长上下，文多语种以及 dual mode reasoning 能力的大语言模型，作者详细介绍了数据，训练以及 model merging 的技巧，来提高模型的表现。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://huggingface.co/blog/smollm3" target="_blank" rel="noopener"
>blog&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on GLM-4.1V-Thinking</title><link>https://maosong2022.github.io/p/notes-on-glm-4.1v-thinking/</link><pubDate>Mon, 14 Jul 2025 10:32:04 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-glm-4.1v-thinking/</guid><description>&lt;p>智谱 AI 在 25 年 7 月份发布了 GLM-4.1V-Thinking, 一个 9B 的多模态大语言模型，其在多个 benchmark 上达到了相同大小 MLLM 的 SOTA&lt;/p>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>已有工作如 &lt;a class="link" href="https://maosong.website/p/notes-on-mimo-vl/" target="_blank" rel="noopener"
>MiMo-VL&lt;/a> 和 V-Triune 主要集中在特定的领域，目前还没有一个在所有领域都能超过 non-reasoning model 表现的多模态 reasoning model.&lt;/p>
&lt;p>基于这个目标，作者提出了 GLM-4.1V-Thinking, 一个 9B 的，用于通用领域多模态 reasoning 的 MLLM. 在预训练阶段，作者通过构建多样化的数据来提升模型的基础能力。在 post-training 阶段，作者构建了 domain-specific 的数据来让模型学会 reasoning. 最后，作者提出了 Reinforcement Learning with Curriculum Sampling (RLCS) 来扩展模型的 reasoning 能力。&lt;/p>
&lt;p>作者主要发现如下：&lt;/p>
&lt;ol>
&lt;li>multi-domain RL 的泛化性非常强，在某一个 domain 上进行 RL 训练也可以提高模型在其他 domain 上的表现&lt;/li>
&lt;li>动态选择 RL 训练的数据可以有效提高模型的表现和训练效率&lt;/li>
&lt;li>一个 robust 以及 precise 的 reward model 对于 multi-domain RL 是至关重要的&lt;/li>
&lt;/ol>
&lt;h2 id="method">Method
&lt;/h2>&lt;h3 id="architecture">Architecture
&lt;/h3>&lt;p>GLM-4.1V-Thinking 是一个标准的 ViT-MLP-LLM 架构，其中：&lt;/p>
&lt;ul>
&lt;li>ViT: ViT 使用的是 AIMv2-Huge&lt;/li>
&lt;li>MLP: 是一个 3 层的 MLP，架构为 &lt;code>linear-LayerNorm-GELU-SwiGLU&lt;/code>, 并且在进入 MLP 之前，GLM-4.1V-Thinking 还会在 spatial 上进行降采样，降采样率为 2&lt;/li>
&lt;li>LLM: LLM 使用的是 GLM&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_Thinking_architecture.png"
width="1804"
height="988"
srcset="https://maosong2022.github.io/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_Thinking_architecture_hu14497388027985393186.png 480w, https://maosong2022.github.io/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_Thinking_architecture_hu7694229193987378695.png 1024w"
loading="lazy"
alt="Architecture of GLM4.1V-Thinking"
class="gallery-image"
data-flex-grow="182"
data-flex-basis="438px"
>&lt;/p>
&lt;p>与 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5-vl/" target="_blank" rel="noopener"
>Qwen2.5-VL&lt;/a> 一样，作者将 encoder 的 2D convolution 替换为了 3D convolution，用于处理视频输入，然后对于 single-image inputs，GLM-4.1V-Thinking 会将输入复制，变成一个含有两帧相同图片的 video 输入&lt;/p>
&lt;p>为了支持不同分辨率图片输入，作者进行了两点改进：&lt;/p>
&lt;p>第一是使用了 2D RoPE 来处理不同分辨率的图片输入&lt;/p>
&lt;p>第二是使用 bicubic interpolation 来将不同分辨率的图片适应到 encoder 的 position embedding 上。具体来讲，对一个拥有 $H_p\times W_p$ patches 的图片，每个 patch 的坐标 $(w,h)$ 首先会被 normalized 到 $[-1,1]$ 中：&lt;/p>
$$
g_{norm} = (w_{norm}, h_{norm}) = 2* \left(\frac{w+0.5}{W_p}, \frac{h+0.5}{H_p}\right) - 1
$$&lt;p>然后，我们再使用 bicubic interpolation $\mathcal{I}_{bicubic}$ 将坐标映射到原始的 position embedding 上：&lt;/p>
$$
P_{adapted} = \mathcal{I}_{bicubic}(P_{orig}, g_{norm})
$$&lt;p>对于视频输入，作者在每一帧之后插入了一个 time index token, 用每一帧的 timestamp string 来表示。这个做法与 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5-vl/" target="_blank" rel="noopener"
>Qwen2.5-VL&lt;/a> 一样。&lt;/p>
&lt;h3 id="pre-training">Pre-training
&lt;/h3>&lt;h4 id="pre-training-data">Pre-training Data
&lt;/h4>&lt;p>Pre-training 数据包括 image-caption data, interleaved image-text data, OCR data, grounding data, video data 和 instruction tuning data&lt;/p>
&lt;p>&lt;strong>Image caption data&lt;/strong>
Image caption data 用于提升模型的世界知识。作者从 LAION, DataComp, DNF 以及 Wukong 等数据集收集了 10B 的 image-text pairs, 然后进行数据清洗：&lt;/p>
&lt;ol>
&lt;li>Heuristic-based filtering: 基于规则，如 resolution, caption length 等过滤掉低质量的图片&lt;/li>
&lt;li>Relevance filtering: 计算 CLIP score, 过滤掉低质量的数据&lt;/li>
&lt;li>Concept-balanced resampling: 基于 MetaCLIP , 来提升数据的覆盖程度，解决长尾分布的问题。这里 &lt;a class="link" href="https://maosong.website/p/notes-on-seed1.5-vl/" target="_blank" rel="noopener"
>Seed1.5-VL&lt;/a> 有一个类似的消融实验。&lt;/li>
&lt;li>Factual-centered re-captioning: 通过 rewrite caption 来提升 caption 的质量和信息密度。Idefics2 通过实验发现，rewriting caption 可以提高模型的表现。作者在最终的数据集里，混合了一部分 re-caption 的数据。&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Interleaved image-text data&lt;/strong>
相比于 image-caption data, interleaved image-text data 体量更大，且逻辑关系更强。但是噪声也更多，因此作者针对不同来源的数据构建了不同的数据清洗策略：&lt;/p>
&lt;ol>
&lt;li>Web data processing pipeline: 数据来源为 MINT, MMC4, OmniCorpus 等。作者首先基于 CLIP-score，去掉与上下文不相关的图片；然后，作者去除掉广告，二维码等低质量的图片；最后，作者将图多文少的样本也给去除掉，比如相册图片。作者还训练了一个 high-knowledge-density image classifier, 来识别信息密度高的样本。&lt;/li>
&lt;li>Academic book processing pipeline: 作者从电子书中收集了 100M 的样本，这些电子书主要是 STEM domain，然后作者构建了一个 PDF parsing tool 来提取文档内容。&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>OCR data&lt;/strong>
OCR 数据包含&lt;strong>220M images&lt;/strong>, 数据集由三部分组成：&lt;/p>
&lt;ol>
&lt;li>Synthetic document images: 基于预训练语料，使用不同的格式来渲染文字生成不同的图片，然后基于 LAION 的图片作为背景&lt;/li>
&lt;li>Natural scene text images: 使用 Paddle-OCR 来从图片中提取文字以及对应的 bounding box&lt;/li>
&lt;li>Academic documents: 使用类似 Nougat 的方法来构建 PDF page-Markdown 的数据。&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Grounding data&lt;/strong>
主要包含自然场景和 GUI 两个 domain 的 grounding 数据：&lt;/p>
&lt;ol>
&lt;li>Natural image grounding: 基于 LAION-115M 得到。作者使用 GLIPv2, 来自动化解析 caption 中的实体，然后预测对应的 bounding box, 作者将 bounding box 较少的样本去除掉。最终得到&lt;strong>40M&lt;/strong>高质量的自然场景数据&lt;/li>
&lt;li>GUI grounding: 基于 CC 提取对应的 url, 然后使用 Playwright 来与网页进行交互，进而获取到所有的 DOM elements 与其对应的 bounding box. 最终收集到 &lt;strong>140M&lt;/strong>高质量的 QA 样本&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Video data&lt;/strong>
Video data 从多个来源得到，作者使用了 fine-grained human annotation 来保证数据质量。作者还标注了 camera motion 等信息&lt;/p>
&lt;p>为了保证数据的质量，作者进行了去重，来减少数据语义的相似性。&lt;/p>
&lt;p>&lt;strong>Instruction tuning data&lt;/strong>
instruction tuning data 的策略如下：&lt;/p>
&lt;ol>
&lt;li>Task coverage and taxonomy: 优化数据分布&lt;/li>
&lt;li>Complex scenario augmentation: 构建复杂的指令跟随数据&lt;/li>
&lt;li>Data contamination check: 数据污染检查&lt;/li>
&lt;/ol>
&lt;p>最终一共收集到 &lt;strong>50M&lt;/strong>样本，覆盖 visual perception, multimodal reasoning, GUI agent 等任务。&lt;/p>
&lt;h4 id="pre-training-training">Pre-training Training
&lt;/h4>&lt;p>Pre-training 由两个 stage 组成：&lt;/p>
&lt;ol>
&lt;li>Multimodal pre-training: 提高模型的通用多模态能力，上下文长度为 8192, global batch size 为 1536. 使用了 data packing 策略，使用了 2-way tensor parallelism&lt;/li>
&lt;li>Long-context continue training: 提升模型在高精度图片，长视频，长上下文场景下的表现，上下文长度为 32768, 使用了 2-way tensor parallelism 和 4-way context parallelism.&lt;/li>
&lt;/ol>
&lt;h3 id="sft">SFT
&lt;/h3>&lt;h4 id="sft-data">SFT Data
&lt;/h4>&lt;p>数据包括 long CoT reasoning 数据，来让模型以标准格式输出 multi-step solution.&lt;/p>
&lt;p>数据包括 verifiable tasks 以及 non-verifiable tasks, 覆盖中英两种语言。作者过滤了非常简单和非常困难的样本。&lt;/p>
&lt;p>数据格式如下&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;lt;think&amp;gt; {think_content} &amp;lt;/think&amp;gt; &amp;lt;answer&amp;gt; {answer_content} &amp;lt;/answer&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>对于 verifiable tasks, 输出格式为&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;lt;answer&amp;gt; &amp;lt;|begin_of_box|&amp;gt; {answer_content} &amp;lt;|end_of_box|&amp;gt; &amp;lt;/answer&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>数据清洗策略包括：&lt;/p>
&lt;ol>
&lt;li>严格遵守格式要求&lt;/li>
&lt;li>reasoning style 不一致或者有噪声&lt;/li>
&lt;li>混合语言输出。&lt;/li>
&lt;/ol>
&lt;p>作者还使用 RL checkpoints 来生成一部分高质量数据，加入到 SFT 阶段的数据集里面。&lt;/p>
&lt;h4 id="sft-training">SFT Training
&lt;/h4>&lt;p>全参数微调。上下文长度为 32768， batch size 为 32.数据包括 long-form reasoning data 以及纯文本数据，包括 math, multi-turn conversation, agent planning 以及 instruction following 等.&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Observation
作者发现，在 SFT 阶段即使是使用带噪声的 reasoning data，模型在 RL 阶段也能继续训练。也就是说，不完美的 reasoning trace 也能提供 guidance. 但是，使用更高质量的数据可以让模型 RL 阶段的训练更稳定且表现更好。&lt;/p>
&lt;/blockquote>
&lt;h3 id="rl">RL
&lt;/h3>&lt;p>与 &lt;a class="link" href="https://maosong.website/p/notes-on-seed1.5-vl/" target="_blank" rel="noopener"
>Seed1.5-VL&lt;/a> 一样，作者在 RL 阶段结合了 RLVR 和 RLHF 两个训练目标。任务包括 STEM problem solving (such as mathematics, physics, chemistry), grounding, optical character recognition (OCR), video understanding, GUI agents, chart and document understanding, logical reasoning, and instruction following.&lt;/p>
&lt;p>&lt;strong>Data&lt;/strong>
作者首先构建了一个任务集合，然后基于这些任务，来过滤或者生成对应的 QA pair, 接下来作者基于难度和质量来过滤，最后作者在每个 domain 上进行 RL 的训练来保证数据的有效性。&lt;/p>
&lt;p>&lt;strong>Reward modelling&lt;/strong>
作者构建了一个 reward system, 用于给不同 domain 的任务进行奖励。为了探究不同 domain 的 reward 对模型整体表现的影响，作者设计了一个 low-quality reward system，实验结果如下，可以看到模型在训练后期出现了 collapse 或者 reward hacking 的情况。&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_reward_ablation.png"
width="1778"
height="876"
srcset="https://maosong2022.github.io/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_reward_ablation_hu1940628557481340942.png 480w, https://maosong2022.github.io/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_reward_ablation_hu12923493226588456991.png 1024w"
loading="lazy"
alt="GLM-4.1V-Thinking reward ablation"
class="gallery-image"
data-flex-grow="202"
data-flex-basis="487px"
>&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Observation
作者发现，不同任务的不同信号奖励会对模型最终表现产生巨大影响。&lt;/p>
&lt;/blockquote>
&lt;p>在使用 LLM 提取模型回答的 answer 的时候，作者要求模型的最终答案为 &lt;code>&amp;lt;|begin_of_box|&amp;gt; {answer_content} &amp;lt;|end_of_box|&amp;gt;&lt;/code> 这种形式，避免大语言模型提取出错。&lt;/p>
&lt;p>作者构建的 reward system 包含如下模块：&lt;/p>
&lt;ol>
&lt;li>shared verification functions: 包括格式检查等&lt;/li>
&lt;li>domain specific modules: 与 domain 相关的模块&lt;/li>
&lt;li>unit testing: 观测 domain 输出的分布，然后基于输出的分布来调整 reward logic&lt;/li>
&lt;/ol>
&lt;p>最终 reward 的 design 如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_reward_design.png"
width="1812"
height="712"
srcset="https://maosong2022.github.io/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_reward_design_hu7442997355963364551.png 480w, https://maosong2022.github.io/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_reward_design_hu4476507324978246593.png 1024w"
loading="lazy"
alt="GLM-4.1V-Thinking reward design"
class="gallery-image"
data-flex-grow="254"
data-flex-basis="610px"
>&lt;/p>
&lt;p>&lt;strong>RLCS&lt;/strong>
作者使用了 GRPO 来进行优化训练。作者发现，仅需 200 training steps 模型在一半以上问题的准确率就达到了 $90%$.&lt;/p>
&lt;p>为了提升模型的训练效率，作者提出了 Reinforcement Learning with Curriculum Sampling (RLCS), 也就是将课程学习与 RL 结合起来。作者基于模型训练情况动态调整训练样本的难度，来保证每个样本对模型训练的提升都是最大的。&lt;/p>
&lt;p>在 RLCS 训练之前，作者先使用模型评测得到每个样本的 pass@k, 然后根据结果将样本进行分类。在训练过程中，作者记录每个样本的 pass@k， 然后动态更新其分类结果。在采样的时候，作者基于难度来对样本进行加权，来降低太简单或者太难样本的采样概率。&lt;/p>
&lt;p>作者还提出了一些提高 RL 表现的方法：&lt;/p>
&lt;ol>
&lt;li>Larger batch size: 使用更大的 batch size 效果更好&lt;/li>
&lt;li>Dynamic sampling expansion via ratio EMA: 作者定义了 naive 样本和高质量样本的比例，然后根据这个比例进行采样，来提升整体的采样效率。&lt;/li>
&lt;li>Force answering: 与 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a> 一样，对于比较难的问题，当输出的 token 数过长时，作者对模型输出进行截断，然后让模型基于已有思考过程直接输出结果。&lt;/li>
&lt;li>Discard KL loss: 与 DAPO 一样，作者也移除了 KL divergence loss&lt;/li>
&lt;li>Clip-higher: 与 DAPO 一样，作者通过修改超参数来提高模型的探索能力&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>[!tip] Observation&lt;/p>
&lt;ol>
&lt;li>RL 阶段的表现与 cold-start SFT 的表现并不完全相关。作者发现，cold-start SFT 取得更好的表现并不一定保证 RL 的表现就更好。&lt;/li>
&lt;li>RL 阶段各个 domain 数据的影响是正交的，这与 cold-start SFT 阶段的结果不同。&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>作者还提出了一些提高训练稳定性的方法：&lt;/p>
&lt;ol>
&lt;li>提高 cold-start SFT 阶段数据的质量&lt;/li>
&lt;li>移除 KL divergence loss&lt;/li>
&lt;li>使用 top-p 为 1，而不是 0.9 (如 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen-llm/" target="_blank" rel="noopener"
>Qwen-LLM&lt;/a> 中的设置)，top-p 为 1 可以 cover 整个 vocabulary，避免某些 token 没有参与训练&lt;/li>
&lt;li>per-sample loss 和 per-token loss 没有显著区别，但是 per-sample loss 稳定性更高&lt;/li>
&lt;li>在 cold-start SFT 阶段让模型学会格式要求，而不是在 RL 阶段加入 format reward&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Infra&lt;/strong>
在 infra 方面，作者做了如下改进：&lt;/p>
&lt;ol>
&lt;li>Load balancing of sequence lengths across DP ranks. 平衡每个 rank 的 sequensequence length 以及 compute load&lt;/li>
&lt;li>Intra-rank training with sequence packing and gradient accumulation. 将 sequence packing 和 gradient accumulation 结合起来&lt;/li>
&lt;li>Sample packing and reorganization within DP ranks. data packing&lt;/li>
&lt;li>Dynamic sampling expansion via ratio EMA. 平衡 naive 样本和高质量样本之间的比例&lt;/li>
&lt;/ol>
&lt;h2 id="evaluation">Evaluation
&lt;/h2>&lt;h3 id="performance">Performance
&lt;/h3>&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_performance.png"
width="1066"
height="1446"
srcset="https://maosong2022.github.io/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_performance_hu4663931888132339157.png 480w, https://maosong2022.github.io/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_performance_hu2656097992392796714.png 1024w"
loading="lazy"
alt="Performance of GLM4.1V Thinking"
class="gallery-image"
data-flex-grow="73"
data-flex-basis="176px"
>&lt;/p>
&lt;p>可以看到，GLM-4.1V-Thinking 在多个 benchmark 上都达到了 SOTA.&lt;/p>
&lt;h3 id="ablation-study">Ablation Study
&lt;/h3>&lt;p>作者评估了一下不同 domain 的 RL 训练对模型整体表现的影响。作者使用不同的数据来进行训练，然后分析结果的相关性，结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_cross_domain_generalization.png"
width="1226"
height="962"
srcset="https://maosong2022.github.io/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_cross_domain_generalization_hu16221901218333137952.png 480w, https://maosong2022.github.io/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_cross_domain_generalization_hu16525324023796861196.png 1024w"
loading="lazy"
alt="Cross-domain generalization in reinforcement learning"
class="gallery-image"
data-flex-grow="127"
data-flex-basis="305px"
>&lt;/p>
&lt;p>实验结果发现：&lt;/p>
&lt;ol>
&lt;li>在某个 domain 上的训练会提高模型在其他 domain 上的表现&lt;/li>
&lt;li>联合多个 domain 进行训练最终的表现会更好&lt;/li>
&lt;/ol>
&lt;p>作者还发现，不同的任务之间存在关联，如 GUI-agent 和 grounding 这两个任务是高度相关的。OCR &amp;amp; Chart 以及 GUI-agent 也是高度相关的。&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>在本文中，作者提出了 GLM-4.1V-Thinking, 一个 9B 的通用多模态 reasoning model, 作者通过构建高质量的数据，多阶段的训练，让模型在 reasoning 和 non-reasoning 任务上均超过了已有的 SOTA 模型的表现&lt;/p>
&lt;p>作者认为，模型有以下局限：&lt;/p>
&lt;ol>
&lt;li>模型并没有提升 reasoning 的质量，在某些情况下，模型结果正确，但是中间过程错误。作者分析这是因为目前只有针对结果的奖励机制。因此，未来需要能够评估中间过程的 reward model&lt;/li>
&lt;li>RL 的训练具有不稳定性，作者发现模型对设置比较敏感，这对 RL 训练的 scaling 提出了挑战。&lt;/li>
&lt;li>模型在复杂场景下表现依旧不太好，比如图片中有物体遮挡的情况，这些感知误差也会影响最终的 reasoning 表现。作者认为如何同时提高 perception 和 reasoning 的表现是一个需要研究的课题。&lt;/li>
&lt;/ol>
&lt;p>未来的工作有：&lt;/p>
&lt;ol>
&lt;li>如何构建更好的 reward 机制，来同时评估结果和中间过程。从而防止模型 reward hacking&lt;/li>
&lt;li>如何基于 multimodal training 来提升模型在纯文本任务上的表现。探究 visual reasoning 和 text reasoning 如何互相促进也是一个课题&lt;/li>
&lt;li>如何更好评估模型的表现，即如何更好评估模型的 failure modes, 比如幻觉，short reasoning 等&lt;/li>
&lt;/ol>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2507.01006" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://huggingface.co/THUDM/GLM-4.1V-9B-Thinking" target="_blank" rel="noopener"
>Huggingface&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="appendix">Appendix
&lt;/h2>&lt;p>GLM-4.1V-Thinking 的 patch merger 代码&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Glm4vVisionPatchMerger&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">context_dim&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_act&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">bool&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="fm">__init__&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">bias&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">post_projection_norm&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">LayerNorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">gate_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">context_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">bias&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">up_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">context_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">bias&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">down_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">context_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">bias&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">act1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">GELU&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">act_fn&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ACT2FN&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">hidden_act&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_state&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_state&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_state&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_state&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">act1&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">post_projection_norm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_state&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">down_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">act_fn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">gate_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_state&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">up_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_state&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Glm4vVisionModel&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">downsample&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Conv2d&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">in_channels&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">out_channels&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">out_hidden_size&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">kernel_size&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">spatial_merge_size&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">stride&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">spatial_merge_size&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">merger&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Glm4vVisionPatchMerger&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">out_hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">context_dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">intermediate_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_act&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_act&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">...&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">spatial_merge_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">spatial_merge_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">permute&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">downsample&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">out_hidden_size&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">merger&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>Notes on Qwen2.5-1M</title><link>https://maosong2022.github.io/p/notes-on-qwen2.5-1m/</link><pubDate>Sat, 12 Jul 2025 11:00:47 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-qwen2.5-1m/</guid><description>&lt;p>Qwen 在 2025 年 1 月提出了 Qwen2.5-1M，一个拥有 1M 上下文长度的大语言模型系列。包含 7B，14B 两个开源模型以及 API 模型 Qwen2.5-Turbo. 主要改进方法包括长上下文数据合成，渐进式预训练以及多阶段 post-training 等。作者还对 inference 进行了优化，提高了 inference 的效率。&lt;/p>
&lt;h2 id="method">Method
&lt;/h2>&lt;p>架构上，Qwen2.5-1M 与 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5/" target="_blank" rel="noopener"
>Qwen2.5&lt;/a> 的架构一致，Qwen2.5-1M 包括 7B，14B 两个 size，还包括一个基于 MoE 的 API 模型 Qwen2.5-Turbo，不同的点在于，Qwen2.5-1M 的上下文长度为 1M，最大生成长度为 8K&lt;/p>
&lt;h3 id="pretraining">Pretraining
&lt;/h3>&lt;p>&lt;strong>Data&lt;/strong>
作者首先从 CC, arxiv, book, code repositories 等 domain 收集了原始数据。但是，作者发现，原始数据的局部相关性强，但是全局相关性弱。因此，作者基于原始数据进行了增广，来提高数据的长上下文依赖关系。具体有三个任务：&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Fill in the middle&lt;/strong>: FIM 是 openAI 提出来的一个做法，核心思想就是将填空类问题转化为 next-token-prediction 的问题。通过这种方式，作者希望提升模型理解长上下文依赖的能力&lt;/li>
&lt;li>&lt;strong>Keyword-based and Position-based retrieval&lt;/strong>: 基于 keywords 或者 position 来找到对应的 paragraph，这个任务的目的是提高模型识别并连接相关信息的能力&lt;/li>
&lt;li>&lt;strong>Paragraph Reordering&lt;/strong>: 对输入的 paragraphs 进行随机打乱，然后要求模型重新组织段落的关系&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Training&lt;/strong>
作者将训练拆分为了 5 个 stage：&lt;/p>
&lt;ol>
&lt;li>stage 1 和 stage 2 与 Qwen2.5 的训练过程一致，stage 1 的上下文长度为 4096，stage 2 的上下文长度为 32768, 训练时，作者使用了 ABF 技巧来将 RoPE 的 base frequency 从 10,000 调整到了 1,000,000.&lt;/li>
&lt;li>stage 3, stage 4 和 stage 5 分别将模型的上下文长度扩展到了 65,536 tokens, 131,072 tokens 以及 262,144 tokens, 对应的 RoPE base frequency 分别为 1M, 5M 和 10M. 训练时，作者使用了 75% 的长文本和 25% 的短文本，这样可以保证模型在短文本任务上的表现&lt;/li>
&lt;/ol>
&lt;p>最后，作者在评估了一下每个 stage 的表现，结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Training Length&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>RULER&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Avg.&lt;/td>
&lt;td>4K&lt;/td>
&lt;td>8K&lt;/td>
&lt;td>16K&lt;/td>
&lt;td>32K&lt;/td>
&lt;td>64K&lt;/td>
&lt;td>128K&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>32,768 Tokens&lt;/td>
&lt;td>82.3&lt;/td>
&lt;td>96.8&lt;/td>
&lt;td>94.7&lt;/td>
&lt;td>95.9&lt;/td>
&lt;td>92.2&lt;/td>
&lt;td>76.4&lt;/td>
&lt;td>37.6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>65,536 Tokens&lt;/td>
&lt;td>86.8&lt;/td>
&lt;td>96.5&lt;/td>
&lt;td>95.5&lt;/td>
&lt;td>93.6&lt;/td>
&lt;td>92.5&lt;/td>
&lt;td>86.7&lt;/td>
&lt;td>56.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>131,072 Tokens&lt;/td>
&lt;td>92.5&lt;/td>
&lt;td>96.5&lt;/td>
&lt;td>95.9&lt;/td>
&lt;td>93.0&lt;/td>
&lt;td>92.6&lt;/td>
&lt;td>93.0&lt;/td>
&lt;td>83.8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>262,144 Tokens&lt;/td>
&lt;td>92.7&lt;/td>
&lt;td>95.6&lt;/td>
&lt;td>93.8&lt;/td>
&lt;td>93.1&lt;/td>
&lt;td>94.1&lt;/td>
&lt;td>91.9&lt;/td>
&lt;td>87.6&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，随着训练的上下文长度的提升，模型在更长上下文下的能力也有提升，说明模型具有一定的泛化性。&lt;/p>
&lt;h3 id="post-training">Post-training
&lt;/h3>&lt;p>Post-training 阶段与 Qwen2.5 一样，也分为了 SFT 和 RL 两个阶段。&lt;/p>
&lt;p>在 SFT 阶段，作者从预训练预料中选择了一部分长文档的片段，然后让 Qwen2.5 来生成对应的 query，query 类型包括 summarization, information retrieval, multi-hop QA 等任务。接下来，作者使用 Qwen-Agent 框架基于全文来回答这些问题。最后，作者基于生成的 query，全文，以及模型产生的回答作为训练数据。&lt;/p>
&lt;p>SFT 训练时，作者拆分为了两个 stage。 stage 1 作者在 32768 的上下文上进行训练，来提高模型短文本回答能力。第二个阶段，作者混合了 262,144 和 32768 上下文长度的训练数据。&lt;/p>
&lt;p>RL 训练时，与 Qwen2.5 不一样的是，作者进使用了 offline RL，也就是 DPO。作者仅在 8192 的上下文长度上面进行训练。作者认为，长上下文的 RL 训练是非常耗时的，并且作者发现，短文本上进行 RL 的训练之后，模型在长文本上的表现也能得到提升。结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Before RL&lt;/th>
&lt;th>After RL&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Qwen2.5-7B-Instruct-1M&lt;/td>
&lt;td>7.32&lt;/td>
&lt;td>8.08 (+0.75)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen2.5-14B-Instruct-1M&lt;/td>
&lt;td>8.56&lt;/td>
&lt;td>8.76 (+0.20)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen2.5-Turbo&lt;/td>
&lt;td>7.60&lt;/td>
&lt;td>8.34 (+0.74)&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="inference">Inference
&lt;/h2>&lt;p>前面是训练部分的优化，主要是提升模型的上下文能力。接下来，作者详细介绍了如何在 Inference 阶段提升整体的推理效率和减少内存占用。&lt;/p>
&lt;h3 id="length-extrapolation">Length Extrapolation
&lt;/h3>&lt;p>与 Qwen2.5 一样，Qwen2.5-1M 也是用了 &lt;a class="link" href="https://maosong.website/p/dual-chunk-attention/" target="_blank" rel="noopener"
>Dual Chunk Attention&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/notes-on-yarn/" target="_blank" rel="noopener"
>YARN&lt;/a> 来在推理阶段扩展模型的上下文长度，作者做了如下实验，来对比 Qwen2.5, Qwen2.5-1M 加上 DCA 之后的影响&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-qwen2.5-1m/Qwen2_5_1M_DCA.png"
width="1351"
height="764"
srcset="https://maosong2022.github.io/p/notes-on-qwen2.5-1m/Qwen2_5_1M_DCA_hu14575896826361884438.png 480w, https://maosong2022.github.io/p/notes-on-qwen2.5-1m/Qwen2_5_1M_DCA_hu2353376534919284344.png 1024w"
loading="lazy"
alt="Qwen2.5 performance of DCA"
class="gallery-image"
data-flex-grow="176"
data-flex-basis="424px"
>&lt;/p>
&lt;p>结果显示，Qwen2.5-1M 的表现比 Qwen2.5 更好，并且加上 DCA 之后，两者的表现都有进一步的提升。&lt;/p>
&lt;h3 id="sparse-attention">Sparse Attention
&lt;/h3>&lt;p>为了进一步提高计算效率，作者基于 MInference 来加速 perfilling phase. 并结合了 前面的技巧来防止模型性能下降。&lt;/p>
&lt;p>&lt;strong>MInference&lt;/strong>
MInference 的主要思想就是在长上下文中，有一些 critical token 对最终结果的影响是更大的。因此我们可以识别出这些 critical token 并只计算这些 token 对应的 attention score. 这些 critical token 对应的 pattern 被称为 Vertical-Slash pattern.&lt;/p>
&lt;p>为了识别出这个 pattern，作者首先进行离线搜索，来决定最优的 configuration。这个 configuration 决定了 attention 应该如何计算。在 Inference 阶段，MInference 首先计算最后一个 query 和前面所有 key 的 attention，然后基于 configuration 来动态选择 pattern。通过 MInference，我们可以降低 10 倍以上的内存和算力消耗。&lt;/p>
&lt;p>&lt;strong>Integrating with Chunked prefill&lt;/strong>
但是 MInference 的问题在于，整个 sequence 是并行处理的，这会导致内存占用持续上升。为了解决这个问题，作者提出了 chunked prefilling 的技巧，来降低 VRAM 的消耗。具体做法就是，将整个 sequence 分为若干个 chunk，然后每个 chunk 里，选取最后 64 个 token 作为 query，在每个 chunk 中分别识别出 critical token，这样就降低了 MInference 的内存占用&lt;/p>
&lt;p>接下来，作者在集成 DCA 的时候，发现性能有所下降。作者认为，这是由于 DCA 的 position id 信息不连续所导致的，为了解决这个问题，作者在选择 critical token 的时候，使用了连续版的 position id 信息。在最终推理的时候，还是使用 DCA 本身的位置信息。&lt;/p>
&lt;p>&lt;strong>Sparsity refinement&lt;/strong>
前面提到，MInference 需要先进行离线搜索决定最优的 configuration，但是对于 1M token 的上下文，这个过程还是非常耗时的。因此，作者构建了一个加速离线搜索的方法，具体做法就是定义两个 attention score，一个是 full attention, 另一个是 sparse attention， 然后计算两者的差值，如果说相差比较小，则说明 critical token 抓住了全局信息，这个配置是有效的。其公式定义如下：&lt;/p>
$$
\mathrm{Attention\_Recall} = \exp\left(\log\sum_{0\leq j\leq i}\exp \left(\frac{q^Tk_j}{\sqrt {d}}\right) - \log\sum_{j\in\mathcal{critical}}\exp \left(\frac{q^Tk_j}{\sqrt {d}}\right)\right)
$$&lt;p>Attention Recall 越高，说明选取的 critical token 越好，其 configuration 也就越好。&lt;/p>
&lt;p>作者进一步分析了 sparse attention 对 accuracy 的影响，结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-qwen2.5-1m/Qwen2_5_1M_sparsity_config_performance.png"
width="1151"
height="836"
srcset="https://maosong2022.github.io/p/notes-on-qwen2.5-1m/Qwen2_5_1M_sparsity_config_performance_hu8216849468217583274.png 480w, https://maosong2022.github.io/p/notes-on-qwen2.5-1m/Qwen2_5_1M_sparsity_config_performance_hu11779227780761532015.png 1024w"
loading="lazy"
alt="Qwen2.5 performance on sparsity refinement"
class="gallery-image"
data-flex-grow="137"
data-flex-basis="330px"
>&lt;/p>
&lt;p>可以看到，仅使用 MInference 会导致模型性能 下降，但是加入 refinement 之后，模型的表现基本上和 full attention 差不太多。&lt;/p>
&lt;h3 id="inference-engine">Inference Engine
&lt;/h3>&lt;p>&lt;strong>Kernel Optimization&lt;/strong>
作者还对 inference engine 进行了优化，作者使用 BladeLLM 作为 Qwen2.5-1M 的推理引擎。&lt;/p>
&lt;p>作者主要做了两点优化，第一是对 sparse attention kernel 进行了优化，提高了 sparse attention 的计算效率，结果发现，在 1M 的上下文下，BladeLLM 比 &lt;a class="link" href="https://maosong.website/p/notes-on-flashattention/" target="_blank" rel="noopener"
>flash attention&lt;/a> 要快 27.8 倍。&lt;/p>
&lt;p>第二是针对 MoE kernel 的优化。作者发现，decoding 的表现是与 memory access speed 相关的。具体来讲，当 batch size 超过 32 之后，获取模型参数成了效率的瓶颈。因此，作者使用了一系列技巧来提高 memory access 的效率&lt;/p>
&lt;p>&lt;strong>Pipeline parallelism&lt;/strong>
作者还对 Chunked pipeline parallelism 进行了优化，Chunked pipeline parallelism 的问题在于，在长上下文的场景下，不同长度的 chunk 会对 attention 的计算时间产生很大影响。不同的计算时间会产生 pipeline bubbles.&lt;/p>
&lt;p>BladeLLm 使用了 Dynamic Chunked pipeline parallelism 来解决这个问题，该方法通过计算复杂度来调整每个 chunk 的大小，进而使得最终的处理时间尽可能一致&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-qwen2.5-1m/Qwen2_5_1M_PP.png"
width="1366"
height="224"
srcset="https://maosong2022.github.io/p/notes-on-qwen2.5-1m/Qwen2_5_1M_PP_hu12616030854955023044.png 480w, https://maosong2022.github.io/p/notes-on-qwen2.5-1m/Qwen2_5_1M_PP_hu3399618306930440628.png 1024w"
loading="lazy"
alt="Qwen2.5-1M DCPP"
class="gallery-image"
data-flex-grow="609"
data-flex-basis="1463px"
>&lt;/p>
&lt;p>&lt;strong>Scheduling&lt;/strong>
作者还在 Scheduling 上进行了优化，已有的推理引擎主要分为四个模块：API server, scheduler, model runner 以及 decoder&lt;/p>
&lt;p>已有方法的问题在于，non-GPU 的操作会占用大量时间，导致 GPU 利用率非常低。因此，作者在 BladeLLM 中进行了改进，使用了 Totally Asynchronous Generator (TAG) 的架构，主要有：&lt;/p>
&lt;ol>
&lt;li>Scheduler：动态分配 KV cache，类似于 speculative sampling, 而不必等前面的结果完成&lt;/li>
&lt;li>Runner: 基于 Scheduler 分配的任务直接进行处理，处理完之后直接处理下一个任务&lt;/li>
&lt;li>Decoder：基于 token id，进行解码，然后发送给前端的 API server&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-qwen2.5-1m/Qwen2_5_1M_PP.png"
width="1366"
height="224"
srcset="https://maosong2022.github.io/p/notes-on-qwen2.5-1m/Qwen2_5_1M_PP_hu12616030854955023044.png 480w, https://maosong2022.github.io/p/notes-on-qwen2.5-1m/Qwen2_5_1M_PP_hu3399618306930440628.png 1024w"
loading="lazy"
alt="Qwen2.5-1M scheduling"
class="gallery-image"
data-flex-grow="609"
data-flex-basis="1463px"
>&lt;/p>
&lt;h2 id="evaluation">Evaluation
&lt;/h2>&lt;p>作者主要在三个 benchmark 上进行了评测：&lt;/p>
&lt;ol>
&lt;li>RULER: RULER 是 Needle-in-ahaystack 任务的一个扩展笨笨，其要求模型从不相关的上下文中找到多个 &amp;ldquo;needles&amp;rdquo; 或者回答多个问题，数据最长为 128K tokens.&lt;/li>
&lt;li>LV-Eval: LV-Eval 要求模型从上文本中同时理解多个 evidence fragments，数据最长为 256K tokens&lt;/li>
&lt;li>Longbench-Chat: 评估模型在长上下文下与人类偏好对齐的程度，数据最长为 100K tokens&lt;/li>
&lt;/ol>
&lt;p>Qwen2.5-1M 与 Qwen2.5 的对比表现如下&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-qwen2.5-1m/Qwen2_5_1M_RULER_performance.png"
width="1340"
height="652"
srcset="https://maosong2022.github.io/p/notes-on-qwen2.5-1m/Qwen2_5_1M_RULER_performance_hu2041726087274462421.png 480w, https://maosong2022.github.io/p/notes-on-qwen2.5-1m/Qwen2_5_1M_RULER_performance_hu5806237305905427717.png 1024w"
loading="lazy"
alt="Qwen2.5-1M perofermence on RULER"
class="gallery-image"
data-flex-grow="205"
data-flex-basis="493px"
>&lt;/p>
&lt;p>可以看到，相比于 Qwen2.5，Qwen2.5 模型的表现有了大幅度的提升。&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>在本文中，作者提出了 Qwen2.5-1M 系列大语言模型，包括 7B，14B 两个 size，以及一个 MoE 架构的 API 模型 Qwen2.5-Turbo。作者在训练和推理两方面进行了改进，最终将模型的上下文长度扩展到了 1M。从现在的角度来看，不管是 Reasoning model 还是 agent 的训练都依赖 long Context 作为基础能力。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2501.15383" target="_blank" rel="noopener"
>Qwen2.5-1M Technical Report&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Qwen2.5</title><link>https://maosong2022.github.io/p/notes-on-qwen2.5/</link><pubDate>Sat, 12 Jul 2025 10:51:42 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-qwen2.5/</guid><description>&lt;p>2024 年 12 月 Qwen 发布了 Qwen 2.5 系列大语言模型，包括 7 个 dense 模型以及两个 MoE 模型，Qwen2.5 在 pre-training 阶段使用了 18T token 进行。在 post-training 阶段使用了 1M 的样本，还使用了 DPO 以及 GRPO 来进行 RL 的训练&lt;/p>
&lt;p>Qwen2.5 主要在以下方面进行了改进&lt;/p>
&lt;ol>
&lt;li>模型方面，提供了更多的 size，&lt;a class="link" href="https://maosong.website/p/notes-on-qwen2/" target="_blank" rel="noopener"
>Qwen2&lt;/a> 中只有 0.5B, 1.5B, 7B, 72B 四个 size, 在 Qwen2.5 中，加入了 3B, 14B 和 32B 三个 size 的模型&lt;/li>
&lt;li>数据方面，pre-training 阶段使用了 18T 的 token， post-training 阶段使用了 1M 的样本&lt;/li>
&lt;li>功能方面，Qwen2.5 支持更长的上下文长度（8K），支持结构化输入和输出，拥有更强的工具调用能力。&lt;/li>
&lt;/ol>
&lt;h2 id="method">Method
&lt;/h2>&lt;h3 id="architecture">Architecture
&lt;/h3>&lt;p>模型架构这方面，Qwen2.5 和 Qwen2 的模型架构是一致的，tokenizer 页没有太大变化。为了支持工具调用，作者额外增加了 18 个 control token&lt;/p>
&lt;h3 id="pre-training">Pre-training
&lt;/h3>&lt;p>&lt;strong>data&lt;/strong>
Qwen2.5 从以下方面提高了预训练数据的质量&lt;/p>
&lt;ol>
&lt;li>Better data filtering: 使用 Qwen2-Instruct 来过滤掉质量的数据，然后从多维度对训练数据进行打分，从而提高数据的质量&lt;/li>
&lt;li>Better math and code data: 加入了 Qwen2.5 Math 以及 Qwen2.5 Coder 的训练数据来提高模型的数学和代码能力&lt;/li>
&lt;li>Better synthetic data: 作者使用 Qwen2-72B-Instruct 以及 Qwen2-Math-72B-Instruct 来合成 math, code, knowledge domain 的数据，然后通过过滤以及 Qwen2-Math-RM-72B 来提高数据的质量&lt;/li>
&lt;li>Better data mixture: 作者使用 Qwen2-Instruct 来分类，然后平衡不同 domain 的数据分布。作者发现 e-commerce, social media 以及 entertainment 的数据重复性高，且大多都是机器生成的。而 technology, science 以及 academic research 等 domain 的数据质量更高。作者对不同 domain 的数据进行了上采样或者下采样。&lt;/li>
&lt;/ol>
&lt;p>基于这个过程，作者一共收集了&lt;strong>18T&lt;/strong> tokens&lt;/p>
&lt;p>&lt;strong>Hyper-parameters&lt;/strong>
作者构建了针对超参数的 scaling law，即决定最优的训练超参数如 batch size, learning rate 等&lt;/p>
&lt;p>作者通过实验得到了 model size $N$ 以及 pre-training data size $D$ 与 learning rate $\mu_{opt}$ 和 batch size $B_{opt}$ 之间的关系。&lt;/p>
&lt;p>&lt;strong>Long context pre-training&lt;/strong>
为了提升模型的上下文长度，作者将 pre-training 拆分为两个 stage，第一个 stage 的上下文长度为 4096， 第二个 stage，作者将上下文长度从 4096 扩展到 32768.&lt;/p>
&lt;p>在提升模型上下文过程中，作者使用 ABF 技巧将 &lt;a class="link" href="https://maosong.website/p/notes-on-position-encoding/" target="_blank" rel="noopener"
>Position Encoding&lt;/a> 的 base frequency 从 10,000 提升到了 1,000,000.&lt;/p>
&lt;p>对于 Qwen2-5-Turbo，作者实现了渐进式上下文长度扩展策略，模型上下文长度扩展经历四个阶段：32768, 65536, 131072 到最终的 262,144. 此时，RoPE 的 base frequency 为 10,000,000. 在训练的每个阶段，作者都使用了 40% 的长文本以及 60% 的短文本，以保证在扩展模型上下文长度的同时，还能保持模型在不同上下文长度下的表现。&lt;/p>
&lt;p>为了提高模型在 inference 时的长上下文表现，作者使用了 &lt;a class="link" href="https://maosong.website/p/dual-chunk-attention/" target="_blank" rel="noopener"
>Dual Chunk Attention&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/notes-on-yarn/" target="_blank" rel="noopener"
>YARN&lt;/a> 两个技巧。通过这两个技巧，作者将 Qwen2.5-Turbo 的上锈阿文扩展到了 1M，将其他模型的上下文长度扩展到了 131072.&lt;/p>
&lt;h3 id="post-training">Post-training
&lt;/h3>&lt;p>Qwen2.5 的 post-training 分为两个大的 stage: SFT 和 RL，其中 RL 又分为两个小的 stage，分别是 offline RL 和 online RL&lt;/p>
&lt;p>在 SFT 阶段，作者主要做了以下改进：&lt;/p>
&lt;ol>
&lt;li>Long-sequence generation: 作者将 Qwen2.5 的输出长度提升到了 8192, 为了扩展模型输出的长度，作者构建了 Long-response 数据集，然后基于 back-translation 来生成对应的 query，最后使用 Qwen2 来过滤低质量的数据&lt;/li>
&lt;li>Math: 作者在 SFT 阶段加入了 Qwen2.5-Math 的 CoT 数据，包括公开数据集，K12 问题集一集合成数据等。作者通过 rejection sampling 以及 annotated answers 来生成 CoT 过程&lt;/li>
&lt;li>Code: 作者加入了 Qwen2.5-Coder 的 SFT 数据，作者基于多个 agent 来生成多样化高质量的 Instruction, 然后还从 code-related QA website 以及 Github 上获取数据来扩展数据集。对于最终的数据，作者使用了 sandbox 来保证代码的质量&lt;/li>
&lt;li>Instruction following: 作者构建了一个基于 code 的验证框架，让 LLM 同时生成 Instruction 和对应的验证代码，验证的单元测试。最后，通过 rejection sampling 来得到最终的数据集&lt;/li>
&lt;li>Structured Data Understanding: 作者还构建了针对 tabular QA, fact verification, error correction 以及 structured understanding 等数据集。作者在回答中加入 CoT，作者提高了模型对 structured data 的理解能力&lt;/li>
&lt;li>Logical Reasoning: 作者构建了 70,000 个不同 domain 的 query，有多种格式，覆盖了 analogical reasoning, causal reasoning 等 domain&lt;/li>
&lt;li>Cross-Lingual Transfer: 作者使用了一个翻译模型，来将 Instruction 转换到 low-resource language 上，进而提高模型在对应语种上的表现&lt;/li>
&lt;li>Robust System Instruction: 作者构建了不同的 system prompt 用于提升 system prompt 的多样性。作者发现，使用不同的 system prompt 可以减少模型的 variance, 提高模型的 robustness.&lt;/li>
&lt;li>Response Filtering: 作者使用了多种自动化标注方法来保证最终 response 的质量&lt;/li>
&lt;/ol>
&lt;p>最终，作者一共收集到 &lt;strong>1M&lt;/strong> 的 SFT 样本，模型训练了两个 epoch&lt;/p>
&lt;p>在 RL 阶段，作者首先基于 SFT model 来进行采样，然后将高质量的回答作为正样本，低质量的回答作为负样本，通过这个过程，一共采集到了&lt;strong>150K&lt;/strong>的样本。最后，作者使用 DPO 来进行训练。&lt;/p>
&lt;p>然后，作者进行了 online stage 的 RL 训练，这一阶段主要是对齐模型与人类的价值观。这一阶段的数据包括公开数据集，私有数据集。作者使用不同的 checkpoint 来进行采样，然后作者使用 GRPO 来进行训练.&lt;/p>
&lt;h3 id="long-cotnext-fine-tuning">Long Cotnext Fine-tuning
&lt;/h3>&lt;p>作者还针对 Qwen2.5-Turbo 做了额外的 post-training, 来进一步提高其在长上下文下的表现。&lt;/p>
&lt;p>在 SFT 阶段，作者使用了一个两阶段方法，第一阶段仅在短文本上进行训练（上下文长度为 32768），这一阶段的训练数据与其他 Qwen2.5 的模型训练数据相同。第二个阶段，作者混合了短文本和长文本（262144）来进行训练，来提高模型在长上下文情景下的指令跟随能力&lt;/p>
&lt;p>在 RL 阶段，作者使用了和其他 Qwen2.5 模型相同的训练策略。作者认为：&lt;/p>
&lt;ol>
&lt;li>长上下文下训练 RL 代价很大&lt;/li>
&lt;li>reward model 更偏向于长文本&lt;/li>
&lt;li>RL 尽管只在短文本上进行训练，其还是可以提高模型在长上下文下的表现。&lt;/li>
&lt;/ol>
&lt;h2 id="evaluation">Evaluation
&lt;/h2>&lt;p>我们仅关注 instruction 版本的 72B,32B 和 7B 模型&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-qwen2.5/Qwen2_5_72b_instruct_performance.png"
width="1112"
height="757"
srcset="https://maosong2022.github.io/p/notes-on-qwen2.5/Qwen2_5_72b_instruct_performance_hu5527761188674703842.png 480w, https://maosong2022.github.io/p/notes-on-qwen2.5/Qwen2_5_72b_instruct_performance_hu735810909869176757.png 1024w"
loading="lazy"
alt="Performance of Qwen2.5 72B"
class="gallery-image"
data-flex-grow="146"
data-flex-basis="352px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-qwen2.5/Qwen2_5_32b_instruct_performance.png"
width="1339"
height="744"
srcset="https://maosong2022.github.io/p/notes-on-qwen2.5/Qwen2_5_32b_instruct_performance_hu1011682911616957757.png 480w, https://maosong2022.github.io/p/notes-on-qwen2.5/Qwen2_5_32b_instruct_performance_hu17652966028923421654.png 1024w"
loading="lazy"
alt="Performance of Qwen2.5 32B"
class="gallery-image"
data-flex-grow="179"
data-flex-basis="431px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-qwen2.5/Qwen2_5_7b_instruct_performance.png"
width="845"
height="746"
srcset="https://maosong2022.github.io/p/notes-on-qwen2.5/Qwen2_5_7b_instruct_performance_hu8947856737493882843.png 480w, https://maosong2022.github.io/p/notes-on-qwen2.5/Qwen2_5_7b_instruct_performance_hu14722873842278627618.png 1024w"
loading="lazy"
alt="Performance of Qwen2.5 7B"
class="gallery-image"
data-flex-grow="113"
data-flex-basis="271px"
>&lt;/p>
&lt;p>可以看到，Qwen2.5 72B 模型表现和 LLaMA3.1 405B 表现差不多，其他两个 size 的模型基本上达到了 SOTA&lt;/p>
&lt;p>最后，作者评估了一下 DCA+YaRN v.s. Full attention 的表现，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-qwen2.5/Qwen2_5_long_context_TTFT.png"
width="1078"
height="958"
srcset="https://maosong2022.github.io/p/notes-on-qwen2.5/Qwen2_5_long_context_TTFT_hu13857754061205515316.png 480w, https://maosong2022.github.io/p/notes-on-qwen2.5/Qwen2_5_long_context_TTFT_hu1131685124791015540.png 1024w"
loading="lazy"
alt="TTFT of Qwen2.5 on long context"
class="gallery-image"
data-flex-grow="112"
data-flex-basis="270px"
>&lt;/p>
&lt;p>可以看到，使用 DCA+YaRN 之后，模型的推理效率比 full attention 要快 3-4 倍。&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>在本文中，作者提出了 Qwen2.5 系列大语言模型，包括 7 个 dense 模型以及两个 MoE 模型，作者详细介绍了模型的 pre-training 和 post-training. 评测结果发现 Qwen2.5 模型基本上达到了 SOTA.&lt;/p>
&lt;p>作者认为，未来工作有：&lt;/p>
&lt;ol>
&lt;li>使用更多更多样化的 pre-training 和 post-training 数据&lt;/li>
&lt;li>多模态大模型的构建，特别是 omni-modal&lt;/li>
&lt;li>提高模型的 Reasoning 能力&lt;/li>
&lt;/ol>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2412.15115" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Dual Chunk Attention</title><link>https://maosong2022.github.io/p/dual-chunk-attention/</link><pubDate>Sat, 12 Jul 2025 10:41:12 +0800</pubDate><guid>https://maosong2022.github.io/p/dual-chunk-attention/</guid><description>&lt;p>Dual Chunk Attention (DCA) 由阿里巴巴在 2024 年 9 月份提出，DCA 是一个无需训练的，扩展 LLM 上下文长度的方法，后续，DCA 被应用于 Qwen2, Qwen2.5, Qwen2.5-1M 以及 Qwen3 中，与 &lt;a class="link" href="https://maosong.website/p/notes-on-yarn/" target="_blank" rel="noopener"
>YARN&lt;/a> 一起作为扩展模型上下文的有效手段&lt;/p>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>提升 LLM 上下文长度的方法可以分为两类：一类是 training-free 的，包括 LM-infinite 和 StreamingLLM 等，这些方法以损失 long range dependency 为代价来保持较低的 perplexity。另一类为了保留全局信息，则是通过外插来扩展模型的上下文，主要工作我们在 &lt;a class="link" href="https://maosong.website/p/notes-on-yarn/" target="_blank" rel="noopener"
>YARN&lt;/a> 中已经回顾了。&lt;/p>
&lt;p>第二类方法的问题在于，其依赖训练，在 training-free 的 setting 下，这些方法也会导致 perplexity 的上升&lt;/p>
&lt;p>因此，在本文中，作者就提出了 Dual Chunk Attention (DCA) ，一个无需训练的，扩展 LLM 上下文长度的方法。DCA 的主要做法是将 attention 的计算进行分块，这样就可以提高计算效率。&lt;/p>
&lt;p>通过实验，作者给出了三点关键发现：&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Extrapolation&lt;/strong>： DCA 可以在无需训练的情况下，将 LLM 的上下文提升到 32K，而不导致 Perplexity 大幅度增加&lt;/li>
&lt;li>&lt;strong>Orthogonality&lt;/strong>： DCA 可以和其他方法一起使用，如 &lt;a class="link" href="https://maosong.website/p/notes-on-yarn/" target="_blank" rel="noopener"
>YARN&lt;/a>, 这一点已经在 Qwen2.5-1M 以及 Qwen3 中得到了应用&lt;/li>
&lt;li>&lt;strong>Long Context Understanding&lt;/strong>: DCA 可以在无需训练的情况下，在长上下文设置下，达到已有 SOTA 模型的表现&lt;/li>
&lt;/ol>
&lt;h2 id="preliminary">Preliminary
&lt;/h2>&lt;p>对于一个长度为 $L$ 的 token 序列，我们首先定义对应的 position id 如下&lt;/p>
$$
P_{\mathbf{q}} = [0,1,\dots,L-1],\quad P_{\mathbf{k}} = [0,1,\dots,L-1]
$$&lt;p>然后，对于第 $i$ 个位置和第 $j$ 个位置的 token，其 attention score 定义为：&lt;/p>
$$
\langle f(\mathbf{q}, i), f(\mathbf{k}, j)\rangle =\langle R_{\theta,i}\mathbf{q}, R_{\theta,j}\mathbf{k}\rangle =\mathbf{q}^TR_{\theta, i-j}\mathbf{k}
$$&lt;p>具体细节参考 &lt;a class="link" href="https://maosong.website/p/notes-on-position-encoding/" target="_blank" rel="noopener"
>Position Encoding&lt;/a> 中的 RoPE 部分介绍。这里面的关键在于，最后的结果只与相对位置 $i-j$ 相关，而与绝对位置 $i$ 和 $j$ 无关。因此，我们可以用一个相对位置矩阵 $M\in\mathbb{R}^{L\times L}$ 来表示这个信息，其中 $M_{ij}=P_{\mathbf{q},i}- P_{\mathbf{k},j}$ 代表了第 $i$ 个位置的 query $\mathbf{q}$ 和第 $j$ 个位置的 key $\mathbf{k}$ 的相对位置信息，其示意图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/dual-chunk-attention/DCA_relative_position_visualization.png"
width="379"
height="384"
srcset="https://maosong2022.github.io/p/dual-chunk-attention/DCA_relative_position_visualization_hu13664291601616356431.png 480w, https://maosong2022.github.io/p/dual-chunk-attention/DCA_relative_position_visualization_hu15347633689096992470.png 1024w"
loading="lazy"
alt="Relative Position Visualization"
class="gallery-image"
data-flex-grow="98"
data-flex-basis="236px"
>&lt;/p>
&lt;p>原始版本的 RoPE 的问题在于，在训练时，模型没有见过更长的上下文，因此其泛化性也最差，这一点在 &lt;a class="link" href="https://maosong.website/p/notes-on-yarn/" target="_blank" rel="noopener"
>YARN&lt;/a> 已经得到了验证&lt;/p>
&lt;h2 id="method">Method
&lt;/h2>&lt;p>DCA 的关键在于将 sequence 分割为若干个 Chunk，然后将 attention 的计算拆分为三个部分：&lt;/p>
&lt;ol>
&lt;li>intra-chunk：负责计算每个 chunk 内部的 attention&lt;/li>
&lt;li>inter-chunk ：负责计算 chunk 之间的 attention&lt;/li>
&lt;li>successive-chunk：负责计算相邻两个 chunk 之间的 attention&lt;/li>
&lt;/ol>
&lt;p>为了更好理解 DCA，我们接下来假设 $L=12$, 这时我们有&lt;/p>
$$
P_{\mathbf{q}} = [0,1,\dots,11],\quad P_{\mathbf{k}} = [0,1,\dots,11]
$$&lt;h3 id="intra-chunk-attention">Intra-Chunk Attention
&lt;/h3>&lt;p>我们首先定义个超参数 chunk size $s&amp;gt;0$, 然后我们将我们的 sequence 分割成 $L/s$ 个 chunk，然后每个 chunk 重新进行编号，就得到了如下的 position id&lt;/p>
$$
P_{\mathbf{q}}^{Intra} = [0,1,\dots,L-1]\mod s,\quad P_{\mathbf{k}}^{Intra} = [0,1,\dots,L-1]\mod s
$$&lt;p>接下来，我们定义 intra-chunk 的相对位置矩阵 $M$， 此时，我们仅在每个 chunk 内部进行计算 attention，即&lt;/p>
$$
M[i][j] = \begin{cases} P_{\mathbf{q},i}^{Intra} - P_{\mathbf{k},j}^{Intra},&amp;\text{if}\lfloor P_{\mathbf{q},i}/s \rfloor = \lfloor P_{\mathbf{k},j} /s\rfloor ,\\
0,&amp;\text{otherwise}
\end{cases}
$$&lt;p>在上面的例子中，假设 $s=6$, 那么我们新的 position id 就变成了&lt;/p>
$$
\begin{aligned}
P_{\mathbf{q}}^{Intra} = [0,1,2,3,4,5,0,1,2,3,4,5]\\
P_{\mathbf{k}}^{Intra} = [\underbrace{0,1,2,3,4,5}_{\text{Chunk 0}},\underbrace{0,1,2,3,4,5}_{\text{Chunk 1}}]
\end{aligned}
$$&lt;p>对其进行可视化，我们就得到&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/dual-chunk-attention/DCA_intra_chunk_attention_visualization.png"
width="362"
height="386"
srcset="https://maosong2022.github.io/p/dual-chunk-attention/DCA_intra_chunk_attention_visualization_hu11134860188566089087.png 480w, https://maosong2022.github.io/p/dual-chunk-attention/DCA_intra_chunk_attention_visualization_hu16302318955909605835.png 1024w"
loading="lazy"
alt="Intra Chunk Attention Visualization"
class="gallery-image"
data-flex-grow="93"
data-flex-basis="225px"
>&lt;/p>
&lt;h3 id="inter-chunk-attention">Inter-Chunk Attention
&lt;/h3>&lt;p>接下来，我们来看一下不同 chunk 之间如何计算彼此的 attention。在 Intra-chunk attention 计算中，我们忽略了跨 chunk 的信息，而且，由于现在的 position id 不再是单调递增的了，我们直接使用 $P_{\mathbf{q}}^{Intra}$ 和 $P_{\mathbf{k}}^{Intra}$ 给出的位置信息不对，这也是为什么我们在 Intra-chunk attention 中要求 query 和 key 在同一个 chunk 中才能计算的原因。&lt;/p>
&lt;p>为了解决这个问题，作者构建了一个新的 position id。首先我们引入一个新的超参数 $c&amp;gt;\max_i P_{\mathbf{q},i}$, $c$ 代表了模型预训练时的上下文长度，如 4096。&lt;/p>
&lt;p>接下来，基于 $c$, 我们定义新的 position id 如下：&lt;/p>
$$
P_{\mathbf{q}}^{Inter} = [c-1,c-1,\dots,c-1]\in\mathbb{R}^s,\quad P_{\mathbf{k}}^{Inter} = P_{\mathbf{k}}^{Intra}
$$&lt;blockquote>
&lt;p>注：这里的 $P_{\mathbf{q}}^{Inter}$ 指的是某一个 chunk 中的 position id，每个 chunk 中的 position id 都相同，请参考例子理解，后面不再赘述。&lt;/p>
&lt;/blockquote>
&lt;p>也就是说，在计算跨 chunk 的 attention 的时候，我们直接把 query 的 position id 设置为最大值，然后 key 的 position id 依然使用 intra-chunk 的位置信息。由于 $\max_i P_{\mathbf{k},i}=s-1$, 因此我们有&lt;/p>
$$
M[i][j] = P_{\mathbf{q},i}^{Inter} - P_{\mathbf{k},j}^{Inter} = c - 1 - P_{\mathbf{k},j}^{Inter}\geq c - 1 - (s- 1) \geq c-s.
$$&lt;p>最后，我们对于 inter chunk 的位置矩阵 $M$ 定义如下：&lt;/p>
$$
M[i][j] = \begin{cases} P_{\mathbf{q},i}^{Inter} - P_{\mathbf{k},j}^{Inter},&amp;\text{if}\lfloor P_{\mathbf{q},i}/s \rfloor \neq \lfloor P_{\mathbf{k},j} /s\rfloor ,\\
0,&amp;\text{otherwise}
\end{cases}
$$&lt;p>在上面的例子中，当 $c=10$ 时，$c-1=9$, 我们有&lt;/p>
$$
P_{\mathbf{q}}^{Inter}=[\underbrace{9,9,9,9,9,9}_{\text{Chunk 0}},\underbrace{9,9,9,9,9,9}_{\text{Chunk 1}}]
$$&lt;p>对其进行可视化，得到&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/dual-chunk-attention/DCA_inter_chunk_attention_visualization.png"
width="351"
height="388"
srcset="https://maosong2022.github.io/p/dual-chunk-attention/DCA_inter_chunk_attention_visualization_hu13434590375947798231.png 480w, https://maosong2022.github.io/p/dual-chunk-attention/DCA_inter_chunk_attention_visualization_hu15403771449886131066.png 1024w"
loading="lazy"
alt="Inter Chunk Attention Visualization"
class="gallery-image"
data-flex-grow="90"
data-flex-basis="217px"
>&lt;/p>
&lt;h3 id="successive-chunk-attention">Successive-Chunk Attention
&lt;/h3>&lt;p>现在我们既可以计算 intra-chunk，也可以计算 inter-block 的 attention，但是问题是对于相邻的 chunk，其位置信息不对了，从上面的可视化中，我们可以看到，当 $P_{\mathbf{q},i}=6$ , $P_{\mathbf{k},j}=5$ 时，我们有&lt;/p>
$$
P_{\mathbf{q},i}^{Inter} - P_{\mathbf{k},j}^{Inter}=9-5=4\neq 1 = P_{\mathbf{q},i}-P_{\mathbf{k},j}
$$&lt;p>也就是说，inter-block 的 attention 会破坏原有的相对位置信息，因此我们就通过 successive chunk attention 来解决这个问题，使得 $P_{\mathbf{q},i}^{Inter} - P_{\mathbf{k},j}^{Inter}\approx P_{\mathbf{q},i}-P_{\mathbf{k},j}$.&lt;/p>
&lt;p>作者发现，这个问题不是所有的 chunk 都有，而是只存在于相邻的 chunk 中，因此，作者又加入了一个超参数 $w&amp;gt;0$ 代表了 local window size，我们可以直接将其设置为 $c-s$, 通过这个 local window，我们调整对应的 position id 如下：&lt;/p>
$$
P_{\mathbf{q}}^{Succ} = [\overbrace{s,s+1,\dots,s+w-1}^{w \text{ elements}},c-1, \dots,c-1]\in\mathbb{R}^s,\quad P_{\mathbf{k}}^{Succ} = P_{\mathbf{k}}^{Inter}
$$&lt;p>对于 successive chunk 的位置矩阵 $M$ 定义如下：&lt;/p>
$$
M[i][j] = \begin{cases} P_{\mathbf{q},i}^{Inter} - P_{\mathbf{k},j}^{Inter},&amp;\text{if}\lfloor P_{\mathbf{q},i}/s \rfloor - \lfloor P_{\mathbf{k},j} /s\rfloor=1 ,\\
0,&amp;\text{otherwise}
\end{cases}
$$&lt;p>在上面的例子中，我们设置 $w=4$, 就得到&lt;/p>
$$
P_{\mathbf{q}}^{Succ}=[\underbrace{6,7,8,9,9,9}_{\text{Chunk 0}},\underbrace{6,7,8,9,9,9}_{\text{Chunk 1}}]
$$&lt;p>对其进行可视化，得到&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/dual-chunk-attention/DCA_successive_chunk_attention_visualization.png"
width="388"
height="388"
srcset="https://maosong2022.github.io/p/dual-chunk-attention/DCA_successive_chunk_attention_visualization_hu1993756046353236827.png 480w, https://maosong2022.github.io/p/dual-chunk-attention/DCA_successive_chunk_attention_visualization_hu13916683337202761882.png 1024w"
loading="lazy"
alt="Successive Chunk Attention Visualization"
class="gallery-image"
data-flex-grow="100"
data-flex-basis="240px"
>&lt;/p>
&lt;h3 id="computation">Computation
&lt;/h3>&lt;p>接下来，我们把所有的改进放在一起，就得到&lt;/p>
$$
M[i][j] = \begin{cases}
P_{\mathbf{q},i}^{Intra} - P_{\mathbf{k},j},&amp;\text{if}\lfloor P_{\mathbf{q},i}/s \rfloor - \lfloor P_{\mathbf{k},j} /s\rfloor=0 ,\\
P_{\mathbf{q},i}^{Succ} - P_{\mathbf{k},j},&amp;\text{if}\lfloor P_{\mathbf{q},i}/s \rfloor - \lfloor P_{\mathbf{k},j} /s\rfloor=1 ,\\
P_{\mathbf{q},i}^{Inter} - P_{\mathbf{k},j},&amp;\text{if}\lfloor P_{\mathbf{q},i}/s \rfloor - \lfloor P_{\mathbf{k},j} /s\rfloor>1.
\end{cases}
$$&lt;p>基于上面的位置矩阵 $M$， 我们再依次计算对应的 attention score&lt;/p>
$$
\langle f(\mathbf{q}, i), f(\mathbf{k}, j)\rangle = \begin{cases}
\langle f(\mathbf{q}, P_{\mathbf{q},i}^{Intra})
, f(\mathbf{k}, P_{\mathbf{k},j})\rangle,&amp;\text{if}\lfloor P_{\mathbf{q},i}/s \rfloor - \lfloor P_{\mathbf{k},j} /s\rfloor=0 ,\\
\langle f(\mathbf{q}, P_{\mathbf{q},i}^{Succ})
, f(\mathbf{k}, P_{\mathbf{k},j})\rangle,&amp;\text{if}\lfloor P_{\mathbf{q},i}/s \rfloor - \lfloor P_{\mathbf{k},j} /s\rfloor=1 ,\\
\langle f(\mathbf{q}, P_{\mathbf{q},i}^{Inter})
, f(\mathbf{k}, P_{\mathbf{k},j})\rangle,&amp;\text{if}\lfloor P_{\mathbf{q},i}/s \rfloor - \lfloor P_{\mathbf{k},j} /s\rfloor>1.
\end{cases}
$$&lt;h2 id="code">Code
&lt;/h2>&lt;p>首先是 &lt;code>RotaryEmbedding&lt;/code> 部分的修改&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">DCARotaryEmbedding&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">max_len&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">chunk_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">local_window&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">max_len&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">max_len&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">chunk_size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">chunk_size&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">local_window&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">local_window&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">inv_freq&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="o">...&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q_t&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">chunk_size&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">qc_t&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">q_t&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">chunk_size&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">clamp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">max&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">chunk_size&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">k_t&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">seq_len&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">chunk_size&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q_freqs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">outer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">q_t&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">inv_freq&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># seq_len x dim/2&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">qc_freqs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">outer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">qc_t&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">inv_freq&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">k_freqs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">outer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">k_t&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">inv_freq&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># seq_len x dim/2&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q_emb&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cat&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">q_freqs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">q_freqs&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># seq_len x dim&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">qc_emb&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cat&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">qc_freqs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">qc_freqs&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">k_emb&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cat&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">k_freqs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k_freqs&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># seq_len x dim&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># compute related sin, cos&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">q_sin&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">q_cos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">qc_sin&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">qc_cos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k_sin&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k_cos&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>attention 计算时的逻辑&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Attention&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">...&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">key_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">apply_rotary_pos_emb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">key_states&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k_cos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k_sin&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">position_ids&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q_states_intra&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">apply_rotary_pos_emb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">query_states&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="p">:&lt;/span>&lt;span class="n">chunk_len&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:],&lt;/span> &lt;span class="n">q_cos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">q_sin&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">position_ids&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="p">:&lt;/span>&lt;span class="n">chunk_len&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">k_states_prev&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">key_states&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="p">:&lt;/span>&lt;span class="n">chunk_len&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">v_states_prev&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">value_states&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="p">:&lt;/span>&lt;span class="n">chunk_len&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># first chunk&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">flash_result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">do_flash_attn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">q_states_intra&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k_states_prev&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">v_states_prev&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">flash_results&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">flash_result&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">remain_len&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">kv_seq_len&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">chunk_len&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="n">remain_len&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">flash_per_chunk&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">begin&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">kv_seq_len&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">remain_len&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">curr_chunk_len&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">chunk_len&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">remain_len&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">end&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">begin&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">curr_chunk_len&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># current chunk, intra-chunk attention&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q_states_intra&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">apply_rotary_pos_emb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">query_states&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="n">begin&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:],&lt;/span> &lt;span class="n">q_cos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">q_sin&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">position_ids&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="n">begin&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">k_states_intra&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">key_states&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="n">begin&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">v_states_intra&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">value_states&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="n">begin&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">flash_result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">do_flash_attn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">q_states_intra&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k_states_intra&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">v_states_intra&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">flash_per_chunk&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">flash_result&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># successive chunk attention&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q_states_succ&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">apply_rotary_pos_emb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">query_states&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="n">begin&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:],&lt;/span> &lt;span class="n">qc_cos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">qc_sin&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">position_ids&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="n">begin&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">flash_result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">do_flash_attn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">q_states_succ&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k_states_prev&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">v_states_prev&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">flash_per_chunk&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">flash_result&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># inter chunk attention&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">begin&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">k_states_prev&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">prev_len&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">k_states_prev&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q_states_inter&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">apply_rotary_pos_emb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">query_states&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="n">begin&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:],&lt;/span> &lt;span class="n">qc_cos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">qc_sin&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">position_ids&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="n">chunk_len&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">][:,&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">repeat&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">curr_chunk_len&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">k_states_inter&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">key_states&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="p">:&lt;/span>&lt;span class="n">begin&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">prev_len&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">v_states_inter&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">value_states&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="p">:&lt;/span>&lt;span class="n">begin&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">prev_len&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">flash_result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">do_flash_attn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">q_states_inter&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k_states_inter&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">v_states_inter&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">flash_per_chunk&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">flash_result&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">flash_results&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">flash_per_chunk&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">k_states_prev&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">k_states_intra&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">v_states_prev&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">v_states_intra&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">remain_len&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">remain_len&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">chunk_len&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># merge the final results&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attn_output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">merge_attn_outputs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">flash_results&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="evaluation">Evaluation
&lt;/h2>&lt;p>&lt;img src="https://maosong2022.github.io/p/dual-chunk-attention/DCA_perplexity_evaluation_PG19.png"
width="1202"
height="418"
srcset="https://maosong2022.github.io/p/dual-chunk-attention/DCA_perplexity_evaluation_PG19_hu3658145395080134465.png 480w, https://maosong2022.github.io/p/dual-chunk-attention/DCA_perplexity_evaluation_PG19_hu3051001898983962189.png 1024w"
loading="lazy"
alt="Perplexity evaluation on PG19"
class="gallery-image"
data-flex-grow="287"
data-flex-basis="690px"
>&lt;/p>
&lt;p>作者还分析了一下 DCA 的效率，结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/dual-chunk-attention/DCA_efficiency_analysis.png"
width="1080"
height="618"
srcset="https://maosong2022.github.io/p/dual-chunk-attention/DCA_efficiency_analysis_hu17500792112674002539.png 480w, https://maosong2022.github.io/p/dual-chunk-attention/DCA_efficiency_analysis_hu12302726781671970184.png 1024w"
loading="lazy"
alt="Efficiency of DCA"
class="gallery-image"
data-flex-grow="174"
data-flex-basis="419px"
>&lt;/p>
&lt;p>可以看到，在 &lt;a class="link" href="https://maosong.website/p/notes-on-flashattention/" target="_blank" rel="noopener"
>flash attention&lt;/a> 的基础上加上 DCA 之后，内存占用和推理时间并没有发生太大变化&lt;/p>
&lt;p>作者还分析了三种 attention 对结果的贡献，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/dual-chunk-attention/DCA_ablation_study.png"
width="1088"
height="404"
srcset="https://maosong2022.github.io/p/dual-chunk-attention/DCA_ablation_study_hu17347492874614111854.png 480w, https://maosong2022.github.io/p/dual-chunk-attention/DCA_ablation_study_hu11352242066928675166.png 1024w"
loading="lazy"
alt="Ablation study on three modules"
class="gallery-image"
data-flex-grow="269"
data-flex-basis="646px"
>&lt;/p>
&lt;p>结果显示，intra block 的 perplexity 是最低的，但是其在下游任务上表现是最差的。当三者结合在一起之后，perplexity 和下游任务上的表现都是最好的。&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>本文中，我们回顾了 Qwen 系列扩展大模型上下文的方法 Dual Chunk Attention （DCA） 通过将 attention 切分成更小的 chunk，然后将 attention 的计算分为 intra-chunk，inter-chunk 和 successive-chunk，分别处理 chunk 内部，chunk 之间以及相邻 chunk 的 attention，通过这种方式，在无需训练的情况下，我们可以有效将模型上下文长度扩展 4 倍以上。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2402.17463" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/HKUNLP/ChunkLlama/tree/main" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Qwen2</title><link>https://maosong2022.github.io/p/notes-on-qwen2/</link><pubDate>Sat, 12 Jul 2025 10:36:43 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-qwen2/</guid><description>&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>2024 年 9 月 Qwen 发布了 Qwen2 系列技术报告，Qwen2 系列包括 4 个 dense 模型（0.5B, 1.5B, 7B, 72B）和一个 MoE 模型（总参数 57B，激活参数 14B），作者主要在架构，数据和长上下文上进行了改进。&lt;/p>
&lt;h2 id="method">Method
&lt;/h2>&lt;h3 id="model">Model
&lt;/h3>&lt;p>对于 dense 模型，Qwen2 在 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen-llm/" target="_blank" rel="noopener"
>Qwen-LLM&lt;/a> 的基础上做了如下改动：&lt;/p>
&lt;ol>
&lt;li>使用 &lt;a class="link" href="https://maosong.website/p/notes-on-gqa/" target="_blank" rel="noopener"
>Group Query Attention (GQA)&lt;/a> 替换 MHA，来优化 KV cache，提高 throughput&lt;/li>
&lt;li>使用 &lt;a class="link" href="https://maosong.website/p/dual-chunk-attention/" target="_blank" rel="noopener"
>Dual Chunk Attention&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/notes-on-yarn/" target="_blank" rel="noopener"
>YARN&lt;/a> 来提高模型上下文长度和训练效率&lt;/li>
&lt;/ol>
&lt;p>其余与 Qwen 一致，包括 SwiGLU，RoPE，RMSNorm 和 pre-normalization&lt;/p>
&lt;p>对于 MoE 模型，Qwen2-MoE 基于 &lt;a class="link" href="https://maosong.website/p/dual-chunk-attention/" target="_blank" rel="noopener"
>Qwen1.5&lt;/a> 进行了改进，主要是 3 点：&lt;/p>
&lt;ol>
&lt;li>作者使用了更细粒度的专家个数，作者认为细粒度的专家可以提供更丰富的 combination，这一点与 olmoe 的结论相同&lt;/li>
&lt;li>与 DeepSeek-MoE 一样，作者使用了共享专家和路由专家&lt;/li>
&lt;li>作者使用了类似 upcycling 的方法来初始化模型。假设一共有 $n$ 个专家，每个专家的维度为 $h_E$, 原始 dense 模型的维度为 $h_{FFN}$, 那么我们会把 dense 模型的参数复制 $[nh_E/h_{FFN}]$ 次，这样就可以扩展到任意个数的 MoE 模型上。作者还对参数进行 shuffle，来提高 diversity。最后，作者还对 50% 的参数进行随机初始化，来提高模型的 capacity。&lt;/li>
&lt;/ol>
&lt;p>模型配置如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Configuration&lt;/th>
&lt;th>0.5B&lt;/th>
&lt;th>1.5B&lt;/th>
&lt;th>7B&lt;/th>
&lt;th>72B&lt;/th>
&lt;th>57B-A14B&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Hidden Size&lt;/td>
&lt;td>896&lt;/td>
&lt;td>1,536&lt;/td>
&lt;td>3,584&lt;/td>
&lt;td>8,192&lt;/td>
&lt;td>3,584&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Layers&lt;/td>
&lt;td>24&lt;/td>
&lt;td>28&lt;/td>
&lt;td>28&lt;/td>
&lt;td>80&lt;/td>
&lt;td>28&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Query Heads&lt;/td>
&lt;td>14&lt;/td>
&lt;td>12&lt;/td>
&lt;td>28&lt;/td>
&lt;td>64&lt;/td>
&lt;td>28&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># KV Heads&lt;/td>
&lt;td>2&lt;/td>
&lt;td>2&lt;/td>
&lt;td>4&lt;/td>
&lt;td>8&lt;/td>
&lt;td>4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Head Size&lt;/td>
&lt;td>64&lt;/td>
&lt;td>128&lt;/td>
&lt;td>128&lt;/td>
&lt;td>128&lt;/td>
&lt;td>128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Intermediate Size&lt;/td>
&lt;td>4,864&lt;/td>
&lt;td>8,960&lt;/td>
&lt;td>18,944&lt;/td>
&lt;td>29,568&lt;/td>
&lt;td>2,560&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Routed Experts&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Activated Experts&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Shared Experts&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Embedding Tying&lt;/td>
&lt;td>True&lt;/td>
&lt;td>True&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Vocabulary Size&lt;/td>
&lt;td>151,646&lt;/td>
&lt;td>151,646&lt;/td>
&lt;td>151,646&lt;/td>
&lt;td>151,646&lt;/td>
&lt;td>151,646&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Trained Tokens&lt;/td>
&lt;td>12T&lt;/td>
&lt;td>7T&lt;/td>
&lt;td>7T&lt;/td>
&lt;td>7T&lt;/td>
&lt;td>4.5T&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="pre-training">Pre-training
&lt;/h3>&lt;p>预训练阶段的数据基于 Qwen 和 Qwen1.5，数据处理策略如下：&lt;/p>
&lt;ol>
&lt;li>使用基于 heuristic 和 model-based 方法来过滤掉低质量的数据&lt;/li>
&lt;li>加入了 code， math 和 multilingual 的数据&lt;/li>
&lt;li>平衡了各个类别的数据分布&lt;/li>
&lt;/ol>
&lt;p>初始数据包括 12T token，经过过滤得到 7T token。作者发现，使用 12T token 进行训练，模型的表现不如使用 7B token 训练得到的模型效果好。因此除了 0.5B 的模型，其他模型使用的都是 7T 的 token&lt;/p>
&lt;p>对于 MoE 模型，作者使用了额外的 4.5T token 来进行预训练。&lt;/p>
&lt;p>在训练过程中，作者还加入了 multi-task instruction 数据，来提高模型的上下文学习能力和指令跟随能力。&lt;/p>
&lt;p>作者还将 Qwen2 模型系列的上下文长度从 4096 扩展到 32768，扩展过程中作了三个改动：&lt;/p>
&lt;ol>
&lt;li>加入了更多高质量的长上下文数据&lt;/li>
&lt;li>将 RoPE 的 frequency 从 10,000 提升到了 1,000,000&lt;/li>
&lt;li>使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-yarn/" target="_blank" rel="noopener"
>YARN&lt;/a> 来扩展上下文长度&lt;/li>
&lt;li>使用了 &lt;a class="link" href="https://maosong.website/p/dual-chunk-attention/" target="_blank" rel="noopener"
>Dual Chunk Attention&lt;/a> 来优化 attention 的计算&lt;/li>
&lt;/ol>
&lt;h3 id="post-training">Post-training
&lt;/h3>&lt;p>post-training 包括 SFT 和 RLHF 两个阶段&lt;/p>
&lt;p>数据包括 SFT 数据和 RLHF 使用的偏好数据&lt;/p>
&lt;p>数据标注过程有：&lt;/p>
&lt;ol>
&lt;li>使用 InsTag 对数据进行打标&lt;/li>
&lt;li>选取高质量的 instruction&lt;/li>
&lt;li>构建了一个 self-evolution 策略，来扩展 instruction 数据&lt;/li>
&lt;li>请人类来标注数据&lt;/li>
&lt;/ol>
&lt;p>作者还合成了一些数据，合成数据的过程如下：&lt;/p>
&lt;ol>
&lt;li>rejection sampling：对 LLM 进行多次采样，然后保留结论正确的数据作为 SFT 数据，以正确和错误的数据对作为偏好数据&lt;/li>
&lt;li>Execution feedback：对于代码任务，使用 Python 来验证答案的正确性&lt;/li>
&lt;li>Data Repurposing：对于写作类任务，以文档为输入，让 LLM 生成对应的 instruction&lt;/li>
&lt;li>Constitutional Feeback：基于预设的 principle 来生成回答&lt;/li>
&lt;/ol>
&lt;p>最终，SFT 数据包括 500, 000 条样本&lt;/p>
&lt;p>RLHF 的训练包括 offline stage 和 online stage，offline stage 就是用收集到的偏好数据。在 online stage，作者使用 reward model 来给输出的回答进行打分，然后再使用 DPO 进行训练。&lt;/p>
&lt;p>与 Qwen 不同，Qwen2 中作者使用了 Online Merging Optimizer 来解决因为 alignment 导致的性能降低&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>本文提出了 Qwen2 系列，在 Qwen2 中，首次使用了 GQA 代替 MHA，Qwen2 在上下文上做出了初步探索&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2407.10671" target="_blank" rel="noopener"
>Qwen2 tech report&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Qwen1.5</title><link>https://maosong2022.github.io/p/notes-on-qwen1.5/</link><pubDate>Thu, 03 Jul 2025 17:37:39 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-qwen1.5/</guid><description>&lt;p>Qwen 在 24 年 1 月份发布了 Qwen1.5，包含 0.5B, 1.8B, 4B, 7B, 14B, 32B, 72B, 以及 110B 6 个 size，还有一个 MoE 模型。&lt;/p>
&lt;h2 id="介绍">介绍
&lt;/h2>&lt;p>Qwen1.5 的主要特点：&lt;/p>
&lt;ol>
&lt;li>支持 12 中语言&lt;/li>
&lt;li>统一支持 32768 tokens 上下文长度 。&lt;/li>
&lt;li>提供 量化版本 （Int4、Int8、AWQ、GGUF）以适应低资源环境或部署需求。&lt;/li>
&lt;/ol>
&lt;p>训练过程使用了 DPO 以及 PPO 来进行对齐&lt;/p>
&lt;h2 id="qwen15-moe">Qwen1.5-MoE
&lt;/h2>&lt;p>Qwen1.5-MoE 的激活参数为 2.7B，一共包含 64 个专家，其中激活 4 个专家，共享 4 个专家&lt;/p>
&lt;p>相比于 Qwen1.5-7B，去训练的 FLOPS 降低了 75%，inference 的速度提高了 174%&lt;/p>
&lt;p>Qwen1.5-MoE 采用了改进的 MoE 架构，主要优化包括：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>细粒度专家（Fine-grained experts）&lt;/strong> ：通过将 FFN 层划分为多个片段，构建更多专家而不增加参数总量。&lt;/li>
&lt;li>&lt;strong>初始化策略（Upcycling）&lt;/strong> ：基于 Qwen-1.8B 初始化模型，并引入随机性以加速收敛。&lt;/li>
&lt;li>&lt;strong>路由机制（Routing Mechanism）&lt;/strong> ：在每个 MoE 层中使用 64 个专家，其中 4 个共享专家始终激活，60 个路由专家中有 4 个被激活，提高了灵活性和效率。&lt;/li>
&lt;/ul>
&lt;h2 id="效率对比">效率对比
&lt;/h2>&lt;p>作者对比了 throughput (requests processed per second) 以及 tokens per second (TPS):&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Throughput&lt;/th>
&lt;th>TPS&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Qwen1.5-7B-Chat&lt;/td>
&lt;td>1.15&lt;/td>
&lt;td>2298.89&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen1.5-MoE-A2.7B-Chat&lt;/td>
&lt;td>2.01&lt;/td>
&lt;td>4010.27&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://qwenlm.github.io/blog/qwen-moe/" target="_blank" rel="noopener"
>Qwen1.5 MoE&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://qwenlm.github.io/blog/qwen1.5/" target="_blank" rel="noopener"
>Qwen 1.5&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on YaRN</title><link>https://maosong2022.github.io/p/notes-on-yarn/</link><pubDate>Thu, 03 Jul 2025 14:40:49 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-yarn/</guid><description>&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>YaRN (Yet Another RoPE extentionN method) 时23年9月EleutherAI等提出来的一个扩展LLM上下文长度的方法，后来被Qwen系列模型所应用。&lt;/p>
&lt;h2 id="preliminary">Preliminary
&lt;/h2>&lt;p>作者首先回顾了一下RoPE, 具体内容请参见上一篇&lt;a class="link" href="https://maosong.website/p/notes-on-position-encoding/" target="_blank" rel="noopener"
>blog&lt;/a>。并使用了 $f_{W}(x_m, m, \theta_d)$ 来表示RoPE：&lt;/p>
$$
f_{W}(x_m, m, \theta_{d}) = \Theta_{\theta, m}^d W x_m
$$&lt;p>其中 $\Theta_{\theta, m}^d\in\mathbb{R}^{d\times d}$ 是多维旋转矩阵, $\theta_d=[\theta_{0,d},\dots,\theta_{(d-2)/2,d}]\in\mathbb{R}^{d/2}$, $\theta_{i,d}=\theta^{2i/d}$, $\theta&amp;gt;0$ 是一个超参数，RoPE中设置为 $\theta=10000$, $W\in{W_q,W_k}\subset\mathbb{R}^{d\times d}$ 是对应的query/key projection layer的权重矩阵， $x\in\mathbb{R}^{d}$ 是输入的hidden states.&lt;/p>
&lt;p>接下来，作者定义了两个新的变量：&lt;/p>
&lt;p>&lt;strong>scaling factor&lt;/strong>
假设预训练的上下文长度为 $L$, 扩展的上下文长度为 $L&amp;rsquo;&amp;gt;L$, 则我们定义scaling factor 为&lt;/p>
$$
s = \frac{L'}{L}
$$&lt;p>易知 $s&amp;gt;1$.&lt;/p>
&lt;p>&lt;strong>wavelength&lt;/strong>
我们将 $\lambda_d$ 定义为 $i$-th 维的RoPE embedding对应的 wavelength：&lt;/p>
$$
\lambda_{i,d} = \frac{2\pi}{\theta_{i,d}} = 2\pi\theta^{2i/d}, \ i=0,\dots,(d-2)/2
$$&lt;p>wavelength描述了对于第$i$ 个维度，RoPE旋转一周 ($2\pi$) 所需要的上下文长度。&lt;/p>
&lt;h2 id="unified-perspective-on-related-work">Unified Perspective on Related Work
&lt;/h2>&lt;p>基于 $f_{W}(x_m, m, \theta_d)$, 作者统一了已有的扩展上下文长度的方法，作者将不同的扩展方法使用一个通用函数 $f_W(x_m,g(m), h(\theta_d))$ 来表示，这里 $g(m)$ 和 $h(\theta_d)$ 分别代表了不同的长度外推方法所使用的变换。&lt;/p>
&lt;h3 id="position-interpolation">Position Interpolation
&lt;/h3>&lt;p>Position Interpolation (PI) 的核心思想在于，我们可以通过Interpolation，将超过预训练长度的文本给压缩到当前最大长度，以此来避免RoPE外推产生的问题，其对应的公式为&lt;/p>
$$
f'_W(x_m,m,\theta_d) = f_W(x_m, \frac{mL}{L'}, \theta_d)
$$&lt;p>其中 $L&amp;rsquo;&amp;gt;L$ 为我们扩展之后的上下文长度， $L$ 为我们预训练的上下文长度。使用通用函数表示的话，我们有&lt;/p>
$$
g(m) = \frac{m}{s},\quad h(\theta_d) = \theta_d
$$&lt;h3 id="ntk-aware-interpolation">NTK-aware Interpolation
&lt;/h3>&lt;p>PI的问题是，并没有考虑不同维度的wavelength。
基于NTK理论，DNN当输入维度比较低，且embedding缺少高频内容时，模型就会很难学习到高频信息。对应到RoPE里面，输入的token position id是低位信息（1维），而输出的RoPE是一个 $d$ 维的复杂向量。因此，当输入token非常相似却距离非常近时，RoPE就会丢失高频的细节信息&lt;/p>
&lt;p>因此，为了解决这个问题，作者对不同的维度使用了不同的缩放策略：&lt;strong>维度比较小时，其缩放的更多，维度比较大是，其缩放的更少。&lt;/strong>&lt;/p>
&lt;p>基于这个策略，作者提出了NTK-aware interpolation，其定义如下：&lt;/p>
$$
g(m) = m, \quad h(\theta_{i,d}) = \theta'^{-2i/d}, i=0,\dots,(d-2)/2
$$&lt;p>其中&lt;/p>
$$
\theta' = \theta\cdot s^{\frac{d}{d-2}}
$$&lt;p>实际中，这种方法会产生out-of-bound的值，因此最终结果会比PI要差一点，为了解决这个问题，一般会使用比 $s$ 更大的scaling factor.&lt;/p>
&lt;blockquote>
&lt;p>上式的推导基于一个简单的假设：我们希望最后一个维度的wavelength在scaling之后，是线性变化的，即 $\theta&amp;rsquo;&lt;em>{(d-2)/2,d}=s\theta&lt;/em>{(d-2)/2,d}$, 求解之后，我们就得到了上面的定义&lt;/p>
&lt;/blockquote>
&lt;p>PI 和NTK-aware interpolation的问题在于，我们对不同的维度的处理都是一样的。这类不在乎wavelength的方法被称为&lt;strong>blind interpolation methods&lt;/strong>, 接下来要介绍的就是基于wavelength的方法，即&lt;strong>target interpolation methods&lt;/strong>.&lt;/p>
&lt;h3 id="ntk-by-parts-interpolation">NTK-by-parts Interpolation
&lt;/h3>&lt;p>与NTK-aware interpolation， NTK-by-parts interpolation基于wavelength来考虑不同维度上所做的变换。&lt;/p>
&lt;p>对于低维度，其 $\theta_{i,d}$ 非常大，因此旋转一周所需要的上下文长度也非常大。实际上就导致某些维度的embedding并不是均匀分布的，（比如说只有 $0\sim\pi$ 这个区间的embedding），这个时候，模型就只能访问到绝对位置信息，而访问不到相对位置信息。
另外，当我们对所有的维度都进行scale的时候，所有的token都会与彼此更加靠近，这损害了模型对于局部信息的获取能力。
基于这些认知，作者基于wavelength，对不同的维度分别进行处理：&lt;/p>
&lt;ol>
&lt;li>如果wavelength远小于上下文长度 $L$， 则我们不做任何处理&lt;/li>
&lt;li>如果wavelength等于或者大于上下文长度 $L$， 则我们使用NTK-aware interpolation 进行处理&lt;/li>
&lt;li>对于中间的其他维度，我们进行了一个trade off&lt;/li>
&lt;/ol>
&lt;p>作者定义了一个ratio $r$ 来描述原始上下文长度 $L$ 和 wavelength 之间的关系&lt;/p>
$$
r(i,d) = \frac{L}{\lambda_{i,d}} = \frac{L}{2\pi\theta'^{2i/d}}
$$&lt;p>基于这个ratio，我们可以定义上面的三种处理方式对应的权重&lt;/p>
$$
\gamma(r) = \begin{cases}
0, &amp;\text{if } r &lt; \alpha\\
1, &amp;\text{if } r > \beta\\
\frac{r-\alpha}{\beta-\alpha}, &amp;\text{otherwise}
\end{cases}
$$&lt;p>其中， $\beta&amp;gt;\alpha&amp;gt;0$ 是超参数， $r&amp;lt;\alpha$, $r&amp;lt;\beta$ 分别代表了上面的第1种，第2种情况。&lt;/p>
&lt;p>最后，NTK-by-parts interpolation的定义如下&lt;/p>
$$
g(m) = m, \quad h(\theta_i) = \left(1-\gamma(r(i,d)\right)\frac{\theta_i}{s} + \gamma(r(i,d))\theta_i
$$&lt;p>作者通过实验发现，在LLaMA上，$\alpha=1$ 和 $\beta=32$ 是一个比较好的选择&lt;/p>
&lt;h3 id="dynamic-ntk-interpolation">Dynamic NTK Interpolation
&lt;/h3>&lt;p>在实际中，一个经常遇到的场景就是sequence length会从1逐步上升到最大上下文长度，比如说inference的时候。对于这种情况，我们有两种解决方法：&lt;/p>
&lt;ol>
&lt;li>在整个inference周期中，RoPE的scaling factor都设置为 $s=K&amp;rsquo;/L$, 其中$L&amp;rsquo;$ 是扩展后的上下文长度&lt;/li>
&lt;li>在每次foward的过程汇总，都更新sclaing factor $s=\max(1, \ell&amp;rsquo;/L)$, 这里 $\ell&amp;rsquo;$ 是当前sequence的长度&lt;/li>
&lt;/ol>
&lt;p>作者发现，方案1在sequence长度小于 $L$的时候性能会下降，并且当上下文长度超过 $L&amp;rsquo;$ 时，性能下降的更快。
但是，方案2可以让模型的性能下降曲线更平缓。因此，作者将这种inference-time方法称为 &lt;strong>Dynamic Scaling method&lt;/strong>, 当其与NTK-aware方法结合时，就得到了 &lt;strong>Dynamic NTK interpolation&lt;/strong>&lt;/p>
&lt;p>作者通过实验发现，Dynamic NTK interpolation在$L&amp;rsquo;=L$ 时，效果非常好&lt;/p>
&lt;h2 id="yarn">YaRN
&lt;/h2>&lt;p>在YaRN中，作者针对Dynamic NTK interpolation做了进一步改进，也就是在计算attention softmax时，加入了一个温度参数 $t&amp;gt;0$, 这样attention的计算就变成了&lt;/p>
$$
\mathrm{Attn}(Q, K, V) = \mathrm{softmax}\left(\frac{QK^T}{t\sqrt{d}}\right)V
$$&lt;p>作者发现，通过这种scaling的方式，YaRN可以在不改变代码的前提下，更改attention的机制。并且，其不增加训练和推理的cost&lt;/p>
&lt;p>作者将YaRN定义为&lt;strong>结合了NTK-by-parts interpolation和上述scaling技巧的方法&lt;/strong>&lt;/p>
&lt;p>对于LLaMA，作者推荐使用如下参数：&lt;/p>
$$
\sqrt{\frac{1}{t}} = 0.1\ln(s) + 1.
$$&lt;p>实验结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-yarn/YaRN_temperature_ablation.png"
width="1049"
height="528"
srcset="https://maosong2022.github.io/p/notes-on-yarn/YaRN_temperature_ablation_hu8422727462868015211.png 480w, https://maosong2022.github.io/p/notes-on-yarn/YaRN_temperature_ablation_hu48072328224348277.png 1024w"
loading="lazy"
alt="Ablation study on temperature"
class="gallery-image"
data-flex-grow="198"
data-flex-basis="476px"
>&lt;/p>
&lt;p>作者发现：&lt;/p>
&lt;ol>
&lt;li>对于合适的 $t$, 扩展上下文之后，perplexity会变的更好&lt;/li>
&lt;li>最好的$t$ 对于不同的位置和样本提升都是一样的&lt;/li>
&lt;/ol>
&lt;h2 id="evaluation">Evaluation
&lt;/h2>&lt;p>实验结果如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Extension Method&lt;/th>
&lt;th>Trained Tokens&lt;/th>
&lt;th>Context Window&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>Evaluation Context Window Size&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>2048&lt;/td>
&lt;td>4096&lt;/td>
&lt;td>6144&lt;/td>
&lt;td>8192&lt;/td>
&lt;td>10240&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>PI (s = 2)&lt;/td>
&lt;td>1B&lt;/td>
&lt;td>8k&lt;/td>
&lt;td>&lt;/td>
&lt;td>3.92&lt;/td>
&lt;td>3.51&lt;/td>
&lt;td>3.51&lt;/td>
&lt;td>3.34&lt;/td>
&lt;td>8.07&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>NTK ($\theta$ = 20k)&lt;/td>
&lt;td>1B&lt;/td>
&lt;td>8k&lt;/td>
&lt;td>&lt;/td>
&lt;td>4.20&lt;/td>
&lt;td>3.75&lt;/td>
&lt;td>3.74&lt;/td>
&lt;td>3.59&lt;/td>
&lt;td>6.24&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>YaRN (s = 2)&lt;/td>
&lt;td>400M&lt;/td>
&lt;td>8k&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;strong>3.91&lt;/strong>&lt;/td>
&lt;td>&lt;strong>3.50&lt;/strong>&lt;/td>
&lt;td>&lt;strong>3.51&lt;/strong>&lt;/td>
&lt;td>3.35&lt;/td>
&lt;td>&lt;strong>6.04&lt;/strong>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，YaRN使用的数据更少，并且当模型扩展到10240的时候，其表现下降的最慢，这说明了YaRN在扩展上下文长度时的有效性&lt;/p>
&lt;p>原始RoPE，dynamic-PI和dynamic-YaRN的对比&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-yarn/YaRN_comparison_RoPE_PI.png"
width="1154"
height="573"
srcset="https://maosong2022.github.io/p/notes-on-yarn/YaRN_comparison_RoPE_PI_hu1868026857572428582.png 480w, https://maosong2022.github.io/p/notes-on-yarn/YaRN_comparison_RoPE_PI_hu6691787302659616423.png 1024w"
loading="lazy"
alt="Comparison with RoPE and PI"
class="gallery-image"
data-flex-grow="201"
data-flex-basis="483px"
>&lt;/p>
&lt;p>可以看到，RoPE的上下文扩展能力很差，Dynamic-YaRN的表现最好。&lt;/p>
&lt;h2 id="code">Code
&lt;/h2>&lt;p>YaRN的实现在&lt;a class="link" href="https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_rope_utils.py" target="_blank" rel="noopener"
>HuggingFace/src/transformers/modeling_rope_utils.py&lt;/a> 里的 &lt;code>_compute_yarn_parameters&lt;/code> 函数里，其返回 &lt;code>inv_freq&lt;/code> 以及 &lt;code>attention_factor&lt;/code> 两个量，前者代表了 $\theta_d$, 后者代表 $t\sqrt{d}$.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">get_mscale&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">scale&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">scale&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="mf">1.0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="mf">0.1&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">math&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">log&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">scale&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mf">1.0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">find_correction_dim&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num_rotations&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">base&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">max_position_embeddings&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Inverse dimension formula to find the dimension based on the number of rotations&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">math&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">log&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">max_position_embeddings&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">num_rotations&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">math&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pi&lt;/span>&lt;span class="p">)))&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">math&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">log&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">base&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">find_correction_range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">low_rot&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">high_rot&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">base&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">max_position_embeddings&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Find dimension range bounds based on rotations&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">low&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">math&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">floor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">find_correction_dim&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">low_rot&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">base&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">max_position_embeddings&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">high&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">math&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ceil&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">find_correction_dim&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">high_rot&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">base&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">max_position_embeddings&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="nb">max&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">low&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="nb">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">high&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">linear_ramp_factor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">min&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">max&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="nb">min&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="nb">max&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">max&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="mf">0.001&lt;/span> &lt;span class="c1"># Prevent singularity&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">linear_func&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="nb">min&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="nb">max&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="nb">min&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ramp_func&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">clamp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">linear_func&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">ramp_func&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">YaRNRotaryEmbedding&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">beta_fast&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;beta_fast&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="ow">or&lt;/span> &lt;span class="mi">32&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">beta_slow&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;beta_slow&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="ow">or&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dim&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;head_dim&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">factor&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">max_position_embeddings&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">original_max_position_embeddings&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pos_freqs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">base&lt;/span> &lt;span class="o">**&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">inv_freq_extrapolation&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">1.0&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">pos_freqs&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">inv_freq_interpolation&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">1.0&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">factor&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">pos_freqs&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">low&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">high&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">find_correction_range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">beta_fast&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">beta_slow&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">base&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">original_max_position_embeddings&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Get n-dimensional rotational scaling corrected for extrapolation&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">inv_freq_extrapolation_factor&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">linear_ramp_factor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">low&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">high&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">inv_freq&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">inv_freq_interpolation&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">inv_freq_extrapolation_factor&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">inv_freq_extrapolation&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">inv_freq_extrapolation_factor&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attention_factor&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">get_mscale&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">factor&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">position_ids&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">...&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">cos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sin&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>实际的transformers代码中，Qwen使用的还是默认的RoPE，在inference时如果我们需要扩展上下文，可以通过修改config的形式：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">transformers&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">pipeline&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">model_name_or_path&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;Qwen/Qwen3-8B&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">generator&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">pipeline&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;text-generation&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">model_name_or_path&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">torch_dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;auto&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">device_map&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;auto&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">model_kwargs&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;max_position_embeddings&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="mi">131072&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;rope_scaling&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;rope_type&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;yarn&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;factor&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="mf">4.0&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;original_max_position_embeddings&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="mi">32768&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>在本文中，作者首先构建了一个统一的表征不同上下文长度扩展的形式，接下来作者分析了不同上下文长度扩展的不足，并提出了YaRN这种上下文长度扩展方式，结果发现，YaRN不仅在短上下文长度下面表现很好，当上下文长度扩展之后，其表现依然非常优秀。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2309.00071" target="_blank" rel="noopener"
>arxiv YaRN&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2306.15595" target="_blank" rel="noopener"
>arxiv PI&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://qwen.readthedocs.io/en/latest/inference/transformers.html#enabling-long-context" target="_blank" rel="noopener"
>Qwen documentation&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Qwen-LLM</title><link>https://maosong2022.github.io/p/notes-on-qwen-llm/</link><pubDate>Thu, 03 Jul 2025 10:47:27 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-qwen-llm/</guid><description>&lt;p>Qwen 在 23 年 9 月份发布了 Qwen 系列大语言模型，包括 1.8B， 7B，14B 三个 size，训练过程使用了 3T token. 作者还基于 Qwen，构建了 Code-Qwen-Chat，Math-Qwen-Chat 等系列领域大语言模型。&lt;/p>
&lt;h2 id="pre-training">Pre-training
&lt;/h2>&lt;h3 id="data">Data
&lt;/h3>&lt;p>数据一共使用了 &lt;strong>3T token&lt;/strong>，主要是 public web documents, encyclopedia, books, codes, etc，覆盖了中文和英文两种语言&lt;/p>
&lt;p>数据处理：&lt;/p>
&lt;ol>
&lt;li>语言识别&lt;/li>
&lt;li>去重，包括 MinHash 和 LSH 算法&lt;/li>
&lt;li>质量过滤，包括基于规则和和基于 ML 的方法&lt;/li>
&lt;li>上采样，特定数据会进行上采样&lt;/li>
&lt;li>加入指令数据，提高模型的 zero-shot 和 few-shot 表现&lt;/li>
&lt;/ol>
&lt;h3 id="tokenization">Tokenization
&lt;/h3>&lt;p>BPE tokenizer，最终的 tokenizer 大小为 152K&lt;/p>
&lt;h3 id="architecture">Architecture
&lt;/h3>&lt;p>模型架构基于 LLaMA， 改动：&lt;/p>
&lt;ol>
&lt;li>tie embdding: input embdding 和 output embdding 使用的权重相同&lt;/li>
&lt;li>position encoding:RoPE, inverse frequency 的精度为 FP32&lt;/li>
&lt;li>bias: 取消了大部分的 bias，增加了 QKV bias，来提高模型的外推能力&lt;/li>
&lt;li>Pre-Norm &amp;amp; RMSNorm&lt;/li>
&lt;li>Activation function: SwiGLU&lt;/li>
&lt;/ol>
&lt;h3 id="training">Training
&lt;/h3>&lt;ul>
&lt;li>上下文长度：2048&lt;/li>
&lt;li>attention：&lt;a class="link" href="https://maosong.website/p/notes-on-flashattention/" target="_blank" rel="noopener"
>flash attention&lt;/a>&lt;/li>
&lt;li>optimizer：AdamW， $\beta_1=0.9$, $\beta_2=0.95$, $\epsilon=10^{-8}$.&lt;/li>
&lt;li>data type: BF16&lt;/li>
&lt;/ul>
&lt;h3 id="context-extention">Context Extention
&lt;/h3>&lt;p>使用了三个技巧：&lt;/p>
&lt;ol>
&lt;li>NTK-aware position interpolation&lt;/li>
&lt;li>log-N scaling&lt;/li>
&lt;li>window attention&lt;/li>
&lt;/ol>
&lt;p>后续前两个统一成了 &lt;a class="link" href="https://maosong.website/p/notes-on-yarn/" target="_blank" rel="noopener"
>YARN&lt;/a>.&lt;/p>
&lt;p>observation: lower layer 对上下文长度扩展更敏感, 因此作者动态调整了 window size&lt;/p>
&lt;h2 id="post-training">Post-training
&lt;/h2>&lt;p>包括 SFT 和 RLHF 两个阶段&lt;/p>
&lt;h3 id="sft">SFT
&lt;/h3>&lt;p>data： 使用了 ChatML 格式&lt;/p>
&lt;h3 id="rlhf">RLHF
&lt;/h3>&lt;p>PPO 算法&lt;/p>
&lt;p>reward model 构建：基于 Qwen-base model&lt;/p>
&lt;p>RL 训练：先更新 value model 50 steps&lt;/p>
&lt;p>发现：top-p 设置为 0.9 比设置为 1.0 更好&lt;/p>
&lt;h3 id="tool-use-and-agent">Tool-use and Agent
&lt;/h3>&lt;p>作者使用了 self-instruct 来进行 SFT，基于 ReAct 构建数据，数据包括 2000 条高质量数据&lt;/p>
&lt;h2 id="specialization">Specialization
&lt;/h2>&lt;h3 id="code-qwen">Code-Qwen
&lt;/h3>&lt;p>code-qwen 基于 qwen continue Pretraining 得到，然后基于 code-qwen 进行 sft 得到 code-qwen-chat，包括 7B 和 14B 两个 size&lt;/p>
&lt;h3 id="math-qwen">Math-Qwen
&lt;/h3>&lt;p>基于 qwen 直接 SFT 得到，包括 7B 和 14B 两个 size&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者在本文中介绍了 Qwen 系列大语言模型，模型使用了 3T token，作者介绍了训练的细节以及如何扩展到领域大语言模型 Code-Qwen 和 Math-Qwen&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://spaces.ac.cn/archives/9444" target="_blank" rel="noopener"
>Length exploration&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2309.16609" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Hands on LLM(2) Transformer</title><link>https://maosong2022.github.io/p/hands-on-llm2-transformer/</link><pubDate>Sun, 29 Jun 2025 11:40:39 +0800</pubDate><guid>https://maosong2022.github.io/p/hands-on-llm2-transformer/</guid><description>&lt;h1 id="cs336-assignment-1">CS336 Assignment 1
&lt;/h1>&lt;p>Transformer实现&lt;/p>
&lt;p>我们采用top-down的形式构建transformer的代码&lt;/p>
&lt;hr>
&lt;h1 id="架构">架构
&lt;/h1>&lt;p>&lt;img src="https://maosong2022.github.io/p/hands-on-llm2-transformer/transformer_architecture.png"
width="1210"
height="1364"
srcset="https://maosong2022.github.io/p/hands-on-llm2-transformer/transformer_architecture_hu2202173807577829293.png 480w, https://maosong2022.github.io/p/hands-on-llm2-transformer/transformer_architecture_hu9835691261630527796.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="88"
data-flex-basis="212px"
>&lt;/p>
&lt;p>我们以Qwen3的代码为例子讲解Assignment1的代码实现&lt;/p>
&lt;hr>
&lt;p>我们通过在transformer架构上加上一个linear layer就可以完成不同的下游任务，比如：&lt;/p>
&lt;ul>
&lt;li>&lt;code>Qwen3ForQuestionAnswering&lt;/code>&lt;/li>
&lt;li>&lt;code>Qwen3ForCausalLM&lt;/code>&lt;/li>
&lt;li>&lt;code>Qwen3ForSequenceClassification&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>因此，大语言模型是transformer的一个附加产物&lt;/p>
&lt;hr>
&lt;h2 id="causallm">CausalLM
&lt;/h2>&lt;p>编写大语言模型的第一步为定义&lt;code>Qwen3ForCausalLM&lt;/code>&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">CausalLM&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">model&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Transformer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">lm_head&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">vocab_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">...&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">outputs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">model&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">***&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">logits&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">lm_head&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">outputs&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">logits&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>这里&lt;code>lm_head&lt;/code>的作用就是构建embedding space到vocabulary的映射，即 $\mathbb{R}^d\to\mathbb{R}^{|V|}$&lt;/p>
&lt;hr>
&lt;h2 id="transformer">Transformer
&lt;/h2>&lt;p>transformer部分包括四个部分：&lt;/p>
&lt;ol>
&lt;li>Embedding Layer：将token映射到embedding space&lt;/li>
&lt;li>layers：Transformer的主体部分，由 $n$ 个 &lt;code>DecodeLayer&lt;/code> 组成&lt;/li>
&lt;li>Norm：在输出之前，进行一次Normalization&lt;/li>
&lt;li>Position Embedding：由于输入的sequence长度是固定的，因此我们提前计算好每一层的position embedding&lt;/li>
&lt;/ol>
&lt;hr>
&lt;p>&lt;code>Transformer&lt;/code>部分的代码&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Transformer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">embedding&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Embedding&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">vocab_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pad_token_id&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">layers&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ModuleList&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">[&lt;/span>&lt;span class="n">DecodeLayer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">layer_idx&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">layer_idx&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_hidden_layers&lt;/span>&lt;span class="p">)]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">norm&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">RMSNorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">eps&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">rms_norm_eps&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">rotary_emb&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">RotaryEmbedding&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">input_ids&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="o">...&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">input_embeds&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">embedding&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">input_ids&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">input_embeds&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">position_embeddings&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">rotary_emb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">position_ids&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">decode_layer&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">layers&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">layer_outputs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">decode_layer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">position_ids&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">position_embeddings&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">layer_outputs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">norm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">logits&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;hr>
&lt;h2 id="decodelayer">DecodeLayer
&lt;/h2>&lt;p>&lt;code>DecodeLayer&lt;/code> 就是transformer的核心部分，里面包含四个模块：&lt;/p>
&lt;ol>
&lt;li>Pre-Normalization：一般是RMSNorm或者LayerNorm&lt;/li>
&lt;li>Attention：self-attention&lt;/li>
&lt;li>Post-Normalization：与Pre-Normalization一致&lt;/li>
&lt;li>MLP：FFN，SwiGLU或者MoE&lt;/li>
&lt;/ol>
&lt;p>&lt;code>DecodeLayer&lt;/code> 还会使用residual connection来防止梯度消失&lt;/p>
&lt;hr>
&lt;p>&lt;code>DecodeLayer&lt;/code>部分的代码&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">DecodeLayer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">layer_idx&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">attn&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Attention&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">layer_idx&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mlp&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">MLP&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pre_norm&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">RMSNorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">eps&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">rms_norm_eps&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">post_norm&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">RMSNorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">eps&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">rms_norm_eps&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">position_ids&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">position_embeddings&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">residual&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">hidden_states&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pre_norm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">attn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">position_ids&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">position_embeddings&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># residual&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">residual&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">residual&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">hidden_states&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">post_norm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mlp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># residual&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">residual&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">hidden_states&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;hr>
&lt;p>我们接下来按照&lt;/p>
&lt;ol>
&lt;li>Normalization&lt;/li>
&lt;li>MLP&lt;/li>
&lt;li>Attention&lt;/li>
&lt;li>Position embedding&lt;/li>
&lt;/ol>
&lt;p>的顺序来介绍&lt;/p>
&lt;hr>
&lt;h2 id="rmsnorm">RMSNorm
&lt;/h2>&lt;p>RMSNorm的作用和LayerNorm是一样的，但是实现上更简单&lt;/p>
$$
\mathrm{LayerNorm}(x) = \frac{x-\mathbb{E}[x]}{\sqrt{\mathrm{var}[x]+\epsilon}}\odot \beta + \gamma
$$&lt;p>其中 $\beta,\gamma\in\mathbb{R}^d$ 是可学习的参数&lt;/p>
$$
\mathrm{RMSNorm}(x) = \frac{x}{\sqrt{\|x\|_2^2+\epsilon}}\odot \gamma
$$&lt;p>其中 $\gamma\in\mathbb{R}^d$是可学习的参数&lt;/p>
&lt;hr>
&lt;p>RMSNorm代码实现&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">RMSNorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">eps&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">weight&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Parameter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ones&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">d&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">eps&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">eps&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">input_dtype&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dtype&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">variance&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pow&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mean&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">keepdim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">rsqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">variance&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">eps&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">weight&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">input_dtype&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;hr>
&lt;h2 id="mlp">MLP
&lt;/h2>&lt;p>现在大语言模型的MLP使用的激活函数一般都是SwiGLU, 其定义为&lt;/p>
$$
\mathrm{SwiGLU}(x) = x\odot \sigma(x)
$$&lt;p>其中 $\sigma(\cdot)$ 是sigmoid函数
MLP的定义为&lt;/p>
$$
y = W_2(W_3x\odot \mathrm{SwiGLU}(W_1x))
$$&lt;p>其中 $W_3,W_1\in\mathbb{R}^{d_{ff}\times d}$, $W_2\in\mathbb{R}^{d\times d_{ff}}$&lt;/p>
&lt;p>一般地，由于FFN只有两个权重矩阵，且 $d_{ff}=4d$, 在SwiGLU中，为了保证参数量一致，其隐藏层大小设置为 $d_{ff}&amp;rsquo;=\frac23d_{ff}=\frac83 d$.&lt;/p>
&lt;hr>
&lt;p>MLP的代码如下所示&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">SwiGLU&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sigmoid&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">MLP&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d_ff&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">gate_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">d&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d_ff&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">up_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">d&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d_ff&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">down_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">d_ff&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">down_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">SwiGLU&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">gate_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">up_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;hr>
&lt;h2 id="attention">Attention
&lt;/h2>&lt;p>我们先不考虑position embedding，直接看attention，attention定义为&lt;/p>
$$
\mathrm{Attention}(X) = \mathrm{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V\in\mathbb{R}^{m\times d}
$$&lt;p>其中$X\in\mathbb{R}^{m\times d}$,&lt;/p>
$$
Q = W_QX\in\mathbb{R}^{m\times d},\quad
K =W_KX\in\mathbb{R}^{n\times d},\quad
V = W_VX\in\mathbb{R}^{n\times d}
$$&lt;hr>
&lt;p>在自回归模型里，我们还会加上mask, 让每个token只能看见前面的token的信息&lt;/p>
$$
\mathrm{Attention}(X) = \mathrm{softmax}\left(\frac{QK^T}{\sqrt{d}}\odot M\right)V
$$&lt;p>其中&lt;/p>
$$
M = [M_{ij}] = \begin{cases}
1, &amp;\text{ if } i &lt; j\\
0, &amp;\text{ otherwise}
\end{cases}
$$&lt;hr>
&lt;p>self-attention的代码如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">scaled_dot_product_attention&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Q&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">K&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">V&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">mask&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">d_k&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Q&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="c1"># d_k&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">scaled_factor&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">d_k&lt;/span>&lt;span class="o">**&lt;/span>&lt;span class="mf">0.5&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">scores&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">einsum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;... s_q d_k, ... s_k d_k -&amp;gt; ... s_q s_k&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Q&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">K&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">scores&lt;/span> &lt;span class="o">*=&lt;/span> &lt;span class="n">scaled_factor&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">mask&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">scores&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">scores&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">masked_fill&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">mask&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">float&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;-inf&amp;#34;&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">scores&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">scores&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">softmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">einsum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;... s_q s_k, ... s_k d_v -&amp;gt; ... s_q d_v&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">scores&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">V&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;hr>
&lt;h2 id="multi-head-attention">Multi-Head Attention
&lt;/h2>$$
\mathrm{MultiHeadAttention}(X) = [\mathrm{Attention}_1(X),\dots,\mathrm{Attention}_h(X)]W_o\in\mathbb{R}^{m\times d}
$$&lt;p>
其中 $W_o\in\mathbb{R}^{d\times d}$, 且每一个Attention heads的维度会从 $d\to d/h$.&lt;/p>
&lt;p>Multi-Head Attention的主要作用为：&lt;/p>
&lt;ol>
&lt;li>让不同的head关注不同的信息&lt;/li>
&lt;li>并行计算，提高计算效率&lt;/li>
&lt;/ol>
&lt;hr>
&lt;p>MHA代码&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">MultiHeadAttention&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d_model&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_heads&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">d_model&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d_model&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">k_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">d_model&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d_model&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">v_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">d_model&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d_model&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">output_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">d_model&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d_model&lt;/span>&lt;span class="p">,)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">position_embeddings&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">mask&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">Q&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">rearrange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="s2">&amp;#34;... seq_len (num_heads head_dim) -&amp;gt; ... num_heads seq_len head_dim&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">num_heads&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_heads&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">head_dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">K&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">rearrange&lt;/span>&lt;span class="p">(&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">k_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="s2">&amp;#34;... seq_len (num_heads head_dim) -&amp;gt; ... num_heads seq_len head_dim&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">num_heads&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_heads&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">head_dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">mask&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">mask&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ones&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Q&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">K&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">mask&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tril&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">mask&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">position_embeddings&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sin&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">cos&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">position_embeddings&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">Q&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">apply_rotary_pos_emb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Q&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sin&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">cos&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">K&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">apply_rotary_pos_emb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">K&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sin&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">cos&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">V&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">rearrange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">v_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="s2">&amp;#34;... seq_len (num_heads head_dim) -&amp;gt; ... num_heads seq_len head_dim&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">num_heads&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_heads&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">head_dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">scaled_dot_product_attention&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Q&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">K&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">V&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">mask&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">mask&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">rearrange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">output&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;... num_heads seq_len head_dim -&amp;gt; ... seq_len (num_heads head_dim)&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">output_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">output&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;hr>
&lt;h2 id="position-encoding">Position Encoding
&lt;/h2>$$
\mathrm{Attention}(Q, \Pi K, \Pi V) = \mathrm{Attention}(Q, K, V)
$$&lt;p>
这里 $\Pi\in {0,1}^{d\times d}$ 是一个置换矩阵(permutation matrix)&lt;/p>
&lt;p>[[Transformer]]的解决方法是在query和key上加上位置信息：&lt;/p>
$$
Q' = Q + PE(Q),\ K'=K + PE(K)
$$&lt;p>这样&lt;/p>
$$
\mathrm{Attention}(Q, K, V) = \mathrm{softmax}\left(\frac{(Q + PE(Q))(K + PE(K))^T}{\sqrt{d}}\right)V\in\mathbb{R}^{m\times d}
$$&lt;p>
就包含了位置信息&lt;/p>
&lt;hr>
&lt;h3 id="绝对位置编码">绝对位置编码
&lt;/h3>&lt;p>[[Transformer]]的使用的位置编码如下所示&lt;/p>
$$
PE(pos, 2i) = \sin\left(\frac{pos}{10000^{2i/d}}\right)
$$$$
PE(pos, 2i+1) = \cos\left(\frac{pos}{10000^{2i/d}}\right)
$$&lt;hr>
&lt;h3 id="rope">RoPE
&lt;/h3>&lt;p>苏剑林老师提出了[[RoPE]]，现在已经被广泛使用&lt;/p>
$$
q' = R_{\theta,m}^dq, k' = R_{\theta,n}^d k
$$&lt;p>这样 $\langle q, k\rangle$ 就&lt;strong>仅&lt;/strong>包含两者的相对位置信息&lt;/p>
$$
\langle q_m, k_n\rangle = x^TW_qR_{\theta, n-m}^d W_kx_n
$$&lt;hr>
&lt;p>RoPE的矩阵定义如下&lt;/p>
$$
R_{\theta,m}^d = \mathrm{diag}(M_1,\dots,M_{d/2})
$$&lt;p>
其中&lt;/p>
$$
M_i = \begin{bmatrix}
\cos m\theta_i &amp; -\sin m\theta_i\\
\sin m\theta_i &amp; \cos m\theta_i
\end{bmatrix}
$$$$
\theta_i = \frac{1}{10000^{2(i-1)/d}}, i\in\{1,2,\dots,d/2\}
$$&lt;hr>
&lt;p>简化后得到&lt;/p>
$$
R_{\theta,m}^dq = \begin{bmatrix}
\cos m\theta_0\\
\cos m\theta_0\\
\vdots\\
\cos m\theta_{d/2}\\
\cos m\theta_{d/2}\\
\end{bmatrix}\odot \begin{bmatrix}
x1\\
x2\\
\vdots\\
x_{d-1}\\
x_d\\
\end{bmatrix} + \begin{bmatrix}
\sin m\theta_0\\
\sin m\theta_0\\
\vdots\\
\sin m\theta_{d/2}\\
\sin m\theta_{d/2}\\
\end{bmatrix}\odot
\begin{bmatrix}
-\ x_2\\
x_1\\
\vdots\\
-x_d\\
x_{d-1}\\
\end{bmatrix}
$$&lt;hr>
&lt;h3 id="rope代码naive实现">RoPE代码naive实现
&lt;/h3>&lt;p>&lt;code>RotaryEmbedding&lt;/code>代码&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">RotaryEmbedding&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">inv_freq&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">1.0&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">theta&lt;/span> &lt;span class="o">**&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)[:&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)]&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">position_ids&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">freqs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">einsum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">max_seq_len&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">inv_freq&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;seq_len, d_k_half -&amp;gt; seq_len d_k_half&amp;#34;&lt;/span>&lt;span class="p">)[&lt;/span>&lt;span class="n">token_positions&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sin&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sin&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">freqs&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">cos&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cos&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">freqs&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">sin&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">cos&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;hr>
&lt;p>计算部分代码&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">apply_rotary_pos_emb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sin&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">cos&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x_even&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">...&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">::&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="c1"># (seq_len, d_k_half)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x_odd&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">...&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">::&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="c1"># (seq_len, d_k_half)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">odds&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">cos&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">x_even&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">sin&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">x_odd&lt;/span> &lt;span class="c1"># (...,seq_len, d_k_half)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">evens&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">sin&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">x_even&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">cos&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">x_odd&lt;/span> &lt;span class="c1"># (...,seq_len, d_k_half)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">stacked&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">stack&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">odds&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">evens&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># (...,seq_len, 2, d_k_half)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">stacked_trans&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">rearrange&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">stacked&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;... seq_len double d_k_half -&amp;gt; ... seq_len d_k_half double&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span> &lt;span class="c1"># (...,seq_len, d_k_half, 2)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">out&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">rearrange&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">stacked_trans&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;... seq_len d_k_half double -&amp;gt; ... seq_len (d_k_half double)&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span> &lt;span class="c1"># (..., seq_len, d_k)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">out&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;hr>
&lt;h3 id="rope-标准实现">RoPE 标准实现
&lt;/h3>&lt;p>&lt;code>RotaryEmbedding&lt;/code>代码 (LLaMA)&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">LlamaRotaryEmbedding&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">LlamaConfig&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">device&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">inv_freq&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">inv_freq&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">1.0&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">base&lt;/span> &lt;span class="o">**&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">int64&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="c1"># d_k_half&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">position_ids&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># (bsz, d_k_half, 1)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">inv_freq_expanded&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">inv_freq&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">expand&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">position_ids&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># (bsz, 1, seq_len)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">position_ids_expanded&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">position_ids&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># (bsz, seq_len, d_k_half)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">freqs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">inv_freq_expanded&lt;/span> &lt;span class="o">@&lt;/span> &lt;span class="n">position_ids_expanded&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transpose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">emb&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cat&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">freqs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">freqs&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># (..., seq_len, d_k)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">cos&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">emb&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cos&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="c1"># (..., seq_len, d_k)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sin&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">emb&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sin&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="c1"># (..., seq_len, d_k)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">cos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sin&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;hr>
&lt;p>计算部分代码&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">rotate_half&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">...&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">...&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="p">:]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cat&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">x2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x1&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">apply_rotary_pos_emb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">q&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">cos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sin&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">position_ids&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">unsqueeze_dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">cos&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">cos&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">unsqueeze&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">unsqueeze_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sin&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">sin&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">unsqueeze&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">unsqueeze_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q_embed&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">q&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">cos&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">rotate_half&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">q&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">sin&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">k_embed&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">k&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">cos&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">rotate_half&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">sin&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">q_embed&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k_embed&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;hr>
&lt;h1 id="参考文献">参考文献
&lt;/h1>&lt;ol>
&lt;li>&lt;a class="link" href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen3/modeling_qwen3.py" target="_blank" rel="noopener"
>Qwen3 transformer source code&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://huggingface.co/blog/designing-positional-encoding" target="_blank" rel="noopener"
>position encoding blog&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>Unified perspective on dLLM and LLM</title><link>https://maosong2022.github.io/p/unified-perspective-on-dllm-and-llm/</link><pubDate>Sat, 28 Jun 2025 15:02:09 +0800</pubDate><guid>https://maosong2022.github.io/p/unified-perspective-on-dllm-and-llm/</guid><description>&lt;h1 id="introduction">Introduction
&lt;/h1>&lt;p>在上一篇blog里，我们介绍了MLE和KL minimization的等价性。在这篇blog里，我们将要基于这个等价性，推导masked diffusion LLM, autoregressive LLM, any-order diffusion LLM之间的等价性。最终，我们发现，这几种建模方式本质上都是一致的。&lt;/p>
&lt;h1 id="preliminary">Preliminary
&lt;/h1>&lt;p>对于 $\bm{x}=(x_1,\dots,x_L)\in\mathbb{R}^L$, 基于概率的链式法则，我们有&lt;/p>
$$
p(\bm{x}) = \prod_{i=1}^Lp(x_i\mid x_{&lt;i})
$$&lt;p>1D的类别分布，对于随机变量 $X\in{1,\dots, K}$以及概率分布 $\bm{p}=(p_1,\dots,p_K)$ 我们定义类别分布为 $\mathrm{Cat}(\bm{x};\bm{p})$, 其中 $\bm{x}\in\mathbb{R}^K$是一个向量，其PMF定义为：&lt;/p>
$$
f(\bm{x}; \bm{p}) = \prod_{i=1}^Kp_i^{[x=i]}
$$&lt;p>这里 $[x=i]$ 当 $x=i$时为$1$，否则为 $0$.&lt;/p>
&lt;p>一个比较好理解的例子是骰子，骰子有$K=6$种可能性，每一面出现的可能性都是 $p_i=1/6$. 当 $\bm{x}$是one-hot向量是，其代表了出现某一面的概率。&lt;/p>
&lt;p>Discrete-Time Discrete Markov Chain. 接下来我们考虑离散时间离散Markov链，我们现在的随机变量不仅与状态 ${1,\dots,K}$ 还与时间有关，我们记 $X_n$ 为 $n$ 时刻的状态分布。这样，我们可以规定不同时刻之间的状态转换矩阵：&lt;/p>
$$
Q_{ij} = \mathrm{Pr}(x_{n+1}=j\mid x_n=i)
$$&lt;h2 id="categorical-distribution">Categorical distribution
&lt;/h2>&lt;h1 id="autoregressive-llm">autoregressive LLM
&lt;/h1>&lt;p>首先是我们最常见的自回归大语言模型，给定文本语料 $\bm{x}=(x_1,\dots,x_L)\sim p_{data}$， 大语言模型的目标在于求解最大似然估计&lt;/p>
$$
\begin{aligned}
\theta^* &amp;= \arg\max_{\theta}\mathbb{E}_{\bm{x}\sim p_{data}}[\log p(\bm{x}\mid \theta))]\\
&amp;= \arg\max_{\theta}\mathbb{E}_{\bm{x}\sim p_{data}}\left[\log \left(\prod_{i=1}^Lp(x_i\mid x_{&lt;i},\theta )\right) \right]\\
&amp;= \arg\max_{\theta}\mathbb{E}_{\bm{x}\sim p_{data}}\left[\sum_{i=1}^L\log p(x_i\mid x_{&lt;i},\theta ) \right]
\end{aligned}
$$&lt;p>这里我们使用了链式法则.&lt;/p>
&lt;h1 id="any-order-autoregressive-llm">Any order autoregressive LLM
&lt;/h1>&lt;p>接下来，我们证明Any order autoregressive LLM的目标函数等价于求解MLE。
令 $S_D$ 为在集合 ${1,\dots, D}$ 上所有可能的permutation，令 $\sigma\in S_D$ 为一个其中的一个permutation，则我们有&lt;/p>
$$
p(\sigma) = \frac{1}{D!}
$$&lt;p>因此&lt;/p>
$$
p(x) = \sum_{\sigma\in S_d}p(x,\sigma) = \sum_{\sigma\in S_D}p(x\mid \sigma)p(\sigma) = \frac{1}{D!}\sum_{\sigma\in S_D}p(x\mid \sigma)=\mathbb{E}_{\sigma\sim U(S_D)}[p(x\mid \sigma)]
$$&lt;p>这里 $U(\cdot)$ 代表均匀分布，从而我们有&lt;/p>
$$
\log p(x) = \log \mathbb{E}_{\sigma\sim U(S_D)}[p(x\mid \sigma)] \geq \mathbb{E}_{\sigma\sim U(S_D)}[\log p(x\mid \sigma)]
$$&lt;p>
这里我们使用了Jensen不等式&lt;/p>
&lt;p>那么对于any-order autoregressive model, 我们有：&lt;/p>
$$
\log p(x) \geq \mathbb{E}_{\sigma\sim U(S_D)}\sum_{t=1}^D \log p(x_{\sigma(t)}\mid x_{\sigma(&lt;t)})
$$&lt;p>也就是说，对于any-order autoregressive model，我们的目标函数是MLE的一个下界。&lt;/p>
&lt;h1 id="masked-language-modeling">Masked Language modeling
&lt;/h1>&lt;h1 id="discrete-diffusion-model">Discrete Diffusion Model
&lt;/h1>&lt;h1 id="any-order-diffusion-llm">Any-order diffusion LLM
&lt;/h1>&lt;h1 id="masked-diffusion-llm">Masked diffusion LLM
&lt;/h1>&lt;h1 id="reference">Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2107.03006" target="_blank" rel="noopener"
>Structured Denoising Diffusion Models in Discrete State-Spaces&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Relationship between MLE and KL divergence</title><link>https://maosong2022.github.io/p/relationship-between-mle-and-kl-divergence/</link><pubDate>Fri, 27 Jun 2025 11:35:33 +0800</pubDate><guid>https://maosong2022.github.io/p/relationship-between-mle-and-kl-divergence/</guid><description>&lt;h1 id="mle">MLE
&lt;/h1>&lt;p>最大似然估计，即MLE (maximum likelihood estimation), 是一个估计参数分布的方法，其核心思想是：模型的参数，应该让观察样本出现的概率最大。&lt;/p>
&lt;p>假设我们有一个参数分布 $p(x\mid \theta)$, 其中 $\theta$ 是参数，如正态分布中的均值和方差。我们从$p(x\mid \theta)$进行采样得到 $i.i.d.$ 的数据 $X={x_1,\dots,x_n}$.&lt;/p>
&lt;p>似然函数 (likelihood function) 定义为给定数据 $X$ 的联合分布，即：&lt;/p>
$$
\mathcal{L}(\theta\mid X) = P(X\mid \theta)
$$&lt;p>由于 $X={x_1,\dots,x_n}$ 是 $i.i.d.$, 因此，我们可以将上式改写为：&lt;/p>
$$
\mathcal{L}(\theta\mid X) = \prod_{i=1}^n p(x_i\mid \theta)
$$&lt;p>这样我们的优化目标就是&lt;/p>
$$
\begin{aligned}
\theta_{MLE}^* &amp;= \arg\max_{\theta} \mathcal{L}(\theta\mid X)\\
&amp;= \arg\max_{\theta} \prod_{i=1}^n p(x_i\mid \theta)\\
&amp;= \arg\max_{\theta} \log\prod_{i=1}^n p(x_i\mid \theta)\\
&amp;=\arg\max_{\theta} \sum_{i=1}^n \log p(x_i\mid \theta)\\
\end{aligned}
$$&lt;p>即&lt;/p>
$$
\theta_{MLE}^* = \arg\max_{\theta} \sum_{i=1}^n \log p(x_i\mid \theta)
$$&lt;h1 id="kl-divergence">KL divergence
&lt;/h1>&lt;p>KL divergence 用于衡量概率分布 $Q(x)$ 到概率分布 $P(x)$ 的不同程度，我们可以将其理解为：如果我们用 $Q(x)$来替换 $P(x)$, 会有多大的信息损失？&lt;/p>
&lt;p>连续概率分布的KL divergence的定义如下&lt;/p>
$$
D_{KL}(P\mid\mid Q) =\int P(x)\log\left(\frac{P(x)}{Q(x)}\right)dx
$$&lt;p>
离散概率分布的KL divergence定义如下&lt;/p>
$$
D_{KL}(P\mid\mid Q) = \sum_{x} P(x)\log\left(\frac{P(x)}{Q(x)}\right)
$$&lt;p>KL divergence有两个关键性质：&lt;/p>
&lt;ol>
&lt;li>非负性：$D_{KL}(P\mid\mid Q)\geq0$, 且 $D_{KL}(P\mid\mid Q)=0$ 当且仅当 $P(x)=Q(x)$ 对任意 $x$成立&lt;/li>
&lt;li>非对称性： 一般情况下，$D_{KL}(P\mid\mid Q)\neq D_{KL}(Q\mid\mid P)$.&lt;/li>
&lt;/ol>
&lt;h1 id="mle和kl-divergence的等价性">MLE和KL Divergence的等价性
&lt;/h1>&lt;p>我们假设 $p_{data}(x)$ 是数据$X$的真实分布， 我们现在需要找到合适的参数 $\theta$ 以及其对应的分布 $p(x\mid \theta)$ 来近似 $p_{data}(x)$, 此时我们可以用KL Divergence作为我们的目标函数，即&lt;/p>
$$
\theta_{KL} = \arg\min_{\theta}D_{KL}(p_{data}(x)\mid\mid p(x\mid \theta))
$$&lt;p>我们将上面的式子进行展开得到&lt;/p>
$$
\begin{aligned}
\theta_{KL}^* &amp;= \arg\min_{\theta}D_{KL}(p_{data}(x)\mid\mid p(x\mid \theta))\\
&amp;= \arg\min_{\theta} \int p_{data}(x)\frac{p_{data}(x)}{p(x\mid \theta)} dx\\
&amp;= \arg\min_{\theta}\int p_{data}(x)\log p_{data}(x) dx - \int p_{data}(x)\log p(x\mid \theta)dx \\
&amp;= \arg\min_{\theta} - \int p_{data}(x)\log p(x\mid \theta)dx \\
&amp;= \arg\max_{\theta} \int p_{data}(x)\log p(x\mid \theta)dx
\end{aligned}
$$&lt;p>实际上，真实的数据分布 $p_{data}(x)$ 是未知的，我们只有从 $p_{data}(x)$ 采样得到的一批数据 $X={x_1,\dots,x_n}\sim p_{data}(x)$.&lt;/p>
&lt;p>基于大数定律，我们有&lt;/p>
$$
\frac{1}{n}\sum_{i=1}^n\log p(\theta_i\mid \theta)=\mathbb{E}_{x\sim p_{data}}[\log p(x\mid \theta)] = \int p_{data}(x)\log p(x\mid \theta)dx, n\to \infty
$$&lt;p>这样，最大似然估计就与最小化KL divergence构建起了联系：&lt;/p>
$$
\begin{aligned}
\theta_{MLE}^*&amp;=\arg\max_{\theta} \sum_{i=1}^n \log p(x_i\mid \theta)\\
&amp;= \arg\max_{\theta} \int p_{data}(x)\log p(x\mid \theta)dx\\
&amp;= \theta_{KL}^*, n\to\infty.
\end{aligned}
$$&lt;p>也就是说，当采样样本足够多的时候，最大似然估计和最小KL divergence是等价的。&lt;/p></description></item><item><title>Notes on MiMo-VL</title><link>https://maosong2022.github.io/p/notes-on-mimo-vl/</link><pubDate>Thu, 05 Jun 2025 10:51:43 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-mimo-vl/</guid><description>&lt;h1 id="介绍">介绍
&lt;/h1>&lt;p>小米在5月30号发布了MiMo-VL 7B多模态推理大模型，模型包括SFT和RL两个版本，作者在技术报告里讲解了MiMo-VL的架构，预训练和后训练过程，最后对MiMo-VL的性能进行了评测&lt;/p>
&lt;h1 id="架构">架构
&lt;/h1>&lt;p>MiMo-VL 7B的架构是一个标准的 &amp;ldquo;ViT-MLP-LLM&amp;quot;的架构，其中：&lt;/p>
&lt;ul>
&lt;li>ViT使用了Qwen2.5-VL的Qwen2.5-ViT&lt;/li>
&lt;li>MLP是一个两层的SwiGLU MLP Layer&lt;/li>
&lt;li>LLM是之前发布的MiMo-7B&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-mimo-vl/architecture.png"
width="1237"
height="798"
srcset="https://maosong2022.github.io/p/notes-on-mimo-vl/architecture_hu14459510433823787530.png 480w, https://maosong2022.github.io/p/notes-on-mimo-vl/architecture_hu17770933799094781156.png 1024w"
loading="lazy"
alt="Architecture of MiMo-VL 7B"
class="gallery-image"
data-flex-grow="155"
data-flex-basis="372px"
>&lt;/p>
&lt;h1 id="预训练">预训练
&lt;/h1>&lt;h2 id="数据">数据
&lt;/h2>&lt;p>预训练阶段一共使用了2.4T的token&lt;/p>
&lt;ul>
&lt;li>Image caption data: 使用了rewrite caption等方法提升和过滤数据，作者还基于MetaCLIP来降低数据的不平衡性&lt;/li>
&lt;li>Interleaved data: 主要从webpage, books, papers中提取，最后基于relevance, complementarity, balance of information density来保证数据的质量&lt;/li>
&lt;li>OCR and grounding data: OCR数据包括文档，表格，数学公式等。grounding data包括单物体和多物体&lt;/li>
&lt;li>video data: rewrite caption, caption有对应的时间戳，加入了video analysis数据&lt;/li>
&lt;li>GUI data: 覆盖mobile, web, desktop三种场景, 加入了GUI grounding和GUI action数据&lt;/li>
&lt;li>Synthetic reasoning data:使用一个reasoning model来生成带有思考过程的reasoning数据&lt;/li>
&lt;/ul>
&lt;h2 id="训练">训练
&lt;/h2>&lt;p>训练有四个阶段，基本上和之前的模型一致，先训练MLP，然后解冻ViT，再训练全部参数，最后扩展模型的上下文。训练过程如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-mimo-vl/pretraining.png"
width="1339"
height="570"
srcset="https://maosong2022.github.io/p/notes-on-mimo-vl/pretraining_hu9951247225304577295.png 480w, https://maosong2022.github.io/p/notes-on-mimo-vl/pretraining_hu7419219620267078791.png 1024w"
loading="lazy"
alt="Pretraining of MiMo-VL 7B"
class="gallery-image"
data-flex-grow="234"
data-flex-basis="563px"
>&lt;/p>
&lt;h1 id="后训练">后训练
&lt;/h1>&lt;p>作者提出了Mixed On-policy Reinforcement Learning (MORL)框架来把RLVR和RLHF结合起来&lt;/p>
&lt;h2 id="数据-1">数据
&lt;/h2>&lt;p>RLVR的数据包括:&lt;/p>
&lt;ol>
&lt;li>visual reasoning, 80K数据，使用rule-based math-verify library来评估&lt;/li>
&lt;li>text reasoning, 使用了MiMo的数学推理数据，评估方式与visual reasoning一致&lt;/li>
&lt;li>Image grounding, 包括general and GUI grounding任务，使用GIoU来计算奖励&lt;/li>
&lt;li>Visual Counting, 使用准确率来打分&lt;/li>
&lt;li>Temporal Video Grounding, temporal video grounding任务，使用IoU来计算奖励&lt;/li>
&lt;/ol>
&lt;p>RLHF的数据包括中英文的query，作者使用MiMo-VL-7B和其他模型来采样，然后使用一个advanced VLM来排序.
RLHF的reward model使用Breadley-Terry model作为训练目标，text-only reward由MiMo-7B初始化得到，多模态reward model由MiMo-VL-7B初始化得到&lt;/p>
&lt;h2 id="训练-1">训练
&lt;/h2>&lt;p>训练过程就是将RLVR和RLHF放在一起进行训练，作者使用了MiMo的seamless rollout engine.&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-mimo-vl/RL_training.png"
width="1328"
height="466"
srcset="https://maosong2022.github.io/p/notes-on-mimo-vl/RL_training_hu16717293030641612071.png 480w, https://maosong2022.github.io/p/notes-on-mimo-vl/RL_training_hu14101128257135095160.png 1024w"
loading="lazy"
alt="RL Training"
class="gallery-image"
data-flex-grow="284"
data-flex-basis="683px"
>&lt;/p>
&lt;p>训练目标与GRPO一致，但是本文中作者使用了on-policy的训练方式。&lt;/p>
&lt;h1 id="评估">评估
&lt;/h1>&lt;p>作者评估了MiMo-VL-7B的通用能力，reasoning能力，GUI交互理解能力，最后还给出了MiMo-VL-7B在ChatbotArena上的排名&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-mimo-vl/Performance.png"
width="1367"
height="688"
srcset="https://maosong2022.github.io/p/notes-on-mimo-vl/Performance_hu14893086811555912207.png 480w, https://maosong2022.github.io/p/notes-on-mimo-vl/Performance_hu14858348037266394856.png 1024w"
loading="lazy"
alt="Performance of MiMo-VL-7B"
class="gallery-image"
data-flex-grow="198"
data-flex-basis="476px"
>&lt;/p>
&lt;h2 id="ablation-study">Ablation study
&lt;/h2>&lt;p>作者首先探究了在预训练阶段加入Long CoT数据对模型表现的影响，结果发现模型正在MMMU, OlympiadBench等benchmark上都有了提升&lt;/p>
&lt;p>作者还比较了原始的GRPO和本文中使用的on-policy版本的GRPO，结果发现on-policy版本的GRPO效果更好&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-mimo-vl/on-policy-GRPO-ablation.png"
width="1368"
height="843"
srcset="https://maosong2022.github.io/p/notes-on-mimo-vl/on-policy-GRPO-ablation_hu2940336130466218128.png 480w, https://maosong2022.github.io/p/notes-on-mimo-vl/on-policy-GRPO-ablation_hu1129411055183592852.png 1024w"
loading="lazy"
alt="Ablation of on-policy GRPO"
class="gallery-image"
data-flex-grow="162"
data-flex-basis="389px"
>&lt;/p>
&lt;p>最后，作者探讨了一下将不同任务放在一起训练导致的互相干扰问题，这个在MiMo里也提到过，核心问题就是有的任务是short CoT的，有的任务是Long CoT的，如何让模型根据任务来选择不同的reasoning length是一个需要探讨的问题，这个问题在&lt;a class="link" href="https://maosong.website/p/notes-on-vc-ppo/" target="_blank" rel="noopener"
>VC-PPO&lt;/a>里进行了初步的探讨。&lt;/p>
&lt;h1 id="结论">结论
&lt;/h1>&lt;p>与MiMo一样，MiMo-VL-7B通过在预训练阶段加入reasoning data，来提高模型的reasoning能力，并指出模型可以通过这个方式来媲美其他模型的表现&lt;/p>
&lt;h1 id="参考文献">参考文献
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://github.com/XiaomiMiMo/MiMo-VL" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Hands on LLM(1) Tokenizer</title><link>https://maosong2022.github.io/p/hands-on-llm1-tokenizer/</link><pubDate>Sat, 24 May 2025 19:56:34 +0800</pubDate><guid>https://maosong2022.github.io/p/hands-on-llm1-tokenizer/</guid><description>&lt;h1 id="tokenizer总结">Tokenizer总结
&lt;/h1>&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>在自然语言处理中，tokenizer的作用是将一个文本序列通过一个字典转化为一个token id的序列。我们回顾图片分类任务，模型在预测的时候，实际上预测的是类别对应的id，而不是类别本身。tokenizer做的事情就是提供一个类似于从类别到对应id的字典。&lt;/p>
&lt;p>一般来说，一个tokenizer处理文本序列的过程有两步：&lt;/p>
&lt;ol>
&lt;li>pre-tokenize，也就是预处理，我们需要将文本序列分割成合适大小的chunks (words)&lt;/li>
&lt;li>tokenize，构建chunks (words)到token id的映射&lt;/li>
&lt;/ol>
&lt;p>实际上, huggingface的tokenizer包括&lt;a class="link" href="https://huggingface.co/docs/tokenizers/pipeline" target="_blank" rel="noopener"
>四个步骤&lt;/a>, 其中第二第三个步骤与上述一致. 在pre-tokenize之前, 我们有一个normalization过程, 该过程会对文本序列进行处理, 如将文本序列变为小写, 删掉声调符号等, 如下面例子所示：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">tokenizers&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">normalizers&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">tokenizers.normalizers&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">NFD&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">StripAccents&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">normalizer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">normalizers&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Sequence&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">NFD&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">StripAccents&lt;/span>&lt;span class="p">()])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">normalizer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">normalize_str&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Héllò hôw are ü?&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># &amp;#34;Hello how are u?&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>在tokenize之后, 我们会有一个post-processing过程, 比如BERT会在生成的token系列前后加入 &lt;code>[CLS]&lt;/code> token 和 &lt;code>[SEP]&lt;/code> token, 例子如下:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">transformers&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">AutoTokenizer&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">tokenizer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">AutoTokenizer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">from_pretrained&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;bert-base-cased&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">token_ids&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tokenizer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">encode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;I love NLP.&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">token_ids&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># [101, 146, 1567, 21239, 2101, 119, 102]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># represents [[CLS], &amp;#34;I&amp;#34;, &amp;#34;love&amp;#34;, &amp;#34;NL&amp;#34;, &amp;#34;##P&amp;#34;, &amp;#34;.&amp;#34;, [SEP]]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>其完整流程如下图所示 (图源: &lt;a class="link" href="https://huggingface.co/learn/llm-course/chapter6/4?fw=pt" target="_blank" rel="noopener"
>huggingface llm-course&lt;/a>)&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/hands-on-llm1-tokenizer/tokenization_pipeline.png"
width="1702"
height="1234"
srcset="https://maosong2022.github.io/p/hands-on-llm1-tokenizer/tokenization_pipeline_hu10346641896860173084.png 480w, https://maosong2022.github.io/p/hands-on-llm1-tokenizer/tokenization_pipeline_hu472026015685023953.png 1024w"
loading="lazy"
alt="tokenization pipeline"
class="gallery-image"
data-flex-grow="137"
data-flex-basis="331px"
>&lt;/p>
&lt;p>构建好tokenizer之后, 我们还要保证tokenizer提供两个接口：&lt;/p>
&lt;ol>
&lt;li>encoding, 给定文本序列, 将其映射到字典中去得到token id序列&lt;/li>
&lt;li>decoding, 给定token id序列, 将其解码成文本序列&lt;/li>
&lt;/ol>
&lt;p>接下来, 我们将简单介绍一下word tokenizer, character tokenizer以及byte tokenizer, 并分析它们各自的不足。
然后, 我们介绍现代大语言模型中使用最多的BPE tokenizer。最后, 我们介绍一些sub-word tokenizer。&lt;/p>
&lt;h2 id="training-free-tokenizer">Training-free tokenizer
&lt;/h2>&lt;p>本节我们将要介绍word tokenizer, character tokenizer以及byte tokenizer, 它们的特点就是简单易懂, 不需要额外的规则和学习.&lt;/p>
&lt;h3 id="word-tokenizer">Word tokenizer
&lt;/h3>&lt;p>给定一个文本序列, 我们现在需要将其转化为一个token序列。一个比较自然的想法是, 我们按照空格将序列拆分成若干个单词, 这样每个单词的语义都能比较好地保留。下面是一个例子：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">tiktoken&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">tokenizer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tiktoken&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_encoding&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;gpt2&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">indices&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tokenizer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">encode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;hello world&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># indices = [31373, 995]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># decode = [&amp;#34;hello&amp;#34;, &amp;#34; world&amp;#34;]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>接下来我们基于一个预定义好的词典, 将其转化为一个token id的序列。&lt;/p>
&lt;p>word tokenizer的优点是能够保留语义信息，且压缩率比较高（每个token包含的bytes数），其问题是不能处理预定义好的词典之外的词 (out of vocabulary, OOV)。现有的处理方法是使用 &lt;code>&amp;lt;UNK&amp;gt;&lt;/code> token来表示这些OOV的词。
但这样显然会丢失语义信息, 因为我们编码成 &lt;code>&amp;lt;UNK&amp;gt;&lt;/code> token之后, 就没办法再解码回原有的语义信息了。&lt;/p>
&lt;p>word tokenizer的缺点为：&lt;/p>
&lt;ol>
&lt;li>单词数量很大, 很多罕见单词的出现频率很低, 降低了tokenizer的利用率&lt;/li>
&lt;li>对于不在词典内的单词只能用&lt;code>&amp;lt;UNK&amp;gt;&lt;/code> token表示, 损害了语义信息&lt;/li>
&lt;/ol>
&lt;p>既然基于word的tokenizer有OOV的问题，我们能否想办法解决这个问题呢？答案是可以的, 我们可以使用 character tokenizer。&lt;/p>
&lt;h3 id="character-tokenizer">Character tokenizer
&lt;/h3>&lt;p>Character tokenizer的基本思想是使用字符而不是单词来编码文本序列。其实现方式如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">CharacterTokenizer&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">encode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">list&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="nb">list&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">ord&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">c&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">c&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">decode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">token_ids&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">list&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">join&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">chr&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">token_id&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">token_id&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">token_ids&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>character tokenizer的词表大小取决于我们的编码方式，UTF-8的编码大概有&lt;a class="link" href="https://en.wikipedia.org/wiki/UTF-8" target="_blank" rel="noopener"
>110K code points&lt;/a>。character tokenizer的缺点总结如下：&lt;/p>
&lt;ol>
&lt;li>character tokenizer会导致我们的词表非常大&lt;/li>
&lt;li>和word tokenizer一样, 很多character非常罕见, 会降低词表的利用率&lt;/li>
&lt;li>token序列的上下文语义信息较差&lt;/li>
&lt;/ol>
&lt;h3 id="byte-tokenizer">Byte tokenizer
&lt;/h3>&lt;p>我们发现, character tokenizer和word tokenizer的词表都很大, 我们能否想办法降低词表大小, 提升每个token的利用率呢？答案是使用Byte tokenizer.&lt;/p>
&lt;p>Byte tokenizer的基本思想是, 所有的字符(character)都是由byte组成的, 比如对于UTF-8编码来说, 每个字符由1-4个byte组成。
因此, 所有满足UTF-8编码的文本, 我们都可以将它们转换为基于byte的token序列。
由于现在的大部分文本都是基于UTF-8的, 因此, 我们只讨论UTF-8编码的文本。&lt;/p>
&lt;p>Byte tokenizer的实现如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">ByteTokenizer&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">encode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">list&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="nb">list&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">encode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;utf-8&amp;#34;&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">decode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">token_ids&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">list&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="nb">bytes&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">token_ids&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">decode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;utf-8&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>byte tokenizer的词表很小, 其词表大小为 &lt;code>256&lt;/code>, 这是因为一个byte可以有256中可能的值.&lt;/p>
&lt;p>尽管byte tokenizer实现简单，并且词表也很小，可以说byte tokenizer解决了character tokenizer和word tokenizer的问题。
但是，byte tokenizer的问题在于，其encode的到的token序列可能会非常长！我们知道，transformer计算量与token序列的长度是平方级关系的，也就是说token序列长度增加10倍，整体的计算量就会增加100倍，因此我们势必需要考虑token序列的长度。&lt;/p>
&lt;p>总之，byte tokenizer的问题为：&lt;/p>
&lt;ol>
&lt;li>产生的token序列过长, 增加了transformer的计算量&lt;/li>
&lt;li>没有上下文语义信息&lt;/li>
&lt;/ol>
&lt;h3 id="总结">总结
&lt;/h3>&lt;p>我们总结一下word tokenizer, character tokenizer以及byte tokenizer三者各自的特点:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Feature&lt;/th>
&lt;th>Word Tokenizer&lt;/th>
&lt;th>Character Tokenizer&lt;/th>
&lt;th>Byte Tokenizer&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Granularity&lt;/td>
&lt;td>Coarse&lt;/td>
&lt;td>Medium&lt;/td>
&lt;td>Fine&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Vocabulary&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>No&lt;/td>
&lt;td>No&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Support OOV&lt;/td>
&lt;td>Bad&lt;/td>
&lt;td>Good&lt;/td>
&lt;td>Best&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>#Tokens&lt;/td>
&lt;td>Small&lt;/td>
&lt;td>Large&lt;/td>
&lt;td>Very Large&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Chinese&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Support Spell Error&lt;/td>
&lt;td>Bad&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Context&lt;/td>
&lt;td>Good&lt;/td>
&lt;td>Bad&lt;/td>
&lt;td>Worst&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>因此, 这三种tokenizer尽管实现起来很简单, 但是其都有各自的问题. 为了解决这些问题, 我们的做法就是折衷, 使用sub-word tokenizer, 也就是介于word tokenizer和byte tokenizer之间的方法.&lt;/p>
&lt;h2 id="bpe">BPE
&lt;/h2>&lt;h3 id="基本原理与实现">基本原理与实现
&lt;/h3>&lt;p>实际生活中，对于出现频率比较高的词，我们会有一个简写的方式，也就是我们使用一个新的单词来表示这个词。比如在英语中，我们会使用&lt;code>plz&lt;/code> 来代替 &lt;code>please&lt;/code> 以及使用&lt;code>how r u&lt;/code> 来代替&lt;code>how are you&lt;/code>。
BPE，即byte pair tokenizer的原理也是类似的，对于出现频率比较高的byte pair或者character pair, 我们会使用一个新的token来表示这个pair，这样就压缩了sequence的长度。&lt;/p>
&lt;p>BPE算法包括以下几个步骤:&lt;/p>
&lt;ol>
&lt;li>对文本序列进行pre-tokenize, 分割成不同的单词&lt;/li>
&lt;li>当&lt;code>len(vocab)&amp;lt;vocab_size&lt;/code>时, 重复以下步骤:
&lt;ol>
&lt;li>对所有单词, 统计其相邻character或者byte pair的频率&lt;/li>
&lt;li>计算出现频率最高的pair, 使用一个新的token来表示这个pair&lt;/li>
&lt;li>将新的token和其对应的&lt;code>token_id&lt;/code>加入到&lt;code>vocab&lt;/code>中, 并更新单词的分割表示&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;p>算法如下图所示 (参考文献2)&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/hands-on-llm1-tokenizer/bpe_algorithm.png"
width="724"
height="828"
srcset="https://maosong2022.github.io/p/hands-on-llm1-tokenizer/bpe_algorithm_hu8572890471772692345.png 480w, https://maosong2022.github.io/p/hands-on-llm1-tokenizer/bpe_algorithm_hu4951491798023889503.png 1024w"
loading="lazy"
alt="BPE algorithm"
class="gallery-image"
data-flex-grow="87"
data-flex-basis="209px"
>&lt;/p>
&lt;blockquote>
&lt;p>注意：实际上，我们实现的是BBPE (byte BPE算法)，BBPE与BPE的区别在于我们的最小单元是character还是bytes。本质上原理是一致的&lt;/p>
&lt;/blockquote>
&lt;p>实现代码见 &lt;a class="link" href="https://github.com/MaoSong2022/assignment1-basics/blob/main/cs336_basics/naive_bpe.py" target="_blank" rel="noopener"
>Github naive BPE&lt;/a>&lt;/p>
&lt;h3 id="高效实现">高效实现
&lt;/h3>&lt;p>BPE的原理很简单, 我们也实现了其naive版本, 但是naive版本的问题是太慢了。因此我们将要优化naive版本的效率。&lt;/p>
&lt;p>首先我们发现, 我们不需要遍历所有的word, 只有含有&lt;code>best_pair&lt;/code>的word我们才会进行处理, 因此, 我们的第一个改进就是使用 &lt;code>pair_to_word&lt;/code> 来记录每个pair的来源, 比如：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">pair_to_word&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">(&lt;/span>&lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39; &amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;t&amp;#39;&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39; the&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39; it&amp;#39;&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">(&lt;/span>&lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;t&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;h&amp;#39;&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;the&amp;#39;&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>这样, 我们在merge的时候, 直接使用 &lt;code>pair_to_word[best_pair]&lt;/code> 来获取需要被更新的token序列就可以了。&lt;/p>
&lt;p>其次, 注意到每次merge之后, 我们都需要重新计算一次 &lt;code>pair_freq&lt;/code>, 而实际上, 只有被merge的token序列才需要被重新计数, 其他大部分token序列都是不需要重新计数的。
因此, 一个改进点就是我们在merge的过程中就更新 &lt;code>pair_freq&lt;/code>, 而不是重新计算。为了达到这个目标, 我们其实只需要两个操作。
我们用&lt;code>(b'x', b'a', b'b', b'y')&lt;/code> 和 &lt;code>best_pair=(b'a', b'b')&lt;/code>来说明, merge之前, 这个序列贡献的&lt;code>pair_freq&lt;/code>为：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">(&lt;/span>&lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;x&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;a&amp;#39;&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">(&lt;/span>&lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;a&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;b&amp;#39;&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">(&lt;/span>&lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;b&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;y&amp;#39;&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>merge之后, token序列变成了&lt;code>(b'x', b'z', b'y')&lt;/code> (假设&lt;code>best_pair&lt;/code>对应的新的token为&lt;code>b'z'&lt;/code>), 这时候的计数为:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">(&lt;/span>&lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;x&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;a&amp;#39;&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">(&lt;/span>&lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;a&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;b&amp;#39;&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">(&lt;/span>&lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;b&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;y&amp;#39;&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">(&lt;/span>&lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;x&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;z&amp;#39;&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">(&lt;/span>&lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;z&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;y&amp;#39;&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>也就是说, merge之后, 三个pair的计数减少了1, 分别是&lt;code>(token_seq[i-1], merge_pair[0])&lt;/code>,&lt;code>merge_pair&lt;/code> 和 &lt;code>(merge_pair[1], token_seq[i+2])&lt;/code>。两个pair的个数增加了1, 分别是 &lt;code>(token_seq[i-1], new_token)&lt;/code>和&lt;code>(new_token, token_seq[i+2])&lt;/code> (这里我们假设&lt;code>merge_pair=(token_seq[i], token_seq[i+1])&lt;/code>)。&lt;/p>
&lt;p>基于这个结论，我们就可以优化BPE算法了，具体逻辑就是：&lt;/p>
&lt;ol>
&lt;li>pretokenize, 将 text 切分为若干个 word&lt;/li>
&lt;li>计算&lt;code>word_count&lt;/code>, &lt;code>pair_freq&lt;/code>, &lt;code>pair_to_word&lt;/code>, 使用&lt;code>splits&lt;/code>记录每个word对应的token分布&lt;/li>
&lt;li>重复以下过程：
&lt;ol>
&lt;li>挑选频率最高的pair将其merge为一个新的token, 基于&lt;code>pair_to_words&lt;/code>更新对应的&lt;code>pair_freq&lt;/code>&lt;/li>
&lt;li>对每个&lt;code>split&lt;/code>, 按照上述方式更新&lt;code>pair_freq&lt;/code>和&lt;code>split&lt;/code>&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;p>其具体实现见 &lt;a class="link" href="https://github.com/MaoSong2022/assignment1-basics/blob/main/cs336_basics/efficient_bpe.py" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/p>
&lt;h2 id="other-subword-tokenizers">Other subword tokenizers
&lt;/h2>&lt;h3 id="wordpiece">WordPiece
&lt;/h3>&lt;p>WordPiece是Google在预训练BERT时采用的tokenizer，WordPiece的基本思想和BPE差不多，都是从一个较小的vocab开始的。&lt;/p>
&lt;p>首先，WordPiece会通过加上prefix &lt;code>##&lt;/code>来把单词进行切分，比如 &lt;code>word&lt;/code> 会被拆分为：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;w&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;##o&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;##r&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;##d&amp;#39;&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>接下来，对于pair $(a, b)$, WordPiece定义了merge pair的规则如下：&lt;/p>
$$
\mathrm{score}((a, b)) = \frac{\#(a, b)}{\#a \times \#b}
$$&lt;p>其中 $#(a, b)$, $#a$, $#b$ 分别代表 pair $(a, b)$, 元素 $a$ 和元素 $b$ 的频率。
通过这个方式，我们会给予包含元素出现频率较低的pair更高的优先级。通过这个方式，我们选取score最高的pair，然后将其用一个新的token表示，然后和BPE算法一样，继续这一过程直到我们的vocab size达到指定大小。&lt;/p>
&lt;p>在tokenize的时候，WordPiece会找出现在vocab中的最长的subword, 比如对于&lt;code>'hugs'&lt;/code>, 假设从左向右在词典中的最长subword是&lt;code>'hug'&lt;/code>, 那么&lt;code>'hugs'&lt;/code> 就会被拆分为 &lt;code>['hug', '##s']&lt;/code>。如果我们在词表中找不到对应的subword，这个时候我们就会使用&lt;code>'[UNK]'&lt;/code>来表示。&lt;/p>
&lt;p>具体实现见 &lt;a class="link" href="https://github.com/MaoSong2022/assignment1-basics/blob/main/cs336_basics/word_piece.py" target="_blank" rel="noopener"
>Github wordpiece&lt;/a> (基于&lt;a class="link" href="https://huggingface.co/learn/llm-course/chapter6/4?fw=pt" target="_blank" rel="noopener"
>huggingface llm course&lt;/a>)。 代码实现除了选择最优pair的方式不同之外，和BPE基本一致。&lt;/p>
&lt;h3 id="unigram">Unigram
&lt;/h3>&lt;p>Unigram也是由Google提出来的tokenizer，与BPE和wordpiece不同，unigram从一个非常大的vocab开始，然后merge token来降低vocab的size，直到达到指定大小。初始的vocab可以基于BPE算法或者使用prefix subword来构建。并且，初始vocab还包含所有的base characters来保证所有的word都可以被tokenize。&lt;/p>
&lt;p>算法的描述如下:&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/hands-on-llm1-tokenizer/unigram.png"
width="619"
height="669"
srcset="https://maosong2022.github.io/p/hands-on-llm1-tokenizer/unigram_hu9742228212950594435.png 480w, https://maosong2022.github.io/p/hands-on-llm1-tokenizer/unigram_hu16226380729442000185.png 1024w"
loading="lazy"
alt="unigram"
class="gallery-image"
data-flex-grow="92"
data-flex-basis="222px"
>&lt;/p>
&lt;p>我们来看一下算法的细节, 首先对于一个word, 我们有多种切割方式, 比如&lt;code>'bug'&lt;/code>可以被切分为如下三种形式:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="p">[[&lt;/span>&lt;span class="s1">&amp;#39;b&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;u&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;g&amp;#39;&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;b&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;ug&amp;#39;&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;bu&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;g&amp;#39;&lt;/span>&lt;span class="p">]]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>unigram 假设每个 word 出现的概率是其 subword 出现概率的乘积, 即对于包含 $n$个subword的单词 $\bm{x}=(x_1,\dots,x_n)$, 我们有:&lt;/p>
$$
p(\bm{x}) = \prod_{i=1}^n p(x_i)
$$&lt;p>其中，对于给定的vocab $\mathcal{V}$, 我们有：&lt;/p>
$$\sum_{v\in\mathcal{V}} p(x)=1$$&lt;p>unigram的目的就是选择合适的切分 $\bm{x}\in S(\bf{x})$ (这里我们用 $\bf{x}$ 表示单词本身, 用 $\bm{x}$ 表示 $\bf{x}$ 的一个切分), 使得 $p(\bm{x})$的概率最大. 这样我们就可以写出unigram的损失函数了:&lt;/p>
$$
\mathcal{L} = \sum_{i=1}^{N} \log\left(\sum_{\bm{x}\in S(\bf{x})}p(\bm{x})\right)
$$&lt;p>其本质就是: 我们希望对每个单词找到一种合适的切分, 切分得到的subword的概率分布满足其求和为1, 并且使得每个单词的概率最大.&lt;/p>
&lt;p>但是直接对上面概率最大化的问题就是我们每个subword的概率是未知的, unigram的做法是使用EM算法求解这个问题.&lt;/p>
&lt;p>当我们求解完成之后, 对每个subword, 我们都尝试将其从 $\mathcal{V}$中移除, 然后计算移除后的损失 $loss_i$, 我们依照$loss_i$对subword进行排序, 然后我们去掉 $\eta %$ 比例的subword.&lt;/p>
&lt;p>unigram的伪代码逻辑如下:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;span class="lnt">9
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">while&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">model&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">vocab_size&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">scores&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">compute_scores&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">model&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sorted_scores&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">sorted&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">scores&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">items&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="k">lambda&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Remove percent_to_remove tokens with the lowest scores.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">model&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">percent_to_remove&lt;/span>&lt;span class="p">)):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">_&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">token_freqs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pop&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">sorted_scores&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">total_sum&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">sum&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">freq&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">token&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">freq&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">token_freqs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">items&lt;/span>&lt;span class="p">()])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">model&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="n">token&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="n">log&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">freq&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">total_sum&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">token&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">freq&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">token_freqs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">items&lt;/span>&lt;span class="p">()}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>其中 &lt;code>compute_scores&lt;/code> 用于计算最优分割以及从&lt;code>model&lt;/code>中去掉每个token之后的loss.&lt;/p>
&lt;p>具体实现见 &lt;a class="link" href="https://github.com/MaoSong2022/assignment1-basics/blob/main/cs336_basics/unigram.py" target="_blank" rel="noopener"
>Github wordpiece&lt;/a> (基于&lt;a class="link" href="https://huggingface.co/learn/llm-course/chapter6/4?fw=pt" target="_blank" rel="noopener"
>huggingface llm course&lt;/a>)。代码实现的关键在于为每个word选取最优分割，huggingface是采取了动态规划的方法，也就是我们使用 &lt;code>dp[i]&lt;/code> 来表示 &lt;code>word[:i]&lt;/code> 的最优score，这样我们有：&lt;/p>
$$
dp[i] = \max_{0 \leq j &lt; i} dp[j]* p(word[j:i]),\quad \mathrm{s.t.}\ word[j:i]\in \mathcal{V}
$$&lt;p>这里的乘法代表 $p(\bm{x}) = \prod_{i=1}^n p(x_i)$, 在实现的时候我们会取log变成加法，然后概率会由频率来代替。&lt;/p>
&lt;h3 id="subword-tokenizer总结">Subword tokenizer总结
&lt;/h3>&lt;p>sub-word tokenizer的对比 (来自&lt;a class="link" href="https://huggingface.co/learn/llm-course/chapter6/4?fw=pt" target="_blank" rel="noopener"
>huggingface llm course&lt;/a>)：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Method&lt;/th>
&lt;th>BPE&lt;/th>
&lt;th>WordPiece&lt;/th>
&lt;th>Unigram&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Start Vocabulary&lt;/td>
&lt;td>Small&lt;/td>
&lt;td>Small&lt;/td>
&lt;td>Large&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Train&lt;/td>
&lt;td>Merge tokens&lt;/td>
&lt;td>Merge tokens&lt;/td>
&lt;td>Remove tokens&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Step&lt;/td>
&lt;td>Merge with most frequent pair&lt;/td>
&lt;td>Merge with best score&lt;/td>
&lt;td>Remove all tokens minimized the loss&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Learns&lt;/td>
&lt;td>Merge rules and a vocab&lt;/td>
&lt;td>A vocab&lt;/td>
&lt;td>A vocab with a score for each token&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Encoding&lt;/td>
&lt;td>Splits into words and applies merge rules&lt;/td>
&lt;td>Find the longest subword from the beginning that is in the vocab&lt;/td>
&lt;td>Finds the most likely split into tokens with learned scores&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Model&lt;/td>
&lt;td>GPT&lt;/td>
&lt;td>BERT&lt;/td>
&lt;td>T5&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="实践">实践
&lt;/h2>&lt;h3 id="tiktoken">tiktoken
&lt;/h3>&lt;p>&lt;a class="link" href="https://github.com/openai/tiktoken" target="_blank" rel="noopener"
>tiktoken&lt;/a>是openAI提出来的一个BPE tokenizer, openAI的模型都基于这个tokenizer, 其主要用于调用GPT系列模型是对token进行计数, 我们可以在&lt;a class="link" href="https://platform.openai.com/tokenizer" target="_blank" rel="noopener"
>tokenizer&lt;/a> 这个网站查看其分词情况.&lt;/p>
&lt;h3 id="sentencepiece">SentencePiece
&lt;/h3>&lt;p>&lt;a class="link" href="https://github.com/google/sentencepiece" target="_blank" rel="noopener"
>SentencePiece&lt;/a>是google开源的一个无监督的text tokenizer，其实现了BPE和unigram两种算法，SentencePiece还是一个语言无关的tokenizer，使其更适合多语种大语言模型的开发。&lt;/p>
&lt;h3 id="tokenizer">Tokenizer
&lt;/h3>&lt;p>&lt;a class="link" href="https://github.com/huggingface/tokenizers" target="_blank" rel="noopener"
>tokenizer&lt;/a> 是huggingface推出的为基于transformer服务的tokenizer库, 其支持BPE, wordpiece和unigram等分词算法, 使用简便. 并且, huggingface的tokenizer包括两种:&lt;/p>
&lt;ol>
&lt;li>fast tokenizer, 即&lt;a class="link" href="https://github.com/huggingface/tokenizers" target="_blank" rel="noopener"
>Tokenizer库&lt;/a>, 这个库是基于Rust开发的&lt;/li>
&lt;li>slow tokenizer, 这个是transformer库里模型自带的, 比如ChatGLM就有自己开发的tokenizer&lt;/li>
&lt;/ol>
&lt;p>huggingface比较了并行处理时两者的区别:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Setting&lt;/th>
&lt;th>Fast Tokenizer&lt;/th>
&lt;th>Slow Tokenizer&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>batched=True&lt;/code>&lt;/td>
&lt;td>10.8s&lt;/td>
&lt;td>4min41s&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>batched=False&lt;/code>&lt;/td>
&lt;td>59.2s&lt;/td>
&lt;td>5min3s&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>huggingface提供的tokenizer库已经非常齐全了, 如果我们要训练新的基于transformer的模型的话，建议直接使用Huggingface的&lt;code>AutoTokenizer&lt;/code>。&lt;/p>
&lt;h3 id="总结-1">总结
&lt;/h3>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>特性&lt;/th>
&lt;th>SentencePiece&lt;/th>
&lt;th>Tokenizer&lt;/th>
&lt;th>tiktoken&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>是否适合中文&lt;/td>
&lt;td>√&lt;/td>
&lt;td>√&lt;/td>
&lt;td>×&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>是否适合英文&lt;/td>
&lt;td>√&lt;/td>
&lt;td>√&lt;/td>
&lt;td>√&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>是否适合训练&lt;/td>
&lt;td>√&lt;/td>
&lt;td>√&lt;/td>
&lt;td>×&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>是否快速&lt;/td>
&lt;td>√&lt;/td>
&lt;td>√&lt;/td>
&lt;td>fast&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>是否用于 GPT 系列&lt;/td>
&lt;td>×&lt;/td>
&lt;td>×&lt;/td>
&lt;td>√&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>是否可解码&lt;/td>
&lt;td>√&lt;/td>
&lt;td>√&lt;/td>
&lt;td>√&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>是否支持多语言&lt;/td>
&lt;td>√&lt;/td>
&lt;td>√&lt;/td>
&lt;td>×&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="结论">结论
&lt;/h2>&lt;p>本文中, 我们介绍了大语言模型中的tokenizer, 我们从byte level, word level到sub-word level, 再到现代大语言模型最常使用的BPE tokenizer, 并给出了其（高效版本）实现。最后, 我们介绍了一下tokenizer-free的大语言模型和huggingface的tokenizer库。在未来, 我们将继续深入了解大语言模型的基本原理和实现细节。&lt;/p>
&lt;h2 id="参考文献">参考文献
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://stanford-cs336.github.io/spring2025/" target="_blank" rel="noopener"
>cs336 Lecture1&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/1508.07909" target="_blank" rel="noopener"
>Neural Machine Translation of Rare Words with Subword Units&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/1808.06226" target="_blank" rel="noopener"
>SentencePiece&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/1804.10959" target="_blank" rel="noopener"
>Unigram&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf" target="_blank" rel="noopener"
>WordPiece&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://huggingface.co/learn/llm-course/chapter6/1" target="_blank" rel="noopener"
>Huggingface LLM Course&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on attention bias</title><link>https://maosong2022.github.io/p/notes-on-attention-bias/</link><pubDate>Thu, 22 May 2025 15:25:07 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-attention-bias/</guid><description>&lt;h1 id="introduction">Introduction
&lt;/h1>&lt;p>我们知道，transformer使用position encoding的一个原因就是，attention layer具有置换不变性，也就是说，我们随机打乱输入token的顺序，并不影响其最终结果 (我们后面会证明，实际上只对key和value具有置换不变性，对query具有置换等变性，也就是改变query的顺序之后，结果的顺序也相应改变)。因此为了让模型学习到正确的上下文知识，我们需要加上position encoding。&lt;/p>
&lt;p>已有的工作大部分都在讨论如何构建更好的position encoding，但是鲜有工作探究为什么attention layer具有置换不变性. 因此，本文将从这一点出发，抽丝剥茧探究其内在原因，最后通过数学公式证明原始transformer是如何具有置换不变性的。&lt;/p>
&lt;h1 id="attention-layer介绍">attention layer介绍
&lt;/h1>&lt;p>原始transformer layer的架构比较简单，其结构具有&lt;code>attention-LayerNorm-FFN-LayerNorm&lt;/code>的形式。给定输入 $X\in\mathbb{R}^{d\times m}$ 和上下文 $Y\in\mathbb{R}^{d\times n}$. 其中，attention的定义为&lt;/p>
$$
\mathrm{Attn}(X, Y, Y) = V\mathrm{softmax}\left(\frac{K^TQ}{\sqrt{d}}\right)\in\mathbb{R}^{d\times m}
$$&lt;p>
其中 $d$是模型的&lt;code>hidden_size&lt;/code>, $Q=W_QX\in\mathbb{R}^{d\times m}$, $K=W_KY\in\mathbb{R}^{d\times n}$, $V=W_VY\in\mathbb{R}^{d\times n}$, $W_Q, W_K, W_V\in\mathbb{R}^{d\times d}$ 分别是QKV projection layer的参数.&lt;/p>
&lt;p>LayerNorm的定义为：&lt;/p>
$$
\mathrm{LayerNorm}(x) = \frac{x-\mathbb{E}[x]}{\sqrt{\mathrm{var}[x]+\epsilon}}\odot \gamma + \beta
$$&lt;p>
其中 $\epsilon&amp;gt;0$是一个超参数， $\gamma, \beta\in\mathbb{R}^d$ 是可学习的参数.&lt;/p>
&lt;p>FFN的定义为：&lt;/p>
$$
\mathrm{FFN}(x) = W_2\max(0, W_1x+b_1)+b_2
$$&lt;p>
其中 $W_1\in\mathbb{R}^{d_{ff}\times d}$, $W_2\in\mathbb{R}^{d\times d_{ff}}$, $b_1\in\mathbb{R}^{d_{ff}}$, $b_2\in\mathbb{R}^d$ 是可学习的参数。&lt;/p>
&lt;p>最后，一个attention layer的的结构可以表达为：&lt;/p>
$$
X = X + \mathrm{LayerNorm}(\mathrm{Attn}(X, Y, Y))\\
X = X + \mathrm{LayerNorm}(\mathrm{FFN}(X))\\
$$&lt;h1 id="置换不变性的定义">置换不变性的定义
&lt;/h1>&lt;p>置换不变性(permutation invariant)的定义：假设 $f:\mathbb{R}^n\to\mathbb{R}^n$，如果&lt;/p>
$$
f(\sigma(\bm{x})) = (f(\bm{x}))
$$&lt;p>则我们说 $f$是置换不变的. 这里 $\sigma:\mathbb{R}^n\to\mathbb{R}^n$ 是一个置换函数 (permutation function). 当输入的是一个矩阵时，我们默认置换其列，即对 $X=[X_1,\dots,X_n]\in\mathbb{R}^{d\times n}$, 我们有 $\sigma(X)=[X_{\sigma_1},\dots, X_{\sigma_n}]=Y\Pi $, 其中 $\Pi\in\mathbb{R}^{n\times n}\in {0,1}^{n\times n}$ 是一个置换矩阵 (permutation matrix)。&lt;/p>
&lt;p>置换等变性 (permutation equivariant)的定义：假设 $f:\mathbb{R}^n\to\mathbb{R}^n$，如果&lt;/p>
$$
f(\sigma(\bm{x})) = \sigma(f(\bm{x}))
$$&lt;p>
则我们说 $f$是置换等变的.&lt;/p>
&lt;h1 id="attention的置换不变性与置换等变性">attention的置换不变性与置换等变性
&lt;/h1>&lt;p>我们首先证明attention 对于key和value是置换不变的，即&lt;/p>
$$
\boxed{\mathrm{Attn}(X, \sigma(Y),\sigma(Y)) = \mathrm{Attn}(X, Y, Y)}
$$&lt;p>&lt;strong>证明&lt;/strong>: 我们直接计算即可得到：&lt;/p>
$$
\begin{aligned}
\mathrm{Attn}(X, \sigma(Y),\sigma(Y)) &amp;= V\Pi\mathrm{softmax}\left(\frac{(K\Pi)^TQ}{\sqrt{d}}\right)\\
&amp;=V\Pi\mathrm{softmax}\left(\frac{\Pi^TK^TQ}{\sqrt{d}}\right)\\
\end{aligned}
$$&lt;p>
由于softmax是按列计算的，置换只是改变了元素的顺序，因此我们自然有&lt;/p>
$$
\mathrm{Attn}(X, \sigma(Y),\sigma(Y)) = V\Pi\Pi^T\mathrm{softmax}\left(\frac{K^TQ}{\sqrt{d}}\right)=V\mathrm{softmax}\left(\frac{K^TQ}{\sqrt{d}}\right)=\mathrm{Attn}(X, Y, Y)
$$&lt;p>
这里我们使用了性质 $\Pi\Pi^T=\mathbf{I}$.&lt;/p>
&lt;p>接下来我们证明，attention对于query是置换等变的，即&lt;/p>
$$
\boxed{\mathrm{Attn}(\sigma(X), Y, Y) = \sigma(\mathrm{Attn}(X,Y,Y))}
$$$$
\begin{aligned}
\mathrm{Attn}(\sigma(X), Y, Y) &amp;= V\mathrm{softmax}\left(\frac{K^TQ\Pi}{\sqrt{d}}\right)\\
&amp;= V\mathrm{softmax}\left(\frac{K^TQ\Pi}{\sqrt{d}}\right)\\
&amp;= V\mathrm{softmax}\left(\frac{K^TQ}{\sqrt{d}}\right)\Pi\\
&amp;= \mathrm{Attn}(X,Y,Y)\Pi\\
&amp;= \sigma(\mathrm{Attn}(X,Y,Y))
\end{aligned}
$$&lt;p>从以上的证明可以看到，attention layer对于key和value具有置换不变性，也就是说，我们改变文字顺序不影响最终的输出结果。
但是，我们发现，尽管我们证明了attention具有置换不变性，我们却忽略了一件事：那就是我们计算query, key和value的时候，没有加上bias! 为什么bias如此重要呢？这是因为，$W\sigma(x) = \sigma(Wx)$, 但是 $W\sigma(X)\neq \sigma(Wx+b)$.
因此，我们就会思考，难道是transformer实际上可以通过增加bias的方式来让模型学习到上下文知识？事实上并非如此，我们将要通过分析表明，我们计算query, key和value时，增加的query bias和key bias会被softmax操作给消除掉，而key bias则会被LayerNorm消除掉。因此，我们加与加bias，对attention的置换不变性没有任何影响。&lt;/p>
&lt;h1 id="bias对attention-layer的影响">Bias对attention layer的影响
&lt;/h1>&lt;p>接下来，我们考虑在计算query, key和value时加入bias。为了简化，我们只考虑query为一个向量的情况，即 $X=\bm{x}\in\mathbb{R}^d$, 我们计算query, key和value如下：&lt;/p>
$$
\bm{q} = W_Q\bm{x}+\bm{b}_Q\in\mathbb{R}^{d}\\
K = W_KY + \bm{b}_K\mathbf{1}^T\in\mathbb{R}^{d\times n}\\
V = W_VY + \bm{b}_V\mathbf{1}^T\in\mathbb{R}^{d\times n}
$$&lt;p>这里 $\mathbf{1}^T\in\mathbb{R}^{n}$. 我们这里简化了scaling的操作，因为其不对结果产生影响。&lt;/p>
&lt;blockquote>
&lt;p>注：以下证明参考了【参考文献2】&lt;/p>
&lt;/blockquote>
&lt;p>我们首先展开attention中的 $V$:&lt;/p>
$$
\begin{aligned}
\mathrm{Attn}(\bm{x}, Y, Y) &amp;= V\mathrm{softmax}\left(K^T\bm{q}\right)\\
&amp;= \left(W_VY + \bm{b}_V\mathbb{1}^T\right)\mathrm{softmax}\left(K^T\bm{q}\right)\\
&amp;= W_VY\mathrm{softmax}\left(K^T\bm{q}\right) + \bm{b}_V\mathbb{1}^T \mathrm{softmax}\left(K^T\bm{q}\right)
\end{aligned}
$$&lt;p>
由于 $\mathrm{softmax}\left(K^T\bm{q}\right)\in\mathbb{R}^{n}$的列求和为$1$, 因此，$\mathbb{1}^T\mathrm{softmax}\left(K^T\bm{q}\right)=1$, 我们有&lt;/p>
$$
\mathrm{Attn}(\bm{x}, Y, Y) = W_VY\mathrm{softmax}\left(K^T\bm{q}\right) + \bm{b}_V
$$&lt;p>接下来，我们展开 $K$:&lt;/p>
$$
\begin{aligned}
\mathrm{Attn}(\bm{x}, Y, Y) &amp;= W_VY\mathrm{softmax}\left(K^T\bm{q}\right) + \bm{b}_V\\
&amp;= W_VY\mathrm{softmax}\left((W_KY + \bm{b}_K\mathbf{1}^T)^T\bm{q}\right) + \bm{b}_V\\
&amp;= W_VY\mathrm{softmax}\left(Y^TW_K^Tq + \mathbf{1}\bm{b}_K^T\bm{q}\right) + \bm{b}_V\\
\end{aligned}
$$$$
\mathrm{softmax}(\bm{x}+\delta)_i = \frac{e^{x_i+\delta}}{\sum_{j}e^{x_j+\delta}} = \frac{e^{x_i} * e^{\delta}}{\sum_{j}e^{x_j} * e^{\delta}} = \mathrm{softmax}(\bm{x})_i
$$&lt;p>
而这里 $\bm{b}_K^T\bm{q}\in\mathbb{R}$，因此我们可以将这一项给去掉，我们得到：&lt;/p>
$$
\mathrm{Attn}(\bm{x}, Y, Y) = W_VY\mathrm{softmax}\left(Y^TW_K^T\bm{q}\right) + \bm{b}_V
$$$$
\boxed{
\begin{aligned}
\mathrm{Attn}(\bm{x}, Y, Y) &amp;= W_VY\mathrm{softmax}\left(Y^TW_K^T\bm{q}\right) + \bm{b}_V\\
&amp;= W_VY\mathrm{softmax}\left(Y^TW_K^T(W_Q\bm{x}+\bm{b}_Q)\right) + \bm{b}_V\\
\end{aligned}}
$$&lt;p>因此，我们最终的结论为： &lt;strong>key bias对attention输出没有任何贡献，query bias和key bias会影响结果。&lt;/strong>&lt;/p>
&lt;p>到这里，看了参考文献3，我本以为可以进一步简化。但实际上并不行。参考文献3关于“transformer block is equivariant&amp;quot;的结果是错的，因为在attention layer之后还有一个LayerNorm，而LayerNorm不是置换不变的，这也是LayerNorm和BatchNorm之间的区别。也就是&lt;em>如果我们在&lt;code>nn.Linear&lt;/code>后加一个BatchNorm，那么&lt;code>nn.Linear&lt;/code>的bias是无效的，反之如果是LayerNorm的话，则bias是有效的&lt;/em>.&lt;/p>
&lt;h1 id="为什么没有bias">为什么没有bias
&lt;/h1>&lt;p>实际上这个问题并没有定论。特别是加入position encoding之后，就更难探究bias对最终结果的影响了。但是，我认为一个原因就是bias其实就是某种先验知识，假设输入满足高斯分布，那么我们有&lt;/p>
$$
\mathbb{E}[W\bm{x}+b] = b
$$&lt;p>加上先验知识后，当训练数据出现distribution shift之后，模型在训练过程中可能就会不稳定(PaLM). 而后来将LayerNorm替换为RMSNorm，使用RoPE而不是其他的additive position encoding, 我认为也是避免模型学习到先验知识，从而影响其泛化性。在未来，我认为transformer里应该是没有bias的，尽管这样效果可能会差一些，但是其稳定性更好，泛化性应该也会更好。&lt;/p>
&lt;h1 id="结论">结论
&lt;/h1>&lt;p>在本文中，我们分析了attention的性质，我们发现，在原始transformer架构中，attention对于key和value有置换不变性，对于query有置换等变性。然后，我们给出了一些猜测，也就是bias会让模型产生先验知识，而这种先验知识很可能会影响训练的稳定性和模型的泛化性。&lt;/p>
&lt;h1 id="参考文献">参考文献
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" target="_blank" rel="noopener"
>Attention is All you Need&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2302.08626" target="_blank" rel="noopener"
>Role of Bias Terms in Dot-Product Attention&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://openreview.net/forum?id=ByxRM0Ntvr" target="_blank" rel="noopener"
>Are Transformers universal approximators of sequence-to-sequence functions?&lt;/a>&lt;/li>
&lt;/ul>
&lt;h1 id="附录">附录
&lt;/h1>&lt;p>下面是测试上面结论的python代码&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;span class="lnt">49
&lt;/span>&lt;span class="lnt">50
&lt;/span>&lt;span class="lnt">51
&lt;/span>&lt;span class="lnt">52
&lt;/span>&lt;span class="lnt">53
&lt;/span>&lt;span class="lnt">54
&lt;/span>&lt;span class="lnt">55
&lt;/span>&lt;span class="lnt">56
&lt;/span>&lt;span class="lnt">57
&lt;/span>&lt;span class="lnt">58
&lt;/span>&lt;span class="lnt">59
&lt;/span>&lt;span class="lnt">60
&lt;/span>&lt;span class="lnt">61
&lt;/span>&lt;span class="lnt">62
&lt;/span>&lt;span class="lnt">63
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">torch&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">torch.nn&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">nn&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">torch.nn.functional&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">F&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 设置随机种子，确保可复现性&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">manual_seed&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">42&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 输入参数&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">batch_size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">seq_len&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">16&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">embed_dim&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1024&lt;/span> &lt;span class="c1"># 嵌入维度&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">num_heads&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">32&lt;/span> &lt;span class="c1"># 多头注意力头数&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">head_dim&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">embed_dim&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="n">num_heads&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 输入张量 (batch_size, seq_len, embed_dim)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">batch_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">seq_len&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">embed_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 有 bias 的 QKV 线性层&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Attention&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">embed_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">q_bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k_bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">v_bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="fm">__init__&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embed_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">embed_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">q_bias&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">k&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embed_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">embed_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">k_bias&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">v&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embed_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">embed_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">v_bias&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">B&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">N&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">C&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reshape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">B&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">N&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_heads&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">head_dim&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transpose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">k&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reshape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">B&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">N&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_heads&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">head_dim&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transpose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">v&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">v&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reshape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">B&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">N&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_heads&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">head_dim&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transpose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attn&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">q&lt;/span> &lt;span class="o">@&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transpose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mf">1.0&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="o">**&lt;/span>&lt;span class="mf">0.5&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attn&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">attn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">softmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">attn&lt;/span> &lt;span class="o">@&lt;/span> &lt;span class="n">v&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transpose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reshape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">B&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">N&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">C&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">attn&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 初始化模型&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">model_no_bias&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Attention&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embed_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">q_bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k_bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">v_bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">model_with_bias&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Attention&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embed_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">q_bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k_bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">v_bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">model_with_bias&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">weight&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">data&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">model_no_bias&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">weight&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">data&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">model_with_bias&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">weight&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">data&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">model_no_bias&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">weight&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">data&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">model_with_bias&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">v&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">weight&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">data&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">model_no_bias&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">v&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">weight&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">data&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 推理&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">out_no_bias&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">attn_no_bias&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">model_no_bias&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">out_with_bias&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">attn_with_bias&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">model_with_bias&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 比较差异&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">diff_output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">abs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">out_no_bias&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">out_with_bias&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mean&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">diff_variance&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">abs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">out_no_bias&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">out_with_bias&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">var&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">diff_attn&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">abs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">attn_no_bias&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">attn_with_bias&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mean&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">Mean difference in output:&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">diff_output&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">item&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Mean difference in variance:&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">diff_variance&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">item&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Mean difference in attention weights:&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">diff_attn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">item&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Mean difference in output: 1.2734082233123445e-08&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Mean difference in variance: 1.7173628739783402e-16&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Mean difference in attention weights: 3.949708116124384e-09&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>Notes on Position encoding</title><link>https://maosong2022.github.io/p/notes-on-position-encoding/</link><pubDate>Mon, 19 May 2025 10:46:39 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-position-encoding/</guid><description>&lt;blockquote>
&lt;p>本文前半部分参考 &lt;a class="link" href="https://huggingface.co/blog/designing-positional-encoding" target="_blank" rel="noopener"
>参考文献1&lt;/a>，推荐大家看博客原文。&lt;/p>
&lt;/blockquote>
&lt;h1 id="position-encoding总结">Position encoding总结
&lt;/h1>&lt;p>在 &lt;a class="link" href="https://maosong.website/p/notes-on-attention-bias/" target="_blank" rel="noopener"
>上一篇blog&lt;/a> 中, 我们介绍了 Attention 的两个性质，也就是在不加 position encoding 的情况下，Attention 对于 query 是 permutation equivariant 的，对于 key 和 value 是 permutation invariant 的。&lt;/p>
&lt;p>但是“我爱你”和“你爱我”这两句话所表示的含义应该是不一样的，我们将这两句话作为 key 和 value 的时候，我们发现模型的输出是一致的，这显然是不能接受的。因此，我们就需要加入 position encoding，让模型学习到语序信息，从而明白不同的语序有不同的含义。&lt;/p>
&lt;p>下面是测试代码 （来自 &lt;a class="link" href="https://huggingface.co/blog/designing-positional-encoding" target="_blank" rel="noopener"
>参考文献1&lt;/a>）&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">torch&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">torch.nn&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">nn&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">transformers&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">AutoTokenizer&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">AutoModel&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">model_id&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;meta-llama/Llama-3.2-1B&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">tok&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">AutoTokenizer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">from_pretrained&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">model_id&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">model&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">AutoModel&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">from_pretrained&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">model_id&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">text&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;The dog chased another dog&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">tokens&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tok&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">text&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">return_tensors&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;pt&amp;#34;&lt;/span>&lt;span class="p">)[&lt;/span>&lt;span class="s2">&amp;#34;input_ids&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">embeddings&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">model&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">embed_tokens&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tokens&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">hdim&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">embeddings&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">W_q&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hdim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hdim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">W_k&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hdim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hdim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">W_v&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hdim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hdim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">mha&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">MultiheadAttention&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embed_dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">hdim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_heads&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">4&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">batch_first&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">with&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">no_grad&lt;/span>&lt;span class="p">():&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">param&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">mha&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">parameters&lt;/span>&lt;span class="p">():&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">init&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">normal_&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">param&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">std&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># Initialize weights to be non-negligible&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">output&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">mha&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">W_q&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embeddings&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">W_k&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embeddings&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">W_v&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embeddings&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">dog1_out&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">output&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">dog2_out&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">output&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;Dog output identical?: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">allclose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dog1_out&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dog2_out&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">atol&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">1e-6&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1">#True&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Position encoding 可以分为绝对位置编码 (absolute position encoding, APE)，相对位置编码 (relative position encoding, RPE) 以及可学习的位置编码。可学习位置编码主要是 BERT 类的模型在使用，其训练成本比较高，本文不做讨论。绝对位置编码是原始 transformer 里提出的编码模式，现在的大多数基于 transformer 模型使用的都是相对位置编码。&lt;/p>
&lt;p>本文中，我们先介绍位置编码应该具有的性质，然后我们分别介绍绝对位置编码和相对位置编码，我们将着重关注苏剑林老师提出来的 RoPE。&lt;/p>
&lt;h2 id="位置编码">位置编码
&lt;/h2>&lt;p>在介绍位置编码之前，我们首先应该关注位置编码的性质，位置编码的目标是为输入的 token embedding 增加位置信息，那么理想的位置编码应该是怎么样的呢？&lt;/p>
&lt;p>我们这里直接引用 &lt;a class="link" href="https://huggingface.co/blog/designing-positional-encoding" target="_blank" rel="noopener"
>参考文献1&lt;/a> 中给定的性质：&lt;/p>
&lt;ol>
&lt;li>&lt;strong>性质 1&lt;/strong>: token sequence 中每个位置的位置编码都是唯一的。这个很好理解，如果不唯一的话，那么根据前面推导的性质，这两个位置的 attention 输出就完全一致了&lt;/li>
&lt;li>&lt;strong>性质 2&lt;/strong>: 线性相关性。也就是说，如果我们知道了位置 $p$ 处的位置编码，那么理想情况下，我们应该能比较简单地得到 $p+k$ 处的位置编码，理想情况下，我们应该有 $PE(p+k)=W_kPE(p)$.&lt;/li>
&lt;li>&lt;strong>性质 3&lt;/strong>: 泛化到长上下文中去。我们希望位置编码不仅在 8K 的上下文起作用，还希望位置编码能够泛化到 32K 的上下文&lt;/li>
&lt;li>&lt;strong>性质 4&lt;/strong>: 生成模式是固定的。固定的模式有助于模型更好地学习位置相关的信息&lt;/li>
&lt;li>&lt;strong>性质 5&lt;/strong>: 可以扩展到多维。我们希望位置编码可以从文本扩展到图片再到视频，也就是从 $1D$ 到 $nD$.&lt;/li>
&lt;/ol>
&lt;h2 id="绝对位置编码">绝对位置编码
&lt;/h2>&lt;p>绝对位置编码依照其名称，其思想就是为每个位置的 token 分配一个固定的位置信息，也就是对于输入的 hidden states $\bm{x}=[\bm{x}_1,\dots,\ \bm{x}_m]\in\mathbb{R}^{m\times d}$, 我们有&lt;/p>
$$
\bm{x}_i' = \bm{x}_i + p_i, i=1,\dots, m
$$&lt;p>这里，$p_i\in\mathbb{R}^d$. 我们的 attention 就变成了&lt;/p>
$$
\mathrm{Attn}(X) = \mathrm{softmax}\left(\frac{(Q+P)(K+P)^T}{\sqrt{d}}\right)V\in\mathbb{R}^{m\times d}
$$&lt;p>这里&lt;/p>
$$
P = [p_1,\dots,p_m]\in\mathbb{R}^{m\times d}， Q= W_QX\in\mathbb{R}^{m\times d}, K=W_KX, V=W_VX\in\mathbb{R}^{n\times d}
$$&lt;h3 id="整数位置编码">整数位置编码
&lt;/h3>&lt;p>一个最简单的想法就是我们使用正整数来标记 token 所在的位置，也就是&lt;/p>
$$
PE(i) = [i, \dots, i]=i\mathbf{1}_{d\times 1}\in\mathbb{R}^d,\ i=1,\dots,m
$$&lt;p>可以看到，这个简单的设计满足性质 1，性质 2，性质 3，性质 4.&lt;/p>
&lt;p>但是，注意到 attention 的输入 $X$ 通常是经过 Layer Normalization 处理过后的，因此其按列符合正态分布，并且均值和方差一般较小。当我们加上整数位置编码之后，其 token 本身的信息就会被污染，也就是信噪比非常低。一个解决方法就是我们对 $PE(i)$ 进行 normalization，即&lt;/p>
$$
PE(i)' = \frac{1}{m}PE(i) = \frac{i}{m}\mathbf{1}_{d\times 1}
$$&lt;p>现在所有的位置编码的值都比较小，但是我们发现新的位置编码不满足性质 2 了，这是因为现在位置编码还和 sequence 长度有关，我们从位置 $p$ 到位置 $p+k$ 不仅取决于 $k$ 还取决于 sequence 长度 $m$&lt;/p>
&lt;h3 id="二进制位置编码">二进制位置编码
&lt;/h3>&lt;p>既然整数位置编码的主要问题是对输入影响太大，我们能否找一个不影响输入的整数位置编码方式呢？ &lt;a class="link" href="https://huggingface.co/blog/designing-positional-encoding" target="_blank" rel="noopener"
>参考文献1&lt;/a> 提出了二进制位置编码，因为每个 token 是 $d$ 维的，因此我们可以使用 $d$ 位二进制来表示 $i$. 比如说，当 $d=3$, $m=4$ 时，我们的位置编码分别为&lt;/p>
$$
PE(0) =p_{(000)_2} = [0, 0, 0],\ PE(1) =p_{(001)_2}= [0, 0, 1],\ PE(2) =p_{(010)_2} = [0, 1, 0],\ PE(3) =p_{(011)_2} = [0, 1, 1]
$$&lt;p>现在，我们二进制位置编码满足性质 1，性质 2. 对于性质 3，由于 $d$ 位二进制的表示范围为 $[0, 2^d-1]$，因此其泛化性受到 $d$ 的影响。&lt;/p>
&lt;p>&lt;a class="link" href="https://huggingface.co/blog/designing-positional-encoding" target="_blank" rel="noopener"
>参考文献1&lt;/a> 画出了不同位置的值的变化情况。我们这里也模仿绘制出类似的曲线图&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-position-encoding/RoPE_binary_position_encoding.png"
width="1400"
height="800"
srcset="https://maosong2022.github.io/p/notes-on-position-encoding/RoPE_binary_position_encoding_hu2257952624178844849.png 480w, https://maosong2022.github.io/p/notes-on-position-encoding/RoPE_binary_position_encoding_hu2340593386986551550.png 1024w"
loading="lazy"
alt="Binary Position Encoding"
class="gallery-image"
data-flex-grow="175"
data-flex-basis="420px"
>&lt;/p>
&lt;p>我们发现，二进制位置编码高位，也就是 $PE(i)&lt;em>{d}$ 的变化很慢，而低位，也就是 $PE(i)&lt;/em>{0}$ 变化很快，&lt;/p>
&lt;p>二进制位置编码解决了整数位置编码的信噪比过低和线性相关性。但是其问题是其对不同位置的 token embedding 产生的影响是不一样的。比如位置 1 和位置 2 的相同的 token embedding 之间的区别是：&lt;/p>
$$
(\bm{x}_2 + PE(2)) - (\bm{x}_1 + PE(1)) = (\bm{x}_2-\bm{x}_1)+ [0, 1, -1]
$$&lt;p>一般来说, $\bm{x}_2-\bm{x}_1$ 比较小，因此使用二进制位置编码的问题是输入位置的微小变化（增加一个 token 或减少一个 token）都会对最终结果产生巨大影响。因此，我们需要想办法解决这个问题。&lt;/p>
&lt;h3 id="sinusoidal">Sinusoidal
&lt;/h3>&lt;p>前面提到二进制位置编码的问题是相邻 token 之间变化太大，不够光滑。因此我们想要增加一个光滑性质，也就是说我们希望：&lt;/p>
&lt;ol>
&lt;li>位置编码值在 $[-1, 1]$ 之间，防止对 token embedding 产生影响&lt;/li>
&lt;li>相邻 token 的位置编码尽可能相近，即 $|PE(k+p)-PE(p)| \leq \delta |k|$, 其中 $\delta&amp;gt;0$ 是一个比较小的数。&lt;/li>
&lt;li>与二进制一样，高位的变化比较慢，低位的变化比较快&lt;/li>
&lt;/ol>
&lt;p>一个想法就是利用三角函数 $\sin$ 或者 $\cos$，三角函数满足前两个性质， 对于第三个性质，我们可以通过控制频率来满足。这样我们得到的位置编码就具有如下形式：&lt;/p>
$$
PE(p, i) = \sin\left(\frac{p}{\theta^{i/d}}\right)
$$&lt;p>其中 $\theta$ 是我们的超参数。&lt;/p>
&lt;p>我们现在来推导一下上面位置编码的线性相关性：&lt;/p>
$$
PE(p+k) = \sin\left(\frac{p+k}{\theta^{i/d}}\right)=PE(p)\cos\left(\frac{k}{\theta^{i/d}}\right) + \cos\left(\frac{p}{\theta^{i/d}}\right)\sin\left(\frac{k}{\theta^{i/d}}\right)
$$&lt;p>我们发现，$\sin$ 位置编码不满足线性相关性。但是出现的 $\cos$ 给了我们启发，也就是我们可以同时使用 $\sin$ 和 $\cos$ 来完成位置编码，这也是原始 transformer 里提出来的 Sinusoidal 位置编码，其形式为：&lt;/p>
$$
\begin{aligned}
PE(p, 2i) &amp;= \sin\left(\frac{p}{\theta^{2i/d}}\right)\\
PE(p, 2i+1) &amp;= \cos\left(\frac{p}{\theta^{2i/d}}\right)
\end{aligned}
$$&lt;p>现在，记 $\omega_i=1/\theta^{2i/d}$, 我们再推导一下线性相关性，就得到：&lt;/p>
$$
\begin{aligned}
\begin{bmatrix}
PE(p+k, 2i)\\
PE(p+k, 2i+1)\\
\end{bmatrix}&amp;=\begin{bmatrix}
\sin \omega_i(p+k)\\
\cos \omega_i(p+k)
\end{bmatrix}\\
&amp;=\begin{bmatrix}
\sin \omega_i(\omega_ip)\cos(\omega_ik)+\cos w_i(\omega_ip)\sin(\omega_ik)\\
\cos \omega_i(\omega_ip)\cos(\omega_ik)-\sin w_i(\omega_ip)\sin(\omega_ik)
\end{bmatrix}\\
&amp;= \begin{bmatrix}
\cos(\omega_ik)&amp; \sin(\omega_ik)\\
-\sin(\omega_ik)&amp; \cos(\omega_ik)
\end{bmatrix}\begin{bmatrix}
PE(p, 2i)\\
PE(p, 2i+1)\\
\end{bmatrix}
\end{aligned}
$$&lt;p>也就是说，Sinusoidal 位置编码满足线性相关性。对于 Sinusoidal 位置编码我们也可以进行可视化：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-position-encoding/RoPE_sinusoidal_position_encoding.png"
width="1400"
height="800"
srcset="https://maosong2022.github.io/p/notes-on-position-encoding/RoPE_sinusoidal_position_encoding_hu12672545324139701707.png 480w, https://maosong2022.github.io/p/notes-on-position-encoding/RoPE_sinusoidal_position_encoding_hu9945773915385487910.png 1024w"
loading="lazy"
alt="Sinusoidal Position Encoding"
class="gallery-image"
data-flex-grow="175"
data-flex-basis="420px"
>&lt;/p>
&lt;h2 id="相对位置编码">相对位置编码
&lt;/h2>&lt;p>前面介绍了绝对位置编码，每个位置的位置编码是固定的。但是绝对位置编码的问题是，模型比较难以学习相对位置关系。&lt;/p>
&lt;p>举个例子，我们提到上下文时，通常会使用“上一节”，“上一章”这些表示相对位置关系的词。&lt;/p>
&lt;p>因此，我们希望让模型学习相对位置关系而不是绝对位置关系，因为相对关系更符合我们的认知。&lt;/p>
&lt;h3 id="rope">RoPE
&lt;/h3>&lt;p>RoPE 由苏剑林老师提出，最早应用于 LLaMA 架构（没有确认），后续被大多数模型所采用。&lt;/p>
&lt;p>之前的 PE 大多数关注于加性位置编码，也就是&lt;strong>假设位置编码的形式为 $\bm{x}+\bm{p}$&lt;/strong>, 基于这种假设，已有的工作基本都集中于优化下面的 Q 和 K 的内积&lt;/p>
$$
\langle f_q(\bm{x}_q, m), f_k(\bm{x}_k, n) \rangle
$$&lt;p>这里 $f_q(\bm{x}_q, m)=W_q(\bm{x}_q+\bm{p}_m)$, $f_k(\bm{x}_k, n)=W_k(\bm{x}_k+ \bm{p}_n)$.&lt;/p>
&lt;p>而 RoPE 里面，作者使用了一个不同的假设： &lt;strong>假设内积应该仅包含两者的相对信息&lt;/strong>，也就是&lt;/p>
$$
\langle f_q(\bm{x}_q, m), f_q(\bm{x}_k, n)\rangle := g(\bm{x}_q,\bm{x}_k, m-n)
$$&lt;p>这里的 $f$ 和 $g$ 都是未知函数。我们的目标就是从这个公式中推导出一个合适的位置编码出来。&lt;/p>
&lt;p>不失一般性，我们可以假设&lt;/p>
$$
f_q(\bm{x}_m,0) = \bm{x}_q,\quad f_q(\bm{x}_n, 0) = \bm{x}_k
$$&lt;p>这个假设代表初始条件下，我们不对输入做任何改变，也就是不增加位置信息。&lt;/p>
&lt;h2 id="2d-推导">2D 推导
&lt;/h2>&lt;p>与 RoPE 一样，我们直接使用复平面来进行推导。&lt;/p>
&lt;p>我们假设 $d=2$, 注意到二维平面上的每个点都可以表示为如下形式&lt;/p>
$$
\bm{z} = (x,y) = re^{i\theta}
$$&lt;p>其中 ($\mathrm{atan2}$ 定义参考 &lt;a class="link" href="https://en.wikipedia.org/wiki/Polar_coordinate_system" target="_blank" rel="noopener"
>维基百科&lt;/a>)&lt;/p>
$$
r = \|\bm{z}\|_2 = \sqrt{x^2+y^2}\in\mathbb{R},\quad \theta = \mathrm{atan2}(y, x)\in\mathbb{R},
$$&lt;p>现在，对于三个向量 $f_q(\bm{x}_q, m)$, $f_q(\bm{x}_k, n)$, $g(\bm{x}_q,\bm{x}_k, m-n)$ 我们可以写出其极坐标形式：&lt;/p>
$$
\begin{aligned}
f_q(\bm{x}_q,m) &amp;:= r_q(\bm{x}_q,m)e^{i\theta_q(\bm{x}_q,m)}\\
f_k(\bm{x}_k, n) &amp;:= r_k(\bm{x}_k, n)e^{i\theta_k(\bm{x}_k, n)}\\
g(\bm{x}_q,\bm{x}_k, m-n) &amp;:= r_g(\bm{x}_q,\bm{x}_k, m-n)e^{i\theta_g(\bm{x}_q,\bm{x}_k, m-n)}
\end{aligned}
$$&lt;p>我们计算内积并比较同类项得到：&lt;/p>
$$
\begin{aligned}
r_g(\bm{x}_q,\bm{x}_k, m-n) &amp;:= r_q(\bm{x}_q,m)r_k(\bm{x}_k, n)\\
\theta_g(\bm{x}_q,\bm{x}_k, m-n) &amp;:= \theta_q(\bm{x}_q,m)-\theta_k(\bm{x}_k, n)
\end{aligned}\tag{3}
$$&lt;p>我们接下来分别推导 $r_g(\bm{x}_q,\bm{x}_k, m-n)$ 和 $\theta_g(\bm{x}_q,\bm{x}_k, m-n)$ 的形式&lt;/p>
&lt;h4 id="r_gbmx_qbmx_k-m-n">$r_g(\bm{x}_q,\bm{x}_k, m-n)$
&lt;/h4>&lt;p>我们令 $m=n=0$ 可以得到初始条件&lt;/p>
$$
r_g(\bm{x}_q,\bm{x}_k, 0) = r_q(\bm{x}_q,0)r_k(\bm{x}_k, 0)=\|\bm{q}\|_2\|\bm{k}\|_2
$$&lt;p>我们再令 $n=0$,得到&lt;/p>
$$
r_g(\bm{x}_q,\bm{x}_k, m) = r_q(\bm{x}_q,m)r_k(\bm{x}_k, 0)=r_q(\bm{x}_q,m)\|\bm{k}\|_2=\frac{r_g(\bm{x}_q,\bm{x}_k, m-n)}{r_k(\bm{x}_k, n)}\|\bm{k}\|_2
$$&lt;p>这里最后一个等式带入了原始等式 (3)，注意到左侧与 $n$ 无关，因此右侧我们选取 $n=1$, 得到&lt;/p>
$$
r_g(\bm{x}_q,\bm{x}_k, m) = \frac{r_g(\bm{x}_q,\bm{x}_k, m-1)}{r_k(\bm{x}_k, 1)}\|\bm{k}\|_2 =\cdots= r_g(\bm{x}_q,\bm{x}_k, 0)\left(\frac{\|\bm{k}\|_2 }{r_k(\bm{x}_k, 1)}\right)^{m+1}
$$&lt;p>令 $m=0$ 我们有&lt;/p>
$$
r_k(\bm{x}_k, 1) = \|\bm{k}\|_2.
$$&lt;p>因此我们最终的表达式为：&lt;/p>
$$
r_g(\bm{x}_q,\bm{x}_k, m) = r_g(\bm{x}_q,\bm{x}_k, 0) = \|\bm{q}\|_2\|\bm{k}\|_2.
$$&lt;p>并且，通过分别设置 $m=0$ 以及 $n=0$ 我们还可以得到&lt;/p>
$$
r_q(\bm{x}_q,m) = \|\bm{q}\|_2,\quad r_k(\bm{x}_k, n) = \|\bm{k}\|_2
$$&lt;h4 id="theta_gbmx_qbmx_k-m-n">$\theta_g(\bm{x}_q,\bm{x}_k, m-n)$
&lt;/h4>&lt;p>令 $m=n=0$, 我们得到初始条件&lt;/p>
$$
\theta_g(\bm{x}_q,\bm{x}_k, 0) = \theta_q(\bm{x}_q,0)-\theta_k(\bm{x}_k, 0)=\theta_q-\theta_k
$$&lt;p>令 $n=1$, 我们有&lt;/p>
$$
\begin{aligned}
\theta_g(\bm{x}_q,\bm{x}_k, m-1) &amp;= \theta_q(\bm{x}_q,m)-\theta_k(\bm{x}_k, 1)\\
&amp;=\theta_g(\bm{x}_q,\bm{x}_k, m-n) + \theta_k(\bm{x}_k, n)-\theta_k(\bm{x}_k, 1)
\end{aligned}
$$&lt;p>这里我们带入了公式 (3)，注意到公式左边与 $n$ 无关，因此在公式右侧我们令 $n=0$, 得到&lt;/p>
$$
\theta_g(\bm{x}_q,\bm{x}_k, m-1) = \theta_g(\bm{x}_q,\bm{x}_k, m)+ \theta_k(\bm{x}_k, 0)-\theta_k(\bm{x}_k, 1)
$$&lt;p>分别令 $m=1,2,\dots$ 并相加这些等式，我们得到&lt;/p>
$$
\theta_g(\bm{x}_q,\bm{x}_k, 0) = \theta_g(\bm{x}_q,\bm{x}_k, m) + m(\theta_k(\bm{x}_k, 0)-\theta_k(\bm{x}_k, 1))
$$&lt;p>即&lt;/p>
$$
\theta_g(\bm{x}_q,\bm{x}_k, m) = m(\theta_k(\bm{x}_k, 1)-\theta_k(\bm{x}_k, 0))+(\theta_q-\theta_k)\tag{4}
$$&lt;p>注意到&lt;/p>
$$
\theta_g(\bm{x}_q,\bm{x}_k, m) = \theta_q(\bm{x}_q,m)-\theta_k(\bm{x}_k, 0)=\theta_q(\bm{x}_q,m)-\theta_k
$$&lt;p>带入上式我们就得到&lt;/p>
$$
\theta_q(\bm{x}_q,m) = m(\theta_k(\bm{x}_k, 1)-\theta_k(\bm{x}_k, 0))+\theta_q
$$&lt;p>在 (4) 式中再令 $m=m-n$，并带入 $\theta_q(\bm{x}_q,m)$ 就有&lt;/p>
$$
\theta_k(\bm{x}_k,n) = n(\theta_k(\bm{x}_k, 1)-\theta_k(\bm{x}_k, 0))+\theta_k
$$&lt;h3 id="汇总">汇总
&lt;/h3>&lt;p>最后，我们将以上结果放在一起，就得到&lt;/p>
$$
f_q(\bm{x}_q,m) = \bm{q}e^{im\theta}, f_v(\bm{x}_k,m) = \bm{k}e^{in\theta}
$$&lt;p>这里 $\theta=\theta_k(\bm{x}_k, 1)-\theta_k(\bm{x}_k, 0)$ 是一个超参数，用于控制频率。&lt;/p>
&lt;p>我们记&lt;/p>
$$
R_{\theta,m} = \begin{bmatrix}
\cos m\theta &amp; -\sin m\theta\\
\sin m\theta &amp; \cos m\theta
\end{bmatrix}
$$&lt;p>则我们有：&lt;/p>
$$
f_q(\bm{x}_q,m) = R_{\theta,m}\bm{q}, f_v(\bm{x}_k,m) = R_{\theta,n}\bm{k}.
$$&lt;p>并且&lt;/p>
$$
\langle f_q(\bm{x}_q,m), f_v(\bm{x}_k,m)\rangle = \bm{q}^TR_{\theta,m-n}\bm{k} \tag{5}
$$&lt;h3 id="多维扩展">多维扩展
&lt;/h3>&lt;p>上面是 2D 的情况，对于多维情况，苏剑林老师通过将两个元素组对，然后分别进行处理，得到了多维的情形：&lt;/p>
$$
R_{\theta,m}^d = \begin{bmatrix}
R_{\theta_1,m} &amp; &amp; &amp;&amp; &amp; \\
&amp; &amp; R_{\theta_2,m} &amp; &amp; &amp; \\
&amp;&amp;&amp;&amp; \ddots &amp; \\
&amp;&amp;&amp;&amp; &amp; R_{\theta_{d/2},m}
\end{bmatrix}\in\mathbb{R}^{d\times d}
$$&lt;p>我们可以验证公式 (5) 仍然是成立的。&lt;/p>
&lt;h2 id="rope-的远程衰减性质">RoPE 的远程衰减性质
&lt;/h2>&lt;p>我们接下来看一下结果与相对距离 $m-n$ 之间的关系， 注意到&lt;/p>
$$
\langle f_q(\bm{x}_q,m), f_v(\bm{x}_k,m)\rangle = \bm{q}^TR_{\theta,m-n}\bm{k} = \sum_{i=1}^{d/2} \bm{q}_i^TR_{\theta, m-n}\bm{k}_i
$$&lt;p>这里 $\bm{q}&lt;em>i=[q&lt;/em>{2i},q_{2i+1}]^T$, $\bm{k}&lt;em>i=[k&lt;/em>{2i},k_{2i+1}]^T$ 分别是对应的 pair，我们考虑其中一个分量，不妨假设 $|\bm{q}|_2=|\bm{k}|_2=1$, 我们有&lt;/p>
$$
\begin{aligned}
\bm{q}_i^TR_{\theta, m-n}\bm{k}_i
&amp;\leq \bm{q}_i^TR_{\theta, m-n}\bm{q}_i\\
&amp;= \bm{q}_i^T\left(\frac{R_{\theta, m-n}+R_{\theta, m-n}^T}{2}\right)\bm{q}_i\\
&amp;\leq \lambda_{\max}\left(\frac{R_{\theta, m-n}+R_{\theta, m-n}^T}{2}\right) \\
&amp;= \cos (m-n)\theta_i
\end{aligned}
$$&lt;p>其中，第一个不等式是因为 两个向量相等时其内积最大，第二个不等式是由于二次型最大值为矩阵的特征值。&lt;/p>
&lt;p>这样我们就有&lt;/p>
$$
\langle f_q(\bm{x}_q,m), f_v(\bm{x}_k,m)\rangle \leq \sum_{i=1}^{d/2}\cos (m-n)\theta_i.
$$&lt;p>我们可以简单画出对应的曲线：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-position-encoding/RoPE_long_term_decay.png"
width="1200"
height="600"
srcset="https://maosong2022.github.io/p/notes-on-position-encoding/RoPE_long_term_decay_hu12721434127033867948.png 480w, https://maosong2022.github.io/p/notes-on-position-encoding/RoPE_long_term_decay_hu8640095232158000116.png 1024w"
loading="lazy"
alt="Long term decay of RoPE"
class="gallery-image"
data-flex-grow="200"
data-flex-basis="480px"
>&lt;/p>
&lt;p>这里针对不同的配置，结果也会略微不同，具体分析可以参加知乎回答 &lt;a class="link" href="https://zhuanlan.zhihu.com/p/705492804" target="_blank" rel="noopener"
>RoPE的远距离衰减&lt;/a>&lt;/p>
&lt;h2 id="rope-代码实现与理解">RoPE 代码实现与理解
&lt;/h2>&lt;h3 id="naive-实现">Naive 实现
&lt;/h3>&lt;p>我们接下来看一下如何实现 RoPE&lt;/p>
$$
\Theta_m\bm{x}=\begin{bmatrix}
\cos m\theta_0 &amp; -\sin m\theta_0 &amp; &amp;&amp;\cdots &amp;\cdots &amp;\cdots \\
\sin m\theta_0 &amp; \cos m\theta_0 &amp; &amp;&amp;\cdots &amp;\cdots &amp;\cdots\\
&amp; &amp; \cos m\theta_1 &amp; -\sin m\theta_1 &amp; \cdots &amp;\cdots &amp;\cdots\\
&amp; &amp; \sin m\theta_1 &amp; \cos m\theta_1 &amp; \cdots &amp;\cdots &amp;\cdots \\
&amp;&amp;&amp;&amp; \ddots &amp;\vdots &amp;\vdots\\
&amp;&amp;&amp;&amp; &amp; \cos m\theta_{d/2} &amp; -\sin m\theta_{d/2}\\
&amp;&amp;&amp;&amp; &amp; \sin m\theta_{d/2} &amp; \cos m\theta_{d/2}
\end{bmatrix}\begin{bmatrix}
x_1\\
x_2\\
\vdots \\
x_d
\end{bmatrix}
$$&lt;p>在实现的时候，我们一般根据 $\sin$ 和 $\cos$ 进行分组，也就是&lt;/p>
$$
\Theta_m\bm{x}=\begin{bmatrix}
\cos m\theta_0\\
\cos m\theta_0\\
\vdots\\
\cos m\theta_{d/2}\\
\cos m\theta_{d/2}\\
\end{bmatrix}\odot \begin{bmatrix}
x1\\
x2\\
\vdots\\
x_{d-1}\\
x_d\\
\end{bmatrix} + \begin{bmatrix}
\sin m\theta_0\\
\sin m\theta_0\\
\vdots\\
\sin m\theta_{d/2}\\
\sin m\theta_{d/2}\\
\end{bmatrix}\odot
\begin{bmatrix}
-\ x_2\\
x_1\\
\vdots\\
-x_d\\
x_{d-1}\\
\end{bmatrix}
$$&lt;p>我们通常按照奇偶 index 来分别计算，然后通过重排序来得到最终的结果，实现代码如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">apply_rotary_pos_emb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sin&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">cos&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x_even&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">...&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">::&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="c1"># (seq_len, d_k_half)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x_odd&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">...&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">::&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="c1"># (seq_len, d_k_half)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">odds&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">cos&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">x_even&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">sin&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">x_odd&lt;/span> &lt;span class="c1"># (...,seq_len, d_k_half)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">evens&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">sin&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">x_even&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">cos&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">x_odd&lt;/span> &lt;span class="c1"># (...,seq_len, d_k_half)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">stacked&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">stack&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">odds&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">evens&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># (...,seq_len, 2, d_k_half)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">stacked_trans&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">rearrange&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">stacked&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;... seq_len double d_k_half -&amp;gt; ... seq_len d_k_half double&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span> &lt;span class="c1"># (...,seq_len, d_k_half, 2)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">out&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">rearrange&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">stacked_trans&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;... seq_len d_k_half double -&amp;gt; ... seq_len (d_k_half double)&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span> &lt;span class="c1"># (..., seq_len, d_k)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">out&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="llama-实现">LLaMA 实现
&lt;/h2>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">precompute_freqs_cis&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">end&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">theta&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">float&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">10000.0&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">freqs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">1.0&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">theta&lt;/span> &lt;span class="o">**&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)[:&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)]&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">t&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">device&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">freqs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">device&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># type: ignore&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">freqs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">outer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">freqs&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="c1"># type: ignore&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">freqs_cis&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">polar&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ones_like&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">freqs&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">freqs&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># complex64&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">freqs_cis&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">reshape_for_broadcast&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">freqs_cis&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ndim&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ndim&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">assert&lt;/span> &lt;span class="mi">0&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">ndim&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">assert&lt;/span> &lt;span class="n">freqs_cis&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">shape&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">d&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="ow">or&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">ndim&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">enumerate&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">)]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">freqs_cis&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">apply_rotary_emb&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">xq&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">xk&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">freqs_cis&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">Tuple&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">xq_&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view_as_complex&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">xq&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reshape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">xq&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[:&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">xk_&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view_as_complex&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">xk&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reshape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">xk&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[:&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">freqs_cis&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">reshape_for_broadcast&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">freqs_cis&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">xq_&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">xq_out&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view_as_real&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">xq_&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">freqs_cis&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">flatten&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">xk_out&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view_as_real&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">xk_&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">freqs_cis&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">flatten&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">xq_out&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">type_as&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">xq&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">xk_out&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">type_as&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">xk&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>在 LLaMA 中，我们首先还是计算 $\theta_i$, 然后在计算的过程中，我们将 $(x_i,x_{i+1})$ 视作一个复数，然后 乘以 $\exp(im\theta)$, 最后再取实部得到最终的结果&lt;/p>
&lt;h2 id="通用实现">通用实现
&lt;/h2>&lt;p>实际上，naive 版本的实现与现在大语言模型所采用的实现并不一致，我们先看一下现有的大语言模型的 RoPE 实现，这里我们将 &lt;a class="link" href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py" target="_blank" rel="noopener"
>LLaMA的transformer代码&lt;/a> 放在下面，&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">rotate_half&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Rotates half the hidden dims of the input.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">...&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">...&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="p">:]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cat&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">x2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x1&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">apply_rotary_pos_emb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">q&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">cos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sin&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">position_ids&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">unsqueeze_dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">cos&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">cos&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">unsqueeze&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">unsqueeze_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sin&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">sin&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">unsqueeze&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">unsqueeze_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q_embed&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">q&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">cos&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">rotate_half&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">q&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">sin&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">k_embed&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">k&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">cos&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">rotate_half&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">sin&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">q_embed&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k_embed&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">LlamaRotaryEmbedding&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">max_position_embeddings&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">2048&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">base&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">10000&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">device&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="fm">__init__&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dim&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">dim&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">max_position_embeddings&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">max_position_embeddings&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">base&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">base&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">inv_freq&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">1.0&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">base&lt;/span> &lt;span class="o">**&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">device&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">register_buffer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;inv_freq&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">inv_freq&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">max_seq_len_cached&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">seq_len&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">t&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">max_seq_len_cached&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">device&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">device&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">inv_freq&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dtype&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">freqs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">einsum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;i,j-&amp;gt;ij&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">t&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">inv_freq&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Different from paper, but it uses a different permutation in order to obtain the same calculation&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">emb&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cat&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">freqs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">freqs&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">register_buffer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;cos_cached&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">emb&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cos&lt;/span>&lt;span class="p">()[&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="p">:]&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dtype&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">persistent&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">register_buffer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;sin_cached&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">emb&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sin&lt;/span>&lt;span class="p">()[&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="p">:]&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dtype&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">persistent&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">seq_len&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># x: [bs, num_attention_heads, seq_len, head_size]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">seq_len&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">max_seq_len_cached&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_set_cos_sin_cache&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">seq_len&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">seq_len&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">device&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">device&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dtype&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cos_cached&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="p">:&lt;/span>&lt;span class="n">seq_len&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">...&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dtype&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sin_cached&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="p">:&lt;/span>&lt;span class="n">seq_len&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">...&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dtype&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>我们将上述代码翻译成公式，现在我们的 $\Theta$ 变成了 (对应 &lt;code>emb = torch.cat((freqs, freqs), dim=-1)&lt;/code>)&lt;/p>
$$
\Theta = [\theta_0,\dots,\theta_{d/2},\theta_0,\dots,\theta_{d/2}]^T
$$&lt;p>实际上 $\sin$ 部分对应的向量现在变成了&lt;/p>
$$
[-x_{d/2+1},
-x_{d/2+2},
\dots,
-x_{d},
x_1,
\dots,
x_{d/2}]^T
$$&lt;p>我们带回到原始公式，可以得到对应的 RoPE 操作变成了&lt;/p>
$$
R_{\theta,m}^d=\begin{bmatrix}
\cos m\theta_0 &amp; &amp; &amp; -\sin m\theta_0 &amp; \cdots &amp;\cdots &amp;\cdots \\
&amp; &amp; \cos m\theta_1 &amp; &amp;-\sin m\theta_1 &amp; \cdots &amp;\cdots \\
&amp; &amp; &amp; \cos m\theta_2 &amp; &amp;-\sin m\theta_2 &amp; \cdots \\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp; \vdots &amp;\vdots &amp;\vdots\\
&amp; &amp; &amp; &amp;\cos m\theta_{d/2 - 1} &amp; &amp; -\sin m\theta_{d/2 - 1} \\
\sin m\theta_0 &amp;&amp; &amp; \cos m\theta_0 &amp;&amp;\cdots &amp;\cdots \\
&amp; &amp; \sin m\theta_1 &amp; &amp;\cos m\theta_1 &amp; \cdots &amp;\cdots \\
&amp; &amp; &amp; \sin m\theta_2 &amp; &amp;\cos m\theta_2 &amp; \cdots \\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp; \vdots &amp;\vdots &amp;\vdots\\
&amp;&amp;&amp;&amp; \sin m\theta_{d/2 - 1}&amp; &amp; \cos m\theta_{d/2 - 1}
\end{bmatrix}
$$&lt;p>这列每一行的 $\cos$ 和 $\sin$ 都相差了 $d/2$ 列.&lt;/p>
&lt;p>因此，这里的区别在于，原始 RoPE 计算的 pair 为 $(x_{i}, x_{i+1})$, 而 LLaMA 里的 RoPE 计算的 pair 为 $(x_{i}, x_{i+d/2})$. transformers library 使用这种方式，可以减少计算量，提高整体的计算效率。&lt;/p>
&lt;p>为了适应使用 LLaMA 中实现的 RoPE 的，Huggingface 对权重进行了转换，使得基于原始 RoPE 实现的模型也可以获得加速.&lt;/p>
&lt;p>假设 $d=8$，原始 RoPE 的 pair 为 &lt;code>[(q_0, q_1), (q_2, q_3), (q_4, q_5), (q_6, q_7)]&lt;/code>, 新的 pair 为 &lt;code>[(q_0, q_4), (q_1, q_5), (q_2, q_6), (q_3, q_7)]&lt;/code>. 我们希望对 index 进行 remap，我们发现一个满足条件的 permutation 为 &lt;code>[0, 2, 4, 6, 1, 3, 5, 7]&lt;/code>, 也就是 &lt;code>q_0-&amp;gt;q_0&lt;/code>, &lt;code>q_2-&amp;gt;q_1&lt;/code>, &amp;hellip;, &lt;code>q_7-&amp;gt;q_7&lt;/code>.&lt;/p>
&lt;p>但是，如果我们在推理时这样做，就会降低整体速度，因此 Huggingface 的做法是改变 $W_Q$ 和 $W_K$ 的权重，具体来说，就是 $\Pi q=(\Pi W_Q)x$， 左边是在线转换，右侧离线转换。转换好 $W_Q$ 之后，正常计算就可以了。&lt;a class="link" href="https://github.com/huggingface/transformers/blob/e42587f596181396e1c4b63660abf0c736b10dae/src/transformers/models/llama/convert_llama_weights_to_hf.py" target="_blank" rel="noopener"
>具体代码&lt;/a> 为&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># permute for sliced rotary&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">permute&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">w&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n_heads&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">n_heads&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim1&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim2&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">w&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n_heads&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim1&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="n">n_heads&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim2&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transpose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reshape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">state_dict&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;model.layers.&lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">layer_i&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">.self_attn.q_proj.weight&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">permute&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">loaded&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;layers.&lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">layer_i&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">.attention.wq.weight&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;model.layers.&lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">layer_i&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">.self_attn.k_proj.weight&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">permute&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">loaded&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;layers.&lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">layer_i&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">.attention.wk.weight&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">...&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="结论">结论
&lt;/h2>&lt;p>本文中，我们回顾了位置编码，包括绝对位置编码和相对位置编码，我们着重介绍了 RoPE 的原理，推导以及代码实现。&lt;/p>
&lt;h2 id="参考文献">参考文献
&lt;/h2>&lt;ol>
&lt;li>&lt;a class="link" href="https://huggingface.co/blog/designing-positional-encoding" target="_blank" rel="noopener"
>You could have designed state of the art positional encoding&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://discuss.huggingface.co/t/is-llama-rotary-embedding-implementation-correct/44509/2" target="_blank" rel="noopener"
>Is LLaMA rotary embedding implementation correct?&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/huggingface/transformers/issues/25199" target="_blank" rel="noopener"
>[LLaMA] Rotary positional embedding differs with official implementation&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://kexue.fm/archives/8130/comment-page-6#comments" target="_blank" rel="noopener"
>RoPE blog&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2104.09864" target="_blank" rel="noopener"
>RoFormer&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://zhuanlan.zhihu.com/p/1894384438206505105" target="_blank" rel="noopener"
>位置编码之路&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://zhuanlan.zhihu.com/p/705492804" target="_blank" rel="noopener"
>RoPE的远距离衰减&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>Notes on Qwen3</title><link>https://maosong2022.github.io/p/notes-on-qwen3/</link><pubDate>Thu, 15 May 2025 14:48:11 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-qwen3/</guid><description>&lt;p>Qwen 在 2025 年 5 月发布了 Qwen3 系列大语言模型，Qwen3 包括 6 个 dense 模型和 2 个 MoE 模型，主要亮点为多语种能力，自适应快慢思考能力以及支持用户设置 thinking budget.&lt;/p>
&lt;p>Qwen3 包括 6 个 dense 模型和 2 个 MoE 模型，其旗舰模型是一个 235B 的 MoE 模型，激活参数为 22B. Qwen3 系列的主要亮点如下:&lt;/p>
&lt;ol>
&lt;li>快慢思考融合，模型原生支持在 reasoning/non-reasoning 模式之间切换&lt;/li>
&lt;li>Reasoning budget, 用户可以指定思考需要的 budget，来平衡 latency 和 performance&lt;/li>
&lt;li>Distillation, 使用蒸馏的方法训练小模型，大幅度提高模型的表现&lt;/li>
&lt;li>多语种支持，相比于 Qwen2.5，Qwen3 支持 119 中语言和方言&lt;/li>
&lt;/ol>
&lt;h2 id="method">Method
&lt;/h2>&lt;h3 id="model">Model
&lt;/h3>&lt;p>Qwen3 的 dense 模型的架构与 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5/" target="_blank" rel="noopener"
>Qwen2.5&lt;/a> 基本一致，包括使用 &lt;a class="link" href="https://maosong.website/p/notes-on-gqa/" target="_blank" rel="noopener"
>GQA&lt;/a> , SwiGLU, RoPE, RMSNorm 和 pre-normalization. Qwen3 进一步移除了 QKV bias, 然 后加入了 &lt;a class="link" href="https://maosong.website/p/notes-on-qk-norm/" target="_blank" rel="noopener"
>QK-Norm&lt;/a> 来提高训练的稳定性。&lt;/p>
&lt;p>Qwen3 的 MoE 架构使用了 128 个专家，激活专家个数为 8 个。与 Qwen2.5-MoE 不同，Qwen3 里没有使用 shard experts。并且，Qwen3 加入了 global-batch load balancing loss,来提高 expert 的特化程度。&lt;/p>
&lt;p>在 tokenizer 方面，Qwen 系列的 tokenizer 一直都是一样的，这也是 Qwen 系列领先的一点。&lt;/p>
&lt;p>模型的具体参数如下两张表所示。&lt;/p>
&lt;p>&lt;strong>MoE 架构&lt;/strong>：上下文长度为 128K，128 个专家，每个 token 由 8 个专家负责处理&lt;/p>
&lt;ul>
&lt;li>Qwen3-235B-A22B, 总参数 235B，激活参数 22B&lt;/li>
&lt;li>Qwen3-30B-A3B, 总参数 30B，激活参数 3B&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Models&lt;/th>
&lt;th>Layers&lt;/th>
&lt;th>Heads (Q / KV)&lt;/th>
&lt;th># Experts (Total / Activated)&lt;/th>
&lt;th>Context Length&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Qwen3-30B-A3B&lt;/td>
&lt;td>48&lt;/td>
&lt;td>32 / 4&lt;/td>
&lt;td>128 / 8&lt;/td>
&lt;td>128K&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen3-235B-A22B&lt;/td>
&lt;td>94&lt;/td>
&lt;td>64 / 4&lt;/td>
&lt;td>128 / 8&lt;/td>
&lt;td>128K&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>dense 架构&lt;/strong>: Qwen3-32B, Qwen3-14B, Qwen3-8B, Qwen3-4B, Qwen3-1.7B, and Qwen3-0.6B&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Models&lt;/th>
&lt;th>Layers&lt;/th>
&lt;th>Heads (Q / KV)&lt;/th>
&lt;th>Tie Embedding&lt;/th>
&lt;th>Context Length&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Qwen3-0.6B&lt;/td>
&lt;td>28&lt;/td>
&lt;td>16 / 8&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>32K&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen3-1.7B&lt;/td>
&lt;td>28&lt;/td>
&lt;td>16 / 8&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>32K&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen3-4B&lt;/td>
&lt;td>36&lt;/td>
&lt;td>32 / 8&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>32K&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen3-8B&lt;/td>
&lt;td>36&lt;/td>
&lt;td>32 / 8&lt;/td>
&lt;td>No&lt;/td>
&lt;td>128K&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen3-14B&lt;/td>
&lt;td>40&lt;/td>
&lt;td>40 / 8&lt;/td>
&lt;td>No&lt;/td>
&lt;td>128K&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen3-32B&lt;/td>
&lt;td>64&lt;/td>
&lt;td>64 / 8&lt;/td>
&lt;td>No&lt;/td>
&lt;td>128K&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="pre-training">Pre-training
&lt;/h3>&lt;p>预训练数据一共包括 &lt;strong>36T token&lt;/strong>，覆盖了 119 种语言。数据包括 coding, STEM, reasoning, books, multilingual texts 以及合成数据。&lt;/p>
&lt;p>为了扩展训练数据，作者微调了 Qwen2.5-VL 来从 PDF 文档中提取文字，然后使用 Qwen2.5 来进行修正。最终收集到了几 T 的 token。另外，作者还使用 Qwen2.5, Qwen2.5-Math, Qwen2.5-Coder 来合成不同格式的数据，包括教科书，QA，指令以及代码片段等。最后，作者加入了更多的多语种数据。&lt;/p>
&lt;p>作者从 educational value, fields, domains 以及 safety 对数据进行了标注。在数据混合时，Qwen3 在 instance 层面进行操作。&lt;/p>
&lt;p>预训练阶段包括 3 个 stage：&lt;/p>
&lt;ol>
&lt;li>General Stage (S1): 这一阶段的目的是让模型掌握世界知识，使用了 &lt;strong>30T&lt;/strong> 的 token，模型上下文长度为 4096&lt;/li>
&lt;li>Reasoning Stage (S2): 这一阶段的目的是提高模型的推理能力，使用了 &lt;strong>5T&lt;/strong> 的高质量 token，模型上下文长度为 4096，数据包括 STEM, coding, reasoning 以及合成数据&lt;/li>
&lt;li>Long Context Stage (S3): 这一阶段的目的是提升模型的长上下文能力，使用了&lt;strong>几百 B&lt;/strong>的 token，模型上下文长度为 32768.训练时数据混合 75% 的长文档数据，25% 的短文本数据。作者将 RoPE 的 frequency 从 10000 提升到了 1,000,000. 作者还是用 YARN 以及 Dual Chunk Attention 来提高 inference 效率&lt;/li>
&lt;/ol>
&lt;p>对 pre-training 的 base model 进行评测之后，作者发现：&lt;/p>
&lt;ol>
&lt;li>&lt;code>Qwen3-235B-A22B-Base&lt;/code> 超过了其他 base 模型的表现，包括 &lt;code>DeepSeek-V3 Base&lt;/code>, &lt;code>Llama-4-Maverick Base&lt;/code>, &lt;code>Qwen2.5-72B Base&lt;/code>&lt;/li>
&lt;li>Qwen3-MoE 模型与相同大小的 Qwen3-Dense 模型参数相比，其只需要 1/5 的参数就可以达到相同的表现&lt;/li>
&lt;li>Qwen3-MoE 模型与 2 倍参数量的 Qwen2.5-MoE 模型表现差不多&lt;/li>
&lt;li>Qwen3-Dense 模型与大一个量级的 Qwen2.5-Dense 模型表现差不多&lt;/li>
&lt;/ol>
&lt;h3 id="post-training">Post-training
&lt;/h3>&lt;p>Qwen3 的 post-training 如下图所示：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-qwen3/Qwen3_post-training.png"
width="1355"
height="614"
srcset="https://maosong2022.github.io/p/notes-on-qwen3/Qwen3_post-training_hu8850245799975621570.png 480w, https://maosong2022.github.io/p/notes-on-qwen3/Qwen3_post-training_hu13715759687519081896.png 1024w"
loading="lazy"
alt="Post-training Pipeline of Qwen3"
class="gallery-image"
data-flex-grow="220"
data-flex-basis="529px"
>&lt;/p>
&lt;p>对于旗舰模型 (&lt;code>Qwen3-235B-A22B&lt;/code>, &lt;code>Qwen3-32B&lt;/code>) 的训练，Qwen3 使用了一个四阶段的训练 pipeline。对于轻量化模型（其他模型）的训练，Qwen3 使用了知识蒸馏。&lt;/p>
&lt;p>旗舰模型的训练包括四个阶段，前两个阶段用于提升模型的 reasoning 能力，后两个阶段用于将 reasoning 和 non-reasoning 能力结合起来。&lt;/p>
&lt;h4 id="flashship-model">Flashship Model
&lt;/h4>&lt;p>&lt;strong>Stage 1 (Long CoT Cold Start)&lt;/strong>
这个阶段的目的是让模型掌握 reasoning 的基础。这个阶段使用了数学，代码，逻辑推理和通用的 STEM 相关问题。每个问题都有参考答案或者 test-cases. 作者使用了 Qwen2.5-72B 来过滤数据，包括 non-verifiable prompts 以及太简单的 prompt. 作者认为，这一阶段应该减少训练使用的样本和训练步数。&lt;/p>
&lt;p>&lt;strong>Stage 2 (Reasoning RL)&lt;/strong>
这个阶段的目的是提升模型的 reasoning 能力。该阶段使用了 3,995 条过滤得到的样本，算法为 GRPO. 作者发现提高 batch size 和每个 query 的 rollouts 可以提高模型的表现。作者通过调整模型的 entropy 来控制 exploration 和 exploitation 的平衡&lt;/p>
&lt;p>&lt;strong>Stage 3 (Thinking Mode Fusion)&lt;/strong>
这一阶段的目的是将 non-reasoning 能力加入到之前的 reasoning 模型中。作者在第二阶段的 model 上进行了 continual SFT，然后构建了一个 chat template 用于融合两种模式。&lt;/p>
&lt;p>reasoning 数据来源于 stage1 的 rejection sampling 和 stage 2 的模型. non-reasoning 数据来源于各种任务，如 coding, math, multilingual 等。为了保证模型的多语种能力，作者还加入了一些翻译相关的数据。&lt;/p>
&lt;p>作者还构建了一个 chat template, 用于统一数据格式。chat template 如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-qwen3/Qwen3_chat_template.png"
width="740"
height="320"
srcset="https://maosong2022.github.io/p/notes-on-qwen3/Qwen3_chat_template_hu7314460585051966683.png 480w, https://maosong2022.github.io/p/notes-on-qwen3/Qwen3_chat_template_hu9999242725563798042.png 1024w"
loading="lazy"
alt="chat template of Qwen3"
class="gallery-image"
data-flex-grow="231"
data-flex-basis="555px"
>&lt;/p>
&lt;p>作者使用 &lt;code>/think&lt;/code> 和 &lt;code>/no_think&lt;/code> 来标记两种模式，对于 non-reasoning mode, 其 &lt;code>&amp;lt;think&amp;gt;&amp;lt;/think&amp;gt;&lt;/code> 会被置空。模型在默认情况下处于 reasoning mode, 因此作者加入了一些不包含 &lt;code>/think&lt;/code> 的 reasoning 数据。&lt;/p>
&lt;p>作者发现，通过这种 Think mode fusion, 模型可以学会在 reasoning mode 和 non-reasoning mode 下进行回答，因此，模型也可以基于中间结果来给出最终的答案。 当超出 budget 之后，作者使用以下 Instruction&lt;/p>
&lt;p>&lt;code>Considering the limit time by the user. I have to give the solution based on the thinking directly now. \n&amp;lt;/think&amp;gt;.\n\n&lt;/code>&lt;/p>
&lt;p>来让模型直接终止思考二给出最终的答案。&lt;/p>
&lt;p>&lt;strong>Stage 4 (General RL)&lt;/strong>
这个阶段的目的是提升模型在不同场景下的能力。作者构建了一个 reward system 来覆盖 20 多种不同的任务。这些任务包括：instruction following, format following, preference alignment, agent ability 以及 abilities for specialized scenarios.&lt;/p>
&lt;p>作者构建了三种不同的 rewards:&lt;/p>
&lt;ol>
&lt;li>Rule-based rewards: 覆盖的任务包括 instruction following 和 format following&lt;/li>
&lt;li>Model-based rewards: 作者使用 Qwen2.5-72B 来判别答案的正确性&lt;/li>
&lt;li>Model-based Reward without reference answer: 作者训练一个 reward model 来给模型的回答进行打分&lt;/li>
&lt;/ol>
&lt;h4 id="lightweight-model">Lightweight Model
&lt;/h4>&lt;p>对于轻量化的模型，作者发现直接通过蒸馏可以有效提高学生模型的表现，并且训练效率也更高。蒸馏训练包括两个阶段：&lt;/p>
&lt;ol>
&lt;li>Off-policy Distillation: 这个阶段的目的是让模型拥有基本的 reasoning 能力并且可以在不同的模式中进行切换。作者使用了教师模型的 reasoning 输出和 non-reasoning 输出来蒸馏学生模型&lt;/li>
&lt;li>On-policy Distillation: 在这个阶段，学生模型生成回答，然后基于教师模型的输出，使用 KL-divergence 来更新学生模型的参数&lt;/li>
&lt;/ol>
&lt;h2 id="evaluation">Evaluation
&lt;/h2>&lt;p>&lt;strong>Thinking budget&lt;/strong>. 作者发现当我们提高 Thinking budget 之后，模型的表现是可以持续提升的。结果如下图&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-qwen3/Qwen3_thinking_budget_performance.png"
width="1355"
height="938"
srcset="https://maosong2022.github.io/p/notes-on-qwen3/Qwen3_thinking_budget_performance_hu13788390577533869064.png 480w, https://maosong2022.github.io/p/notes-on-qwen3/Qwen3_thinking_budget_performance_hu12962124605035138392.png 1024w"
loading="lazy"
alt="Performance according to thinking budget"
class="gallery-image"
data-flex-grow="144"
data-flex-basis="346px"
>&lt;/p>
&lt;p>&lt;strong>Efficiency of distillation&lt;/strong>. 作者发现使用 distillation 可以大幅度提高模型的表现和训练效率。下面是结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-qwen3/Qwen3_distillation_ablation.png"
width="1309"
height="200"
srcset="https://maosong2022.github.io/p/notes-on-qwen3/Qwen3_distillation_ablation_hu2369750054666948761.png 480w, https://maosong2022.github.io/p/notes-on-qwen3/Qwen3_distillation_ablation_hu7032629541131820710.png 1024w"
loading="lazy"
alt="Comparison between distillation and RL"
class="gallery-image"
data-flex-grow="654"
data-flex-basis="1570px"
>&lt;/p>
&lt;p>&lt;strong>Effects of Thinking mode fusion and RL&lt;/strong> 作者进一步探究了三个 stage 对模型表现的影响，为此，作者构建了 in-house benchmarks 来评估模型的表现，这些 benchmarks 包括：&lt;/p>
&lt;ol>
&lt;li>CounterFactrQA. 问题是不符合事实的，用于评估模型的幻觉&lt;/li>
&lt;li>LengthCtrl. 有长度要求的写作任务，评估生成内容长度和给定长度之间的差别&lt;/li>
&lt;li>ThinkFollow. 多轮对话，每轮对话随机插入 &lt;code>/think&lt;/code> 和 &lt;code>/no_think&lt;/code> flag，评估模型是否能在两种模式之间切换&lt;/li>
&lt;li>Tooluse. 评估模型的工具调用能力&lt;/li>
&lt;/ol>
&lt;p>结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-qwen3/Qwen3_stage_ablation.png"
width="1358"
height="684"
srcset="https://maosong2022.github.io/p/notes-on-qwen3/Qwen3_stage_ablation_hu2887782245149393248.png 480w, https://maosong2022.github.io/p/notes-on-qwen3/Qwen3_stage_ablation_hu16197092104876694984.png 1024w"
loading="lazy"
alt="Performance of difference stages"
class="gallery-image"
data-flex-grow="198"
data-flex-basis="476px"
>&lt;/p>
&lt;p>结论如下：&lt;/p>
&lt;ol>
&lt;li>Stage3 可以提高模型在两种 reasoning mode 切换的能力，并且 stage3 还可以提高模型的通用以及 instruction following 能力&lt;/li>
&lt;li>Stage4 进一步提高模型在两种模式下的通用，instruction following 和 agent 能力&lt;/li>
&lt;li>Stage3 和 stage4 并没有显著提高模型在 knowledge, STEM, math 和 coding 相关任务上的表现。甚至在一些竞赛如 AIME24 上模型的表现还有所下降，作者认为这是由于我们提升了模型的通用能力而导致其特化能力下降导致的，作者认为作为一个通用模型，这是可以接受的。&lt;/li>
&lt;/ol>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>在本文中，作者提出了 Qwen3 系列大语言模型，包括 6 个 Dense 模型和 2 个 MoE 模型。Qwen3 模型标志了一个新的 SOTA，其特点主要是快慢思考结合，thinking budget，以及多语种。&lt;/p>
&lt;p>作者认为后续工作有以下几点：&lt;/p>
&lt;ol>
&lt;li>使用更高质量的数据来进行预训练&lt;/li>
&lt;li>优化模型架构和训练方式，提升模型的上下文&lt;/li>
&lt;li>提高针对 RL 的计算资源，来进一步提高模型的 agent 能力&lt;/li>
&lt;/ol>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2505.09388" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/QwenLM/Qwen3" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Seed1.5-VL</title><link>https://maosong2022.github.io/p/notes-on-seed1.5-vl/</link><pubDate>Wed, 14 May 2025 09:28:07 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-seed1.5-vl/</guid><description>&lt;p>字节Seed在5月11号发布了Seed1.5-VL技术报告。技术报告详细介绍了Seed1.5-VL的架构，训练和评估细节&lt;/p>
&lt;h1 id="简介">简介
&lt;/h1>&lt;p>Seed1.5-VL是多模态大模型，它是一个标准的 Vision Encoder - MLP - LLM 的架构，Vision Encoder是自研的Seed-ViT，参数量为532M，大语言模型的激活参数为20B（虽然论文未提及总参数量，但是从seed-Thinking v1.5来看，总参数量应该是200B）。Seed1.5-VL强调了agent能力，比如GUI control和gameplay。&lt;/p>
&lt;h1 id="介绍">介绍
&lt;/h1>&lt;p>现有的LVLM还不能与人类的通用能力相比，特别是在3D空间理解，物体技术，交互游戏等方面。并且，LVLM训练预料的多样性也不如LLM。最后，多模态数据的异质性也对模型的训练和推理提出了挑战。&lt;/p>
&lt;p>基于这些问题，作者提出了Seed1.5-VL，一个视觉多模态大模型。通过构建高质量的训练数据，作者大幅度提高了模型的表现。作者还进一步探究了模型的scaling law以及pre-training和post training算法的设计。为了提高训练和推理效率，作者在infra上也进行了改进&lt;/p>
&lt;h1 id="架构">架构
&lt;/h1>&lt;p>Seed1.5-VL的架构如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-seed1.5-vl/architecture.png"
width="1387"
height="1062"
srcset="https://maosong2022.github.io/p/notes-on-seed1.5-vl/architecture_hu6358939600119516298.png 480w, https://maosong2022.github.io/p/notes-on-seed1.5-vl/architecture_hu16024849637560253520.png 1024w"
loading="lazy"
alt="Architecture of See1.5-VL"
class="gallery-image"
data-flex-grow="130"
data-flex-basis="313px"
>&lt;/p>
&lt;p>Seed1.5-VL的架构包含三个部分：&lt;/p>
&lt;ol>
&lt;li>Vision Encoder: vision encoder是seed自研的Seed-ViT，拥有532M参数&lt;/li>
&lt;li>Adapter：一个两层的MLP&lt;/li>
&lt;li>LLM：Seed1.5-LLM，一个MoE架构的LLM&lt;/li>
&lt;/ol>
&lt;p>Seed1.5-VL支持文本，图片和视频输入，输入为文本。其可以进行理解和推理(reasoning)。&lt;/p>
&lt;h2 id="seed-vit">Seed-ViT
&lt;/h2>&lt;p>Seed-ViT主要是在两个方面进行了改进：&lt;/p>
&lt;ol>
&lt;li>在pre-training阶段使用了视频数据，来提高模型的时空间感知能力&lt;/li>
&lt;li>使用了2D-RoPE来处理动态分辨率的图片&lt;/li>
&lt;/ol>
&lt;p>Seed-ViT的超参数如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Patch size&lt;/th>
&lt;th>Pos Embedding&lt;/th>
&lt;th>Head dim&lt;/th>
&lt;th>#heads&lt;/th>
&lt;th>Embedding dim&lt;/th>
&lt;th>MLP ratio&lt;/th>
&lt;th>#layers&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>14&lt;/td>
&lt;td>2D RoPE&lt;/td>
&lt;td>64&lt;/td>
&lt;td>20&lt;/td>
&lt;td>1280&lt;/td>
&lt;td>4.0&lt;/td>
&lt;td>27&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>对于输入的图片，Seed-ViT首先将图片resize到28*28的倍数，接下来图片会被分割为14*14的patch. 与NaViT一样，Seed-ViT也是用了token-packing的操作，来将多个图片packing到一个sequence里进行训练。最后，对于Seed-ViT输出的图片特征，作者还是用了 $2\times 2$的 average pooling来降低token个数。&lt;/p>
&lt;p>Seed-ViT的预训练包括三个阶段，其设置和超参数如下表所示：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Categories&lt;/th>
&lt;th>Unlabeled Image&lt;/th>
&lt;th>Image-text pairs&lt;/th>
&lt;th>Video-audio-text tuples&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Training samples&lt;/td>
&lt;td>2.2B&lt;/td>
&lt;td>4.8B&lt;/td>
&lt;td>65M&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Token percentages&lt;/td>
&lt;td>4.0%&lt;/td>
&lt;td>91.2%&lt;/td>
&lt;td>4.8%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Batch sizes&lt;/td>
&lt;td>55,296&lt;/td>
&lt;td>32,768&lt;/td>
&lt;td>1,024&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LR warm up steps&lt;/td>
&lt;td>1,692&lt;/td>
&lt;td>2,000&lt;/td>
&lt;td>12,800&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Maximum LR&lt;/td>
&lt;td>$7.06\times 10^{-3}$&lt;/td>
&lt;td>$1.0\times 10^{-4}$&lt;/td>
&lt;td>$5.0\times 10^{-5}$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Minimum LR&lt;/td>
&lt;td>$1.05\times 10^{-5}$&lt;/td>
&lt;td>$1.2\times 10^{-6}$&lt;/td>
&lt;td>$2.02\times 10^{-7}$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>在预训练时，作者考虑了三点：&lt;/p>
&lt;ol>
&lt;li>训练效率：尽可能提高模型的训练效率&lt;/li>
&lt;li>原生图片精度输入处理：让模型可以尽早处理不同分辨率图片输入&lt;/li>
&lt;li>数据利用率：可以使用不同类型的数据&lt;/li>
&lt;/ol>
&lt;p>基于这三点考虑，Seed-ViT的训练包括三个阶段：&lt;/p>
&lt;ol>
&lt;li>Masked Image modeling with 2D RoPE：这一阶段的目的是提高模型对与几何和结构的感知能力。作者使用 &lt;code>EVA02-CLIP-E&lt;/code>作为教师模型，然后随机将75%的patches进行mask，最后让Seed-ViT输出的特征尽可能匹配教师模型输出的特征。这里的损失函数是cosine similarity. 作者发现，scale up MIM之后，VLM在OCR和文档理解任务上的表现得到了大幅度的提升。&lt;/li>
&lt;li>Native Resolution Contrastive Learning：这一阶段和CLIP,SigLIP的训练差不多，text encoder与&lt;code>EVA02-CLIP-E&lt;/code>相同。损失函数为SigLIP loss和SuperClass loss&lt;/li>
&lt;li>Omni-modal pret-raining：这一阶段使用了MiCo框架，来将video frames, audio, visual captions以及audio captions进行对齐，text encoder编码caption，ViT编码其他部分。通过这一阶段的训练，模型可以学习omni-model representations,并且还可以提高模型的图片和视频理解人物上的表现。&lt;/li>
&lt;/ol>
&lt;h2 id="视频编码">视频编码
&lt;/h2>&lt;p>对于视频输入，Seed1.5-VL采用了 &lt;strong>Dynamic Frame-Resolution Sampling&lt;/strong>技巧，核心思想在于根据budget和处理的任务来动态调整采样率(frame)和每一帧的图片精度(resolution)。&lt;/p>
&lt;p>具体来讲，在时序上，默认的FPS为1，如果要处理有关时序信息任务的话，FPS调整为2，如果要处理motion tracking等任务的话，FPS提高到5.为了进一步提高模型的时间感知能力，Seed 1.5-VL加入了timestamp tokens (e.g., &lt;code>[1.5 second]&lt;/code>)。&lt;/p>
&lt;p>在空间上，每个frame的可选精度为 ${640, 512, 384, 256, 160, 128}$，选择的精度根据我们的budget进行决定。如果说我们使用最低配置还是不能处理视频输入的话，我们就会先进行均匀采样，然后再进行处理，这样可以让模型处理所有的视频信息。&lt;/p>
&lt;h1 id="pre-training">Pre-training
&lt;/h1>&lt;p>pre-training阶段一共包括了3T token. 作者详细介绍了每一个类别。这里我们就简单总结一下。数据一共包括以下7个类别&lt;/p>
&lt;ol>
&lt;li>Generic Image-text pairs &amp;amp; knowledge data. 这部分数据主要是图文交错以及image text-pairs数据。作者介绍了一下针对长尾数据（稀有物种识别）的数据构建方式&lt;/li>
&lt;li>OCR. 这部分数据有1B左右的sample，包括documents, scene texts, tables, charts以及flowcharts等。其中，作者通过SynthDog和Latex合成了200M左右的文本密集图片。作者还对合成的数据进行了增广;Chart数据包括公开数据集和合成的数据集，最终一共有100M的样本; Table数据集也是通过提取得到的，一共包含50M左右的图片。作者还基于OCR结果构建了VQA数据集，来提高模型理解图片文字的能力&lt;/li>
&lt;li>Visual Grounding &amp;amp; Counting: 提高模型识别和定位物体的能力。数据格式包括bounding box和center points. 其中bounding box主要是公开数据集；作者还是用Grounding DINO来标记了一部分数据，最后一共收集了200M的样本; point数据主要是PixMo, 作者还使用了Molmo以及CountGD标注了一部分数据，最后一共包括170M的指令以及110B的token; Counting数据从bounding box 和point数据集中筛选得到，包括了8M的样本和13B的token。最后在训练时，与InternVL一样，坐标会被normalize到 &lt;code>[1,1000]&lt;/code>&lt;/li>
&lt;li>3D spatial understanding. 主要包括相对深度，绝对深度和3D Grounding数据；对于相对深度，作者使用DepthAnything V2来标注，得到了3.2B的token; 对于绝对深度，作者使用了公开数据集，最终包含18M的指令以及28B的token;对于3D Grounding，作者使用了公开数据集，最终包含770的指令以及1.3B的token&lt;/li>
&lt;li>Video. 主要包含general video understanding数据, video temporal grounding and moment retrieval以及video streaming data.&lt;/li>
&lt;li>STEM. 主要包含image comprehension data和problem-solving data. 前者包括收集的3.2M教育相关的数据，以及10M结构化表格，4.5M化学结构表达式，1.5M坐标系数据。后者包括100M K12的习题&lt;/li>
&lt;li>GUI. 作者使用UI-TARS来合成数据。数据主要包括perception, grounding 以及 reasoning&lt;/li>
&lt;/ol>
&lt;p>预训练包括三个stage，其实验设置如下表所示：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Stages&lt;/th>
&lt;th>Stage 0&lt;/th>
&lt;th>Stage 1&lt;/th>
&lt;th>Stage 2&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Training budget (tokens)&lt;/td>
&lt;td>16B&lt;/td>
&lt;td>3T&lt;/td>
&lt;td>240B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Sequence length&lt;/td>
&lt;td>32,768&lt;/td>
&lt;td>32,768&lt;/td>
&lt;td>131,072&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Trainable components&lt;/td>
&lt;td>MLP adaptor&lt;/td>
&lt;td>all&lt;/td>
&lt;td>all&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Batch sizes (tokens)&lt;/td>
&lt;td>8.4M&lt;/td>
&lt;td>71M&lt;/td>
&lt;td>71M&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LR warmup steps&lt;/td>
&lt;td>100&lt;/td>
&lt;td>500&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Maximum LR&lt;/td>
&lt;td>$2.52 \times 10^{-4}$&lt;/td>
&lt;td>$5.22 \times 10^{-5}$&lt;/td>
&lt;td>$5.22 \times 10^{-6}$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Minimum LR&lt;/td>
&lt;td>$4.50 \times 10^{-5}$&lt;/td>
&lt;td>$5.22 \times 10^{-6}$&lt;/td>
&lt;td>$5.22 \times 10^{-6}$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;ul>
&lt;li>Stage 0: 仅训练MLP，用于对齐vision encoder和LLM,使用了16B token&lt;/li>
&lt;li>Stage 1：训练全部参数，用于获取知识以及掌握grounding和OCR能力，使用了3T token&lt;/li>
&lt;li>Stage 2：训练全部参数，提升模型的长上下文能力以及对下游任务的适应性，包括视频理解，coding和3D空间理解，上下文长度从32,768提升到131,072&lt;/li>
&lt;/ul>
&lt;p>作者还进行了消融实验，采取了和Qwen2-VL一样的训练方式，结果发现，本文的训练方式效果更好。作者认为，解冻vision encoder，可能让vision encoder弥补LLM的不足，进而损害其自身的感知能力。&lt;/p>
&lt;p>作者还探究了模型的scaling law，使用的建模函数为：&lt;/p>
$$
\log(\hat{L}) \sim \log(B) - \beta \log(D) = -a \log(D) + b
$$&lt;p>
这里 $L$ 是NLL loss, D是训练的token数，$a$和$b$是待拟合的参数。实验结果如下图所示。
&lt;img src="https://maosong2022.github.io/p/notes-on-seed1.5-vl/scaling_law.png"
width="1392"
height="1065"
srcset="https://maosong2022.github.io/p/notes-on-seed1.5-vl/scaling_law_hu5357680134013757132.png 480w, https://maosong2022.github.io/p/notes-on-seed1.5-vl/scaling_law_hu12709952869440683564.png 1024w"
loading="lazy"
alt="scaling law"
class="gallery-image"
data-flex-grow="130"
data-flex-basis="313px"
>&lt;/p>
&lt;p>结论是在特定领域数据训练的loss可以作为模型在下游任务上表现的一个估计&lt;/p>
&lt;h1 id="post-training">Post-training
&lt;/h1>&lt;p>post-training分为两个阶段：SFT和RL，其中RL包括RLHF, RLVR. 总流程如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-seed1.5-vl/post_training.png"
width="1367"
height="598"
srcset="https://maosong2022.github.io/p/notes-on-seed1.5-vl/post_training_hu5528687103497423085.png 480w, https://maosong2022.github.io/p/notes-on-seed1.5-vl/post_training_hu7246407921116532780.png 1024w"
loading="lazy"
alt="post training"
class="gallery-image"
data-flex-grow="228"
data-flex-basis="548px"
>&lt;/p>
&lt;h2 id="sft">SFT
&lt;/h2>&lt;p>SFT数据主要包括General Instruction data以及Long Chain-of-Thought (LongCoT) data，劝着途胜模拟徐保国的指令跟随能力，后者提升模型的reasoning能力。&lt;/p>
&lt;p>通用指令微调数据包括13,000条收集的数据和40,0000高质量的公开数据集。&lt;/p>
&lt;h2 id="rlhf">RLHF
&lt;/h2>&lt;p>RLHF的数据包括人类标注数据和合成数据。对于reward model，作者使用了LVLM作为reward model，来给不同的response进行打分。最后，使用变体的PPO算法进行训练。&lt;/p>
&lt;h2 id="rlvr">RLVR
&lt;/h2>&lt;p>RLVR的训练数据主要包含Visual STEM和Visual Perception and Reasoning. 前者包含1M左右的问题，主要是STEM和K12的相关问题。后者包括grounding, visual instruction following以及visual puzzles &amp;amp; games等&lt;/p>
&lt;h2 id="hybrid-rl">Hybrid RL
&lt;/h2>&lt;p>RL的训练过程包含了RLHF和RLVR两个RL算法。作者介绍了以下细节：&lt;/p>
&lt;ol>
&lt;li>format reward. 要求模型输出格式为 &lt;code>&amp;lt;think&amp;gt;{thought}&amp;lt;/think&amp;gt;{solution}&lt;/code>&lt;/li>
&lt;li>hybrid reward. reward包括RM和verifier两部分&lt;/li>
&lt;li>Shared critic. 使用一个critic model来估计value function&lt;/li>
&lt;li>KL coefficients. 对于RLHF和RLVE，使用不同的KL divergence稀疏&lt;/li>
&lt;/ol>
&lt;h2 id="iterative-update-by-rejection-sampling-fine-tuning">Iterative Update by Rejection Sampling Fine-tuning
&lt;/h2>&lt;p>在训练的过程中，作者使用了一个迭代训练策略。一个开始模型在低质量的Long-CoT数据上进行SFT，在RL过后，作者会重新评估问题的难度，然后过程掉一部分数据，再进行SFT，这样反复进行迭代训练以提高模型的表现。&lt;/p>
&lt;blockquote>
&lt;p>Infra部分好像都是一些细节，这里就跳过了。&lt;/p>
&lt;/blockquote>
&lt;h1 id="evaluation">Evaluation
&lt;/h1>&lt;p>作者评估了&lt;/p>
&lt;ol>
&lt;li>Seed-ViT的表现&lt;/li>
&lt;li>Seed1.5-VL的通用视觉理解和推理能力&lt;/li>
&lt;li>Seed1.5-VL的视频相关能力&lt;/li>
&lt;li>Seed1.5-VL的agent能力&lt;/li>
&lt;li>Seed1.5-VL在内部benchmark上的表现&lt;/li>
&lt;/ol>
&lt;p>由于Seed1.5-VL是一个API模型，并且其内部benchmark不公开，因此Seed-ViT以及其在内部benchmark上的表现我们就不列出来了。我们主要看一下其和领先多模态大模型的对比&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-seed1.5-vl/vision_benchmarks.png"
width="1240"
height="1184"
srcset="https://maosong2022.github.io/p/notes-on-seed1.5-vl/vision_benchmarks_hu11436903269627804540.png 480w, https://maosong2022.github.io/p/notes-on-seed1.5-vl/vision_benchmarks_hu11903969968901952136.png 1024w"
loading="lazy"
alt="vision benchmarks, 论文表6"
class="gallery-image"
data-flex-grow="104"
data-flex-basis="251px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-seed1.5-vl/video_benchmarks.png"
width="1254"
height="1025"
srcset="https://maosong2022.github.io/p/notes-on-seed1.5-vl/video_benchmarks_hu1700365900879711657.png 480w, https://maosong2022.github.io/p/notes-on-seed1.5-vl/video_benchmarks_hu5982671056146884601.png 1024w"
loading="lazy"
alt="Video benchmarks, 论文表7"
class="gallery-image"
data-flex-grow="122"
data-flex-basis="293px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-seed1.5-vl/gui_benchmarks.png"
width="1190"
height="393"
srcset="https://maosong2022.github.io/p/notes-on-seed1.5-vl/gui_benchmarks_hu10558902132270732879.png 480w, https://maosong2022.github.io/p/notes-on-seed1.5-vl/gui_benchmarks_hu8916295417278482733.png 1024w"
loading="lazy"
alt="GUI benchmarks"
class="gallery-image"
data-flex-grow="302"
data-flex-basis="726px"
>&lt;/p>
&lt;h1 id="conclusion">Conclusion
&lt;/h1>&lt;p>在本文中，作者介绍了Seed1.5-VL，一个领先的多模态大模型，可以处理图片和视频输入。作者介绍了Seed-ViT以及Seed1.5-VL的数据，训练和评估。&lt;/p>
&lt;p>作者还分析了以下Seed1.5-VL的不足以及未来的改进方向&lt;/p>
&lt;ol>
&lt;li>Seed1.5-VL在复杂视觉感知任务中，其计数能力会下降。&lt;/li>
&lt;li>Seed1.5-VL的高阶推理能力还尚有不足，作者拿Klotski举了一个例子&lt;/li>
&lt;li>Seed1.5-VL的3D spatial reasoning能力不足&lt;/li>
&lt;li>Seed1.5-VL的temporal reasoning能力不足&lt;/li>
&lt;li>Seed1.5-VL与其他模型一样，存在幻觉问题&lt;/li>
&lt;/ol>
&lt;p>未来，有以下值得探索的方向：&lt;/p>
&lt;ol>
&lt;li>涌现现象(emergent abilities)&lt;/li>
&lt;li>scaling law，进一步探索模型参数量，算力以及数据对模型表现的影响&lt;/li>
&lt;li>提高模型的3D spatial reasoning能力，降低模型的幻觉，提高模型分组合搜索能力&lt;/li>
&lt;/ol>
&lt;h1 id="参考文献">参考文献
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2505.07062" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>分布式训练：参数量与计算量分析</title><link>https://maosong2022.github.io/p/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E5%8F%82%E6%95%B0%E9%87%8F%E4%B8%8E%E8%AE%A1%E7%AE%97%E9%87%8F%E5%88%86%E6%9E%90/</link><pubDate>Tue, 13 May 2025 11:26:36 +0800</pubDate><guid>https://maosong2022.github.io/p/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E5%8F%82%E6%95%B0%E9%87%8F%E4%B8%8E%E8%AE%A1%E7%AE%97%E9%87%8F%E5%88%86%E6%9E%90/</guid><description>&lt;p>在本文中，我们将要分析与大语言模型相关的参数量和计算量。在计算之前，我们会首先回顾一下大语言模型的架构&lt;/p>
&lt;h1 id="大语言模型架构">大语言模型架构
&lt;/h1>&lt;h1 id="大语言模型参数计算">大语言模型参数计算
&lt;/h1>&lt;h1 id="计算量估计">计算量估计
&lt;/h1>&lt;h1 id="checkpointing">checkpointing
&lt;/h1>&lt;h1 id="kv-cache">KV cache
&lt;/h1>&lt;h1 id="参考文献">参考文献
&lt;/h1>&lt;ol>
&lt;li>&lt;a class="link" href="https://zhuanlan.zhihu.com/p/624740065" target="_blank" rel="noopener"
>回旋托马斯x 文章&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>分布式训练：如何训练一个模型</title><link>https://maosong2022.github.io/p/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AA%E6%A8%A1%E5%9E%8B/</link><pubDate>Tue, 13 May 2025 11:26:36 +0800</pubDate><guid>https://maosong2022.github.io/p/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AA%E6%A8%A1%E5%9E%8B/</guid><description>&lt;p>本节中，我们将介绍模型训练的基本数学原理，以及在分布式训练中我们需要考虑的精度，优化器等问题。&lt;/p>
&lt;h1 id="训练的数学原理">训练的数学原理
&lt;/h1>&lt;p>在最优化里面，我们需要解决的问题一般有如下形式：&lt;/p>
$$
\min_x\ f(x)
$$&lt;p>
这里 $f$是我们的目标函数, $x$是我们的变量。一个比较简单的例子就是求一个给定函数的最小值。&lt;/p>
&lt;p>如果说，我们想要基于数据来训练一个模型，这个时候，我们目标函数的输入就包括两部分，一部分是模型参数，另一部分是数据，为了方便起见，我们使用$\theta$来代表模型的参数，用 ${x_i,y_i}_{i=1}^N$ 来表示模型的训练集。上述的优化问题改写如下：&lt;/p>
$$
\min_{\theta}\ \frac{1}{N}\sum_{i=1}^Nf(x_i,y_i\;\theta)
$$&lt;p>比如我们训练 resnet 作为分类器，那么 resnet 的模型参数就是这里的 $\theta$, 训练集就是我们的图片和对应的标签，比如ImageNet等，对应的$f$可以设定为 cross entropy loss.&lt;/p>
&lt;p>现在有了优化问题之后，我们就需要设计算法求解这个优化问题。一个最简单的优化算法就是梯度下降算法：&lt;/p>
$$
\theta^{k+1} = \theta^k - \alpha_k\frac{1}{N}\sum_{i=1}^N\nabla_{\theta}f(x_i,y_i;\theta^k)
$$&lt;p>
这里 $\nabla_{\theta}f(x;\theta^k)$ 是 $f$ 相对于 $\theta$ 在 $\theta^k$ 处的梯度。&lt;/p>
&lt;p>但是，当我们模型过于复杂的时候，梯度往往计算起来非常复杂。为了简化模型的训练，现在的框架如tensorflow和pytorch都支持自动微分。因此，我们只需要定义如何从输入 $(x_i,y_i)$计算得到 $f(x_i,y_i;\theta)$ 就可以了，框架会帮我们计算参数的梯度。&lt;/p>
&lt;h1 id="自动微分">自动微分
&lt;/h1>&lt;p>自动微分的目的是将求导的过程交给框架，从而让用户专注于模型的开发（也就是设计&lt;code>forward&lt;/code>函数）。&lt;/p>
&lt;p>自动微分的核心思想就是链式法则.&lt;/p>
$$
\frac{dy}{dx} = \frac{dy}{df}\frac{df}{dg}\frac{dg}{dh}\frac{dh}{dx}
$$&lt;p>
如果我们的中间函数 $g$, $h$非常复杂的话，那么整个求导过程就会非常复杂。而链式法则则是将这样一个全局过程给分解成了若干个局部过程。我们将 $y$ 表示为：&lt;/p>
$$
\begin{aligned}
y &amp;= f(y_1)\\
y_1&amp;=g(y_2)\\
y_2&amp;=h(y_3)\\
y_3&amp;=x
\end{aligned}
$$&lt;p>接下来，&lt;/p>
&lt;h1 id="总结">总结
&lt;/h1>&lt;p>在本文中，我们简单介绍了一下如何训练一个模型，我们使用pytorch作为例子展示了现在训练框架的工作方式。在下一篇博客中，我们将会探究训练精度和优化器。训练精度和优化器是模型在训练过程中需要考虑的重点之一。&lt;/p>
&lt;h1 id="训练">训练
&lt;/h1>&lt;h1 id="优化器">优化器
&lt;/h1>&lt;h2 id="sgd">SGD
&lt;/h2>&lt;h2 id="adam">Adam
&lt;/h2>&lt;h2 id="adamw">AdamW
&lt;/h2>&lt;h1 id="精度">精度
&lt;/h1></description></item><item><title>Distributed training--Basic</title><link>https://maosong2022.github.io/p/distributed-training--basic/</link><pubDate>Mon, 12 May 2025 10:15:17 +0800</pubDate><guid>https://maosong2022.github.io/p/distributed-training--basic/</guid><description>&lt;blockquote>
&lt;p>说明：本文参考了 &lt;a class="link" href="https://huggingface.co/spaces/nanotron/ultrascale-playbook" target="_blank" rel="noopener"
>nanotron/ultrascale-playbook&lt;/a> 和 &lt;a class="link" href="https://colossalai.org/docs/concepts/distributed_training" target="_blank" rel="noopener"
>Colossal-AI Concepts&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;h1 id="什么是分布式系统">什么是分布式系统
&lt;/h1>&lt;p>分布式系统允许一个软件的多个组件运行在不同的机器上。与传统集中式系统不一样，分布式系统可以有效提高系统的稳健性。
一个比较比较经典的分布式就是Git，Git允许我们把代码保存在多个remote上。这样当一个remote宕机时，其他remote也能提供服务。&lt;/p>
&lt;p>评估一个分布式系统的重要标准就是规模效益(scalablity)，也就是说，我们希望使用8台设备应该要比4台设备快2倍。但是，由于通信带宽等原因，实际上加速比并不是和设备数量成线性关系。因此，我们需要设计分布式算法，来有效提高分布式系统的效率。&lt;/p>
&lt;h1 id="为什么需要分布式训练">为什么需要分布式训练
&lt;/h1>&lt;p>我们需要分布式训练的原因主要是以下几点：&lt;/p>
&lt;ol>
&lt;li>模型越来越大。当下（2025）领先模型如Qwen，LLaMA系列的最大模型都超过了100B [2][3]。LLaMA系列最大的模型甚至超过了1000B。Scaling law告诉我们模型表现与参数量，算力，数据量成正相关关系。&lt;/li>
&lt;li>数据集越来越大。现在领先的模型需要的数据量基本都需要100M以上，而大语言模型训练需要的token数量也都超过了10T的量级 [2][3].&lt;/li>
&lt;li>算力越来越强。现有最强的GPU H100其显存为80GB，拥有3.35TB/s 的带宽 (PcIe)，这让训练大规模模型成为可能。&lt;/li>
&lt;/ol>
&lt;p>超大的模型使得我们很难在一张GPU上进行训练，甚至我们都很难使用单张GPU进行部署。而10T级的数据也也需要几个月的时间才能训练完毕。因此，如何高效利用多张GPU在大规模数据上训练超大模型就是我们需要解决的问题。&lt;/p>
&lt;h1 id="基本概念">基本概念
&lt;/h1>&lt;p>我们先来熟悉一下分布式训练中的一些基本概念：&lt;/p>
&lt;ul>
&lt;li>Host: host (master address)是分布式训练中通信网络的主设备(main device). 一般我们需要对其进行初始化&lt;/li>
&lt;li>Node: 一个物理或虚拟的计算单元，可以是一台机器，一个容器或者一个虚拟机&lt;/li>
&lt;li>Port: port (master port)主要是用于通信的master port&lt;/li>
&lt;li>Rank: rank是通信网络中每个设备唯一的ID&lt;/li>
&lt;li>world size: world size是通信网络中设备的数量&lt;/li>
&lt;li>process group: 一个process group是通信网络中所有设备集合的一个子集。通过process group, 我们可以限制device只在group内部进行通信&lt;/li>
&lt;/ul>
&lt;p>我们以下图为例：
&lt;img src="https://maosong2022.github.io/p/distributed-training--basic/basic_concepts.png"
width="1893"
height="1098"
srcset="https://maosong2022.github.io/p/distributed-training--basic/basic_concepts_hu10475259466228097343.png 480w, https://maosong2022.github.io/p/distributed-training--basic/basic_concepts_hu8267018694859831408.png 1024w"
loading="lazy"
alt="basic concepts"
class="gallery-image"
data-flex-grow="172"
data-flex-basis="413px"
>
上图中一共包含2个node (2台机器)，每台机器包含4个GPU (device)，当我们初始化分布式环境时，我们一共启动了8个进程（每台机器4个进程），每个进程绑定一个GPU。&lt;/p>
&lt;p>在初始化分布式环境之间，我们需要指定host和port。假设我们指定host为&lt;code>node 0&lt;/code>和port为 &lt;code>29500&lt;/code>，接下来，所有的进程都会基于这个host和port来与其他进程连接。默认的process group（包含所有device）的world size 为8. 其细节展示如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>process ID&lt;/th>
&lt;th>rank&lt;/th>
&lt;th>Node index&lt;/th>
&lt;th>GPU index&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>0&lt;/td>
&lt;td>0&lt;/td>
&lt;td>0&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;td>0&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2&lt;/td>
&lt;td>2&lt;/td>
&lt;td>0&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>3&lt;/td>
&lt;td>3&lt;/td>
&lt;td>0&lt;/td>
&lt;td>3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4&lt;/td>
&lt;td>4&lt;/td>
&lt;td>1&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>5&lt;/td>
&lt;td>5&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>6&lt;/td>
&lt;td>6&lt;/td>
&lt;td>1&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>7&lt;/td>
&lt;td>7&lt;/td>
&lt;td>1&lt;/td>
&lt;td>3&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>我们可以创建一个新的process group，使其仅包含ID为偶数的process：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>process ID&lt;/th>
&lt;th>rank&lt;/th>
&lt;th>Node index&lt;/th>
&lt;th>GPU index&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>0&lt;/td>
&lt;td>0&lt;/td>
&lt;td>0&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2&lt;/td>
&lt;td>1&lt;/td>
&lt;td>0&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4&lt;/td>
&lt;td>2&lt;/td>
&lt;td>1&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>6&lt;/td>
&lt;td>3&lt;/td>
&lt;td>1&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Remark&lt;/strong>: 注意，rank与process group相关，一个process在不同的process group里可能会有不同的rank.&lt;/p>
&lt;h1 id="通信方式">通信方式
&lt;/h1>&lt;p>接下来，我们需要介绍一下设备间的通信方式，这是我们后面分布式训练算法的基础。根据设备数量的不同，我们可以将设备间通信分为：&lt;/p>
&lt;ol>
&lt;li>one-to-one: 两个device之间互相进行通信&lt;/li>
&lt;li>one-to-many: 一个device与多个device进行通信&lt;/li>
&lt;li>many-to-one: 多个device与一个device之间进行通信&lt;/li>
&lt;li>many-to-many: 多个device之间互相进行通信&lt;/li>
&lt;/ol>
&lt;h2 id="one-to-one">One-to-one
&lt;/h2>&lt;p>One-to-one的情况很简单，一个process与另一个process进行通信，通信通过 &lt;code>send&lt;/code> 和 &lt;code>recv&lt;/code> 完成。还有对应的 immediate版本，即 &lt;code>isend&lt;/code> 和 &lt;code>irecv&lt;/code>，示意图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/distributed-training--basic/send_recv.png"
width="364"
height="411"
srcset="https://maosong2022.github.io/p/distributed-training--basic/send_recv_hu380319568110573848.png 480w, https://maosong2022.github.io/p/distributed-training--basic/send_recv_hu11611900734872621806.png 1024w"
loading="lazy"
alt="point to point communication"
class="gallery-image"
data-flex-grow="88"
data-flex-basis="212px"
>&lt;/p>
&lt;p>测试代码如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># send_recv.py&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">os&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">torch&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">torch.distributed&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">dist&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">init_process&lt;/span>&lt;span class="p">():&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">init_process_group&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">backend&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;nccl&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cuda&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">set_device&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">example_send&lt;/span>&lt;span class="p">():&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">tensor&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">4&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cuda&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">send&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dst&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">elif&lt;/span> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">tensor&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zeros&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cuda&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;Before send on rank &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">recv&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">src&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;After send on rank &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">init_process&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">example_send&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># run with&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># torchrun --nproc_per_node=2 send_recv.py&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>结果输出&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Before send on rank 1: tensor([0., 0., 0., 0., 0.], device=&amp;#39;cuda:1&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">After send on rank 1: tensor([1., 2., 3., 4., 5.], device=&amp;#39;cuda:1&amp;#39;)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>注：为了方便，后续代码仅定义函数和运行方式，&lt;code>init_process()&lt;/code>和import部分省略&lt;/p>
&lt;/blockquote>
&lt;p>&lt;code>send/recv&lt;/code>的特点是在完成通信之前，两个process是锁住的。与之相反，&lt;code>isend/irecv&lt;/code> 则不会加锁，代码会继续执行然后返回&lt;code>Work&lt;/code>对象，为了让通信顺利进行，我们可以在返回之前加入&lt;code>wait()&lt;/code>&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># isend_irecv.py&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">example_isend&lt;/span>&lt;span class="p">():&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">req&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">None&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">tensor&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">4&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cuda&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">req&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">isend&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dst&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Rank 0 is sending&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">elif&lt;/span> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">tensor&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zeros&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cuda&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;Before irecv on rank &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">req&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">irecv&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">src&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Rank 1 is receiving&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">req&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">wait&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;After isend on rank &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">init_process&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">example_isend&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># run with&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># torchrun --nproc_per_node=2 isend_irecv.py&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>结果输出&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Before irecv on rank 1: tensor([0., 0., 0., 0., 0.], device=&amp;#39;cuda:1&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Rank 0 is sending
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Rank 1 is receiving
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">After isend on rank 1: tensor([1., 2., 3., 4., 5.], device=&amp;#39;cuda:1&amp;#39;)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>由于&lt;code>isend/irecv&lt;/code>这种不锁的特性，我们不应该&lt;/p>
&lt;ol>
&lt;li>在&lt;code>dist.isend()&lt;/code>之前修改发送的内容&lt;code>tensor&lt;/code>&lt;/li>
&lt;li>在&lt;code>dist.irecv()&lt;/code>之后读取接受的内容&lt;code>tensor&lt;/code>&lt;/li>
&lt;/ol>
&lt;p>&lt;code>req.wait()&lt;/code> 可以保证这次通信顺利完成，因此我们可以在&lt;code>req.wait()&lt;/code>之后再进行修改和读取。&lt;/p>
&lt;h2 id="one-to-many">One-to-many
&lt;/h2>&lt;p>One-to-many 情形下，可以分为两种：scatter 和 broadcast&lt;/p>
&lt;p>scatter的作用是将一个process的数据均分并散布到其他process。broadcast的作用是将一个process的数据广播到其他process。两者不同的地方在于其他process获取到的是全量数据(copy)还是部分数据(slice)，其示意图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/distributed-training--basic/scatter_broadcast.png"
width="1641"
height="413"
srcset="https://maosong2022.github.io/p/distributed-training--basic/scatter_broadcast_hu10197535290360460314.png 480w, https://maosong2022.github.io/p/distributed-training--basic/scatter_broadcast_hu12737660502149133054.png 1024w"
loading="lazy"
alt="scatter and broadcast"
class="gallery-image"
data-flex-grow="397"
data-flex-basis="953px"
>&lt;/p>
&lt;p>scatter 测试代码：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># scatter.py&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">example_scatter&lt;/span>&lt;span class="p">():&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">scatter_list&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cuda&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_world_size&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;Rank 0 scatter list: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">scatter_list&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">scatter_list&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">None&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">tensor&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zeros&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cuda&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;Before scatter on rank &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">scatter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">scatter_list&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">src&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;After scatter on rank &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">init_process&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">example_broadcast&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># run with&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># torchrun --nproc_per_node=4 broadcast.py&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>结果输出以下内容（输出内容有优化，后续不再说明）：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Rank 0 scatter list: [
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([1., 1., 1., 1., 1.], device=&amp;#39;cuda:0&amp;#39;),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([2., 2., 2., 2., 2.], device=&amp;#39;cuda:0&amp;#39;),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([3., 3., 3., 3., 3.], device=&amp;#39;cuda:0&amp;#39;),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([4., 4., 4., 4., 4.], device=&amp;#39;cuda:0&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Before scatter on rank 2: tensor([0., 0., 0., 0., 0.], device=&amp;#39;cuda:2&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Before scatter on rank 1: tensor([0., 0., 0., 0., 0.], device=&amp;#39;cuda:1&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Before scatter on rank 3: tensor([0., 0., 0., 0., 0.], device=&amp;#39;cuda:3&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Before scatter on rank 0: tensor([0., 0., 0., 0., 0.], device=&amp;#39;cuda:0&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">After scatter on rank 0: tensor([1., 1., 1., 1., 1.], device=&amp;#39;cuda:0&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">After scatter on rank 2: tensor([3., 3., 3., 3., 3.], device=&amp;#39;cuda:2&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">After scatter on rank 3: tensor([4., 4., 4., 4., 4.], device=&amp;#39;cuda:3&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">After scatter on rank 1: tensor([2., 2., 2., 2., 2.], device=&amp;#39;cuda:1&amp;#39;)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>broadcast 测试代码:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># broadcast.py&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">example_broadcast&lt;/span>&lt;span class="p">():&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">tensor&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">4&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cuda&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">tensor&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zeros&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cuda&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;Before broadcast on rank &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">broadcast&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">src&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;After broadcast on rank &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">init_process&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">example_broadcast&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># run with&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># torchrun --nproc_per_node=3 broadcast.py&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>结果输出：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Before broadcast on rank 1: tensor([0., 0., 0., 0., 0.], device=&amp;#39;cuda:1&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Before broadcast on rank 2: tensor([0., 0., 0., 0., 0.], device=&amp;#39;cuda:2&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Before broadcast on rank 0: tensor([1., 2., 3., 4., 5.], device=&amp;#39;cuda:0&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">After broadcast on rank 0: tensor([1., 2., 3., 4., 5.], device=&amp;#39;cuda:0&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">After broadcast on rank 1: tensor([1., 2., 3., 4., 5.], device=&amp;#39;cuda:1&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">After broadcast on rank 2: tensor([1., 2., 3., 4., 5.], device=&amp;#39;cuda:2&amp;#39;)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="many-to-one">Many-to-one
&lt;/h2>&lt;p>Many-to-one 情形下，也可以分为两种：gather 和 reduce, Gather对应one-to-many的scatter操作，负责将多个process的内容汇聚到一起，形成一个完整的向量。而reduce的操作则是通过一个函数 $f(\cdot)$ 来把数据进行汇总，常见的函数有求和以及求平均，示意图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/distributed-training--basic/gather_reduce.png"
width="1672"
height="413"
srcset="https://maosong2022.github.io/p/distributed-training--basic/gather_reduce_hu15876724359186343988.png 480w, https://maosong2022.github.io/p/distributed-training--basic/gather_reduce_hu5737310159982010255.png 1024w"
loading="lazy"
alt="gather and reduce"
class="gallery-image"
data-flex-grow="404"
data-flex-basis="971px"
>&lt;/p>
&lt;p>gather 测试代码：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># gather.py&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">example_gather&lt;/span>&lt;span class="p">():&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">tensor&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cuda&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">gather_list&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zeros&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cuda&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_world_size&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;Rank 0 gather list: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">gather_list&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">gather_list&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">None&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;Before gather on rank &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">gather&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">gather_list&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dst&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;After gather on rank &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">gather_list&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">init_process&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">example_gather&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># run with&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># torchrun --nproc_per_node=4 gather.py&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>结果输出：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Before gather on rank 3: tensor([4., 4., 4., 4., 4.], device=&amp;#39;cuda:3&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Before gather on rank 2: tensor([3., 3., 3., 3., 3.], device=&amp;#39;cuda:2&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Before gather on rank 0: tensor([1., 1., 1., 1., 1.], device=&amp;#39;cuda:0&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Before gather on rank 1: tensor([2., 2., 2., 2., 2.], device=&amp;#39;cuda:1&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">After gather on rank 0: [
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([1., 1., 1., 1., 1.], device=&amp;#39;cuda:0&amp;#39;),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([2., 2., 2., 2., 2.], device=&amp;#39;cuda:0&amp;#39;),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([3., 3., 3., 3., 3.], device=&amp;#39;cuda:0&amp;#39;),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([4., 4., 4., 4., 4.], device=&amp;#39;cuda:0&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>reduce 测试代码:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># example_reduce.py&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">example_reduce&lt;/span>&lt;span class="p">():&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">tensor&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cuda&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;Before reduce on rank &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reduce&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dst&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">op&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ReduceOp&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">SUM&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;After reduce on rank &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">init_process&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">example_reduce&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># run with&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># torchrun --nproc_per_node=3 example_reduce.py&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>这里我们使用求和&lt;code>dist.ReduceOp.SUM&lt;/code>作为我们的汇总操作，Pytorch还支持其他的&lt;a class="link" href="https://docs.pytorch.org/docs/stable/distributed.html#torch.distributed.ReduceOp" target="_blank" rel="noopener"
>reduce operations&lt;/a>. 结果输出以下内容：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Before reduce on rank 2: tensor([3., 3., 3., 3., 3.], device=&amp;#39;cuda:2&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Before reduce on rank 3: tensor([4., 4., 4., 4., 4.], device=&amp;#39;cuda:3&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Before reduce on rank 0: tensor([1., 1., 1., 1., 1.], device=&amp;#39;cuda:0&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Before reduce on rank 1: tensor([2., 2., 2., 2., 2.], device=&amp;#39;cuda:1&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">After reduce on rank 0: tensor([10., 10., 10., 10., 10.], device=&amp;#39;cuda:0&amp;#39;)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="many-to-many">Many-to-many
&lt;/h2>&lt;p>Many-to-many 情形下的两种通信方式为：All-Reduce 和 All-Gather，分别是reduce和gather的升级版，all-reduce对所有process都执行一次reduce操作，而all-gather则对所有process执行一次gather操作，其示意图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/distributed-training--basic/all_gather_reduce.png"
width="1713"
height="411"
srcset="https://maosong2022.github.io/p/distributed-training--basic/all_gather_reduce_hu5361697916204291357.png 480w, https://maosong2022.github.io/p/distributed-training--basic/all_gather_reduce_hu13086253325925651598.png 1024w"
loading="lazy"
alt="all-gather and all-reduce"
class="gallery-image"
data-flex-grow="416"
data-flex-basis="1000px"
>&lt;/p>
&lt;p>all-gather 测试代码：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># example_all_gather.py&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">example_all_gather&lt;/span>&lt;span class="p">():&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">tensor&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cuda&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">gather_list&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zeros&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cuda&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_world_size&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;Before all gather on rank &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">all_gather&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">gather_list&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">tensor&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;After all gather on rank &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">gather_list&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">init_process&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">example_all_gather&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># run with&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># torchrun --nproc_per_node=3 example_all_gather.py&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>测试输出结果：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Before all gather on rank 2: tensor([3., 3., 3., 3., 3.], device=&amp;#39;cuda:2&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Before all gather on rank 3: tensor([4., 4., 4., 4., 4.], device=&amp;#39;cuda:3&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Before all gather on rank 0: tensor([1., 1., 1., 1., 1.], device=&amp;#39;cuda:0&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Before all gather on rank 1: tensor([2., 2., 2., 2., 2.], device=&amp;#39;cuda:1&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">After all gather on rank 0: [
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([1., 1., 1., 1., 1.], device=&amp;#39;cuda:0&amp;#39;),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([2., 2., 2., 2., 2.], device=&amp;#39;cuda:0&amp;#39;),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([3., 3., 3., 3., 3.], device=&amp;#39;cuda:0&amp;#39;),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([4., 4., 4., 4., 4.], device=&amp;#39;cuda:0&amp;#39;)]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">After all gather on rank 2: [
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([1., 1., 1., 1., 1.], device=&amp;#39;cuda:2&amp;#39;),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([2., 2., 2., 2., 2.], device=&amp;#39;cuda:2&amp;#39;),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([3., 3., 3., 3., 3.], device=&amp;#39;cuda:2&amp;#39;),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([4., 4., 4., 4., 4.], device=&amp;#39;cuda:2&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">After all gather on rank 3: [
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([1., 1., 1., 1., 1.], device=&amp;#39;cuda:3&amp;#39;),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([2., 2., 2., 2., 2.], device=&amp;#39;cuda:3&amp;#39;),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([3., 3., 3., 3., 3.], device=&amp;#39;cuda:3&amp;#39;),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([4., 4., 4., 4., 4.], device=&amp;#39;cuda:3&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">After all gather on rank 1: [
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([1., 1., 1., 1., 1.], device=&amp;#39;cuda:1&amp;#39;),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([2., 2., 2., 2., 2.], device=&amp;#39;cuda:1&amp;#39;),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([3., 3., 3., 3., 3.], device=&amp;#39;cuda:1&amp;#39;),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([4., 4., 4., 4., 4.], device=&amp;#39;cuda:1&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>all-reduce 测试代码：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># example_all_reduce.py&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">example_all_reduce&lt;/span>&lt;span class="p">():&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">tensor&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cuda&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;Before all reduce on rank &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">all_reduce&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">op&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ReduceOp&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">SUM&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;After all reduce on rank &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">init_process&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">example_all_reduce&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># run with&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># torchrun --nproc_per_node=3 example_all_reduce.py&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>测试输出结果&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Before all reduce on rank 1: tensor([2., 2., 2., 2., 2.], device=&amp;#39;cuda:1&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Before all reduce on rank 0: tensor([1., 1., 1., 1., 1.], device=&amp;#39;cuda:0&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Before all reduce on rank 2: tensor([3., 3., 3., 3., 3.], device=&amp;#39;cuda:2&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Before all reduce on rank 3: tensor([4., 4., 4., 4., 4.], device=&amp;#39;cuda:3&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">After all reduce on rank 0: tensor([10., 10., 10., 10., 10.], device=&amp;#39;cuda:0&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">After all reduce on rank 2: tensor([10., 10., 10., 10., 10.], device=&amp;#39;cuda:2&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">After all reduce on rank 3: tensor([10., 10., 10., 10., 10.], device=&amp;#39;cuda:3&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">After all reduce on rank 1: tensor([10., 10., 10., 10., 10.], device=&amp;#39;cuda:1&amp;#39;)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="barrier">Barrier
&lt;/h2>&lt;p>除了之前这些传输数据的方式之外，我们还有Barrier，用于在所有process之间进行同步。Barrier会确保所有的process在同一时间点完成某些操作。其流程为，先让每个process完成各自的任务，然后当process到达barrier时，process会通知系统自己已到达。最后当所有process都到达barrier之后，阻塞会解除，所有process继续执行下一步操作。&lt;/p>
&lt;p>barrier 测试代码&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># example_barrier.py&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">example_barrier&lt;/span>&lt;span class="p">():&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kn">import&lt;/span> &lt;span class="nn">time&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">rank&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">t_start&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">time&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">time&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;Rank &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">rank&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2"> sleeps &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">rank&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2"> seconds&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">time&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sleep&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">rank&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">barrier&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;Rank &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">rank&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2"> is done at &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">time&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">time&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">t_start&lt;/span>&lt;span class="si">:&lt;/span>&lt;span class="s2">.4f&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2"> seconds&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">init_process&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">example_barrier&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># run with&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># torchrun --nproc_per_node=3 example_barrier.py&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>结果输出&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;span class="lnt">9
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Rank 2 sleeps 2 seconds
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Rank 0 sleeps 0 seconds
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Rank 1 sleeps 1 seconds
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Rank 3 sleeps 3 seconds
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Rank 3 is done at 3.3046 seconds
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Rank 1 is done at 3.3229 seconds
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Rank 2 is done at 3.8437 seconds
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Rank 0 is done at 3.6613 seconds
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>可以看到，四个process的到达时间都在3s左右，这是因为rank 3需要3s才能完成当前任务&lt;/p>
&lt;h2 id="advanced">Advanced
&lt;/h2>&lt;p>除了前面的通信方式之外，还有 Reduce-Scatter和Ring All-Reduce，这两个通信方式等我们学习ZeRO的时候再一并讲解。&lt;/p>
&lt;h1 id="reference">Reference
&lt;/h1>&lt;ol>
&lt;li>&lt;a class="link" href="https://colossalai.org/docs/concepts/distributed_training" target="_blank" rel="noopener"
>Colossal-AI&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/" target="_blank" rel="noopener"
>LLaMA 4 blog&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://qwenlm.github.io/blog/qwen3/" target="_blank" rel="noopener"
>Qwen3 blog&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://docs.pytorch.org/tutorials/intermediate/dist_tuto.html" target="_blank" rel="noopener"
>Pytorch tutorial&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://huggingface.co/spaces/nanotron/ultrascale-playbook" target="_blank" rel="noopener"
>nanotron/ultrascale-playbook&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>Notes on LLaMA4 blog</title><link>https://maosong2022.github.io/p/notes-on-llama4-blog/</link><pubDate>Wed, 30 Apr 2025 10:44:19 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-llama4-blog/</guid><description>&lt;h1 id="介绍">介绍
&lt;/h1>&lt;p>Meta在2025年4月10号发布了LLaMA4系列，包含三个模型：Llama 4 Scout, Llama 4 Maverick 以及Llama 4 Behemoth, 三个模型都基于MoE架构，且支持多模态&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Layers&lt;/th>
&lt;th>Heads (Q / KV)&lt;/th>
&lt;th>Context Length&lt;/th>
&lt;th>#Parameters (activated/total)&lt;/th>
&lt;th>#Experts (activated/total)&lt;/th>
&lt;th>#Tokens&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>LLaMA 4 Behemoth&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>288B / 2T&lt;/td>
&lt;td>(1 shared + 1 routed) / 16&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LLaMA 4 Maverick&lt;/td>
&lt;td>48&lt;/td>
&lt;td>40/8&lt;/td>
&lt;td>1M&lt;/td>
&lt;td>17B / 109B&lt;/td>
&lt;td>(1 shared + 1 routed) / 128&lt;/td>
&lt;td>~22T&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LLaMA 4 Scout&lt;/td>
&lt;td>48&lt;/td>
&lt;td>40/8&lt;/td>
&lt;td>10M&lt;/td>
&lt;td>17B / 400B&lt;/td>
&lt;td>(1 shared + 1 routed) / 16&lt;/td>
&lt;td>~40T&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>训练数据截止到2024年8月。LLaMa4支持200多种语言，其中100多种语言的训练token数超过了1B&lt;/p>
&lt;h1 id="亮点">亮点
&lt;/h1>&lt;ol>
&lt;li>原生多模态
LLaMA 4是一个原生多模态架构&lt;/li>
&lt;li>超长上下文
LLaMA 4的上下文超过了1M&lt;/li>
&lt;li>iRoPE
通过交替dense和MoE MLP来提高整体推理效率&lt;/li>
&lt;li>基于MetaCLIP的vision encoder&lt;/li>
&lt;li>MetaP
使用MetaP来调整超参数&lt;/li>
&lt;li>FP8精度训练&lt;/li>
&lt;/ol>
&lt;h1 id="pre-training">Pre-training
&lt;/h1>&lt;p>LLaMA 4 仍然是一个基于transformer的架构，但是引入了MoE，其示意图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-llama4-blog/architecture.png"
width="1920"
height="1308"
srcset="https://maosong2022.github.io/p/notes-on-llama4-blog/architecture_hu4137367911887700008.png 480w, https://maosong2022.github.io/p/notes-on-llama4-blog/architecture_hu13148231107128236913.png 1024w"
loading="lazy"
alt="architecture of LLaMA 4"
class="gallery-image"
data-flex-grow="146"
data-flex-basis="352px"
>&lt;/p>
&lt;p>MoE架构中包含1个shared expert以及1个routed expert. 并且，与其他LLM不同，LLaMA 4使用了一个交替MLP个MoE的架构，即iRoPE，即特定的transformer layer是MoE架构，其余的是MLP架构，其&lt;a class="link" href="https://github.com/huggingface/transformers/blob/5f8d17268ced2ca5f51b0216782356b16be0d6f4/src/transformers/models/llama4/modeling_llama4.py#L380" target="_blank" rel="noopener"
>核心代码&lt;/a>如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">is_moe_layer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">layer_idx&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">moe_layers&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">if&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">is_moe_layer&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="c1"># the 128E model interleaves dense / sparse&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">feed_forward&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Llama4TextMoe&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">feed_forward&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Llama4TextMLP&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">intermediate_size&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">intermediate_size_mlp&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>early fusion. LLaMA 4称其一个原生多模态大模型，但是其架构仍然是 Vision Encoder-MLP-LLM 的形式，其不同点在于patch embedding没有使用convolution, 而是使用 &lt;code>nn.Unfold&lt;/code>直接进行展平，然后使用一个线性层与vision encoder进行对齐。&lt;a class="link" href="https://github.com/huggingface/transformers/blob/5f8d17268ced2ca5f51b0216782356b16be0d6f4/src/transformers/models/llama4/modeling_llama4.py#L1409" target="_blank" rel="noopener"
>代码&lt;/a>如下&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Llama4UnfoldConvolution&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="fm">__init__&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">kernel_size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">patch_size&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="nb">isinstance&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">kernel_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">kernel_size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">kernel_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">kernel_size&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">unfold&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Unfold&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">kernel_size&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">kernel_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">stride&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">patch_size&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">linear&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_channels&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">kernel_size&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">kernel_size&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">unfold&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">permute&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">hidden_states&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>其他训练优化技巧如下：&lt;/p>
&lt;ol>
&lt;li>MetaP：用于选择超参数&lt;/li>
&lt;li>FP8 precision：与DeepSeek-V3一样，使用FP8精度进行训练&lt;/li>
&lt;li>mid-training：在预训练阶段之后，额外增加了一个训练阶段，来提高模型的长上下文等关键能力&lt;/li>
&lt;/ol>
&lt;h1 id="post-training">Post-training
&lt;/h1>&lt;p>post-training包括三个阶段：&lt;/p>
&lt;ol>
&lt;li>SFT&lt;/li>
&lt;li>online RL&lt;/li>
&lt;li>DPO&lt;/li>
&lt;/ol>
&lt;p>作者发现SFT和DPO会限制模型的探索能力，特别是在math, coding等domain。为了解决这个问题，作者使用LlaMA对问题进行难度分级，然后移除了50%的简单数据。&lt;/p>
&lt;p>在online RL阶段，作者设计了一个continuous online RL策略，让模型在训练和筛选问题两种模式之间进行切换，以平衡效率和准确率。&lt;/p>
&lt;p>DPO的目的是为了提升模型输出的质量&lt;/p>
&lt;h1 id="评测">评测
&lt;/h1>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>benchmark&lt;/th>
&lt;th>LLaMA 4 Maverick&lt;/th>
&lt;th>LLaMA 4 Maverick&lt;/th>
&lt;th>LLaMA 4 Scout&lt;/th>
&lt;th>Gemeni 2.0 Flash&lt;/th>
&lt;th>GPT-4o&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>MMMU&lt;/td>
&lt;td>76.1&lt;/td>
&lt;td>73.4&lt;/td>
&lt;td>69.4&lt;/td>
&lt;td>71.7&lt;/td>
&lt;td>69.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Math Vista&lt;/td>
&lt;td>73.7&lt;/td>
&lt;td>70.7&lt;/td>
&lt;td>73.1&lt;/td>
&lt;td>63.8&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ChartQA&lt;/td>
&lt;td>-&lt;/td>
&lt;td>90.0&lt;/td>
&lt;td>88.8&lt;/td>
&lt;td>88.3&lt;/td>
&lt;td>85.7&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DocVQA&lt;/td>
&lt;td>-&lt;/td>
&lt;td>94.4&lt;/td>
&lt;td>94.4&lt;/td>
&lt;td>-&lt;/td>
&lt;td>92.8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LiveCodeBench&lt;/td>
&lt;td>49.4&lt;/td>
&lt;td>43.4&lt;/td>
&lt;td>32.8&lt;/td>
&lt;td>34.5&lt;/td>
&lt;td>32.3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MMLU Pro&lt;/td>
&lt;td>82.2&lt;/td>
&lt;td>80.5&lt;/td>
&lt;td>74.3&lt;/td>
&lt;td>77.6&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GPQA Diamond&lt;/td>
&lt;td>73.7&lt;/td>
&lt;td>69.8&lt;/td>
&lt;td>57.2&lt;/td>
&lt;td>60.1&lt;/td>
&lt;td>53.6&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h1 id="结论">结论
&lt;/h1>&lt;p>LLaMA 4 采用了MoE架构，是一个原生的多模态大模型系列。在架构上，与DeepSeek-MoE, aria和OLMoE不同，LLaMA4并没有增加expert granularity，OLMoE分析认为，增加granularity可以提高模型的flexibility，
下面总结了一下相关模型的参数&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Layers&lt;/th>
&lt;th>Heads (Q / KV)&lt;/th>
&lt;th>Context Length&lt;/th>
&lt;th>#Parameters (activated/total)&lt;/th>
&lt;th>#Experts (activated/total)&lt;/th>
&lt;th>#Tokens&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>DeepSeek-MoE(144.6B)&lt;/td>
&lt;td>62&lt;/td>
&lt;td>32/32&lt;/td>
&lt;td>2048&lt;/td>
&lt;td>22.2B/144.6B&lt;/td>
&lt;td>(1 shared + 7 routed)/64&lt;/td>
&lt;td>245B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DeepSeek-V3&lt;/td>
&lt;td>61&lt;/td>
&lt;td>128(MLA)&lt;/td>
&lt;td>128K&lt;/td>
&lt;td>37B/671B&lt;/td>
&lt;td>(1 shared + 8 routed)/257&lt;/td>
&lt;td>14.8T&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Aria&lt;/td>
&lt;td>28&lt;/td>
&lt;td>20/20&lt;/td>
&lt;td>64K&lt;/td>
&lt;td>3.5B/24.9B&lt;/td>
&lt;td>(2 shared+ 6 routed)/66&lt;/td>
&lt;td>6.4T(text)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OLMoE&lt;/td>
&lt;td>16&lt;/td>
&lt;td>16/16&lt;/td>
&lt;td>4096&lt;/td>
&lt;td>1.3B/6.9B&lt;/td>
&lt;td>8/64&lt;/td>
&lt;td>5T&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LLaMA 4 Maverick&lt;/td>
&lt;td>48&lt;/td>
&lt;td>40/8&lt;/td>
&lt;td>1M&lt;/td>
&lt;td>17B / 109B&lt;/td>
&lt;td>(1 shared + 1 routed) / 128&lt;/td>
&lt;td>~22T&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LLaMA 4 Scout&lt;/td>
&lt;td>48&lt;/td>
&lt;td>40/8&lt;/td>
&lt;td>10M&lt;/td>
&lt;td>17B / 400B&lt;/td>
&lt;td>(1 shared + 1 routed) / 16&lt;/td>
&lt;td>~40T&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h1 id="reference">Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md" target="_blank" rel="noopener"
>LLaMA 4 Model Card&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/" target="_blank" rel="noopener"
>LLaMA 4 Blog&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Qwen3 blog</title><link>https://maosong2022.github.io/p/notes-on-qwen3-blog/</link><pubDate>Tue, 29 Apr 2025 11:23:04 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-qwen3-blog/</guid><description>&lt;h1 id="介绍">介绍
&lt;/h1>&lt;p>Qwen3发布，包含两种架构的模型，每种架构均包含对应的base model和post-trained model.&lt;/p>
&lt;p>&lt;strong>MoE架构&lt;/strong>：上下文长度为128K，128个专家，每个token由8个专家负责处理&lt;/p>
&lt;ul>
&lt;li>Qwen3-235B-A22B, 总参数235B，激活参数22B，&lt;/li>
&lt;li>Qwen3-30B-A3B, 总参数30B，激活参数3B&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Models&lt;/th>
&lt;th>Layers&lt;/th>
&lt;th>Heads (Q / KV)&lt;/th>
&lt;th># Experts (Total / Activated)&lt;/th>
&lt;th>Context Length&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Qwen3-30B-A3B&lt;/td>
&lt;td>48&lt;/td>
&lt;td>32 / 4&lt;/td>
&lt;td>128 / 8&lt;/td>
&lt;td>128K&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen3-235B-A22B&lt;/td>
&lt;td>94&lt;/td>
&lt;td>64 / 4&lt;/td>
&lt;td>128 / 8&lt;/td>
&lt;td>128K&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>dense架构&lt;/strong>: Qwen3-32B, Qwen3-14B, Qwen3-8B, Qwen3-4B, Qwen3-1.7B, and Qwen3-0.6B&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Models&lt;/th>
&lt;th>Layers&lt;/th>
&lt;th>Heads (Q / KV)&lt;/th>
&lt;th>Tie Embedding&lt;/th>
&lt;th>Context Length&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Qwen3-0.6B&lt;/td>
&lt;td>28&lt;/td>
&lt;td>16 / 8&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>32K&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen3-1.7B&lt;/td>
&lt;td>28&lt;/td>
&lt;td>16 / 8&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>32K&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen3-4B&lt;/td>
&lt;td>36&lt;/td>
&lt;td>32 / 8&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>32K&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen3-8B&lt;/td>
&lt;td>36&lt;/td>
&lt;td>32 / 8&lt;/td>
&lt;td>No&lt;/td>
&lt;td>128K&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen3-14B&lt;/td>
&lt;td>40&lt;/td>
&lt;td>40 / 8&lt;/td>
&lt;td>No&lt;/td>
&lt;td>128K&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen3-32B&lt;/td>
&lt;td>64&lt;/td>
&lt;td>64 / 8&lt;/td>
&lt;td>No&lt;/td>
&lt;td>128K&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h1 id="亮点">亮点
&lt;/h1>&lt;ol>
&lt;li>Hybrid Thinking modes
qwen3支持两种思考模式： thinking mode 和 non-thinking mode，前者用于解决复杂的问题，后者用于解决简单的问题&lt;/li>
&lt;li>multilingual support
qwen3支持119中语言和方言&lt;/li>
&lt;li>Improved agentic capabilities
提升了qwen3的coding和agentic能力，并支持MCP&lt;/li>
&lt;/ol>
&lt;h1 id="训练">训练
&lt;/h1>&lt;h2 id="pre-training">Pre-training
&lt;/h2>&lt;p>Qwen3使用了36T token进行训练 （与之相比，Qwen2.5使用方的token数量为18T），数据来源于互联网和PDF，作者使用Qwen2.5-VL来提取内容，然后使用Qwen2.5来提升内容质量。作者还基于Qwen2.5-Math和Qwen2.5-Coder来合成数学以及代码数据&lt;/p>
&lt;p>训练包含三个stage：&lt;/p>
&lt;ol>
&lt;li>上下文长度为4K tokens，训练数据为30T tokens, 目标是让模型掌握初步的语言能力和知识&lt;/li>
&lt;li>上下文长度为4K tokens，训练数据为5T tokens,这部分数据主要是knowledge intensive的数据，比如STEM, coding和reasoning等&lt;/li>
&lt;li>上下文长度扩展到32K tokens，训练数据主要是高质量长上下文的数据&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-qwen3-blog/qwen3-base.jpg"
width="1554"
height="1058"
srcset="https://maosong2022.github.io/p/notes-on-qwen3-blog/qwen3-base_hu12486593419265846584.jpg 480w, https://maosong2022.github.io/p/notes-on-qwen3-blog/qwen3-base_hu13961341124876178588.jpg 1024w"
loading="lazy"
alt="performance of qwen3 base"
class="gallery-image"
data-flex-grow="146"
data-flex-basis="352px"
>&lt;/p>
&lt;h2 id="post-training">Post-training
&lt;/h2>&lt;p>Post-training包含四个阶段，如下图所示
&lt;img src="https://maosong2022.github.io/p/notes-on-qwen3-blog/post-training.png"
width="4143"
height="1640"
srcset="https://maosong2022.github.io/p/notes-on-qwen3-blog/post-training_hu12371199527398711262.png 480w, https://maosong2022.github.io/p/notes-on-qwen3-blog/post-training_hu2234000358067062540.png 1024w"
loading="lazy"
alt="post training of qwen3"
class="gallery-image"
data-flex-grow="252"
data-flex-basis="606px"
>&lt;/p>
&lt;ul>
&lt;li>Stage 1, Long CoT code start: 作者基于math, coding, logical reasoning, STEM等domain的Long CoT数据来微调模型，让模型拥有初步的推理能力，这和Kimi-VL是类似的&lt;/li>
&lt;li>Stage 2, reasoning-based RL: 作者使用rule-based rewards来提供奖励，然后使用RL来训练模型，经一部提高模型的exploration和exploitation能力&lt;/li>
&lt;li>Stage 3, thinking mode fusion: 作者混合了一部分instruction-following和Long CoT数据来提升模型的non-thinking能力，这样可以让模型在两种思考模式之间切换&lt;/li>
&lt;li>Stage 4,general RL： 作者使用RL在20多个general-domain任务上进一步提高模型的通用能力，包括instruction following, format following以及agent capability等&lt;/li>
&lt;/ul>
&lt;h1 id="future-work">Future work
&lt;/h1>&lt;p>作者希望在未来能够在模型架构和训练方式上进行提升，包括：scaling data, increasing model size, extending context length, broadening modalities, advancing RL with environmental feedback for long-horizon reasoning.&lt;/p>
&lt;h1 id="结论">结论
&lt;/h1>&lt;p>与Gemini2.5 pro，Kimi-VL等reaosning model不同，qwen3可以在快思考和慢思考之间进行转换。感觉未来有两个趋势，一个是如何在快思考和慢思考之间进行切换，切换的逻辑是什么？第二个就是qwen3以及qwen2.5-vl都在强调的agent能力，也就是我们不仅仅是在做一个LLM，而是逐步延伸到了agent这个层面。&lt;/p>
&lt;h1 id="参考文献">参考文献
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://qwenlm.github.io/blog/qwen3/" target="_blank" rel="noopener"
>blog&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Data mixture in MLLM</title><link>https://maosong2022.github.io/p/data-mixture-in-mllm/</link><pubDate>Fri, 25 Apr 2025 10:25:48 +0800</pubDate><guid>https://maosong2022.github.io/p/data-mixture-in-mllm/</guid><description>&lt;h1 id="介绍">介绍
&lt;/h1>&lt;p>简单总结一下已有的介绍了训练数据配比的多模态大模型，方便后续使用。
根据之前看的一些论文进行总结，如有缺漏，欢迎批评指正。&lt;/p>
&lt;h1 id="相关工作">相关工作
&lt;/h1>&lt;h2 id="llava">LLaVA
&lt;/h2>&lt;p>&lt;a class="link" href="https://arxiv.org/abs/2310.03744" target="_blank" rel="noopener"
>LLaVA 1.5&lt;/a> SFT数据配比如下：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/data-mixture-in-mllm/llava_v1_5_sft_mixture.png"
width="693"
height="569"
srcset="https://maosong2022.github.io/p/data-mixture-in-mllm/llava_v1_5_sft_mixture_hu8071442616939015820.png 480w, https://maosong2022.github.io/p/data-mixture-in-mllm/llava_v1_5_sft_mixture_hu6393677682649649864.png 1024w"
loading="lazy"
alt="LLaVA 1.5 SFT data mixture"
class="gallery-image"
data-flex-grow="121"
data-flex-basis="292px"
>&lt;/p>
&lt;h2 id="llava-onevision">LLaVA-OneVision
&lt;/h2>&lt;p>&lt;a class="link" href="http://arxiv.org/abs/2408.03326" target="_blank" rel="noopener"
>LLaVA OneVision&lt;/a> 的数据集统计在表16里，这里总结一下数据配比(总量为3.15M)&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">Category&lt;/th>
&lt;th style="text-align: right">Ratio&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">general vqa&lt;/td>
&lt;td style="text-align: right">36.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Doc/Chart/Screen&lt;/td>
&lt;td style="text-align: right">20.6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Math/Reasoning&lt;/td>
&lt;td style="text-align: right">20.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">general OCR&lt;/td>
&lt;td style="text-align: right">8.9&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">text only&lt;/td>
&lt;td style="text-align: right">14.3&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="apollo">Apollo
&lt;/h2>&lt;p>&lt;a class="link" href="http://arxiv.org/abs/2412.10360" target="_blank" rel="noopener"
>Apollo&lt;/a> 是Meta发布的一个视频理解多模态大模型，其数据集没有开源，论文中给出了其训练数据配比
&lt;img src="https://maosong2022.github.io/p/data-mixture-in-mllm/apollo.png"
width="1389"
height="589"
srcset="https://maosong2022.github.io/p/data-mixture-in-mllm/apollo_hu330258808799378426.png 480w, https://maosong2022.github.io/p/data-mixture-in-mllm/apollo_hu9473230649314306977.png 1024w"
loading="lazy"
alt="apollo data mixture"
class="gallery-image"
data-flex-grow="235"
data-flex-basis="565px"
>&lt;/p>
&lt;h2 id="cambiran-1">Cambiran-1
&lt;/h2>&lt;p>&lt;a class="link" href="http://arxiv.org/abs/2406.16860" target="_blank" rel="noopener"
>Cambiran-1&lt;/a> 是纽约大学提出了一个多模态大模型系列，论文发表在了 NeurIPS 2024(Oral) 上，作者给出了 Cambrain-10M 和 Cambrain-7M 两个数据集，Cambrain-7M 的数据分布如下&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/data-mixture-in-mllm/cambrain-7M.png"
width="1294"
height="513"
srcset="https://maosong2022.github.io/p/data-mixture-in-mllm/cambrain-7M_hu3233025464605170789.png 480w, https://maosong2022.github.io/p/data-mixture-in-mllm/cambrain-7M_hu7236826763219101351.png 1024w"
loading="lazy"
alt="Cambrain-7M"
class="gallery-image"
data-flex-grow="252"
data-flex-basis="605px"
>&lt;/p>
&lt;h2 id="idefics">Idefics
&lt;/h2>&lt;p>Idefics系列(1/2/3)是huggingface提出的视觉多模态大模型系列，在 &lt;a class="link" href="https://arxiv.org/abs/2405.02246" target="_blank" rel="noopener"
>Idefics2&lt;/a> 中，作者构建了The Cauldron数据集，其数据配比在表14里面。总结如下：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">Category&lt;/th>
&lt;th style="text-align: right">Ratio&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">general vqa&lt;/td>
&lt;td style="text-align: right">11.02&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">captioning&lt;/td>
&lt;td style="text-align: right">5.14&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">OCR&lt;/td>
&lt;td style="text-align: right">17.47&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">chart/figures&lt;/td>
&lt;td style="text-align: right">14.05&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">table&lt;/td>
&lt;td style="text-align: right">11.3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">reasoning&lt;/td>
&lt;td style="text-align: right">10.32&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">textbook&lt;/td>
&lt;td style="text-align: right">1.58&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">difference&lt;/td>
&lt;td style="text-align: right">2.38&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">screenshot2code&lt;/td>
&lt;td style="text-align: right">0.31&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">text only&lt;/td>
&lt;td style="text-align: right">26.41&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>在 &lt;a class="link" href="http://arxiv.org/abs/2408.12637" target="_blank" rel="noopener"
>Idefics3&lt;/a> 中，作者基于The Cauldron进行了扩展，最终数据集的比例如下：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/data-mixture-in-mllm/mixture_the_cauldron.png"
width="3430"
height="886"
srcset="https://maosong2022.github.io/p/data-mixture-in-mllm/mixture_the_cauldron_hu1608030439426456491.png 480w, https://maosong2022.github.io/p/data-mixture-in-mllm/mixture_the_cauldron_hu1014414555286361174.png 1024w"
loading="lazy"
alt="data mixture of SmolVLM blog"
class="gallery-image"
data-flex-grow="387"
data-flex-basis="929px"
>&lt;/p>
&lt;h2 id="molmo">Molmo
&lt;/h2>&lt;p>&lt;a class="link" href="http://arxiv.org/abs/2409.17146" target="_blank" rel="noopener"
>Molmo&lt;/a> 是Allen AI发布的一个多模态大模型，其SFT数据配比如下&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/data-mixture-in-mllm/molmo_sft_data_mixture.png"
width="693"
height="785"
srcset="https://maosong2022.github.io/p/data-mixture-in-mllm/molmo_sft_data_mixture_hu5476556090381513170.png 480w, https://maosong2022.github.io/p/data-mixture-in-mllm/molmo_sft_data_mixture_hu4031536362211311589.png 1024w"
loading="lazy"
alt="SFT data mixture of Molmo"
class="gallery-image"
data-flex-grow="88"
data-flex-basis="211px"
>&lt;/p>
&lt;h2 id="eagle-225">Eagle 2/2.5
&lt;/h2>&lt;p>Eagle (1/2/2.5) 是NVLab提出了系列多模态大模型，&lt;a class="link" href="http://arxiv.org/abs/2501.14818" target="_blank" rel="noopener"
>Eagle 2&lt;/a> 给出了 stage 1.5 和 stage 2的数据配比&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/data-mixture-in-mllm/eagle2_data_mixture.png"
width="693"
height="400"
srcset="https://maosong2022.github.io/p/data-mixture-in-mllm/eagle2_data_mixture_hu8404894763143763992.png 480w, https://maosong2022.github.io/p/data-mixture-in-mllm/eagle2_data_mixture_hu12620148260005926647.png 1024w"
loading="lazy"
alt="Eagle 2 data mixture"
class="gallery-image"
data-flex-grow="173"
data-flex-basis="415px"
>&lt;/p>
&lt;p>&lt;a class="link" href="http://arxiv.org/abs/2504.15271" target="_blank" rel="noopener"
>Eagle 2.5&lt;/a> 在 Eagle 2的基础上加入了一些 long context 数据，其数据列表在表11里&lt;/p>
&lt;h2 id="smolvlm">SmolVLM
&lt;/h2>&lt;p>&lt;a class="link" href="http://arxiv.org/abs/2504.05299" target="_blank" rel="noopener"
>SmolVLM&lt;/a> 是huggingface开发的一款轻量化视觉多模态大模型，论文中的数据配比如下：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/data-mixture-in-mllm/smol_vlm_data_mixture.png"
width="1382"
height="560"
srcset="https://maosong2022.github.io/p/data-mixture-in-mllm/smol_vlm_data_mixture_hu8265249450615094784.png 480w, https://maosong2022.github.io/p/data-mixture-in-mllm/smol_vlm_data_mixture_hu8724599476712824453.png 1024w"
loading="lazy"
alt="data mixture of SmolVLM paper"
class="gallery-image"
data-flex-grow="246"
data-flex-basis="592px"
>&lt;/p>
&lt;h2 id="mm115">MM1/1.5
&lt;/h2>&lt;p>&lt;a class="link" href="http://arxiv.org/abs/2403.09611" target="_blank" rel="noopener"
>MM1&lt;/a> 通过实验确定了训练数据的配比：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">Category&lt;/th>
&lt;th style="text-align: right">Ratio&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">interleaved image-text&lt;/td>
&lt;td style="text-align: right">45&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">imagetext&lt;/td>
&lt;td style="text-align: right">45&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">text only&lt;/td>
&lt;td style="text-align: right">10&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;a class="link" href="http://arxiv.org/abs/2409.20566" target="_blank" rel="noopener"
>MM1.5&lt;/a> 的SFT数据配比如下：
&lt;img src="https://maosong2022.github.io/p/data-mixture-in-mllm/MM1_5_data_mixture.png"
width="1183"
height="845"
srcset="https://maosong2022.github.io/p/data-mixture-in-mllm/MM1_5_data_mixture_hu15017125515046800186.png 480w, https://maosong2022.github.io/p/data-mixture-in-mllm/MM1_5_data_mixture_hu8312701426794256519.png 1024w"
loading="lazy"
alt="MM1.5 SFT data mixture"
class="gallery-image"
data-flex-grow="140"
data-flex-basis="336px"
>&lt;/p>
&lt;h2 id="internvl">InternVL
&lt;/h2>&lt;p>&lt;a class="link" href="http://arxiv.org/abs/2412.05271" target="_blank" rel="noopener"
>InternVL2.5&lt;/a> 在论文里总结了其使用的pretraining数据和SFT数据，但是没有具体的数据配比，请参考论文的表4和表5&lt;/p>
&lt;p>SFT数据配比&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">Category&lt;/th>
&lt;th style="text-align: left">Ratio&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">single-image&lt;/td>
&lt;td style="text-align: left">45.92%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">multi-image&lt;/td>
&lt;td style="text-align: left">9.37%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">video&lt;/td>
&lt;td style="text-align: left">39.79%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">pure-text&lt;/td>
&lt;td style="text-align: left">4.92%&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="minicpm-v">MiniCPM V
&lt;/h2>&lt;p>&lt;a class="link" href="http://arxiv.org/abs/2408.01800" target="_blank" rel="noopener"
>MiniCPM V&lt;/a> 是 OpenBMB发布的轻量化多模态大模型，其在表1和表2列出了ptraining和SFT数据的具体量级和类别。&lt;/p>
&lt;h2 id="flash-vl">Flash-VL
&lt;/h2>&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">Category&lt;/th>
&lt;th style="text-align: right">Ratio&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">Special Enhancement&lt;/td>
&lt;td style="text-align: right">4%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Text&lt;/td>
&lt;td style="text-align: right">21%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Caption&lt;/td>
&lt;td style="text-align: right">4%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Chart&lt;/td>
&lt;td style="text-align: right">16%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Math&lt;/td>
&lt;td style="text-align: right">11%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">OCR&lt;/td>
&lt;td style="text-align: right">3%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Code&lt;/td>
&lt;td style="text-align: right">8%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">General&lt;/td>
&lt;td style="text-align: right">33%&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>具体数据参见&lt;a class="link" href="http://arxiv.org/abs/2505.09498" target="_blank" rel="noopener"
>原论文&lt;/a>&lt;/p>
&lt;h1 id="参考文献">参考文献
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2310.03744" target="_blank" rel="noopener"
>LLaVA 1.5&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2408.03326" target="_blank" rel="noopener"
>LLaVA OneVision&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2412.10360" target="_blank" rel="noopener"
>Apollo&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2406.16860" target="_blank" rel="noopener"
>Cambiran-1&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2405.02246" target="_blank" rel="noopener"
>Idefics2&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2408.12637" target="_blank" rel="noopener"
>Idefics3&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2409.17146" target="_blank" rel="noopener"
>Molmo&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2501.14818" target="_blank" rel="noopener"
>Eagle 2&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2504.15271" target="_blank" rel="noopener"
>Eagle 2.5&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2504.05299" target="_blank" rel="noopener"
>SmolVLM&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2403.09611" target="_blank" rel="noopener"
>MM1&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2409.20566" target="_blank" rel="noopener"
>MM1.5&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2412.05271" target="_blank" rel="noopener"
>InternVL2.5&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2408.01800" target="_blank" rel="noopener"
>MiniCPM V&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>随笔-身体健康</title><link>https://maosong2022.github.io/p/%E9%9A%8F%E7%AC%94-%E8%BA%AB%E4%BD%93%E5%81%A5%E5%BA%B7/</link><pubDate>Wed, 23 Apr 2025 13:24:02 +0800</pubDate><guid>https://maosong2022.github.io/p/%E9%9A%8F%E7%AC%94-%E8%BA%AB%E4%BD%93%E5%81%A5%E5%BA%B7/</guid><description>&lt;p>上大学以及读研的时候，从没生过病，女朋友还说我身体素质强大。工作之后，先是新冠，然后轻度脂肪肝，脂肪肝，今年体检又有颈椎曲度变直，腰椎间盘突出一系列问题。说实话，这些毛病说大也不算大，工作久了都有类似的问题。但是前两天因为睡觉着凉感冒了两天，就忽然觉得年少时以为身体强健，不过是身体在替你硬抗。现在工作之后，久坐，不健康饮食，熬夜等坏习惯就开始了反击。都说人类理想的寿命不过25岁，25岁你应该已经完成结婚生子，可以安享晚年了。而也是在这个年龄段，身体开始摆烂，让你自己硬抗。现在来看，果然如此。&lt;/p>
&lt;p>有时候，还在那里想，人要完成一个什么样的目标才算是成功呢，是发很多顶会成为学术大佬，还是赚很多钱，实现人生自由。现在来看，身体健康，家庭美满，有一份说得过去的工作就已经很好了，不要好高骛远。人可以向前看，向前走，但是不能不看自己的身体状况。&lt;/p>
&lt;p>想起王立群老师的话，“人的一生就像一场马拉松，只要你在路上，就还有机会”。但是现在来看，看着身边那么多优秀的人，很容易就把马拉松跑成百米赛跑，没跑过人家不说，百米过了，还浑身酸痛。&lt;/p>
&lt;p>又想起高职务的那句话，“有时候想想这官当多大才叫大啊”。现在看，人也是这样啊，赚多少的钱才算人生自由，发多少的论文才算成功。向上看，总是会给自己无形的压力，让自己陷入无尽的焦虑之中。&lt;/p>
&lt;p>我觉得我已经算是比较豁达的人了，只会跟自己比，就算这样也还是会有一些焦虑。我的焦虑主要是来自于自己啥都没做，就已经周三了。周日晚上着凉，周一周二脑子就完全宕机，一周的计划直接泡汤一半。这种计划被打乱的挫败感对于T人来说确实难以接受。而细细一想，学生时代对于时间真的没什么概念，每天就瞎搞，没有目标，反而时间就过得慢。现在则是，一低头一抬头可能两个小时就过去了。难说这是年龄大了之后，对时间的迟钝性，还是工作之后，磨平了人心中的激情。&lt;/p>
&lt;p>最近开始了健身，希望能花半年时间降低体脂，我越来越感觉到身体健康的重要性，很多东西失去了才真正懂得拥有是多么难得，感冒鼻塞了才知道呼吸到空气不是那么一件简单的事情。摔伤之后才知道洗澡也可能是奢望。&lt;/p>
&lt;p>在明朝那些事的结尾，当年明月没有提到书中的大人物，而是以徐霞客收尾，“所谓百年功名、千秋霸业、万古流芳，与一件事情相比，其实算不了什么。这件事情就是——用你喜欢的方式度过一生。”希望大家都有健康的体魄，毕竟身体是革命的本钱。与诸君共勉。&lt;/p></description></item><item><title>Notes on VAPO</title><link>https://maosong2022.github.io/p/notes-on-vapo/</link><pubDate>Thu, 17 Apr 2025 09:41:51 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-vapo/</guid><description>&lt;h1 id="abstract">Abstract
&lt;/h1>&lt;p>字节Seed团队提出了 Value-based Augmented Proximal Policy Optimization (VAPO), 用于提高 reasoning model 的表现。 VAPO 通过集成 DAPO 和 VC-PPO 的优点，进一步提高了 value-based 方法的表现。&lt;/p>
&lt;h1 id="introduction">Introduction
&lt;/h1>&lt;p>现有的RL 训练方法可以分为 value-free 和 value-based 两大类。
其中 value-free 方法不需要使用 value model, 比如 GRPO 和 GRPO 的变体 DAPO, 这类方法通过多次采样，然后使用 leave-one-out estimate 来代替 value model. 这类方法的优点是不需要训练value model, 但是缺点是在复杂的任务中表现不是很稳定。&lt;/p>
&lt;p>另一方面，value-based 方法需要训练一个 value model, 比如 &lt;a class="link" href="https://maosong.website/p/notes-on-vc-ppo/" target="_blank" rel="noopener"
>VC-PPO&lt;/a>, 这类方法的优点是：&lt;/p>
&lt;ol>
&lt;li>提供更细粒度的奖励信号&lt;/li>
&lt;li>提供lower-varaince value estimation, 从而提高训练的稳定性&lt;/li>
&lt;li>拥有更好的泛化能力&lt;/li>
&lt;/ol>
&lt;p>但是，value-based 方法在训练过程中存在一些问题：&lt;/p>
&lt;ol>
&lt;li>训练一个low-bias 的 value model 比较困难， 尤其是在long trajectory 上，因为 bias 会随着 trajectory 的长度增加而增加&lt;/li>
&lt;li>在heterogeneous sequence lengths during training 中表现不佳，对于短文本和长文本，我们需要考虑 bias-variance 的trade-off&lt;/li>
&lt;li>在sparse reward signal 中表现不佳&lt;/li>
&lt;/ol>
&lt;p>为了解决这些问题，字节Seed团队提出了 VAPO, 一个基于value-based 的 RL 训练方法，VAPO 通过结合 DAPO 和 &lt;a class="link" href="https://maosong.website/p/notes-on-vc-ppo/" target="_blank" rel="noopener"
>VC-PPO&lt;/a> 的优点，进一步提高了 value-based 方法的表现。&lt;/p>
&lt;h1 id="preliminary">Preliminary
&lt;/h1>&lt;p>Preliminary包括 token-level MDP, RLHF, PPO 三个部分，这部分请参考 &lt;a class="link" href="https://maosong.website/p/notes-on-vc-ppo/" target="_blank" rel="noopener"
>VC-PPO&lt;/a>. 这里不做重复。&lt;/p>
&lt;h1 id="vapo">VAPO
&lt;/h1>&lt;p>作者针对value-based 方法在训练过程中存在的三个问题，在VAPO中分别进行了解决。&lt;/p>
&lt;h2 id="mitigating-value-model-bias-over-long-sequences">Mitigating Value Model Bias over Long Sequences
&lt;/h2>&lt;p>在 &lt;a class="link" href="https://maosong.website/p/notes-on-vc-ppo/" target="_blank" rel="noopener"
>VC-PPO&lt;/a> 中，作者提到了 value model 和 reward model 的不一致性，这种不一致性会导致 bias, 尤其是在long sequences上。 在VAPO中，作者就直接使用了 VC-PPO的做法，包括value pretraining 和 decoupled GAE 来解决这个问题。&lt;/p>
&lt;h2 id="managing-heterogeneous-sequence-lengths-during-training">Managing Heterogeneous Sequence Lengths during Training
&lt;/h2>&lt;p>针对heterogeneous sequence的问题，作者提出了 &lt;strong>Length-Adaptive GAE&lt;/strong>. 在&lt;a class="link" href="https://maosong.website/p/notes-on-vc-ppo/" target="_blank" rel="noopener"
>VC-PPO&lt;/a>中， $\lambda_{\mathrm{policy}}$ 被设置为 $0.95$. 但是当 sequence 非常长时， TD-error会迅速下降，导致GAE被一部分TD-error所主导，从而不利于模型的训练。&lt;/p>
&lt;p>为了解决这个问题，作者将 $\lambda_{\mathrm{policy}}$ 与 sequence 的长度 $\ell$ 联系起来，具体来说， 两者的关系如下：&lt;/p>
$$
\sum_{t=0}^{\infty}\lambda_{\mathrm{policy}}^t = \frac{1}{1-\lambda_{\mathrm{policy}}} := \alpha\ell
$$&lt;p>其中 $\alpha$ 是一个超参数，用来控制 bias-variance 的trade-off. 给定 $\ell$, $\lambda_{\mathrm{policy}}$ 可以被计算为：&lt;/p>
$$
\lambda_{\mathrm{policy}} = 1 - \frac{1}{\alpha\ell}
$$&lt;p>同时，为了平衡短文本和长文本的贡献，基于 &lt;a class="link" href="https://maosong.website/p/notes-on-dapo/" target="_blank" rel="noopener"
>DAPO&lt;/a>, 作者构建了 token-level policy gradient loss， 其具体形式如下：&lt;/p>
$$
\mathcal{L}_{\mathrm{PPO}}(\theta) = \frac{1}{\sum_{t=1}^G|o_i|}\sum_{i=1}^G\sum_{t=1}^{|o_i|}\min\left(r_{i,t}(\theta)\hat{A}_{i,t},\mathrm{clip}\left(r_{i,t}(\theta), 1-\epsilon, 1+\epsilon\right)\hat{A}_{i,t}\right)
$$&lt;h2 id="dealing-with-sparsity-of-reward-signal-in-verifier-based-tasks">Dealing with Sparsity of Reward Signal in Verifier-based Tasks
&lt;/h2>&lt;p>与DAPO一致，为了解决reward signal的稀疏性问题，作者提出了Clip-Higher, 来让更小概率的输出也能获得较大的更新概率。其更新公式如下：&lt;/p>
$$
\mathcal{L}_{\mathrm{PPO}}(\theta) = \frac{1}{\sum_{t=1}^G|o_i|}\sum_{i=1}^G\sum_{t=1}^{|o_i|}\min\left(r_{i,t}(\theta)\hat{A}_{i,t},\mathrm{clip}\left(r_{i,t}(\theta), 1-\epsilon_{low}, 1+\epsilon_{high}\right)\hat{A}_{i,t}\right)
$$&lt;p>Clip-Higher的介绍见&lt;a class="link" href="https://maosong.website/p/notes-on-dapo/" target="_blank" rel="noopener"
>DAPO&lt;/a>&lt;/p>
&lt;p>然后，作者还将next-token prediction loss 和 PPO loss 结合起来，来降低奖励的稀疏程度。&lt;/p>
$$
\mathcal{L}_{\mathrm{NTP}}(\theta) = -\frac{1}{N}\sum_{o_i\in\mathcal{T}}\sum_{t=1}^{|o_i|}\log \pi_{\theta}(a_{i,t}|s_{i,t})
$$&lt;p>其中 $\mathcal{T}$ 是正确答案的集合。 最终的loss为：&lt;/p>
$$
\mathcal{L}_{\mathrm{VAPO}}(\theta) = \mathcal{L}_{\mathrm{PPO}}(\theta) +\mu \mathcal{L}_{\mathrm{NTP}}(\theta)
$$&lt;p>其中 $\mu$ 是一个超参数，用来平衡PPO loss和NTP loss。&lt;/p>
&lt;h1 id="experiments">Experiments
&lt;/h1>&lt;p>模型使用Qwen-32B来进行训练, 大部分细节与&lt;a class="link" href="https://maosong.website/p/notes-on-vc-ppo/" target="_blank" rel="noopener"
>VC-PPO&lt;/a>和DAPO一致，这里不再赘述。最后与DAPO以及R1的对比结果如下：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-vapo/performance.png"
width="1396"
height="600"
srcset="https://maosong2022.github.io/p/notes-on-vapo/performance_hu12016151919211234052.png 480w, https://maosong2022.github.io/p/notes-on-vapo/performance_hu8043626083862234148.png 1024w"
loading="lazy"
alt="performance"
class="gallery-image"
data-flex-grow="232"
data-flex-basis="558px"
>&lt;/p>
&lt;h2 id="ablation-study">Ablation study
&lt;/h2>&lt;p>针对本文使用的模块，作者进行了消融实验，结果如下：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-vapo/ablation_results.png"
width="768"
height="547"
srcset="https://maosong2022.github.io/p/notes-on-vapo/ablation_results_hu16606869449475845197.png 480w, https://maosong2022.github.io/p/notes-on-vapo/ablation_results_hu15929942015572033019.png 1024w"
loading="lazy"
alt="ablation study"
class="gallery-image"
data-flex-grow="140"
data-flex-basis="336px"
>&lt;/p>
&lt;p>从实验结果可以看到：&lt;/p>
&lt;ol>
&lt;li>value pretraining 和 decoupled GAE 可以显著提高模型的表现&lt;/li>
&lt;li>clip-higer 可以提升模型的探索能力&lt;/li>
&lt;li>length-adaptive GAE 可以平衡模型在短文本和长文本上的表现&lt;/li>
&lt;/ol>
&lt;h2 id="training-dynamics">Training Dynamics
&lt;/h2>&lt;p>与DAPO类似，作者也分析了VAPO的训练动态，结果如下：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-vapo/mean_response_length.png"
width="623"
height="463"
srcset="https://maosong2022.github.io/p/notes-on-vapo/mean_response_length_hu7154681057320845113.png 480w, https://maosong2022.github.io/p/notes-on-vapo/mean_response_length_hu14198165635304465194.png 1024w"
loading="lazy"
alt="Mean Response Length"
class="gallery-image"
data-flex-grow="134"
data-flex-basis="322px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-vapo/reward_score.png"
width="620"
height="466"
srcset="https://maosong2022.github.io/p/notes-on-vapo/reward_score_hu9683818189450227083.png 480w, https://maosong2022.github.io/p/notes-on-vapo/reward_score_hu17145085370284665859.png 1024w"
loading="lazy"
alt="reward score"
class="gallery-image"
data-flex-grow="133"
data-flex-basis="319px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-vapo/generation_entropy.png"
width="634"
height="472"
srcset="https://maosong2022.github.io/p/notes-on-vapo/generation_entropy_hu16020487093924223451.png 480w, https://maosong2022.github.io/p/notes-on-vapo/generation_entropy_hu5628025501830599565.png 1024w"
loading="lazy"
alt="generation entropy"
class="gallery-image"
data-flex-grow="134"
data-flex-basis="322px"
>&lt;/p>
&lt;p>从上面三张图可以看到：&lt;/p>
&lt;ol>
&lt;li>VAPO相比于DAPO来说，其训练更加稳定&lt;/li>
&lt;li>从response length来看，VAPO的response length更长，说明VAPO的length scaling更强&lt;/li>
&lt;li>从reward score来看，VAPO的reward score更高，说明VAPO的reward signal提供的指导信息更多&lt;/li>
&lt;li>从generation entropy来看，在训练末期，VAPO的generation entropy更低，说明VAPO的生成多样性要更低一些，但这也说明了VAPO的生成更加稳定。 作者认为这个时候稳定性更重要。&lt;/li>
&lt;/ol>
&lt;h1 id="conclusion">Conclusion
&lt;/h1>&lt;p>作者在DAPO和VC-PPO的基础上，提出了VAPO，一个基于value-based 的 RL 训练方法，VAPO 通过结合 DAPO 和 &lt;a class="link" href="https://maosong.website/p/notes-on-vc-ppo/" target="_blank" rel="noopener"
>VC-PPO&lt;/a> 的优点，进一步提高了 value-based 方法的表现。 后续，作者又提出了Seed-thiking-1.5的技术报告。可以说，这一系列论文的连贯性是非常高的。&lt;/p>
&lt;h1 id="reference">Reference
&lt;/h1>&lt;ol>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2504.05118" target="_blank" rel="noopener"
>Arxiv VAPO: Efficient and Reliable Reinforcement Learning for Advanced Reasoning Tasks&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://maosong.website/p/notes-on-dapo/" target="_blank" rel="noopener"
>Notes onDAPO&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://maosong.website/p/notes-on-vc-ppo/" target="_blank" rel="noopener"
>Notes on VC-PPO&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>Notes on VC-PPO</title><link>https://maosong2022.github.io/p/notes-on-vc-ppo/</link><pubDate>Mon, 14 Apr 2025 17:36:15 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-vc-ppo/</guid><description>&lt;h1 id="abstract">Abstract
&lt;/h1>&lt;p>字节Seed团队提出了 Value-Calibrated PPO (VC-PPO), 用于解决PPO的value initialization bias 以及 reward signal decay 问题. 具体来讲：&lt;/p>
&lt;ol>
&lt;li>VC-PPO增加了 value pretraining 来解决 value initialization bias 的问题&lt;/li>
&lt;li>VC-PPO分离了 actor 和 critic 的GAE的计算，避免了 reward signal decay 的问题&lt;/li>
&lt;/ol>
&lt;h1 id="introduction">Introduction
&lt;/h1>&lt;p>已有的reasoning model的训练方法，主要包括两个个stage:&lt;/p>
&lt;ol>
&lt;li>SFT: 这个阶段主要使用了一些标注好的long CoT数据，初步激活模型的reasoning能力(参考KImi-VL)&lt;/li>
&lt;li>RL: 这个阶段使用收集到的数据使用RL算法进行训练，任务包括math, code, logic reasoning等&lt;/li>
&lt;/ol>
&lt;p>已有PPO算法在处理Long CoT任务时，存在的问题在 &lt;a class="link" href="https://maosong.website/p/notes-on-dapo/" target="_blank" rel="noopener"
>DAPO&lt;/a> 中已经介绍过了，GRPO的解决方式为
使用leave-one-out estimate来替换value model. 但是GRPO相比于PPO能够提供token级别的奖励来说，只能提供response level的奖励，因此限制了模型的性能。&lt;/p>
&lt;h1 id="preliminary">Preliminary
&lt;/h1>&lt;p>Preliminary包括MDP, RLHF, PPO三个部分，RLHF和PPO我们在 &lt;a class="link" href="https://maosong.website/p/notes-on-dapo/" target="_blank" rel="noopener"
>DAPO&lt;/a> 中已经介绍过了，这里不再赘述。&lt;/p>
&lt;h2 id="token-level-mdp">Token-level MDP
&lt;/h2>&lt;p>给定prompt $x$ 和 response $y$, 我们可以将 $x$ 和 $y$ 分解为 token 序列，比如 $y = y_1, y_2, \cdots, y_n$, 其中 $y_i\in\mathcal{A}$, $\mathcal{A}$ 是我们的词表。&lt;/p>
&lt;p>我们将 token-level MDP定义为：&lt;/p>
$$
\mathcal{M} = \langle \mathcal{S}, \mathcal{A}, P, r, d_0, \omega \rangle
$$&lt;p>其中：&lt;/p>
&lt;ul>
&lt;li>$\mathcal{S}$ 是状态空间，表示当前的token序列， $t$时刻的状态可以表示为 $s_t = (x, y_1, y_2, \cdots, y_t)$&lt;/li>
&lt;li>$\mathcal{A}$ 是动作空间，表示下一个token， $t$时刻的动作可以表示为 $a_t = y_{t+1}\in\mathcal{A}$&lt;/li>
&lt;li>$P$ 是状态转移概率，表示在 $t$时刻，从状态 $s_t$ 转移到状态 $s_{t+1}$ 的概率&lt;/li>
&lt;li>$r$ 是奖励函数，表示在 $t$时刻，从状态 $s_t$ 采取动作 $a_t$ 转移到状态 $s_{t+1}$ 的奖励&lt;/li>
&lt;li>$d_0$ 是初始状态分布，表示初始状态 $s_0$ 的概率分布&lt;/li>
&lt;li>$\omega$ 是终止状态分布，表示终止状态 $s_n$ 的概率分布，通常表示 &lt;code>&amp;lt;eos&amp;gt;&lt;/code> token&lt;/li>
&lt;/ul>
&lt;h1 id="方法">方法
&lt;/h1>&lt;p>首先作者分析了一下为什么 PPO 在处理 long CoT 任务时效果不佳。&lt;/p>
&lt;p>PPO的传统设置为：&lt;/p>
&lt;ul>
&lt;li>将GAE的参数 $\lambda$ 设置为 0.95&lt;/li>
&lt;li>使用一个 reward model 来初始化 value model&lt;/li>
&lt;/ul>
&lt;p>一方面，作者认为，$\lambda=0.95$ 是为了减少模型在 Mujoco 以及 Atari 等环境中的variance，但是对于Long CoT任务，这个设置会导致模型缺乏足够的探索能力。&lt;/p>
&lt;p>另一方面，作者认为 reward model 和 value model 虽然都是提供关于 response 的信息，但是他们之间还是存在一些差距的。作者给出了使用PPO来进行 long CoT 任务相关的实验结果&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-vc-ppo/failue_PPO.png"
width="1375"
height="582"
srcset="https://maosong2022.github.io/p/notes-on-vc-ppo/failue_PPO_hu4138122479658325450.png 480w, https://maosong2022.github.io/p/notes-on-vc-ppo/failue_PPO_hu5334362614363854722.png 1024w"
loading="lazy"
alt="failue_PPO"
class="gallery-image"
data-flex-grow="236"
data-flex-basis="567px"
>&lt;/p>
&lt;p>可以看到，随着训练的进行，模型的context length以及在AIME benchmark上的表现都出现了下降。&lt;/p>
&lt;p>在本文中，作者主要是在 verifiable tasks 上进行实验，答案的正确性与 response length 长度关系不大，因此, response length 可以反应模型的训练情况。 作者绘制了训练过程中， value 和 advantage 与 token position 的关系图&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-vc-ppo/value_against_pos.png"
width="1390"
height="592"
srcset="https://maosong2022.github.io/p/notes-on-vc-ppo/value_against_pos_hu2396407115477348672.png 480w, https://maosong2022.github.io/p/notes-on-vc-ppo/value_against_pos_hu8102099948194175826.png 1024w"
loading="lazy"
alt="value_advantage_position"
class="gallery-image"
data-flex-grow="234"
data-flex-basis="563px"
>&lt;/p>
&lt;p>从上图可以看到，value 和 advantage 更倾向与给最初的 token 更高的 bias, 作者认为出现这个情况的原因是 value model 和 reward model 的目标并不匹配。 reward model 的目标是给 $\omega$ 也就是 &lt;code>&amp;lt;eos&amp;gt;&lt;/code> token reward, 对于一开始的 token, reward model会给出比较低的奖励；而 value model 的目标是给整个 response 一个奖励，因此，value model 会倾向于给最初的 token 更高的奖励。我们对 GAE 进行改写得到：&lt;/p>
$$
\hat{A}_t = \sum_{i=t}^{T-t-1} \lambda^{i} \left( r_{t+i} + V(s_{t+i+1}) - V(s_{t+i}) \right)
$$&lt;p>从上式可以看到，一开始 $r_{t+i}$ 的值会比较小，而 $V(s_{t+i})$ 的变化比较大，因此，一开始的 token 的 advantage 会比较大，这个 bias 会持续影响整个 trajectory。&lt;/p>
&lt;p>为了解决这个问题，作者提出了 value-pretraining, 也就是对value model 进行离线与训练，直到其收敛到一个具体的 policy 上。具体训练步骤为：&lt;/p>
&lt;ol>
&lt;li>基于一个policy， 如 $\pi_{\mathrm{sft}}$ 进行采样，然后更新value model ($\lambda=1.0$)&lt;/li>
&lt;li>基于收集到的数据训练 value model, 直到 value loss 或者 explain variance 收敛&lt;/li>
&lt;/ol>
&lt;p>接下来，在训练 value model 时，我们还需要考虑 variance reduction 的问题。作者首先改写了 GAE 的公式：&lt;/p>
$$
\hat{A}_t = \begin{cases}
\sum_{i=0}^{T-t-1} \lambda^{i} \left( r_{t+i} + V(s_{t+i+1}) - V(s_{t+i}) \right)+V(s_t) &amp; \text{if } \lambda &lt; 1.0 \\
\sum_{i=0}^{T-t-1} r_{t+i} &amp; \text{if } \lambda=1.0
\end{cases}
$$&lt;p>可以看到，当 $\lambda &amp;lt; 1.0$ 并且 $T-t-1$ 比较大时，&lt;code>&amp;lt;eos&amp;gt;&lt;/code> token 的reward就非常接近于0了，作者通过实验验证了这一点。&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-vc-ppo/reward_signal_decay.png"
width="1117"
height="623"
srcset="https://maosong2022.github.io/p/notes-on-vc-ppo/reward_signal_decay_hu11087213378716453589.png 480w, https://maosong2022.github.io/p/notes-on-vc-ppo/reward_signal_decay_hu8955023302766031291.png 1024w"
loading="lazy"
alt="reward_signal_decay"
class="gallery-image"
data-flex-grow="179"
data-flex-basis="430px"
>&lt;/p>
&lt;p>可以看到，当我们降低 $\lambda$ 时，reward signal 的稀疏性会显著增加，从而提高了模型的训练难度。&lt;/p>
&lt;p>但是，我们又不能不使用 variance reduction, 因为这会导致训练的不稳定性。 作者从 TD error 的角度分析了 variance:&lt;/p>
$$
\begin{aligned}
\mathrm{Var}[A_{t}^{\lambda}] &amp;= \mathrm{Var}\left[\sum_{i=0}^{T-t-1} \lambda^{i}\delta_{t+i}\right] \\
&amp;= \sum_{i=1}^{T-t-1} \lambda^{2i} \mathrm{Var}[\delta_{t+i}] + 2\sum_{i=1}^{T-t-1}\sum_{j=0}^{i-1} \lambda^{i+j} \mathrm{Cov}[\delta_{t+i}, \delta_{t+j}]
\end{aligned}
$$&lt;p>因为 $\lambda\in[0,1]$, 因此未来的 TD error 会衰减的更快，因此就降低了 advantage 的 variance.&lt;/p>
&lt;p>那么如何在降低variance的同时，又不至于使得 reward signal 过于稀疏呢？作者的解决方案是分离GAE的计算，也就是将 GAE 的计算分为两部分：&lt;/p>
$$
G_{t:t+h} = \begin{cases}
\sum_{i=0}^{h-1} r_{t+i} + \bar{V}(s_{t+h}) &amp; \text{if } t+h&lt;T \\
\sum_{i=0}^{T-h} r_{t+i} &amp; \text{if } t+h=T
\end{cases}
$$&lt;p>基于这个公式，我们可以写出policy gradient的公式：&lt;/p>
$$
\begin{aligned}
\nabla_{\theta} J(\theta) &amp;= \mathbb{E}_{t}[\nabla_{\theta} \log \pi_{\theta}(a_t|s_t)A_t] \\
&amp;= \mathbb{E}_{t}\left[\nabla_{\theta} \log \pi_{\theta}(a_t|s_t) \sum_{i=0}^{T-t-1} \lambda^{i}\left( r_{t+i} + \bar{V}(s_{t+i+1}) - \bar{V}(s_{t+i}) \right)\right] \\
&amp;= \mathbb{E}_{t}\left[\nabla_{\theta} \log \pi_{\theta}(a_t|s_t) \left((1-\lambda) \sum_{i=1}^{T-t-1} \lambda^{i-1}G_{t:t+i}+\lambda^{T-t-1}G_{t:T}- \bar{V}(s_{t})\right)\right] \\
&amp;= \mathbb{E}_{t}\left[\nabla_{\theta} \log \pi_{\theta}(a_t|s_t) \left((1-\lambda) \sum_{i=1}^{T-t-1} \lambda^{i-1}G_{t:t+i}+\lambda^{T-t-1}G_{t:T}\right)\right]
\end{aligned}
$$&lt;p>通过这种方式，我们就可以避免 value function 对 policy gradient 的影响，因而我们可以对 value model 和 policy model 使用不同的 $\lambda$ 进行训练。&lt;/p>
&lt;p>最终，我们就可以得到 VC-PPO 的算法：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-vc-ppo/vc-ppo.png"
width="1376"
height="713"
srcset="https://maosong2022.github.io/p/notes-on-vc-ppo/vc-ppo_hu18113332919137762506.png 480w, https://maosong2022.github.io/p/notes-on-vc-ppo/vc-ppo_hu3523005463027398596.png 1024w"
loading="lazy"
alt="vc_ppo"
class="gallery-image"
data-flex-grow="192"
data-flex-basis="463px"
>&lt;/p>
&lt;h1 id="实验">实验
&lt;/h1>&lt;h2 id="setup">setup
&lt;/h2>&lt;ol>
&lt;li>作者在AIME, GPQA 以及Codeforces三个数据集上进行评测&lt;/li>
&lt;li>作者首先进行了code-start，作者构建了一批样本然后要求模型在 &lt;code>&amp;lt;thinking&amp;gt;&lt;/code> 和 &lt;code>&amp;lt;/thinking&amp;gt;&lt;/code> 之间生成推理过程，然后使用Verifier来针对答案部分提供奖励，正确则奖励为1，错误则奖励为-1&lt;/li>
&lt;li>RL的Baseline使用的是PPO&lt;/li>
&lt;li>value pretraining 时，作者将 GAE的 $\lambda$ 设置为 1.0， 其他参数与PPO一致&lt;/li>
&lt;li>对于decoupled GAE，作者使用 $\lambda_{\text{critic}}=1.0$, $\lambda_{\text{actor}}=0.95$&lt;/li>
&lt;/ol>
&lt;h2 id="实验结果">实验结果
&lt;/h2>&lt;p>实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-vc-ppo/results.png"
width="905"
height="260"
srcset="https://maosong2022.github.io/p/notes-on-vc-ppo/results_hu6699412279798280280.png 480w, https://maosong2022.github.io/p/notes-on-vc-ppo/results_hu7308485645305468575.png 1024w"
loading="lazy"
alt="results"
class="gallery-image"
data-flex-grow="348"
data-flex-basis="835px"
>&lt;/p>
&lt;p>作者还分析了以下模型在AIME数据集上随着训练步数增加准确率的变化情况&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-vc-ppo/AIME_acc_dynamics.png"
width="1119"
height="619"
srcset="https://maosong2022.github.io/p/notes-on-vc-ppo/AIME_acc_dynamics_hu18029786748235472233.png 480w, https://maosong2022.github.io/p/notes-on-vc-ppo/AIME_acc_dynamics_hu11436355547314266769.png 1024w"
loading="lazy"
alt="results_aime"
class="gallery-image"
data-flex-grow="180"
data-flex-basis="433px"
>&lt;/p>
&lt;h2 id="ablation-study">Ablation Study
&lt;/h2>&lt;p>作者首先探究了 value pretraining 以及 decoupled GAE 对于模型性能的影响&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-vc-ppo/ablation_vc_ppo_componenets.png"
width="1013"
height="319"
srcset="https://maosong2022.github.io/p/notes-on-vc-ppo/ablation_vc_ppo_componenets_hu9686497278113478400.png 480w, https://maosong2022.github.io/p/notes-on-vc-ppo/ablation_vc_ppo_componenets_hu4048457361115586377.png 1024w"
loading="lazy"
alt="ablation_study"
class="gallery-image"
data-flex-grow="317"
data-flex-basis="762px"
>&lt;/p>
&lt;p>从上图可以看到，直接使用PPO并不能提升模型的表现，而使用value pretraining 以及 decoupled GAE 能够显著提升模型的表现。&lt;/p>
&lt;p>作者接下来探究了不同的value pretraining steps对模型的影响，结果如下图&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-vc-ppo/ablation_value_pretraining.png"
width="954"
height="297"
srcset="https://maosong2022.github.io/p/notes-on-vc-ppo/ablation_value_pretraining_hu1243801788441141163.png 480w, https://maosong2022.github.io/p/notes-on-vc-ppo/ablation_value_pretraining_hu18087424118727601877.png 1024w"
loading="lazy"
alt="ablation_study_2"
class="gallery-image"
data-flex-grow="321"
data-flex-basis="770px"
>&lt;/p>
&lt;p>从上表可以看到，value pretraining 的训练步数并不是越多越好，随着训练步数的增加，模型可能会出现过拟合的现象。&lt;/p>
&lt;p>最后作者还分析了以下 $\lambda_{\text{actor}}$ 对于模型性能的影响，结果如下图&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-vc-ppo/ablation_lambda_actor.png"
width="712"
height="321"
srcset="https://maosong2022.github.io/p/notes-on-vc-ppo/ablation_lambda_actor_hu10229919570665430433.png 480w, https://maosong2022.github.io/p/notes-on-vc-ppo/ablation_lambda_actor_hu1744348048046763005.png 1024w"
loading="lazy"
alt="ablation_study_3"
class="gallery-image"
data-flex-grow="221"
data-flex-basis="532px"
>&lt;/p>
&lt;p>可以看到， $\lambda_{\text{actor}} =1.0$ 的效果是最差的，但是 $\lambda_{\text{actor}}$ 也不是越小越好，实验发现当 $\lambda_{\text{actor}} \in [0.95, 1.0)$ 时结果比较好&lt;/p>
&lt;h2 id="findings">Findings
&lt;/h2>&lt;p>作者还提供了一些发现。&lt;/p>
&lt;ol>
&lt;li>作者认为，在LLM中进行RL的训练与传统的RL训练不同，我们不再是从一个随机policy开始，而是从一个SFT之后的policy开始，因此，这就会引入 prior，我们需要将 value model 与 policy model 进行对齐，才能使得训练更加稳定。&lt;/li>
&lt;li>作者认为，value pretraining 可以王value model 中注入先验知识，作者通过实验发现，value pretraining 的过程可以分为两个阶段，第一个阶段是random alignment，这个和传统的RL训练类似，第二个阶段是knowledge injection，这个阶段，value model 开始学习如何给重要的token 更高的权重。
&lt;img src="https://maosong2022.github.io/p/notes-on-vc-ppo/value-pretraining.png"
width="634"
height="532"
srcset="https://maosong2022.github.io/p/notes-on-vc-ppo/value-pretraining_hu1494787409475055775.png 480w, https://maosong2022.github.io/p/notes-on-vc-ppo/value-pretraining_hu6848133809477616518.png 1024w"
loading="lazy"
alt="value-pretraining"
class="gallery-image"
data-flex-grow="119"
data-flex-basis="286px"
>&lt;/li>
&lt;li>作者发现， value model 倾向于更大的 $\lambda$, 因此结果会导致更小的 bias 和 更大的 variance. 而 policy model 倾向于更小的 $\lambda$. 这种差异启发我们需要使用一些基于policy gradient 的目标来训练 value model.&lt;/li>
&lt;/ol>
&lt;h1 id="结论">结论
&lt;/h1>&lt;p>本文提出了VC-PPO，一个通过使用value-pretraining 以及 decoupled GAE 来解决PPO 的 value initialization bias 以及 reward signal decay 问题的算法。&lt;/p>
&lt;h1 id="参考文献">参考文献
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2503.01491" target="_blank" rel="noopener"
>VC-PPO&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://maosong.website/p/notes-on-dapo/" target="_blank" rel="noopener"
>DAPO&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on DAPO</title><link>https://maosong2022.github.io/p/notes-on-dapo/</link><pubDate>Wed, 09 Apr 2025 21:40:33 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-dapo/</guid><description>&lt;h1 id="introduction">Introduction
&lt;/h1>&lt;p>字节Seed团队和清华合作提出了DAPO，一种全开源的，基于PPO的强化学习方法，一用于提升LLM的reasoning能力。&lt;/p>
&lt;h1 id="preliminary">Preliminary
&lt;/h1>&lt;p>Preliminary包括PPO，GRPO还有KL divergence&lt;/p>
&lt;h2 id="ppo">PPO
&lt;/h2>&lt;p>PPO的训练目标为：&lt;/p>
$$
\mathcal{J}_{\mathrm{PPO}}(\theta) = \mathbb{E}_{(q,a)\sim\mathcal{D},o_{\leq t}\sim \pi_{\theta_{old}}(\cdot\mid q)}\left[ \min\left(r_t(\theta)\hat{A}_t,\mathrm{clip}\left(r_t(\theta), 1-\epsilon, 1+\epsilon\right)\hat{A}_t\right) \right]
$$&lt;p>其中&lt;/p>
$$
r_t(\theta) = \frac{\pi_{\theta}(o_t\mid q, o_{&lt; t})}{\pi_{\theta_{old}}(o_t\mid q, o_{&lt; t})}
$$&lt;p>$(q,a)$ 是从数据集 $\mathcal{D}$ 采样的QA pair，$\epsilon&amp;gt;0$ 是一个超参数，$\hat{A}_t$ 是 $t$时刻的优势估计 (advantage estimator). 给定 value function $V$ 以及 reward function $R$, $\hat{A}_t$ 通过计算GAE得到：&lt;/p>
$$
\hat{A}_t^{\mathrm{GAE}(\gamma, \lambda)}=\sum_{k=0}^{\infty}(\gamma\lambda)^k\delta_{t+k}
$$&lt;p>其中&lt;/p>
$$
\delta_k = R_k + \gamma V(s_{k+1})-V(s_k),\quad 0\leq \gamma,\lambda\leq 1
$$&lt;h2 id="grpo">GRPO
&lt;/h2>&lt;p>相比于PPO，GRPO不依赖于value function, 因此不需要使用reward model. GRPO通过一组输出来估计value $V(s)$, 然后进行更新。具体来说，给定 QA pair $(q,a)$, 我们从$\pi_{\theta_{old}}$中采样$G$个输出 ${o_i}&lt;em>{i=1}^G$, 接下来我们基于reward ${R_i}&lt;/em>{i=1}^G$ 使用如下表达式来估计group-level reward:&lt;/p>
$$
\hat{A}_{i,t} = \frac{r_i - \mathrm{mean}(\{R_i\}_{i=1}^G)}{\mathrm{std}(\{R_i\}_{i=1}^G)}
$$&lt;p>最后，GRPO的训练目标与PPO类似，只不过将 $\hat{A}&lt;em>t$ 替换为 $\hat{A}&lt;/em>{i,t}$, 然后在分组上进行了归一化：&lt;/p>
$$
\mathcal{J}_{\mathrm{GRPO}}(\theta) = \mathbb{E}_{(q,a)\sim\mathcal{D},\{o_i\}_{i=1}^G\sim \pi_{\theta_{old}}(\cdot\mid q)}\left[ \frac{1}{G}\sum_{i=1}^G\frac{1}{|o_i|}\sum_{t=1}^{|o_i|}\min\left(r_{i,t}(\theta)\hat{A}_{i,t},\mathrm{clip}\left(r_{i,t}(\theta), 1-\epsilon, 1+\epsilon\right)\hat{A}_{i,t}\right) \right]
$$&lt;p>其中，&lt;/p>
$$
r_{i,t}(\theta) = \frac{\pi_{\theta}(o_{i,t}\mid q, o_{i,&lt; t})}{\pi_{\theta_{old}}(o_{i,t}\mid q, o_{i,&lt; t})}
$$&lt;h2 id="kl-divergence">KL divergence
&lt;/h2>&lt;p>在传统的RLHF框架中，我们再PPO的基础上，增加了一个KL divergence正则项，用于约束新旧策略的差异。具体来说，给定旧策略 $\pi_{\theta_{old}}$ 和新策略 $\pi_{\theta}$，我们实际上优化的损失函数为&lt;/p>
$$
\mathcal{J}_{\mathrm{RLHF}}(\theta) = \mathcal{J}_{\mathrm{PPO}}(\theta) - \beta\mathrm{KL}\left(\pi_{\theta_{old}}(\cdot\mid q)\|\pi_{\theta}(\cdot\mid q)\right)
$$&lt;p>其中，$\beta$ 是一个超参数，用于平衡PPO损失和KL divergence损失。上面的PPO损失函数也可以改为GRPO损失函数。&lt;/p>
&lt;p>作者在本文中认为，reasoning model和RLHF的训练目标是不一样的，RLHF加上KL divergence正则项，会使得模型在推理时过于保守，偏向于explotition而reasoning model则需要exploration。因此，作者在本文中去掉了这一项。&lt;/p>
&lt;h2 id="rule-based-reward-modeling">Rule-based reward modeling
&lt;/h2>&lt;p>作者基于final accuracy来作为outcome reward, 避免模型训练出现reward hacking问题。reward function 如下：&lt;/p>
$$
R(\hat{y},y) = \begin{cases}
1, &amp; \text{if is\_equivalent}(\hat{y},y) \\
-1, &amp; \text{otherwise}
\end{cases}
$$&lt;h1 id="dapo">DAPO
&lt;/h1>&lt;p>DAPO基于GRPO改进，其优化的目标函数为：&lt;/p>
$$
\mathcal{J}_{\mathrm{DAPO}}(\theta) = \mathbb{E}_{(q,a)\sim\mathcal{D},\{o_i\}_{i=1}^G\sim \pi_{\theta_{old}}(\cdot\mid q)}\left[ \frac{1}{\sum_{t=1}^G|o_i|}\sum_{i=1}^G\sum_{t=1}^{|o_i|}\min\left(r_{i,t}(\theta)\hat{A}_{i,t},\mathrm{clip}\left(r_{i,t}(\theta), 1-\epsilon_{low}, 1+\epsilon_{high}\right)\hat{A}_{i,t}\right) \right]
s.t. \quad 0&lt; \vert \{o_i\mid \text{is\_equivalent}(o_i,a)\} \vert &lt; G
$$&lt;p>其中，$\alpha$ 是一个超参数，用于约束输出数量, $r_{i,t}(\theta)$ 和 $\hat{A}_{i,t}$ 的定义与GRPO相同。&lt;/p>
&lt;p>接下来就是DAPO算法的几个关键点：&lt;/p>
&lt;h2 id="clip-higher">Clip-Higher
&lt;/h2>&lt;p>作者认为，PPO和GRPO存在entropy collapse问题，也就是policy的entropy会迅速的下降，导致最终每个group的输出基本都差不多，模型很难探索到新的知识。因此，作者提出了Clip-Higher策略，也就是说，让模型能够充分探索。&lt;/p>
&lt;p>作者举了一个例子，当 $\epsilon=0.2$ 时，如果一个group的输出为 $[0.01, 0.9]$, 那么在clip之后，最大的更新幅度为 $[0.01, 0.9]\times 0.2=[0.002, 0.18]$. 这对于概率比较小的输出来说，很难帮助模型提升。因此，作者修改了其阈值。进一步地，作者分离了clip的阈值，分别使用 $\epsilon_{low}$ 和 $\epsilon_{high}$ 来约束更新幅度。作者通过实验验证了这个例子，结果如下图所示：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-dapo/entropy.png"
width="1395"
height="581"
srcset="https://maosong2022.github.io/p/notes-on-dapo/entropy_hu8179302749229083691.png 480w, https://maosong2022.github.io/p/notes-on-dapo/entropy_hu10259359026417120206.png 1024w"
loading="lazy"
alt="entropy collapse"
class="gallery-image"
data-flex-grow="240"
data-flex-basis="576px"
>&lt;/p>
&lt;p>最后，加入Clip-Higher策略的DAPO的训练目标为，与GRPO的训练目标相比，我们使用 $\epsilon_{low}$ 和 $\epsilon_{high}$ 来约束更新幅度。&lt;/p>
$$
\mathcal{J}_{\mathrm{DAPO}}(\theta) = \mathbb{E}_{(q,a)\sim\mathcal{D},\{o_i\}_{i=1}^G\sim \pi_{\theta_{old}}(\cdot\mid q)}\left[ \frac{1}{\sum_{t=1}^G|o_i|}\sum_{i=1}^G\sum_{t=1}^{|o_i|}\min\left(r_{i,t}(\theta)\hat{A}_{i,t},\mathrm{clip}\left(r_{i,t}(\theta), 1-\epsilon_{low}, 1+\epsilon_{high}\right)\hat{A}_{i,t}\right) \right]
s.t. \quad 0&lt; \vert \{o_i\mid \text{is\_equivalent}(o_i,a)\} \vert &lt; G
$$&lt;p>在实际训练中，作者使用了一个比较大的 $\epsilon_{high}$ 来保证概率较小的token也能有较大的概率被采样到。
Clip-Higher的实验结果如下图所示：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-dapo/clip-higher.png"
width="1396"
height="598"
srcset="https://maosong2022.github.io/p/notes-on-dapo/clip-higher_hu15543817945356973980.png 480w, https://maosong2022.github.io/p/notes-on-dapo/clip-higher_hu5887700003268347485.png 1024w"
loading="lazy"
alt="Clip-Higher"
class="gallery-image"
data-flex-grow="233"
data-flex-basis="560px"
>&lt;/p>
&lt;h2 id="dynamic-sampling">Dynamic Sampling
&lt;/h2>&lt;p>作者认为，在GRPO中，如果一个group的输出全都是对的，那么其advantage会趋近于0，结果导致 policy 不会得到更新，这样就降低了采样效率。&lt;/p>
&lt;p>为了解决这个问题，坐着提出了over-sample以及filtering策略，来过滤accracy等于1或者0的prompts,也就是DAPO中的约束项：&lt;/p>
$$
s.t. \quad 0&lt; \vert \{o_i\mid \text{is\_equivalent}(o_i,a)\} \vert &lt; G
$$&lt;p>通过增加这个约束项，我们可以保证每个group的输出不会趋同，从保证模型能够得到更新，以提高采样效率。实验结果如下图所示：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-dapo/dynamic-sampling.png"
width="1169"
height="554"
srcset="https://maosong2022.github.io/p/notes-on-dapo/dynamic-sampling_hu6577790916460662313.png 480w, https://maosong2022.github.io/p/notes-on-dapo/dynamic-sampling_hu11456810036780733562.png 1024w"
loading="lazy"
alt="Dynamic Sampling"
class="gallery-image"
data-flex-grow="211"
data-flex-basis="506px"
>&lt;/p>
&lt;h2 id="token-level-policy-gradient-loss">Token-level policy gradient loss
&lt;/h2>&lt;p>GRPO采用了一个sample-level的loss计算方式，也就是GRPO首先在每个sample中计算每个token的损失并进行平均，然后在不同的sample中在此进行平均。在这种方式下，每个sample对最终的loss贡献是相同的。这样就导致两个问题：&lt;/p>
&lt;ol>
&lt;li>对于高质量的long answer，通过在token层面进行平均，因为answe比较长，因此整体loss会比较低，也就会降低这个sample的贡献，导致模型很难学习到长答案。&lt;/li>
&lt;li>一些非常长的sample，其往往包含gibberish或者repetitive tokens，这些sample对模型训练是有害的&lt;/li>
&lt;/ol>
&lt;p>因此，为了解决这个问题，作者提出了Token-level policy gradient loss，也就是我们改变了求和方式&lt;/p>
$$
\frac{1}{G}\sum_{i=1}^G\frac{1}{|o_i|}\sum_{t=1}^{|o_i|}\left(\cdot\right)\to \frac{1}{\sum_{t=1}^G|o_i|}\sum_{i=1}^G\sum_{t=1}^{|o_i|}\left(\cdot\right)
$$&lt;p>通过这种方式，我们让长回答的loss贡献更大，从而提高模型学习长回答的能力。另一方面，现在每个token对整体loss的贡献是相同的，一些关键token的loss也会被放大，从而提高模型的表现。&lt;/p>
&lt;p>实验结果如下图所示：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-dapo/token-loss.png"
width="1391"
height="564"
srcset="https://maosong2022.github.io/p/notes-on-dapo/token-loss_hu502314068130756686.png 480w, https://maosong2022.github.io/p/notes-on-dapo/token-loss_hu9534175777293042147.png 1024w"
loading="lazy"
alt="Token-level policy gradient loss"
class="gallery-image"
data-flex-grow="246"
data-flex-basis="591px"
>&lt;/p>
&lt;h2 id="overlong-reward-shaping">Overlong reward shaping
&lt;/h2>&lt;p>与Kimi-k1.5类似，DAPO也增加了length penalty，用于惩罚过长的回答。作者首先使用了一个overlong filtering技巧，来mask掉truncated samples的loss，作者发现通过这个技巧，可以提升模型的训练稳定性以及表现，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-dapo/overlong-filtering.png"
width="1394"
height="605"
srcset="https://maosong2022.github.io/p/notes-on-dapo/overlong-filtering_hu4525038643484855881.png 480w, https://maosong2022.github.io/p/notes-on-dapo/overlong-filtering_hu2947757528018929037.png 1024w"
loading="lazy"
alt="Overlong filtering"
class="gallery-image"
data-flex-grow="230"
data-flex-basis="552px"
>&lt;/p>
&lt;p>作者还提出了soft overlong punishment， 用于reshape truncated samples的reward，表达式如下：&lt;/p>
$$
R_{length}(y) = \begin{cases}
0, &amp; \text{if } |y|\leq L_{\max}-L_{cache} \\
\frac{(L_{\max}-L_{cache})-|y|}{L_{cache}}, &amp; \text{if } L_{\max}-L_{cache}&lt;|y|\leq L_{\max} \\
-1, &amp; \text{if } |y|>L_{\max}
\end{cases}
$$&lt;h2 id="算法">算法
&lt;/h2>&lt;p>DAPO的算法流程如下：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-dapo/dapo-algorithm.png"
width="1391"
height="536"
srcset="https://maosong2022.github.io/p/notes-on-dapo/dapo-algorithm_hu511434881505261352.png 480w, https://maosong2022.github.io/p/notes-on-dapo/dapo-algorithm_hu3096415489348530806.png 1024w"
loading="lazy"
alt="DAPO"
class="gallery-image"
data-flex-grow="259"
data-flex-basis="622px"
>&lt;/p>
&lt;h1 id="实验">实验
&lt;/h1>&lt;h2 id="数据集">数据集
&lt;/h2>&lt;p>作者构建了DAPO-Math-17K数据集用于DAPO的训练，该数据集从AoPS得到，包含了17K prompts，每个prompt都对应一个整数作为答案。&lt;/p>
&lt;h2 id="训练细节">训练细节
&lt;/h2>&lt;p>作者以GRPO作为baseline， $G=16$, $L_{\max}=16384$, $L_{cache}=4096$, $\epsilon_{low}=0.2$, $\epsilon_{high}=0.28$&lt;/p>
&lt;p>评估时，使用AIME作为benchmark，以 avg@32作为指标，temperature设置为1.0, topp设置为0.7。&lt;/p>
&lt;h2 id="实验结果">实验结果
&lt;/h2>&lt;p>DAPO与GRPO的对比如下图所示：
&lt;img src="https://maosong2022.github.io/p/notes-on-dapo/dapo-performance.png"
width="1381"
height="605"
srcset="https://maosong2022.github.io/p/notes-on-dapo/dapo-performance_hu7946030748495081666.png 480w, https://maosong2022.github.io/p/notes-on-dapo/dapo-performance_hu10627399252545047766.png 1024w"
loading="lazy"
alt="dapo performance"
class="gallery-image"
data-flex-grow="228"
data-flex-basis="547px"
>&lt;/p>
&lt;h2 id="ablation-study">Ablation study
&lt;/h2>&lt;p>作者探究了每一个部分对最终表现的影响，结果如下：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-dapo/ablation-study.png"
width="787"
height="415"
srcset="https://maosong2022.github.io/p/notes-on-dapo/ablation-study_hu12477267731290906231.png 480w, https://maosong2022.github.io/p/notes-on-dapo/ablation-study_hu1931468525880383603.png 1024w"
loading="lazy"
alt="ablation study"
class="gallery-image"
data-flex-grow="189"
data-flex-basis="455px"
>&lt;/p>
&lt;h2 id="traing-dynamics">traing dynamics
&lt;/h2>&lt;p>作者还探究了mean response length, reward score, generation entropy以及mean probability随训练轮数的变化，其中：&lt;/p>
&lt;ul>
&lt;li>mean response length: 在一定程度上反应了模型训练的稳定性和表现&lt;/li>
&lt;li>reward score: 反应模型的表现，作者认为，给定一个reliable reward signal， LLM可以很好的fit到训练数据集上，但是作者发现最终的reward与val score相关性比较大，很可能是因为模型过拟合了&lt;/li>
&lt;li>generation entropy &amp;amp; mean probability: 代表了模型的探索能力，通过实验结果可以看到，DAPO初期的探索能力比较强，但是随着训练的进行，探索能力下降，利用能力增强。&lt;/li>
&lt;/ul>
&lt;p>结果如下：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-dapo/training-dynamics.png"
width="1402"
height="1027"
srcset="https://maosong2022.github.io/p/notes-on-dapo/training-dynamics_hu2718737616169362162.png 480w, https://maosong2022.github.io/p/notes-on-dapo/training-dynamics_hu6245583490424385514.png 1024w"
loading="lazy"
alt="training dynamics"
class="gallery-image"
data-flex-grow="136"
data-flex-basis="327px"
>&lt;/p>
&lt;h1 id="结论">结论
&lt;/h1>&lt;p>作者基于GRPO提出了DAPO，一种全开源的，基于PPO的强化学习方法，用于提升LLM的reasoning能力。作者首先分析了GRPO损失函数存在的不足，然后进行了针对性改进，包括Clip-Higher, Dynamic Sampling, Token-level policy gradient loss以及Overlong reward shaping。作者通过实验验证了DAPO的有效性，并探究了DAPO的训练动态。&lt;/p>
&lt;h1 id="reference">Reference
&lt;/h1>&lt;ol>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2503.14476" target="_blank" rel="noopener"
>DAPO: An Open-Source LLM Reinforcement Learning System at Scale&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>Notes on Qwen2.5 omni</title><link>https://maosong2022.github.io/p/notes-on-qwen2.5-omni/</link><pubDate>Tue, 01 Apr 2025 10:29:00 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-qwen2.5-omni/</guid><description>&lt;h1 id="介绍">介绍
&lt;/h1>&lt;p>2025年3月26号，Qwen团队发布了Qwen2.5 omni，Qwen2.5 omni是一个多模态模型，支持文本、音频、视频、图像等多个模态的输入，支持文本，音频的输出。
作者提出了TMRoPE来对齐不同的模态，然后还是用了Thinker-Talker的架构来同时生成文本和音频。
Thinker是一个大语言模型，Talker是一个dual-track自回归模型，Talker基于Thinker的hideen states来生成audio token，最后通过一个sliding-window DiT来解码audio token&lt;/p>
&lt;p>现有omni-model存在的问题：&lt;/p>
&lt;ol>
&lt;li>缺乏一个系统的，联合训练多个不同模态的方法&lt;/li>
&lt;li>需要处理不同模态输出时互相干扰的问题&lt;/li>
&lt;li>不能实时理解或者流式输出多模态信息&lt;/li>
&lt;/ol>
&lt;p>Qwen2.5 omni的解决方法：&lt;/p>
&lt;ol>
&lt;li>使用TMRoPE来对齐不同的模态。作者将audio以及video frame按照时间顺序组织成一个交替的架构，然后使用TMRoPE来对齐&lt;/li>
&lt;li>使用Thinker-Talker的架构来同时生成文本和音频。Thinker负责文本输出，Talker负责音频的流式输出。&lt;/li>
&lt;li>在multimodal encoder中使用Block-wise streaming处理技巧来实现实时理解；在输出时，实现了一个dual-track自回归架构来生成audio token，以及一个DiT模型来解码audio token&lt;/li>
&lt;/ol>
&lt;p>Qwen2.5 omni的贡献：&lt;/p>
&lt;ol>
&lt;li>提出了Qwen2.5-omni,可以实时理解多模态信息，并流式输出文本和音频&lt;/li>
&lt;li>提出了TMRoPE，一个可以对齐不同模态的RoPE算法&lt;/li>
&lt;li>提出了Thinker-Talker的架构，来完成实时理解和音频输出&lt;/li>
&lt;li>在多个benchmark上取得了SOTA&lt;/li>
&lt;/ol>
&lt;h1 id="架构">架构
&lt;/h1>&lt;h2 id="总览">总览
&lt;/h2>&lt;p>Qwen2.5-omni的架构如下图所示
&lt;img src="https://maosong2022.github.io/p/notes-on-qwen2.5-omni/architecture.png"
width="1346"
height="1110"
srcset="https://maosong2022.github.io/p/notes-on-qwen2.5-omni/architecture_hu13856471526692175307.png 480w, https://maosong2022.github.io/p/notes-on-qwen2.5-omni/architecture_hu13696785578383343851.png 1024w"
loading="lazy"
alt="architecture of Qwen2.5-omni"
class="gallery-image"
data-flex-grow="121"
data-flex-basis="291px"
>&lt;/p>
&lt;p>其中Talker类似于人的嘴，负责基于脑中的信息来生成对话；Thinker类似于人的大脑，负责理解输入的信息，并生成对话的上下文。&lt;/p>
&lt;h2 id="输入">输入
&lt;/h2>&lt;ul>
&lt;li>text: Qwen的tokenizer&lt;/li>
&lt;li>audio： Qwen2-Audio的tokenizer&lt;/li>
&lt;li>vision： Qwen2.5-VL的vision tokenizer&lt;/li>
&lt;/ul>
&lt;p>视频以及TMRoPE。 对于视频来说，Qwen2.5-omni采取了和Qwen2.5-VL一样的MRoPE算法。只是这里的temporal ID对应40ms&lt;/p>
&lt;p>音频：对于音频来说，Qwen2.5-omni也是以40ms进行采样然后encoding。&lt;/p>
&lt;p>视频+音频：对于视频+音频来说，Qwen2.5-omni将视频和音频的token进行交替的排列，来保证时间上信息一致&lt;/p>
&lt;h2 id="输出">输出
&lt;/h2>&lt;ul>
&lt;li>Text：text由Thinker直接输出，这和LLM的输出方式是一样的。&lt;/li>
&lt;li>Speech：Qwen2.5-omni首先构建了&lt;code>qwen-tts-tokenizer&lt;/code>，一个speech codec，用于表示speech的关键信息。
然后基于这些关键信息，Talker通过自回归的方式生成audio tokens以及text tokens。&lt;/li>
&lt;/ul>
&lt;h2 id="流式输出">流式输出
&lt;/h2>&lt;p>对于流式输出来说，现在的模型存在latency，具体影响因素如下：&lt;/p>
&lt;ol>
&lt;li>处理多模态输入的延迟&lt;/li>
&lt;li>TTFT （time to first token）&lt;/li>
&lt;li>将第一段speech token解码为audio的时间&lt;/li>
&lt;li>模型架构导致的latency，如模型参数等&lt;/li>
&lt;/ol>
&lt;p>为了解决这个问题，Qwen2.5-omni采取了以下措施：&lt;/p>
&lt;ol>
&lt;li>Support prefilling. 作者修改了audio以及vision encoder来在temporal dimension上支持block-wise attention.
具体来讲，audio encoder只关注2秒内的信息，而vision encoder和Qwen2.5-VL一样，使用了一个MLP patch merger来压缩视觉token&lt;/li>
&lt;li>Streaming Codec generation. 为了提高流式输出的效率，作者还是用了基于Flow-Matching的DiT，输出的code收线使用Flow-Matching转化为一个mel-spectrogram.
然后再通过一个BigVGAN来重建得到对应的waveform. 作者在这里修改了DiT的attention，将其receptive field 限制为4个block，包括lookback of 2 blocks以及lookahead of 1 block.&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-qwen2.5-omni/DiT-sliding-window.png"
width="410"
height="433"
srcset="https://maosong2022.github.io/p/notes-on-qwen2.5-omni/DiT-sliding-window_hu6563920421928967677.png 480w, https://maosong2022.github.io/p/notes-on-qwen2.5-omni/DiT-sliding-window_hu2093921625058248866.png 1024w"
loading="lazy"
alt="Sliding Window DiT"
class="gallery-image"
data-flex-grow="94"
data-flex-basis="227px"
>&lt;/p>
&lt;h1 id="pre-training">Pre Training
&lt;/h1>&lt;p>模型参数初始化：&lt;/p>
&lt;ul>
&lt;li>LLM： 从Qwen2.5进行初始化&lt;/li>
&lt;li>vision encoder：从Qwen2.5-VL的vision encoder进行初始化&lt;/li>
&lt;li>audio encoder：从Whisper-large-v3进行初始化&lt;/li>
&lt;/ul>
&lt;p>Pretraining和Qwen2.5-VL的训练过程类似，分成了3个stage：&lt;/p>
&lt;ol>
&lt;li>冻结LLM的参数，仅训练vision encoder和audio encoder， 该阶段使用了audio-text以及image-text pairs来进行训练。 数据：image-text, video-text, video-audio, audio-text以及text corpus，
跟Qwen2-Audio类似，作者将hierarchical tags用自然语言prompt进行替换&lt;/li>
&lt;li>训练所有参数，使用更多的多模态数据进行训练。数据包括800B token的image和Video相关数据，300B token的audio数据，100B token的video-audio相关数据。&lt;/li>
&lt;li>将模型的上下文扩展到32K。前两个阶段的上下文长度为8192，本阶段使用了long video data等数据来将模型的上下文扩展到32K。&lt;/li>
&lt;/ol>
&lt;h1 id="post-training">Post Training
&lt;/h1>&lt;h2 id="thinker">Thinker
&lt;/h2>&lt;p>数据格式为ChatML，数据类型包括pure text-based dialogue data, visual-modality conversation data, audio-modality conversation data and mix-modality conversation data.&lt;/p>
&lt;h2 id="talker">Talker
&lt;/h2>&lt;p>包括三个阶段&lt;/p>
&lt;ol>
&lt;li>ICL training: 训练Talker学习context continuation&lt;/li>
&lt;li>DPO training: 提升speech generation的stability&lt;/li>
&lt;li>multi-speaker instruction fine-tuning: 提升模型的naturalness和controllability&lt;/li>
&lt;/ol>
&lt;h1 id="evaluation">Evaluation
&lt;/h1>&lt;p>Qwen2.5-omni在两类benchmark上进行来的评测，分别时多模态理解（X-&amp;gt;text）以及音频生成(X-&amp;gt; Speech)&lt;/p>
&lt;p>我们这里主要关注一下speech understanding以及speech generation的benchmark.&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-qwen2.5-omni/audio-to-text-performance.png"
width="1093"
height="1125"
srcset="https://maosong2022.github.io/p/notes-on-qwen2.5-omni/audio-to-text-performance_hu3995772258484959637.png 480w, https://maosong2022.github.io/p/notes-on-qwen2.5-omni/audio-to-text-performance_hu12796047373868839849.png 1024w"
loading="lazy"
alt="Audio-to-Text Performance"
class="gallery-image"
data-flex-grow="97"
data-flex-basis="233px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-qwen2.5-omni/zero-shot-speech-generation.png"
width="904"
height="685"
srcset="https://maosong2022.github.io/p/notes-on-qwen2.5-omni/zero-shot-speech-generation_hu4247592615732321508.png 480w, https://maosong2022.github.io/p/notes-on-qwen2.5-omni/zero-shot-speech-generation_hu2294091350184113498.png 1024w"
loading="lazy"
alt="Speech Generation Performance"
class="gallery-image"
data-flex-grow="131"
data-flex-basis="316px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-qwen2.5-omni/single-speaker-speech-generation.png"
width="864"
height="513"
srcset="https://maosong2022.github.io/p/notes-on-qwen2.5-omni/single-speaker-speech-generation_hu18113694195638600776.png 480w, https://maosong2022.github.io/p/notes-on-qwen2.5-omni/single-speaker-speech-generation_hu13691510460279495977.png 1024w"
loading="lazy"
alt="single speaker speech generation"
class="gallery-image"
data-flex-grow="168"
data-flex-basis="404px"
>&lt;/p>
&lt;h1 id="总结">总结
&lt;/h1>&lt;p>Qwen2.5-omni相当于是结合了Qwen2.5-VL以及Mini-omni，跟mini-omni2的输入输出是类似的。不同的点在于，Qwen2.5-omni使用了Thinker-Talker的架构，然后还使用了TMRoPE来对齐不同的模态。总的来说，感觉模型还是更偏重于audio的理解与生成。&lt;/p>
&lt;h1 id="reference">Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2503.20215" target="_blank" rel="noopener"
>Qwen2.5-omni&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2408.16725" target="_blank" rel="noopener"
>Mini-omni&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2410.11190" target="_blank" rel="noopener"
>Mini-omni2&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Understanding Sigmoid Loss in SigLip</title><link>https://maosong2022.github.io/p/understanding-sigmoid-loss-in-siglip/</link><pubDate>Fri, 28 Mar 2025 14:55:50 +0800</pubDate><guid>https://maosong2022.github.io/p/understanding-sigmoid-loss-in-siglip/</guid><description>&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>A simple note to understand Sigmoid Loss in SigLip &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Supported by DeepSeek&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;h2 id="binary-cross-entropy-loss">Binary cross entropy loss
&lt;/h2>&lt;p>Suppose we want to solve the binary classification problem, with label $y\in{0, 1}$, a common option is to use binary cross entropy loss:&lt;/p>
$$\mathcal{L}(x, y) = -[y\log (\sigma(z)) + (1-y)\log (1-\sigma(z))]$$&lt;p>where $z=f_\theta(x)$ is the logits predicted by our model $f_\theta$, and $\sigma$ is the sigmoid function:&lt;/p>
$$\sigma(z) := \frac{1}{1 + e^{-z}}$$&lt;p>Let $\sigma(\cdot)$ be the sigmoid function, then we have:&lt;/p>
$$
\sigma(-z) = \frac{1}{1 + e^{z}} = \frac{e^{-z}}{1 + e^{-z}} = 1 - \frac{1}{1 + e^{-z}} = 1- \sigma(z)
$$&lt;p>Now we substitute $\sigma(-z)=1-\sigma(z)$ into the loss function, we obtain:&lt;/p>
$$\mathcal{L}(x, y) = -[y\log (\sigma(z)) + (1-y)\log (\sigma(-z))]$$&lt;p>Note that $y\in{0, 1}$ thus for each instance, there are two cases:&lt;/p>
&lt;ul>
&lt;li>If $y=0$, then $\mathcal{L}(x, y) =-\log (\sigma(-z))$&lt;/li>
&lt;li>If $y=1$, then $\mathcal{L}(x, y) =-\log (\sigma(z))$&lt;/li>
&lt;/ul>
&lt;p>Now we want to use a unified expression to express these two cases. Note that this requires fitting a curve that passes two points $(0, -1)$ and $(1, 1)$. The simplest curve is a straight line $y=2x-1$. So, we can further simplify the loss expression into:&lt;/p>
$$\mathcal{L}(x, y) = -\log\left[\sigma((2y-1)z)\right]$$&lt;h2 id="sigmoid-loss-in-siglip">Sigmoid Loss in SigLip
&lt;/h2>&lt;p>Now we recall the sigmoid loss in SigLip:&lt;/p>
$$\mathcal{L}(\{\bm{x}, \bm{y}\}_{i=1}^N)=-\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^N\log \frac{1}{1+\exp\left[z_{ij}(-t\bm{x}_i\cdot \bm{y_j}+b)\right]}$$&lt;p>where $t, b$ are learnable parameters, and $z_{ij}=1$ if $i=j$ and $z_{ij}=-1$ otherwise.&lt;/p>
&lt;p>To understand Sigmoid loss, notice that $z_{ij}=2\mathbb{I}_{i=j}-1$, which exactly matches the form we derived earlier.&lt;/p>
&lt;h2 id="why-use-sigmoid-loss">Why Use Sigmoid Loss?
&lt;/h2>&lt;ol>
&lt;li>More stable: avoids $\log 0$.&lt;/li>
&lt;li>More efficient: Compute Sigmoid once.&lt;/li>
&lt;li>More Precise: one line of code without condition checking.&lt;/li>
&lt;/ol>
&lt;h1 id="references">References
&lt;/h1>&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>&lt;a class="link" href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhai_Sigmoid_Loss_for_Language_Image_Pre-Training_ICCV_2023_paper.pdf" target="_blank" rel="noopener"
>SigLip&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>&lt;a class="link" href="https://chat.deepseek.com/" target="_blank" rel="noopener"
>DeepSeek&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Notes on Aya Vision</title><link>https://maosong2022.github.io/p/notes-on-aya-vision/</link><pubDate>Mon, 17 Mar 2025 17:58:24 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-aya-vision/</guid><description>&lt;p>Aya Vision是一个多模态大语言模型，包含8B, 32B两个size，支持23种语言。Aya Vision基于 Aya Expanse大语言模型。&lt;/p>
&lt;h2 id="模型架构">模型架构
&lt;/h2>&lt;p>Aya Vision的模型架构如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-aya-vision/architecture.png"
width="3624"
height="1316"
srcset="https://maosong2022.github.io/p/notes-on-aya-vision/architecture_hu7451901003157889581.png 480w, https://maosong2022.github.io/p/notes-on-aya-vision/architecture_hu7590094345800415710.png 1024w"
loading="lazy"
alt="Aya Vision模型架构"
class="gallery-image"
data-flex-grow="275"
data-flex-basis="660px"
>&lt;/p>
&lt;ul>
&lt;li>Vision Encoder: SigLip2-patch14-384&lt;/li>
&lt;li>Vision-text connector: 2 layer MLP&lt;/li>
&lt;li>LLM: Aya Expanse 8B/ 32B&lt;/li>
&lt;/ul>
&lt;h2 id="训练">训练
&lt;/h2>&lt;p>训练包含两个stage：&lt;/p>
&lt;ol>
&lt;li>Vision-language alignment: 仅训练vision-text connector，基于image-text pairs进行训练&lt;/li>
&lt;li>SFT：训练connector和LLM，基于合成的多语种数据进行训练&lt;/li>
&lt;/ol>
&lt;h2 id="多语种数据">多语种数据
&lt;/h2>&lt;p>为了提高模型的多语种能力，作者先基于English的高质量数据集合成了annotation，然后作者讲这些数据转化为22中语言对应的文本&lt;/p>
&lt;h2 id="model-merging">Model merging
&lt;/h2>&lt;p>最后为了提高模型在纯文本任务上的表现，作者还使用了model merging的技巧。具体做法就是merge使用的base language model和SFT之后的vision-language model&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ol>
&lt;li>&lt;a class="link" href="https://huggingface.co/blog/aya-vision" target="_blank" rel="noopener"
>Aya Vision Blog&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>Notes on Gemma3</title><link>https://maosong2022.github.io/p/notes-on-gemma3/</link><pubDate>Sat, 15 Mar 2025 11:15:29 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-gemma3/</guid><description>&lt;img src="https://maosong2022.github.io/p/notes-on-gemma3/cover.png" alt="Featured image of post Notes on Gemma3" />&lt;p>作者提出了Gemma3系列大模型，包括1B, 4B, 12B, 27B四个size。4B, 12B, 27B三个size均支持多模态，128K的上下文长度以及140种语言。&lt;/p>
&lt;h1 id="方法">方法
&lt;/h1>&lt;h2 id="数据处理">数据处理
&lt;/h2>&lt;h3 id="数据格式">数据格式
&lt;/h3>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">[BOS]&amp;lt;start_of_turn&amp;gt;user
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Who are you?&amp;lt;end_of_turn&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&amp;lt;start_of_turn&amp;gt;model
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">My name is Gemma!&amp;lt;end_of_turn&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&amp;lt;start_of_turn&amp;gt;user
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">What is 2+2?&amp;lt;end_of_turn&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&amp;lt;start_of_turn&amp;gt;model
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">2+2=4.&amp;lt;end_of_turn&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>pretrain和SFT的区别在于,pretrain时模型输出以&lt;code>&amp;lt;eos&amp;gt;&lt;/code>结束,SFT时模型输出以&lt;code>&amp;lt;end_of_turn&amp;gt;&lt;/code>结束.&lt;/p>
&lt;h3 id="图片处理">图片处理
&lt;/h3>&lt;p>输入的图片都会被resize到896x896，如果图片精度过大或者不是正方形，则会通过Pan &amp;amp; Scan技巧裁剪为多个子图，然后每个子图分别进行resize。核心处理代码为：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">crop_size_w&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">math&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ceil&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">width&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">num_crops_w&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">crop_size_h&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">math&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ceil&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">height&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">num_crops_h&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Don&amp;#39;t apply PaS if crop size is too small.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">if&lt;/span> &lt;span class="nb">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">crop_size_w&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">crop_size_h&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">pan_and_scan_min_crop_size&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">crop_positions_w&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">crop_size_w&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num_crops_w&lt;/span>&lt;span class="p">)]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">crop_positions_h&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">crop_size_h&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num_crops_h&lt;/span>&lt;span class="p">)]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">image_crops&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">image&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">pos_h&lt;/span> &lt;span class="p">:&lt;/span> &lt;span class="n">pos_h&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">crop_size_h&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">pos_w&lt;/span> &lt;span class="p">:&lt;/span> &lt;span class="n">pos_w&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">crop_size_w&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">pos_h&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">pos_w&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">itertools&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">product&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">crop_positions_h&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">crop_positions_w&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="模型架构">模型架构
&lt;/h2>&lt;p>模型包括4个size，分别是1B, 4B, 12B, 27B。1B的模型是单模态的，4B, 12B, 27B的模型是多模态的。对于多模态模型来说：&lt;/p>
&lt;ul>
&lt;li>Vision encoder: Siglip-400M&lt;/li>
&lt;li>Projection layer: linear layer&lt;/li>
&lt;li>LLM: Gemma 3&lt;/li>
&lt;/ul>
&lt;h2 id="模型参数">模型参数
&lt;/h2>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Vision Encoder&lt;/th>
&lt;th>Embedding Parameters&lt;/th>
&lt;th>Non-embedding Parameters&lt;/th>
&lt;th>context length&lt;/th>
&lt;th>multilingual&lt;/th>
&lt;th>training data&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>1B&lt;/td>
&lt;td>0&lt;/td>
&lt;td>302M&lt;/td>
&lt;td>698M&lt;/td>
&lt;td>32K&lt;/td>
&lt;td>English&lt;/td>
&lt;td>2T tokens&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4B&lt;/td>
&lt;td>417M&lt;/td>
&lt;td>675M&lt;/td>
&lt;td>3,209M&lt;/td>
&lt;td>128K&lt;/td>
&lt;td>140+ languages&lt;/td>
&lt;td>4T tokens&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>12B&lt;/td>
&lt;td>417M&lt;/td>
&lt;td>1,012M&lt;/td>
&lt;td>10,759M&lt;/td>
&lt;td>128K&lt;/td>
&lt;td>140+ languages&lt;/td>
&lt;td>12T tokens&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>27B&lt;/td>
&lt;td>417M&lt;/td>
&lt;td>1,416M&lt;/td>
&lt;td>25,600M&lt;/td>
&lt;td>128K&lt;/td>
&lt;td>140+ languages&lt;/td>
&lt;td>14T tokens&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="attention-layers">Attention layers
&lt;/h2>&lt;p>为了提高效率，作者将部分layer的self-attention替换为sliding window attention。论文里是将6 layers为一组，替换一组中最后一层为sliding window attention，其余层为self attention。判断某layer是否为sliding window attention的代码为：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># config.sliding_window_pattern = 6&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">is_sliding&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">bool&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">layer_idx&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sliding_window_pattern&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="long-context">Long context
&lt;/h2>&lt;p>作者将global self-attention的RoPE的base frequency从10K提升到了1M，对于sliding window attention，RoPE的base frequency保持10k不变。&lt;/p>
&lt;h2 id="预训练">预训练
&lt;/h2>&lt;ol>
&lt;li>数据在模型参数里进行了汇总&lt;/li>
&lt;li>tokenizer使用的是Gemini2.0的tokenizer，基于SentencePiece，vocab大小为262K&lt;/li>
&lt;li>使用知识蒸馏的方法进行训练，每个token按照教师模型的概率采样256个logits，然后使用cross-entropy loss进行训练&lt;/li>
&lt;li>4B, 12B, 27B的模型先在32K的context length上进行训练，然后在128K的context length上进行训练&lt;/li>
&lt;/ol>
&lt;h2 id="quantization-aware-training">Quantization Aware training
&lt;/h2>&lt;p>作者提供了quantized版本的模型，模型基于预训练好的模型使用QAT方法进行SFT 5000 steps左右。最后是各个模型的内存占用情况&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Raw (GB)&lt;/th>
&lt;th>&lt;/th>
&lt;th>Quantized (GB)&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Model&lt;/td>
&lt;td>bf16&lt;/td>
&lt;td>Int4&lt;/td>
&lt;td>Int4(blocks=32)&lt;/td>
&lt;td>SFP8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1B&lt;/td>
&lt;td>2.0&lt;/td>
&lt;td>0.5&lt;/td>
&lt;td>0.7&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>+KV&lt;/td>
&lt;td>2.9&lt;/td>
&lt;td>1.4&lt;/td>
&lt;td>1.6&lt;/td>
&lt;td>1.9&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4B&lt;/td>
&lt;td>8.0&lt;/td>
&lt;td>2.6&lt;/td>
&lt;td>2.9&lt;/td>
&lt;td>4.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>+KV&lt;/td>
&lt;td>12.7&lt;/td>
&lt;td>7.3&lt;/td>
&lt;td>7.6&lt;/td>
&lt;td>9.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>12B&lt;/td>
&lt;td>24.0&lt;/td>
&lt;td>6.6&lt;/td>
&lt;td>7.1&lt;/td>
&lt;td>12.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>+KV&lt;/td>
&lt;td>38.9&lt;/td>
&lt;td>21.5&lt;/td>
&lt;td>22.0&lt;/td>
&lt;td>27.3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>27B&lt;/td>
&lt;td>54.0&lt;/td>
&lt;td>14.1&lt;/td>
&lt;td>15.3&lt;/td>
&lt;td>27.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>+KV&lt;/td>
&lt;td>72.7&lt;/td>
&lt;td>32.8&lt;/td>
&lt;td>34.0&lt;/td>
&lt;td>46.1&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="后训练">后训练
&lt;/h2>&lt;p>数据过滤：筛选掉包含PII，不安全的或者有毒的输出等。留下上下文依赖高，幻觉小的数据&lt;/p>
&lt;p>训练包括升级版的知识蒸馏和基于RL的finetuning，其中RL的reward来自于weight averaged reward models, code execution feedback, ground-truth rewards&lt;/p>
&lt;h1 id="实验">实验
&lt;/h1>&lt;h2 id="表现">表现
&lt;/h2>&lt;p>Gemma3的表现如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-gemma3/performance.png"
width="1762"
height="767"
srcset="https://maosong2022.github.io/p/notes-on-gemma3/performance_hu14555874255169857788.png 480w, https://maosong2022.github.io/p/notes-on-gemma3/performance_hu15720531032123846925.png 1024w"
loading="lazy"
alt="performance of Gemma3"
class="gallery-image"
data-flex-grow="229"
data-flex-basis="551px"
>&lt;/p>
&lt;p>与PaliGemma 2的表现对比&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-gemma3/comparison_with_pali_gemma_2.png"
width="843"
height="933"
srcset="https://maosong2022.github.io/p/notes-on-gemma3/comparison_with_pali_gemma_2_hu9558711894193063106.png 480w, https://maosong2022.github.io/p/notes-on-gemma3/comparison_with_pali_gemma_2_hu6898425563427315909.png 1024w"
loading="lazy"
alt="Comparison with PaliGemma 2"
class="gallery-image"
data-flex-grow="90"
data-flex-basis="216px"
>&lt;/p>
&lt;h2 id="消融实验">消融实验
&lt;/h2>&lt;h3 id="local-attention-layers">Local attention layers
&lt;/h3>&lt;ol>
&lt;li>&lt;code>sliding_window_pattern&lt;/code>对模型的表现影响不大&lt;/li>
&lt;li>sliding window size对模型的表现也不是很大，如下图&lt;/li>
&lt;li>使用sliding window attention可以降低KV cache的内存占用&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-gemma3/sliding_window_size.png"
width="865"
height="618"
srcset="https://maosong2022.github.io/p/notes-on-gemma3/sliding_window_size_hu17309206232616813496.png 480w, https://maosong2022.github.io/p/notes-on-gemma3/sliding_window_size_hu10295050076214734954.png 1024w"
loading="lazy"
alt="sliding window size"
class="gallery-image"
data-flex-grow="139"
data-flex-basis="335px"
>&lt;/p>
&lt;h3 id="long-context-ablation">Long context ablation
&lt;/h3>&lt;p>结论为long context会降低模型的性能&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-gemma3/long_context.png"
width="864"
height="887"
srcset="https://maosong2022.github.io/p/notes-on-gemma3/long_context_hu9509380818164822673.png 480w, https://maosong2022.github.io/p/notes-on-gemma3/long_context_hu1683040308469995607.png 1024w"
loading="lazy"
alt="long context"
class="gallery-image"
data-flex-grow="97"
data-flex-basis="233px"
>&lt;/p>
&lt;h3 id="知识蒸馏">知识蒸馏
&lt;/h3>&lt;p>训练token个数比较少的时候，使用小的教师模型效果更好；训练token个数比较多的时候，使用大的教师模型效果更好。&lt;/p>
&lt;h3 id="pan--scan">Pan &amp;amp; Scan
&lt;/h3>&lt;p>使用图片原始的aspect ratio训练的模型效果更好。&lt;/p>
&lt;h3 id="memorization">memorization
&lt;/h3>&lt;p>memorization指模型输出的文本与训练数据中文本的重复率。结果发现Gemma3的memorization要更低一些。&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-gemma3/memorization.png"
width="865"
height="824"
srcset="https://maosong2022.github.io/p/notes-on-gemma3/memorization_hu3042760849725965340.png 480w, https://maosong2022.github.io/p/notes-on-gemma3/memorization_hu3418502301793720798.png 1024w"
loading="lazy"
alt="memorization"
class="gallery-image"
data-flex-grow="104"
data-flex-basis="251px"
>&lt;/p>
&lt;h1 id="结论">结论
&lt;/h1>&lt;ol>
&lt;li>sliding window attention在Qwen2.5-VL里已经验证过有效,这里在Gemma3上同样验证了有效性.&lt;/li>
&lt;li>模型架构与PaliGemma系列基本一致，只是attention改变了，然后LLM从Gemma 2升级到了Gemma 3。&lt;/li>
&lt;/ol>
&lt;h1 id="参考文献">参考文献
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf" target="_blank" rel="noopener"
>Gemma3 Technical Report&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/huggingface/transformers/tree/main/src/transformers/models/gemma3" target="_blank" rel="noopener"
>transformers-Gemma3 code&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Overview of Qwen-VL series</title><link>https://maosong2022.github.io/p/overview-of-qwen-vl-series/</link><pubDate>Sun, 09 Mar 2025 15:11:29 +0800</pubDate><guid>https://maosong2022.github.io/p/overview-of-qwen-vl-series/</guid><description>&lt;h1 id="timeline">Timeline
&lt;/h1>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>时间&lt;/th>
&lt;th>模型系列&lt;/th>
&lt;th>Feature&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>2023.08&lt;/td>
&lt;td>Qwen-VL&lt;/td>
&lt;td>Vision-centric understanding&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>Multi-lingual&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>Multi-image&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>Fine-grained visual understanding&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2024.09&lt;/td>
&lt;td>Qwen2-VL&lt;/td>
&lt;td>Image understanding&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>Video understanding (20min)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>Agent capability&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>Multilingual support&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2025.02&lt;/td>
&lt;td>Qwen2.5-VL&lt;/td>
&lt;td>Document parsing&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>Object grounding&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>Long video understadning and grouding (1 hour)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>Agent functionality&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h1 id="models">Models
&lt;/h1>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>name&lt;/th>
&lt;th>size&lt;/th>
&lt;th>Vision&lt;/th>
&lt;th>Adapter&lt;/th>
&lt;th>LLM&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>1.0&lt;/td>
&lt;td>Qwen-VL&lt;/td>
&lt;td>9.6B&lt;/td>
&lt;td>ViT(1.9B)&lt;/td>
&lt;td>Cross-attention(0.08B)&lt;/td>
&lt;td>Qwen-LLM(7B)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2.0&lt;/td>
&lt;td>Qwen2-VL-2B&lt;/td>
&lt;td>2B&lt;/td>
&lt;td>ViT(675M)&lt;/td>
&lt;td>MLP(0.0341B)&lt;/td>
&lt;td>Qwen2-LLM(1.5B)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Qwen2-VL-7B&lt;/td>
&lt;td>7B&lt;/td>
&lt;td>&lt;/td>
&lt;td>MLP(0.0446B)&lt;/td>
&lt;td>Qwen2-LLM(7.6B)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Qwen2-VL-72B&lt;/td>
&lt;td>72B&lt;/td>
&lt;td>&lt;/td>
&lt;td>MLP(0.0682B)&lt;/td>
&lt;td>Qwen2-LLM(72B)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2.5&lt;/td>
&lt;td>Qwen2.5-VL-3B&lt;/td>
&lt;td>3B&lt;/td>
&lt;td>ViT(675M)&lt;/td>
&lt;td>MLP(0.1817B)&lt;/td>
&lt;td>Qwen2.5-LLM(3B)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Qwen2.5-VL-7B&lt;/td>
&lt;td>7B&lt;/td>
&lt;td>&lt;/td>
&lt;td>MLP(0.3179B)&lt;/td>
&lt;td>Qwen2.5-LLM(7.6B)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Qwe2.5n-VL-72B&lt;/td>
&lt;td>72B&lt;/td>
&lt;td>&lt;/td>
&lt;td>MLP(0.7267B)&lt;/td>
&lt;td>Qwen2.5-LLM(72B)&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Remark&lt;/p>
&lt;ul>
&lt;li>Qwen2-VL的MLP使用的是LayerNorm&lt;/li>
&lt;li>Qwen2.5-VL的MLP使用的是RMSNorm&lt;/li>
&lt;li>出了Qwen-VL之外，所有模型均有对应的Instruct版本，这里为了方便没有列出。Qwen-VL对应的是Qwen-VL-chat。&lt;/li>
&lt;/ul>
&lt;h1 id="data--training">Data &amp;amp; training
&lt;/h1>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model series&lt;/th>
&lt;th>stage&lt;/th>
&lt;th>data&lt;/th>
&lt;th>Vision&lt;/th>
&lt;th>Adapter&lt;/th>
&lt;th>LLM&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Qwen-VL&lt;/td>
&lt;td>pretraining&lt;/td>
&lt;td>1.4B&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Multi-task pretraining&lt;/td>
&lt;td>96.8M&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>SFT&lt;/td>
&lt;td>350K&lt;/td>
&lt;td>&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen2-VL&lt;/td>
&lt;td>pretraining&lt;/td>
&lt;td>600b tokens&lt;/td>
&lt;td>✅？&lt;/td>
&lt;td>✅？&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Multi-task pretraining&lt;/td>
&lt;td>800b tokens&lt;/td>
&lt;td>✅？&lt;/td>
&lt;td>✅？&lt;/td>
&lt;td>✅？&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>SFT&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>✅？&lt;/td>
&lt;td>✅？&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen2.5-VL&lt;/td>
&lt;td>Visual Pre-Training&lt;/td>
&lt;td>1.5T token&lt;/td>
&lt;td>✅？&lt;/td>
&lt;td>✅？&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Multimodal Pre-Training&lt;/td>
&lt;td>2T token&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Long-Context Pre-Training&lt;/td>
&lt;td>0.6T token&lt;/td>
&lt;td>✅？&lt;/td>
&lt;td>✅？&lt;/td>
&lt;td>✅？&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>SFT&lt;/td>
&lt;td>2M samples&lt;/td>
&lt;td>&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>DPO&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Remark&lt;/p>
&lt;ol>
&lt;li>Qwen2-VL按照技术报告的说法是follow了Qwen-VL的训练方式，因此这里也使用了同样的表示&lt;/li>
&lt;li>带问号的地方代表不确定，为个人猜测。请谨慎参考。&lt;/li>
&lt;li>难以确定的原因是Qwen-VL系列将projection layer和ViT作为一个模块，因此比较难区分是否训练了projection layer&lt;/li>
&lt;/ol>
&lt;p>技术报告的训练框架图&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/overview-of-qwen-vl-series/qwen-vl-training.png"
width="1908"
height="868"
srcset="https://maosong2022.github.io/p/overview-of-qwen-vl-series/qwen-vl-training_hu10403710283858859645.png 480w, https://maosong2022.github.io/p/overview-of-qwen-vl-series/qwen-vl-training_hu10541227880671039729.png 1024w"
loading="lazy"
alt="qwen-vl-training"
class="gallery-image"
data-flex-grow="219"
data-flex-basis="527px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/overview-of-qwen-vl-series/qwen2-5-training.png"
width="1354"
height="468"
srcset="https://maosong2022.github.io/p/overview-of-qwen-vl-series/qwen2-5-training_hu9762734632441640035.png 480w, https://maosong2022.github.io/p/overview-of-qwen-vl-series/qwen2-5-training_hu16613519819175232642.png 1024w"
loading="lazy"
alt="qwen2-5-training"
class="gallery-image"
data-flex-grow="289"
data-flex-basis="694px"
>&lt;/p>
&lt;p>References&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2308.12966" target="_blank" rel="noopener"
>Qwen-VL&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2409.12191" target="_blank" rel="noopener"
>Qwen2-VL&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2502.13923" target="_blank" rel="noopener"
>Qwen2.5-VL&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on QwQ-32B</title><link>https://maosong2022.github.io/p/notes-on-qwq-32b/</link><pubDate>Sat, 08 Mar 2025 09:46:16 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-qwq-32b/</guid><description>&lt;h1 id="introduction">Introduction
&lt;/h1>&lt;p>通义团队在3月6号发布了QwQ-32B，一个基于RL的，32B参数的reasoning model，QwQ-32B的表现可以与DeepSeek-R1相比&lt;/p>
&lt;h1 id="模型架构">模型架构
&lt;/h1>&lt;p>QwQ-32B基于Qwen2.5-32B开发。主要通过基于outcome-based rewards的RL进行训练。训练过程包括两个stage&lt;/p>
&lt;ol>
&lt;li>Stage 1：本阶段在math和coding domain上进行RL的训练，作者使用了一个verifier来保证最终数学问题结果的正确性，使用了一个code execution server来保证最终生成代码的正确性。&lt;/li>
&lt;li>Stage 2：本阶段在通用领域上进行RL的训练。模型基于general reward model和一些rule-based verifier进行训练。应该是类似DeepSeek-R1的规则。&lt;/li>
&lt;/ol>
&lt;h1 id="实验结果">实验结果
&lt;/h1>&lt;p>实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-qwq-32b/qwq-32b-final.jpg"
width="3035"
height="1713"
srcset="https://maosong2022.github.io/p/notes-on-qwq-32b/qwq-32b-final_hu8354107095607630009.jpg 480w, https://maosong2022.github.io/p/notes-on-qwq-32b/qwq-32b-final_hu6845858612094320689.jpg 1024w"
loading="lazy"
alt="QwQ_evaluation"
class="gallery-image"
data-flex-grow="177"
data-flex-basis="425px"
>&lt;/p>
&lt;h1 id="reference">Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://qwenlm.github.io/blog/qwq-32b/" target="_blank" rel="noopener"
>blog&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://chat.qwen.ai/?models=Qwen2.5-Plus" target="_blank" rel="noopener"
>demo&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>compression is intelligence</title><link>https://maosong2022.github.io/p/compression-is-intelligence/</link><pubDate>Thu, 06 Mar 2025 17:57:51 +0800</pubDate><guid>https://maosong2022.github.io/p/compression-is-intelligence/</guid><description>&lt;p>我们知道，基于decoder-only transformer的LLM的训练目标是最小化next-token-prediction loss，即给定sequence $x=(x_1,\dots, x_n)\in D$，我们的目标为求解以下优化问题&lt;/p>
$$
\min_{\theta} -\sum_{x\in D}\log P_{\theta}(x_i|x_1,\dots,x_{i-1})
$$&lt;p>这里 $\theta$ 就是我们的模型参数，$D$ 是我们的训练数据集。&lt;/p>
&lt;p>无数模型通过实际效果告诉我们，这个优化目标可以很好地训练出具有良好泛化能力的大语言模型。但是，我们的问题是，为什么这个优化目标可以训练出智能的模型？ 本文将从压缩即智能的角度来理解这个问题。&lt;/p>
&lt;h1 id="压缩即智能">压缩即智能
&lt;/h1>&lt;h2 id="一个例子">一个例子
&lt;/h2>&lt;p>我们首先来看一个简单的例子。给定如下三个0-1字符串：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">01010101010101010101
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">01001000100001000001
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">01101000101010100101
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>我们该如何描述这三个字符串的规律？显然，第一个字符串最简单，它是&lt;code>01&lt;/code>字符串重复得到的结果；第二个字符串稍微复杂一些，它在每个&lt;code>1&lt;/code>之前插入重复次数的&lt;code>0&lt;/code>；第三个字符串则最复杂，它是我随手写的一个字符串，基本没有任何规律，因此，我们只能直接存储这个字符串。&lt;/p>
&lt;p>这个例子告诉我们，一个字符串的规律越简单，我们越容易描述它，因此，我们越容易压缩它。实际上，大语言模型做的也是类似的事情。它们的核心思想是，压缩即智能。&lt;/p>
&lt;h2 id="heading">
&lt;/h2>&lt;h1 id="结论">结论
&lt;/h1>&lt;p>本文中，我们从压缩即智能的角度来理解大语言模型的原理。我们发现，大语言模型的next-token-prediction其实就是压缩。我们通过压缩让大语言模型学习到了语言中的规律，从而让模型具有了智能。&lt;/p>
&lt;h1 id="参考文献">参考文献
&lt;/h1></description></item><item><title>Notes on Qwen2.5 VL</title><link>https://maosong2022.github.io/p/notes-on-qwen2.5-vl/</link><pubDate>Tue, 04 Mar 2025 10:46:42 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-qwen2.5-vl/</guid><description>&lt;h1 id="introduction">Introduction
&lt;/h1>&lt;p>2025年2月20号Qwen团队发布了Qwen2.5 VL技术报告，Qwen2.5 VL包括3B，7B， 72B三个size。Qwen2.5-VL主要在架构，数据上进行了改进。通过评测，Qwen2.5-VL在多个benchmark上取得了SOTA。&lt;/p>
&lt;p>Qwen2.5 VL认为已有模型的缺点为：&lt;/p>
&lt;ul>
&lt;li>计算复杂度高&lt;/li>
&lt;li>上下文理解能力有限&lt;/li>
&lt;li>细粒度visual perception能力不足&lt;/li>
&lt;li>在不同上下文长度下表现不一致&lt;/li>
&lt;/ul>
&lt;p>Qwen2.5 VL的贡献为：&lt;/p>
&lt;ol>
&lt;li>使用了一个从零开始训练的ViT作为vision encoder，并且在ViT中使用了window attention，来提高计算效率&lt;/li>
&lt;li>使用了dynamic FPS sampling，用于处理不同采样率的视频输入&lt;/li>
&lt;li>将MRoPE扩展到了temporal domain上，进一步提高了模型在与时间相关任务上的表现&lt;/li>
&lt;li>使用了更高质量的数据集，其中预训练阶段使用了4.1T的token&lt;/li>
&lt;/ol>
&lt;p>Qwen2.5 VL的主要亮点为：&lt;/p>
&lt;ul>
&lt;li>优秀的document parsing能力&lt;/li>
&lt;li>精确的object grounding能力&lt;/li>
&lt;li>针对长视频的理解和细粒度grounding能力&lt;/li>
&lt;li>针对UI的agent functionality&lt;/li>
&lt;/ul>
&lt;h1 id="模型架构">模型架构
&lt;/h1>&lt;h2 id="总览">总览
&lt;/h2>&lt;p>Qwen2.5 VL和Qwen2 VL的架构基本一致，包括Vision Encoder，Language Encoder，以及projector layer三个部分，其架构图如下：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-qwen2.5-vl/architecture.png"
width="1345"
height="889"
srcset="https://maosong2022.github.io/p/notes-on-qwen2.5-vl/architecture_hu17125015043072785911.png 480w, https://maosong2022.github.io/p/notes-on-qwen2.5-vl/architecture_hu16983634930428889570.png 1024w"
loading="lazy"
alt="Qwen2.5 VL架构图"
class="gallery-image"
data-flex-grow="151"
data-flex-basis="363px"
>&lt;/p>
&lt;ul>
&lt;li>LLM： 使用Qwen2.5 LLM作为LLM，并且将1D-RoPE升级为了MRoPE&lt;/li>
&lt;li>Vision Encoder： 使用一个从零开始训练的ViT架构，patch_size为14，position embedding为2D-RoPE， attention为window attention和self-attention的混合，其中，只有四层使用的是self-attention.对于输入的图片，ViT会将图片resize到28的整数倍。&lt;/li>
&lt;li>Projector Layer： 使用的是一个两层的MLP&lt;/li>
&lt;/ul>
&lt;p>模型的参数配置如下图&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-qwen2.5-vl/model_config.png"
width="1180"
height="861"
srcset="https://maosong2022.github.io/p/notes-on-qwen2.5-vl/model_config_hu6133933021768467739.png 480w, https://maosong2022.github.io/p/notes-on-qwen2.5-vl/model_config_hu685888814144466819.png 1024w"
loading="lazy"
alt="Qwen2.5 VL模型参数配置"
class="gallery-image"
data-flex-grow="137"
data-flex-basis="328px"
>&lt;/p>
&lt;h2 id="vision-encoder">Vision Encoder
&lt;/h2>&lt;p>Vision encoder的主要改进点为：&lt;/p>
&lt;ol>
&lt;li>使用了window attention来提升计算效率，window attention的size为 $112\times 112$， 对应为 $8\times 8$ 个patch. 这样做的好处是可以不用对图片做scaling&lt;/li>
&lt;li>使用了2D RoPE来捕捉空间信息，使用3D RoPE来捕捉视频输入的时间信息&lt;/li>
&lt;li>与LLM的结构进行对齐，包括使用RMSNorm替换LayerNorm，使用SwiGLU替换ReLU&lt;/li>
&lt;/ol>
&lt;h2 id="输入处理">输入处理
&lt;/h2>&lt;p>对于图片输入，Qwen2.5 VL使用原始图片的空间信息来构建坐标，而不是将坐标normalize到[0, 1000]之间，这样让模型可以处理不同精度的图片输入。&lt;/p>
&lt;p>对于视频输入，因为使用了3D RoPE，因此Qwen2.5 VL可以处理不同帧率的视频输入，这样就避免了不同帧率视频对模型视频理解带来的影响。这一点和Apollo里的想法是一样的。具体来说，Qwen2.5 VL首先将连续的两帧group到了一起，然后使用了temporal ID来将position和视频所对应的时间进行对齐。这里可以看Qwen2.5 VL的代码：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Temporal (Time): 3 patches, representing different segments of the video in time.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Height: 2 patches, dividing each frame vertically.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Width: 2 patches, dividing each frame horizontally.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">We also have some important parameters:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">fps (Frames Per Second): The video&amp;#39;s frame rate, set to 1. This means one frame is processed each second.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">tokens_per_second: This is a crucial parameter. It dictates how many &amp;#34;time-steps&amp;#34; or &amp;#34;temporal tokens&amp;#34; are conceptually packed into a one-second interval of the video. In this case, we have 25 tokens per second. So each second of the video will be represented with 25 separate time points. It essentially defines the temporal granularity.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">temporal_patch_size: The number of frames that compose one temporal patch. Here, it&amp;#39;s 2 frames.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">interval: The step size for the temporal position IDs, calculated as tokens_per_second * temporal_patch_size / fps. In this case, 25 * 2 / 1 = 50. This means that each temporal patch will be have a difference of 50 in the temporal position IDs.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">input_ids: [V V V V V V V V V V V V T T T T T], here V is for vision.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">vision temporal position_ids: [0, 0, 0, 0, 50, 50, 50, 50, 100, 100, 100, 100]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">vision height position_ids: [0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">vision width position_ids: [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">text temporal position_ids: [101, 102, 103, 104, 105]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">text height position_ids: [101, 102, 103, 104, 105]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">text width position_ids: [101, 102, 103, 104, 105]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Here we calculate the text start position_ids as the max vision position_ids plus 1.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>这里&lt;code>fps&lt;/code>为1，表示每一帧对应一秒，&lt;code>tokens_per_second&lt;/code>为25，表示每秒包含25个token，&lt;code>temporal_patch_size&lt;/code>为2，表示每个temporal patch包含2个frame。因此一个patch里面，就包含了2frame, 对应50tokens. 然后前面提到连续两帧会被group到一起，因此每个temporal patch对应4个spatial patches. 其position_ids为：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="p">[(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="训练">训练
&lt;/h1>&lt;h2 id="预训练">预训练
&lt;/h2>&lt;h3 id="数据">数据
&lt;/h3>&lt;p>预训练阶段使用了4.1T的token，包括image captions, interleaved image-text data, optical character recognition (OCR) data, visual knowledge (e.g., celebrity, landmark, flora, and fauna identification), multi-modal academic questions, localization data, document parsing data, video descriptions, video localization, and agent-based interaction data. 作者详细介绍了以下几种数据：&lt;/p>
&lt;ul>
&lt;li>Interleaved image-text data： 主要1. 提高模型的上下文学习能力；2. 保持模型的text-only能力；3.包含一些通用信息。数据清洗包括：1. 基于text-only quality过滤； 2. 基于image-text相关性过滤；3. 基于image-text互补程度过滤；4.基于information density balance过滤.&lt;/li>
&lt;li>Grounding data：作者使用了Grounding DINO, SAM等模型来生成一些grounding data.为了提升模型在open-vocabulary detection上的能力，作者将训练数据集扩展到了1万个object category上，作者还使用了一些point-based object grounding data来提升模型的能力&lt;/li>
&lt;li>Document Parsing data：作者基于text,image, music sheets和chemical formulas合成了一些HTML格式的数据，然后根据tag和bounding box来提升模型document parsing的能力&lt;/li>
&lt;li>OCR data：作者使用了开源，合成和in-house的数据集，数据集主要提到multi-lingual以及1M的chart-type数据&lt;/li>
&lt;li>Video data：作者通过pipeline构建了long video caption来提升模型长视频的理解能力&lt;/li>
&lt;li>Agent data：作者收集了一些mobile device和网页的screenshots,然后基于agent框架来合成控制轨迹&lt;/li>
&lt;/ul>
&lt;h3 id="训练-1">训练
&lt;/h3>&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-qwen2.5-vl/pretraining.png"
width="1217"
height="428"
srcset="https://maosong2022.github.io/p/notes-on-qwen2.5-vl/pretraining_hu13120081010368224081.png 480w, https://maosong2022.github.io/p/notes-on-qwen2.5-vl/pretraining_hu851622592674460936.png 1024w"
loading="lazy"
alt="Qwen2.5 VL预训练阶段"
class="gallery-image"
data-flex-grow="284"
data-flex-basis="682px"
>&lt;/p>
&lt;p>如上图所示，Qwen2.5 VL的预训练阶段包括了三个阶段：&lt;/p>
&lt;ol>
&lt;li>Visual Pretraining:这一阶段使用了image-caption, knowledge, OCR数据集，旨在提升ViT提取视觉特征的能力&lt;/li>
&lt;li>multimodal pretraining:这一阶段在第一阶段的基础上增加了pure-text, interleaved data, VQA, Video grounding, agent data, 旨在提升模型处理复杂视觉信息的能力&lt;/li>
&lt;li>long-context pretraining: 这一阶段，在第二阶段的基础上，增加了long video, long agent, long document data，旨在提升模型处理长上下文的能力&lt;/li>
&lt;/ol>
&lt;h2 id="后训练">后训练
&lt;/h2>&lt;h3 id="数据-1">数据
&lt;/h3>&lt;p>SFT阶段使用大概2M的样本进行训练，其中纯文本和多模态数据占比为1:1，语言主要是中文和英文。&lt;/p>
&lt;p>为了保证数据质量，作者提供了一个数据清洗的pipeline，包括：&lt;/p>
&lt;ol>
&lt;li>domain-specific categorization: 作者基于Qwen2-VL-72B构建了Qwen2-VL-Instag，用于将QA pair分为8个大类别，30个小类别&lt;/li>
&lt;li>Domain-tailed filtering:作者使用了rule-based和model-based方法来提升数据的质量
&lt;ul>
&lt;li>rule-based filtering: 重复性检测，格式检测等&lt;/li>
&lt;li>model-based filtering: 使用基于Qwen2.5-VL训练的reward model来从多个维度评估QA pair的质量&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>为了进一步提升模型的推理能力，作者还是用了rejection sampling来refine 数据集。&lt;/p>
&lt;h3 id="训练-2">训练
&lt;/h3>&lt;p>post-training阶段分为SFT和DPO两个小阶段，这个阶段都会冻结VIT. SFT阶段使用大多数的训练数据，而DPO阶段专注于image-text data和pure text data，以更好地进行对齐。具体做法就是基于Grounding truth，使用checkpoint来评估数据的质量，然后只保留答案正确的数据来训练。&lt;/p>
&lt;h1 id="评测">评测
&lt;/h1>&lt;ol>
&lt;li>通用VQA
&lt;img src="https://maosong2022.github.io/p/notes-on-qwen2.5-vl/general_vqa.png"
width="1356"
height="656"
srcset="https://maosong2022.github.io/p/notes-on-qwen2.5-vl/general_vqa_hu892650520576289201.png 480w, https://maosong2022.github.io/p/notes-on-qwen2.5-vl/general_vqa_hu9870708508441533774.png 1024w"
loading="lazy"
alt="通用VQA"
class="gallery-image"
data-flex-grow="206"
data-flex-basis="496px"
>&lt;/li>
&lt;li>文档理解和OCR
&lt;img src="https://maosong2022.github.io/p/notes-on-qwen2.5-vl/document_understanding.png"
width="1358"
height="655"
srcset="https://maosong2022.github.io/p/notes-on-qwen2.5-vl/document_understanding_hu17694059179260147209.png 480w, https://maosong2022.github.io/p/notes-on-qwen2.5-vl/document_understanding_hu7262732638369499565.png 1024w"
loading="lazy"
alt="文档理解和OCR"
class="gallery-image"
data-flex-grow="207"
data-flex-basis="497px"
>&lt;/li>
&lt;li>空间理解
&lt;img src="https://maosong2022.github.io/p/notes-on-qwen2.5-vl/grounding.png"
width="1324"
height="714"
srcset="https://maosong2022.github.io/p/notes-on-qwen2.5-vl/grounding_hu5967843440894939227.png 480w, https://maosong2022.github.io/p/notes-on-qwen2.5-vl/grounding_hu13144193697505905228.png 1024w"
loading="lazy"
alt="空间理解"
class="gallery-image"
data-flex-grow="185"
data-flex-basis="445px"
>&lt;/li>
&lt;li>视频理解和Grounding
&lt;img src="https://maosong2022.github.io/p/notes-on-qwen2.5-vl/video.png"
width="1320"
height="672"
srcset="https://maosong2022.github.io/p/notes-on-qwen2.5-vl/video_hu2773206847940642085.png 480w, https://maosong2022.github.io/p/notes-on-qwen2.5-vl/video_hu9652119302145499627.png 1024w"
loading="lazy"
alt="视频理解和Grounding"
class="gallery-image"
data-flex-grow="196"
data-flex-basis="471px"
>&lt;/li>
&lt;li>Agent
&lt;img src="https://maosong2022.github.io/p/notes-on-qwen2.5-vl/agent.png"
width="1374"
height="337"
srcset="https://maosong2022.github.io/p/notes-on-qwen2.5-vl/agent_hu14999408161773977095.png 480w, https://maosong2022.github.io/p/notes-on-qwen2.5-vl/agent_hu8333246447475526185.png 1024w"
loading="lazy"
alt="Agent"
class="gallery-image"
data-flex-grow="407"
data-flex-basis="978px"
>&lt;/li>
&lt;/ol>
&lt;h1 id="reference">Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://github.com/QwenLM/Qwen2.5-VL" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2502.13923" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Git authentication error</title><link>https://maosong2022.github.io/p/git-authentication-error/</link><pubDate>Sat, 22 Feb 2025 10:51:27 +0800</pubDate><guid>https://maosong2022.github.io/p/git-authentication-error/</guid><description>&lt;h1 id="问题描述">问题描述
&lt;/h1>&lt;p>在使用 &lt;code>git clone&lt;/code> 命令时，可能会遇到认证错误。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&amp;gt; git clone https://github.com/user-name/some-repository.git
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">正克隆到 &lt;span class="s1">&amp;#39;some-repository&amp;#39;&lt;/span>...
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">remote: Invalid username or password.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">fatal: &lt;span class="s1">&amp;#39;https://github.com/user-name/some-repository.git/&amp;#39;&lt;/span> 鉴权失败
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="问题原因解决方案">问题原因&amp;amp;解决方案
&lt;/h1>&lt;p>本地git配置问题，需要重新认证本地和GitHub的连接。一般是因为本地Git无法和Github的SSH服务器连接。&lt;/p>
&lt;h2 id="方案1ssh">方案1：SSH
&lt;/h2>&lt;p>本方法将Github视为一个SSH服务器，本机通过SSH连接到Github。该方法主要需要在本地生成SSH密钥，并将其添加到Github的SSH密钥列表中。最后通过SSH的方式进行Git操作&lt;/p>
&lt;ol>
&lt;li>本地生成SSH秘钥&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">ssh-keygen
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>默认的秘钥地址是&lt;code>~/.ssh/id_rsa&lt;/code>，对应的公钥就是&lt;code>~/.ssh/id_rsa.pub&lt;/code>。如果需要指定秘钥地址，可以使用&lt;code>-f&lt;/code>选项，对于指定的秘钥地址，需要通过以下命令将秘钥地址加入到搜索列表中.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 1. Start the SSH agent&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&amp;gt; &lt;span class="nb">eval&lt;/span> &lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="k">$(&lt;/span>ssh-agent -s&lt;span class="k">)&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 2. Add your custom key file&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&amp;gt; ssh-add path/to/id_rsa
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ol start="2">
&lt;li>
&lt;p>将公钥添加到Github的SSH密钥列表中。使用 &lt;code>cat ~/.ssh/id_rsa.pub&lt;/code> 复制公钥，然后打开&lt;code>Github-&amp;gt;设置-&amp;gt;SSH and GPG keys-&amp;gt;New SSH key&lt;/code>，将公钥粘贴到&lt;code>Key&lt;/code>中，然后点击&lt;code>Add SSH key&lt;/code>。将公钥&lt;code>Key&lt;/code>中并保存。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>本机通过SSH连接到Github，可以通过以下方式验证&lt;/p>
&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&amp;gt; ssh -T git@github.com
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># ssh -vT git@github.com # 查看ssh连接的秘钥文件搜索列表&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># ssh -i ~/.ssh/id_rsa git@github.com # 自定义密钥的地址，如果使用默认的密钥地址，则不需要指定密钥地址，如果之前已经加入到搜索列表中，则也不需要指定密钥地址&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Hi user-name! You&lt;span class="err">&amp;#39;&lt;/span>ve successfully authenticated, but GitHub does not provide shell access.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ol start="4">
&lt;li>克隆仓库：在clone的时候选择&lt;code>SSH&lt;/code>的方式（不是&lt;code>HTTPS&lt;/code>的方式），即repo的地址为&lt;code>git@github.com:user-name/some-repository.git&lt;/code>&lt;/li>
&lt;li>设置remote repo：&lt;code>git remote set-url origin git@github.com:user-name/some-repository.git&lt;/code>&lt;/li>
&lt;/ol>
&lt;p>通过以上步骤，即可正常使用Git进行操作。&lt;/p>
&lt;h2 id="方案2personal-access-token">方案2：Personal Access Token
&lt;/h2>&lt;p>该方法通过生成一个Personal Access Token来认证本地和GitHub的连接。该方法需要在Github的设置中生成一个Personal Access Token，然后通过该Token来连接remote repo&lt;/p>
&lt;ol>
&lt;li>生成Personal Access Token. 打开&lt;code>Github-&amp;gt;设置-&amp;gt;Developer settings-&amp;gt;Personal access tokens-&amp;gt;Token(classic)-&amp;gt;Generate new token&lt;/code>，在&lt;code>Note&lt;/code>中输入本机相关信息，在&lt;code>Expiration&lt;/code>中选择token失效日期，在&lt;code>Permissions&lt;/code>中选择&lt;code>repo&lt;/code>（必选，其他可选），然后点击&lt;code>Generate token&lt;/code>。将生成的&lt;code>&amp;lt;token&amp;gt;&lt;/code>复制到本地。&lt;/li>
&lt;li>克隆仓库：&lt;code>git clone https://user-name:&amp;lt;token&amp;gt;@github.com/user-name/some-repository.git&lt;/code>&lt;/li>
&lt;li>设置remote repo：&lt;code>git remote set-url origin https://user-name:&amp;lt;token&amp;gt;@github.com/user-name/some-repository.git&lt;/code>&lt;/li>
&lt;/ol>
&lt;p>通过以上步骤，即可正常使用Git进行操作。&lt;/p>
&lt;h1 id="参考资料">参考资料
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://docs.github.com/en/authentication/troubleshooting-ssh/error-permission-denied-publickey" target="_blank" rel="noopener"
>Error: Permission denied (publickey)
&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens" target="_blank" rel="noopener"
>Managing your personal access tokens
&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Kimi k1.5</title><link>https://maosong2022.github.io/p/notes-on-kimi-k1.5/</link><pubDate>Sat, 08 Feb 2025 10:09:52 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-kimi-k1.5/</guid><description>&lt;p>论文提出了Kimi k1.5， 一个基于强化学习训练的多模态推理模型。作者介绍了Kimi k1.5的训练方法，以及infra上的优化。作者的主要贡献如下：&lt;/p>
&lt;ol>
&lt;li>作者发现，模型的上下文长度可以有效提升LLM的推理能力。&lt;/li>
&lt;li>作者提出了基于online mirror descent的强化学习训练方法。&lt;/li>
&lt;li>作者发现可以通过long context scaling和policy optimization来提升模型的推理能力。&lt;/li>
&lt;li>作者提出了long2short的推理方法，可以有效提升short CoT模型的推理能力。&lt;/li>
&lt;/ol>
&lt;h2 id="architecture">Architecture
&lt;/h2>&lt;p>论文中包括两个模型，一个是k1.5 base model， 其是一个多模态大模型。另一个是Kimi-1.5 reasoning model（简称为k1.5），k1.5是基于kimi-1.5 base model， 通过强化学习训练得到的推理模型。&lt;/p>
&lt;h3 id="kimi-15-base-model">Kimi-1.5 base model
&lt;/h3>&lt;p>k1.5 base model是一个基于transformer的多模态大模型，论文没有给出详细的模型结构，只有如下的示意图&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-kimi-k1.5/k1-5-model-arch.png"
width="1210"
height="287"
srcset="https://maosong2022.github.io/p/notes-on-kimi-k1.5/k1-5-model-arch_hu1937206545208930311.png 480w, https://maosong2022.github.io/p/notes-on-kimi-k1.5/k1-5-model-arch_hu10349051156390148271.png 1024w"
loading="lazy"
alt="Architecture of k1.5 base model"
class="gallery-image"
data-flex-grow="421"
data-flex-basis="1011px"
>&lt;/p>
&lt;p>k1.5 base model的训练包括三个阶段：&lt;/p>
&lt;ol>
&lt;li>Vision-language pretraining stage: 这一阶段的目的是让模型拥有语言和视觉的表征能力。训练时，模型先在纯文本的数据上进行训练，然后提高加入图文交错的数据进行训练，直到训练数据中图文交错的数据占比达到30%。模型的更新方式为先冻结LLM更新visual tower, 然后解冻LLM更新所有参数。&lt;/li>
&lt;li>Vision-language cooldown stage: 这一阶段的目的是让模型保持多模态理解能力。作者发现，使用合成数据可以提高模型在这一阶段的表现。因此，作者使用闭源大语言模型，基于math, knowledge和code domain的pretraining data来合成了一些QA pair数据对，然后讲这些QA pair数据对加入到训练数据中。&lt;/li>
&lt;li>Long-context activation stage: 这一阶段的目的是让模型在长上下文的数据上进行训练，以提升模型在长上下文上的理解能力。这一阶段包含了40%的full attention data以及60%的partial attention data。其中，full attention data是基于真实数据和合成的QA以及summary数据得到的，partial attention data是在cooldown data找那个通过均匀采样得到的。这一阶段，模型的上下文长度从4096逐步增加到131072。&lt;/li>
&lt;/ol>
&lt;h3 id="kimi-k15">Kimi k1.5
&lt;/h3>&lt;p>Kimi k1.5是基于k1.5 base model， 通过进一步训练得到的推理模型。k1.5的训练包括四个阶段：&lt;/p>
&lt;ol>
&lt;li>pretraining：这一步就是k1.5 base model的训练， 包括三个小阶段。前面已经介绍过了。&lt;/li>
&lt;li>vanilla SFT：这一步的目的是使得k1.5 base model具有指令跟随能力。对于non-reasoning任务，作者先使用人类标注产生一个seed dataset，然后基于seed dataset训练一个seed model，最后根据收集的prompt来使用seed model来生成回答。对于reasoning任务，作者使用了rejection sampling来扩展SFT数据集&lt;/li>
&lt;li>long-CoT SFT： 这一步的目的是让模型能够像人类一样进行推理，能够掌握最基本的推理策略，即：planning，evaluation，reflection和exploration。从而为RL训练提供一个良好的初始化。&lt;/li>
&lt;li>RL：这一步就是通过RL来提高模型的推理能力。我们在下一节中进行详细的介绍。&lt;/li>
&lt;/ol>
&lt;h2 id="rl">RL
&lt;/h2>&lt;h3 id="problem-definition">Problem Definition
&lt;/h3>&lt;p>给定一个训练数据集 $\mathcal{D} = {(x_i, y_i^\star)}_ {i=1}^n$， 其中 $x_ i$ 是问题， $y_{i}^\star$ 是ground truth。我们希望找到一个模型 $\pi_{\theta}$ ，来解决这个问题。通常问题比较难，因此我们使用chain of thought（CoT）的方法来解决这个问题。具体做法就是让模型输出中间步骤 $z=(z_1, z_2, &amp;hellip;, z_m)$， 来连接问题 $x_i$ 和答案 $y$。其中， $z_j$ 是模型在第 $j$ 步的推理结果，即 $z_t\sim\pi_{\theta}(x_i, z_{&amp;lt;t})$, $y\sim \pi_{\theta}(x_i, z)$。&lt;/p>
&lt;p>通常，我们还会使用一个reward model $r_{\phi}$，来评估模型输出的答案的质量。一个常用的reward model是基于答案的正确性来评估的，即 $r_{\phi}(x_i, y, y^\star) = \mathbb{I}[y=y_i^\star]$ 。这样，我们要求解的问题就变成了最大化中间步骤和最终答案的reward，即&lt;/p>
$$
\max_{\theta} \mathbb{E}_ {(x, y^\star)\sim\mathcal{D},(y,z,)\sim\pi_{\theta}} r_{\phi}(x, y, y^\star)
$$&lt;h3 id="policy-optimization">Policy Optimization
&lt;/h3>&lt;p>作者使用了online policy mirror descent来解决上面提到的优化问题。在每个iteration中，存在一个reference model $\pi_{\theta_r}$ ， 以及一个当前要更新的模型 $\pi_{\theta}$ 。online policy mirror descent要解决的优化问题为：&lt;/p>
$$
\min_{\theta} \mathbb{E}_ {(x, y^\star)\sim\mathcal{D}} \left[\mathbb{E}_ {(y,z,)\sim\pi_{\theta}}\left[r_{\phi}(x, y, y^\star) - \beta \mathcal{D}_ {KL}(\pi_{\theta}(y,z\mid x)\Vert \pi_{\theta_r}(y,z\mid x))\right]\right]
$$&lt;p>其中， $\beta$ 是超参数，用于平衡reward和KL散度 $\mathcal{D}_{KL}(\cdot)$.&lt;/p>
&lt;p>上述问题有一个闭式解，即：&lt;/p>
$$
\pi^\star(y,z\mid x)=\pi_{\theta_r}(y,z\mid x)\exp\left(\frac{r_{\phi}(x, y, y^\star)}{\beta}\right)/Z(x;\beta)
$$&lt;p>其中，&lt;/p>
$$
Z(x;\beta):=\sum_{y,z}\pi_{\theta_r}(y,z\mid x)\exp\left(\frac{r_{\phi}(x, y, y^\star)}{\beta}\right)
$$&lt;p>是归一化因子。在闭式解的表达式中，我们对两边取对数，得到：&lt;/p>
$$
r_{\phi}(x, y, y^\star) - \beta \log Z - \beta \log\frac{\pi^\star(y,z\mid x)}{\pi_{\theta_r}(y,z\mid x)}=0
$$&lt;p>这是最优策略满足的等式。作者这里使用了L2 loss来优化当前策略 $\pi_{\theta}$ ，即：&lt;/p>
$$
\min_{\theta} \mathbb{E}_ {(x, y^\star)\sim\mathcal{D}} \left[\mathbb{E}_ {(y,z,)\sim\pi_ {\theta_r}}\left[r_{\phi}(x, y, y^\star) - \beta \log Z - \beta \log\frac{\pi_{\theta}(y,z\mid x)}{\pi_{\theta_r}(y,z\mid x)}\right]^2\right]
$$&lt;p>这里需要注意的一点是response $(y,z)$ 是基于reference model $\pi_{\theta_r}$ 生成的，而不再是基于当前策略 $\pi_{\theta}$ 生成的。这是Kimi-1.5的RL训练和传统RL训练的一个不同点。这也是为什么k1.5说他是off-policy的原因。&lt;/p>
&lt;p>接下来，我们就可以对上面的目标函数求梯度，我们将里层的函数展开为 $(a-b-c)^2=(a-b)^2-2(a-b)c+c^2$ 的形式，然后对 $\theta$ 求梯度，得到：&lt;/p>
$$
\mathbb{E}_ {(x, y^\star)\sim\mathcal{D}} \left[\mathbb{E}_ {(y,z,)\sim\pi_{\theta_r}}\left[\nabla_{\theta}\log \pi_{\theta}(y,z\mid x)(r_{\phi}(x, y, y^\star) - \beta \log Z) - \frac{\beta}{2} \nabla_{\theta}\left(\log\frac{\pi_{\theta}(y,z\mid x)}{\pi_{\theta_r}(y,z\mid x)}\right)^2\right]\right]
$$&lt;p>如果我们对每个问题$x$,采样 $k$ 个response $(y,z)$ ，那么我们可以近似上式中内部的期望，得到：&lt;/p>
$$
\frac{1}{k}\sum_{i=1}^k\left[\nabla_{\theta}\log \pi_{\theta}(y_i,z_i\mid x)(r_{\phi}(x, y_i, y^\star) - \bar{r}) - \frac{\beta}{2} \nabla_{\theta}\left(\log\frac{\pi_{\theta}(y_i,z_i\mid x)}{\pi_{\theta_r}(y_i,z_i\mid x)}\right)^2\right]
$$&lt;p>这里， $\bar{r}$ 是 $\beta \log Z$ 的估计值。作者提供了两种估计方法：&lt;/p>
&lt;ol>
&lt;li>基于采样的 $(y_i,z_i)\sim \pi_{\theta_r}$ 对 $\beta \log Z$ 进行估计：&lt;/li>
&lt;/ol>
$$
\bar{r}=\beta\log\frac{1}{k}\sum_{j=1}^k\exp\left(\frac{r_{\phi}(x, y_j, y^\star)}{\beta}\right)
$$&lt;ol start="2">
&lt;li>直接使用samples rewards的平均值来近似：&lt;/li>
&lt;/ol>
$$
\bar{r}=\frac{1}{k}\sum_{i=1}^k r_{\phi}(x, y_i, y^\star)
$$&lt;p>作者提到，第二种方法效果很好并且计算效率更高，因此作者在实验中使用了第二种方法。&lt;/p>
&lt;p>训练时，作者每次从数据集中采样一个batch的样本，然后使用上述的梯度更新模型。模型参数更新完之后，作者将新的模型作为reference model，然后重复上述过程。&lt;/p>
&lt;blockquote>
&lt;p>Remark
作者还探究了为什么不使用value network。作者认为，在强化学习中，value network通常用于评估当前状态的价值，但是在reasoning model中，训练的目的是让模型能够充分探索，以提高模型在不同任务中的泛化性。因此，作者认为使用value network可能会限制模型的探索能力。&lt;/p>
&lt;/blockquote>
&lt;h3 id="training-strategy">Training Strategy
&lt;/h3>&lt;p>作者提出了两个提高采样效率的方法：&lt;/p>
&lt;ol>
&lt;li>Curriculum Sampling. 这种采样方式会将问题按照难度进行排序，然后按照难度从易到难进行采样。以逐步提高模型的推理能力。&lt;/li>
&lt;li>Prioritized Sampling.作者还采用了prioritized sampling来提高采样效率。也就是说，对于回答错误的问题，模型会进行更多的采样。&lt;/li>
&lt;/ol>
&lt;h2 id="infra">Infra
&lt;/h2>&lt;h3 id="training-system">Training System
&lt;/h3>&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-kimi-k1.5/k1-5-training-system.png"
width="1112"
height="530"
srcset="https://maosong2022.github.io/p/notes-on-kimi-k1.5/k1-5-training-system_hu56126063401246354.png 480w, https://maosong2022.github.io/p/notes-on-kimi-k1.5/k1-5-training-system_hu14529664632197223449.png 1024w"
loading="lazy"
alt="Large Scale Reinforcement Learning Training System for LLM"
class="gallery-image"
data-flex-grow="209"
data-flex-basis="503px"
>&lt;/p>
&lt;p>为了高效地进行RL训练，作者构建了一个iterative synchronous RL训练框架。框架如上图所示。框架的创新之处在于&lt;strong>Partial Rollout&lt;/strong>，该方法可以更高效的处理复杂的推理轨迹。框架包含了以下几部分：&lt;/p>
&lt;ol>
&lt;li>rollout worker: 该部分负责生成推理轨迹。然后将生成的推理轨迹加入到replay buffer中。&lt;/li>
&lt;li>master: 管理data flow和replay buffer和train worker之间的通信。&lt;/li>
&lt;li>train worker: 根据获取的rollout轨迹更新模型。&lt;/li>
&lt;li>code execution service: 在代码相关任务中，该部分负责执行代码，并返回执行结果。&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Partial Rollout&lt;/strong>：Partial Rollout的目的是为了整体的训练效率。其做法就是给定一个固定的outpu token budget，然后限制推理轨迹的长度。当长度超过token限制时，没有完成的部分就会被存入到replay buffer中，以供后续的训练使用。&lt;/p>
&lt;h3 id="deployment-framework">Deployment Framework
&lt;/h3>&lt;p>根据前面的介绍，我们知道k1.5每个iteration的训练包括两步：&lt;/p>
&lt;ol>
&lt;li>inference phase: 基于Reference model进行采样，生成推理轨迹。&lt;/li>
&lt;li>training phase: 基于采样得到的推理轨迹，更新模型。&lt;/li>
&lt;/ol>
&lt;p>如果我们直接使用Megatron和vLLM来实现上述的训练和推理，那么会存在以下问题：&lt;/p>
&lt;ol>
&lt;li>Megatron和vLLM的并行策略可能并不一致，因此很难进行参数共享&lt;/li>
&lt;li>在训练过程中，vLLM可能会保留一些GPU，这就导致了训练和推理的资源分配不均。&lt;/li>
&lt;li>推理和训练的资源需求可能并不一致，无法动态调控显卡资源&lt;/li>
&lt;/ol>
&lt;p>因此，作者构建了一个hybrid deployment framework来解决上述问题，其框架如下图所示。&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-kimi-k1.5/k1-5-hybrid-deployment-framework.png"
width="867"
height="560"
srcset="https://maosong2022.github.io/p/notes-on-kimi-k1.5/k1-5-hybrid-deployment-framework_hu6906289130535590140.png 480w, https://maosong2022.github.io/p/notes-on-kimi-k1.5/k1-5-hybrid-deployment-framework_hu10807671325725203551.png 1024w"
loading="lazy"
alt="Hybrid Deployment Framework"
class="gallery-image"
data-flex-grow="154"
data-flex-basis="371px"
>&lt;/p>
&lt;p>该框架的主要优势在于使用Kubernetes Sidecar containers来在一个pod中共享所有的GPU资源。这样就避免了资源分配不均的问题。&lt;/p>
&lt;h2 id="数据">数据
&lt;/h2>&lt;h3 id="数据处理">数据处理
&lt;/h3>&lt;ol>
&lt;li>RL prompt set. 作者认为prompt的quality和diversity对RL训练至关重要。因此作者基于diverse coverage, balanced difficuty以及accurate evaluation来构建了一个RL prompt set。对于多样性，作者确保数据集覆盖多个domain。对于平衡难度，作者通过多次采样，根据通过率来给每个样本的难易程度打分。对于准确性，作者筛掉了多选题，正确题和证明题，确保模型能够生成正确的推理步骤。最后，作者还让模型进行猜测，如果模型猜测的正确率比较高，那么就认为该样本是easy-to-hack的，就会筛掉。&lt;/li>
&lt;li>Test Case Generation for Coding. 作者使用&lt;a class="link" href="https://github.com/luogu-dev/cyaron" target="_blank" rel="noopener"
>CYaRon&lt;/a>来生成coding任务的测试用例。通过与Kimi k1.5结合，作者构建了一个包含323个测试用例的训练集。&lt;/li>
&lt;li>Reward modeling for Math. 作者使用了两种方法来提高reward model的准确性。第一个是classic RM，作者基于InstructGPT，构造了大约800K的数据训练了一个value-head based RM。模型会基于问题，参考答案和模型生成的答案来判断模型生成的答案是否正确。第二个是Chain of Thought RM，作者基于收集的800k CoT数据对kimi进行了finetune，然后对模型生成的推理步骤进行打分。最终发现，Chain of Thought RM的效果更好。因此，作者在RL训练中使用了Chain of Thought RM。&lt;/li>
&lt;li>Vision RL Data。 作者从三个方面构建了vision RL数据集：real-world data, synthetic visual reasoning data以及text-rendered data.&lt;/li>
&lt;/ol>
&lt;h3 id="数据集">数据集
&lt;/h3>&lt;ol>
&lt;li>pretraining：其中，纯文本数据包括Engligh, Chinese, Code, math reasoning以及Knowledge这5个领域。多模态数据包括captioning, image-text interleaving, OCR, Knowledge以及QA数据集等。附录B介绍了数据集的来源和处理过程。&lt;/li>
&lt;li>SFT： vanilla SFT数据集包含1M的纯文本数据，其中包含500K的general QA数据，200K的coding数据，200K的数学和科学数据，5K的创作写作数据以及20K的长上下文任务（总结，QA，翻译和写作等）。作者还构建了1M的图文数据，包括chart理解，OCR，grounding，visual coding, visual reasoning以及visual aided math/science problems等&lt;/li>
&lt;li>long-CoT SFT：该阶段的数据基于refined RL prompt set，使用了prompt engineering来构建了一个少量但高质量的long-CoT数据集。用于将reasoning能力内化到模型中。&lt;/li>
&lt;/ol>
&lt;h2 id="long2short">Long2short
&lt;/h2>&lt;p>为了降低推理成本，作者提出了long2short的推理方法。该方法通过将长上下文推理转化为短上下文推理，让模型能够更加高效的完成推理。作者尝试了几种方法来实现long2short：&lt;/p>
&lt;ol>
&lt;li>Model merging:将long-CoT模型和short-CoT模型进行合并，然后进行推理。这里采用的办法是对两个模型的权重取平均。&lt;/li>
&lt;li>shortest rejection sampling：通过对同一个问题进行多次采样（论文中每个问题采样8次），然后选择最短的，正确的推理步骤作为最终答案。&lt;/li>
&lt;li>DPO：通过对同一个问题进行多次采样，选择最短的正确的回答作为正样本，最长的回答作为负样本，然后使用DPO进行训练。&lt;/li>
&lt;li>Long2short RL：作者在RL阶段，选择一个在performance和token efficiency之间达到平衡的模型作为 base model，然后加入了一个long2short RL训练阶段。该阶段，作者加入了length penalty以及降低了maximum rollout length来鼓励模型生成更短的推理步骤。&lt;/li>
&lt;/ol>
&lt;h3 id="length-penalty">Length penalty
&lt;/h3>&lt;p>Length penalty的目的是降低模型的overthinking和训练成本。作者加入了一个length reward，对于问题 $x$ 和采样的回答 $(y_i,z_i)$ ，回答 $(y_i,z_i)$ 的length定义为 $len(x_i)=\text{length}([z_i, y_i])$ length reward定义为：&lt;/p>
$$
r_{\text{length}}(x, (y_i,z_i))=\begin{cases}
\lambda, &amp; \text{if } r(x, y_i, y^\star) = 1 \\
\min(0,\lambda), &amp; \text{if } r(x, y_i, y^\star) = 0 \\
\end{cases}
$$&lt;p>其中&lt;/p>
$$
\lambda = 0.5 - \frac{len(x_i)-\min_{j}len(x_j)}{\max_{j}len(x_j)-\min_{j}len(x_j)}
$$&lt;p>也就是说，当回答正确时，我们鼓励模型生成更长的推理步骤。反之，我们鼓励模型生成更短的推理步骤。&lt;/p>
&lt;h2 id="实验">实验
&lt;/h2>&lt;h3 id="实验结果">实验结果
&lt;/h3>&lt;ol>
&lt;li>K1.5在long-CoT任务上的表现&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-kimi-k1.5/k1-5-long-cot-benchmark.png"
width="1118"
height="425"
srcset="https://maosong2022.github.io/p/notes-on-kimi-k1.5/k1-5-long-cot-benchmark_hu6952644691331158148.png 480w, https://maosong2022.github.io/p/notes-on-kimi-k1.5/k1-5-long-cot-benchmark_hu1430225520164276888.png 1024w"
loading="lazy"
alt="Long-CoT-benchmark"
class="gallery-image"
data-flex-grow="263"
data-flex-basis="631px"
>&lt;/p>
&lt;ol start="2">
&lt;li>K1.5在Short-CoT任务上的表现&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-kimi-k1.5/k1-5-short-cot-benchmark.png"
width="1357"
height="566"
srcset="https://maosong2022.github.io/p/notes-on-kimi-k1.5/k1-5-short-cot-benchmark_hu3005992739822340121.png 480w, https://maosong2022.github.io/p/notes-on-kimi-k1.5/k1-5-short-cot-benchmark_hu15186336803595586566.png 1024w"
loading="lazy"
alt="Short-CoT-benchmark"
class="gallery-image"
data-flex-grow="239"
data-flex-basis="575px"
>&lt;/p>
&lt;ol start="3">
&lt;li>
&lt;p>Long Context Scaling
作者在实验中还探究了上下文长度对模型推理能力的影响。作者针对几个模型进行了实验，结果在图5和图6里面。结果发现，随着上下文长度的增加，模型的推理能力会逐渐提升。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Long2short&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-kimi-k1.5/k1-5-long2short.png"
width="1354"
height="590"
srcset="https://maosong2022.github.io/p/notes-on-kimi-k1.5/k1-5-long2short_hu3211336959358196266.png 480w, https://maosong2022.github.io/p/notes-on-kimi-k1.5/k1-5-long2short_hu6402801847399781857.png 1024w"
loading="lazy"
alt="Long2short"
class="gallery-image"
data-flex-grow="229"
data-flex-basis="550px"
>&lt;/p>
&lt;h3 id="消融实验">消融实验
&lt;/h3>&lt;ol>
&lt;li>Scaling of model size and context length. 作者探究了模型大小和上下文长度对模型推理能力的影响，结果在图8里面。结果发现
&lt;ol>
&lt;li>小模型可以通过Long CoT来达到大模型的效果&lt;/li>
&lt;li>大模型具有更高的token效率&lt;/li>
&lt;li>理想情况下，使用具有更长上下文的大模型，可以同时达到更高的推理能力和更高的token效率，否则，使用上下文长度更长的小模型&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Effects of using negative gradients. 作者探究了使用负梯度对模型推理能力的影响。作者与ReST方法进行了对比，结果在图10里面。结果发现，ReST回到只模型产生更长的推理步骤。作者认为，选取合适的优化方法，可以有效提升模型的推理能力。&lt;/li>
&lt;li>Sampling strategies. 作者还探究了采样策略对模型推理能力的影响。作者与均匀采样进行了对比，结果在图9里面。结果发现，论文提出的采样策略可以有效提升模型的推理能力。&lt;/li>
&lt;/ol>
&lt;h2 id="结论">结论
&lt;/h2>&lt;p>作者提出了Kimi k1.5，一个基于强化学习训练的多模态推理模型。作者介绍了Kimi k1.5的训练方法，数据集，infra上的优化以及long2short的推理方法。作者的主要贡献在于：&lt;/p>
&lt;ol>
&lt;li>作者发现，模型的上下文长度可以有效提升LLM的推理能力。&lt;/li>
&lt;li>作者提出了基于online mirror descent的强化学习训练方法。&lt;/li>
&lt;li>作者提出了long2short的推理方法，可以有效提升short CoT模型的推理能力。&lt;/li>
&lt;/ol>
&lt;p>论文的问题在于对于多模态的内容介绍较少，感觉还是更倾向于文本推理任务。&lt;/p>
&lt;h2 id="参考文献">参考文献
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2501.12599" target="_blank" rel="noopener"
>Kimi k1.5: Scaling Reinforcement Learning with LLMs&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Screen usage</title><link>https://maosong2022.github.io/p/screen-usage/</link><pubDate>Wed, 05 Feb 2025 15:14:43 +0800</pubDate><guid>https://maosong2022.github.io/p/screen-usage/</guid><description>&lt;h1 id="introduction">Introduction
&lt;/h1>&lt;p>Screen is a terminal multiplexer, which allows you to create multiple sessions in a single terminal window.&lt;/p>
&lt;h1 id="installation">Installation
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">sudo apt-get install screen
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="usage">Usage
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">screen &lt;span class="o">[&lt;/span>-option&lt;span class="o">]&lt;/span> &lt;span class="o">[&lt;/span>command&lt;span class="o">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Options:&lt;/p>
&lt;ul>
&lt;li>&lt;code>-S&lt;/code>: specify the session name&lt;/li>
&lt;li>&lt;code>-d&lt;/code>: detach from the session&lt;/li>
&lt;li>&lt;code>-r&lt;/code>: reattach to the session&lt;/li>
&lt;li>&lt;code>-c&lt;/code>: execute the command in the session&lt;/li>
&lt;li>&lt;code>-L&lt;/code>: list all sessions&lt;/li>
&lt;li>&lt;code>-wipe&lt;/code>: wipe out all sessions&lt;/li>
&lt;li>&lt;code>-x&lt;/code>: execute the command in the session&lt;/li>
&lt;li>&lt;code>-p&lt;/code>: specify the port number&lt;/li>
&lt;li>&lt;code>-m&lt;/code>: specify the mode&lt;/li>
&lt;li>&lt;code>-t&lt;/code>: specify the title&lt;/li>
&lt;li>&lt;code>-v&lt;/code>: specify the version&lt;/li>
&lt;li>&lt;code>-h&lt;/code>: display help information&lt;/li>
&lt;li>&lt;code>-v&lt;/code>: display version information&lt;/li>
&lt;li>&lt;code>-q&lt;/code>: quit the session&lt;/li>
&lt;/ul>
&lt;h1 id="reference">Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://www.gnu.org/software/screen/manual/screen.html" target="_blank" rel="noopener"
>Screen usage&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Phi-4</title><link>https://maosong2022.github.io/p/notes-on-phi-4/</link><pubDate>Mon, 16 Dec 2024 17:33:52 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-phi-4/</guid><description>&lt;h1 id="introduction">Introduction
&lt;/h1>&lt;p>微软在12月12号发布了Phi-4的技术报告,主要关注了数据的构建，特别是。Phi-4是一个14B的大语言模型，主要在STEM相关的QA任务以及推理相关的任务上表现比较好。Phi-4主要在三个方面进行了改进：&lt;/p>
&lt;ol>
&lt;li>在pre-training和mid-training阶段使用了合成数据进行训练&lt;/li>
&lt;li>&lt;/li>
&lt;/ol>
&lt;h1 id="data">Data
&lt;/h1>&lt;h1 id="pre-training">Pre-training
&lt;/h1>&lt;h2 id="architecture">Architecture
&lt;/h2>&lt;h1 id="post-training">Post-training
&lt;/h1>&lt;h2 id="dpo">DPO
&lt;/h2>&lt;h2 id="sft">SFT
&lt;/h2>&lt;h1 id="references">References
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://techcommunity.microsoft.com/blog/aiplatformblog/introducing-phi-4-microsoft%E2%80%99s-newest-small-language-model-specializing-in-comple/4357090" target="_blank" rel="noopener"
>Blog&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2412.08905" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>An overview of adaption layer in multimodal large language models.</title><link>https://maosong2022.github.io/p/an-overview-of-adaption-layer-in-multimodal-large-language-models./</link><pubDate>Sat, 09 Nov 2024 09:53:43 +0800</pubDate><guid>https://maosong2022.github.io/p/an-overview-of-adaption-layer-in-multimodal-large-language-models./</guid><description>&lt;h1 id="introduction">Introduction
&lt;/h1>&lt;p>A multimodal large language model (MLLM) usually consists of three parts: an encoder $E$ that ingests the information from different modality, a large language model (LLM) that is corresponds to complete various of downstream tasks given multimodal input such as image and text, and an adaption layer $C$ that aligns features of different modality to word embedding space of the LLM.
Below is an example MLLM adopting aforementioned architecture: LLaVA [1]&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/an-overview-of-adaption-layer-in-multimodal-large-language-models./llava.png"
width="1032"
height="360"
srcset="https://maosong2022.github.io/p/an-overview-of-adaption-layer-in-multimodal-large-language-models./llava_hu14087429020602296246.png 480w, https://maosong2022.github.io/p/an-overview-of-adaption-layer-in-multimodal-large-language-models./llava_hu5281182413389194983.png 1024w"
loading="lazy"
alt="Architecture of LlaVA"
class="gallery-image"
data-flex-grow="286"
data-flex-basis="688px"
>&lt;/p>
&lt;p>Efforts have been made to improve the performance of MLLMs. In this post, we aim to review the design of adaption layer and its potential effect on the downstream tasks.&lt;/p>
&lt;h1 id="method">Method
&lt;/h1>&lt;p>Suppose the hidden size of the LLM is $d$, the feature produced by encoder $E$ is $V\in\mathbb{R}^{P\times d_v}$, where $P$ is the number of features (number of visual patches if $E$ is an visual encoder) and $d_v$ is the channel dimension.
The adaption layer $C$ then aligns the feature $V$ with the word embedding space with $x=C(V)\in\mathbb{R}^{Q\times d}$, where $Q$ is the number of tokens. As we can see, $C$ is actually a mapping from $\mathbb{R}^{P\times d_v}$ to $\mathbb{R}^{Q\times d}$.&lt;/p>
&lt;p>Based on relationship between $d_v$ and $d$, we can divide projection layers into two types:&lt;/p>
&lt;ol>
&lt;li>Feature-preserving adaption layer, where $P=Q$&lt;/li>
&lt;li>Feature-compressing adaption layer, where $P&amp;gt;Q$.&lt;/li>
&lt;/ol>
&lt;h2 id="feature-preserving-adaption-layer">Feature-preserving adaption layer
&lt;/h2>$$ x = VW^T, \text{ where } W\in\mathbb{R}^{d\times d_v}$$&lt;p>
the code reads as:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># linear layer&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">adaption_layer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_features&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>$$ x = \phi(VW_1^T)W_2^T$$&lt;p>
where $W_1\in\mathbb{R}^{d\times d_v}$, $W_2\in\mathbb{R}^{d\times d}$, $\phi$ is a activation function, specified as &lt;code>nn.GELU()&lt;/code>. The code reads as:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># two-layer MLP&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">adaption_layer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Sequential&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_features&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">GELU&lt;/span>&lt;span class="p">(),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="feature-compressing-adaption-layer">Feature-compressing adaption layer
&lt;/h2>&lt;p>The feature compression adaption layers can be categorized into three types:&lt;/p>
&lt;ol>
&lt;li>average pooling&lt;/li>
&lt;li>attention pooling&lt;/li>
&lt;li>convolution mapping&lt;/li>
&lt;/ol>
&lt;p>They usually comprise two steps:&lt;/p>
&lt;ol>
&lt;li>reduce the number of features from $P$ to $Q$ with a pooling operation:
$$ f' = \mathcal{P}(f)\in\mathbb{R}^{Q\times d_v} $$&lt;/li>
&lt;li>project compressed features $f&amp;rsquo;$ to word embedding space with a transformation $\mathcal{T}$:
$$ x = \mathcal{T}(f')\in\mathbb{R}^{Q\times d} $$&lt;/li>
&lt;/ol>
$$ f'_i = \frac{1}{n}\sum_{j=1}^{n}f_{(i-1)n+j}, i=1,\dots,Q $$$$ K = W_kf\in\mathbb{R}^{d_c}, V=W_vf\in\mathbb{R}^{d_c}, f'=\mathrm{softmax}\left(\frac{QK^T}{\sqrt{d_c}}\right)V\in\mathbb{R}^{Q\times d_v} $$&lt;p>
where $W_k, W_v\in\mathbb{R}^{d_c\times d_v}$ and $Q\in\mathbb{R}^{Q\times d_c}$ is a learnable query.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">PerceiverResampler&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_queries&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_features&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_heads&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_queries&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">num_queries&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">hidden_size&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_features&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">num_features&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">query_tokens&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Parameter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zeros&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_queries&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_features&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">requires_grad&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">query_tokens&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">data&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">normal_&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">mean&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">std&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.02&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">attention&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">MultiheadAttention&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_heads&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">layer_norm_kv&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">LayerNorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">layer_norm_q&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">LayerNorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">attention_mask&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">layer_norm_kv&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">permute&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">N&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">layer_norm_q&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">query_tokens&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">q&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">unsqueeze&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">repeat&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">N&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">out&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">attention&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">q&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">v&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">attention_mask&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">attention_mask&lt;/span>&lt;span class="p">)[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">out&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">out&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">permute&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">adaption_layer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Sequential&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">PerceiverResampler&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num_queries&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_features&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_heads&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">MLP&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">intermediate_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_size&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>$$ f_i' = \frac{1}{n}\sum_{j=1}^n w_jf_{(i-1)n+j},\quad x_i = \sum_{k=-K}^Kw_k'f_{i+k}' $$&lt;p>
where $W=[w_1,\dots,w_n]^T\in\mathbb{R}^n$ and $W&amp;rsquo;=[w_1,\dots,w_n]^T\in\mathbb{R}^{2K}$ are the weights of the convolution layers.&lt;/p>
&lt;p>&lt;strong>D-Abstractor&lt;/strong> aa&lt;/p>
&lt;h1 id="usages">Usages
&lt;/h1>&lt;h1 id="comparisons">Comparisons
&lt;/h1>&lt;h1 id="references">References
&lt;/h1>&lt;ol>
&lt;li>&lt;a class="link" href="https://papers.nips.cc/paper_files/paper/2023/hash/6dcf277ea32ce3288914faf369fe6de0-Abstract-Conference.html" target="_blank" rel="noopener"
>LLaVA&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Improved_Baselines_with_Visual_Instruction_Tuning_CVPR_2024_paper.pdf" target="_blank" rel="noopener"
>LLaVA 1.5&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/haotian-liu/LLaVA/blob/main/llava/model/multimodal_projector/builder.py" target="_blank" rel="noopener"
>LLaVA adaption layer code&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2405.10739v1" target="_blank" rel="noopener"
>survey&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>592. Fraction Addition and Subtraction</title><link>https://maosong2022.github.io/p/592.-fraction-addition-and-subtraction/</link><pubDate>Fri, 23 Aug 2024 20:16:54 +0800</pubDate><guid>https://maosong2022.github.io/p/592.-fraction-addition-and-subtraction/</guid><description>&lt;p>Given a string expression representing an expression of fraction addition and subtraction, return the calculation result in string format.&lt;/p>
&lt;h1 id="intuition">Intuition
&lt;/h1>&lt;p>Simulate the calculation of fraction addition and subtraction process.&lt;/p>
&lt;h1 id="approach">Approach
&lt;/h1>&lt;p>First, to simplify addition and subtraction, we move the &lt;code>-&lt;/code> to numerator part, for example, &lt;code>3/4-2/3&lt;/code> becomes &lt;code>3/4+(-2)/3&lt;/code>, this step makes all operations become addition.&lt;/p>
&lt;p>Second, we use a initial value &lt;code>0/1&lt;/code> to record result to prevent from parsing the first fraction. Thus &lt;code>3/4-2/3&lt;/code> is actually equivalent to &lt;code>0/1+3/4-2/3&lt;/code>.&lt;/p>
&lt;p>Now, in each loop, we record the sign, the numerator, the denominator. Then we compute the result with the previous result with the formula&lt;/p>
$$
\frac{a}{b} + \frac{c}{d} = \frac{ad-bc}{bd}
$$&lt;p>after computation, we use &lt;code>gcd()&lt;/code> function to make the resulted fraction irreducible.&lt;/p>
&lt;h1 id="complexity">Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n)$$&lt;/li>
&lt;li>
$$O(1)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">string&lt;/span> &lt;span class="n">fractionAddition&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">string&lt;/span> &lt;span class="n">expression&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">expression&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">numerator1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dominator1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">numerator2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dominator2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">sign&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">expression&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="sc">&amp;#39;-&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sign&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">expression&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="sc">&amp;#39;+&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sign&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">numerator2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="sc">&amp;#39;0&amp;#39;&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">expression&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="n">expression&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="sc">&amp;#39;9&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">numerator2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">numerator2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">10&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">expression&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="sc">&amp;#39;0&amp;#39;&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">numerator2&lt;/span> &lt;span class="o">*=&lt;/span> &lt;span class="n">sign&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="c1">// move sign to numerator
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="n">sign&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="c1">// reset sign
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="c1">// division operator
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="n">dominator2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="sc">&amp;#39;0&amp;#39;&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">expression&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="n">expression&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="sc">&amp;#39;9&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dominator2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">dominator2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">10&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">expression&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="sc">&amp;#39;0&amp;#39;&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">numerator1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">numerator1&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">dominator2&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">numerator2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">dominator1&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dominator1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">dominator1&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">dominator2&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">numerator1&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="n">dominator1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">gcd_num&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">gcd&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">numerator1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dominator1&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">numerator1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">numerator1&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">gcd_num&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dominator1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">dominator1&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">gcd_num&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// corner case
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">numerator1&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="s">&amp;#34;0/1&amp;#34;&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="nf">to_string&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">numerator1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="s">&amp;#34;/&amp;#34;&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">to_string&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dominator1&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="reference">Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/fraction-addition-and-subtraction/description" target="_blank" rel="noopener"
>leetcode 592&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>506. Relative Ranks</title><link>https://maosong2022.github.io/p/506.-relative-ranks/</link><pubDate>Thu, 22 Aug 2024 21:36:01 +0800</pubDate><guid>https://maosong2022.github.io/p/506.-relative-ranks/</guid><description>&lt;p>Given an integer, flip all bits in its binary representation.&lt;/p>
&lt;h1 id="intuition">Intuition
&lt;/h1>&lt;p>Use XOR operation to complete this task.&lt;/p>
&lt;h1 id="approach">Approach
&lt;/h1>&lt;p>We can use the XOR operation to complete this task. However, notice that we cannot simply use &lt;code>num ^ INT_MAX&lt;/code> since the leading &lt;code>1&lt;/code>s are still &lt;code>1&lt;/code>s if &lt;code>num&lt;/code> is too small. Instead, we need to compute the number of digits in &lt;code>num&lt;/code>, and use the corresponding masks.&lt;/p>
&lt;p>Notice that if &lt;code>2^30 &amp;lt; num &amp;lt;= 2^31-1&lt;/code>, in this case the corresponding mask will exceeds the integer ranges, thus we use &lt;code>INT_MAX&lt;/code> directly in this case.&lt;/p>
&lt;h1 id="complexity">Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(\log n)$$&lt;/li>
&lt;li>
$$O(1)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">findComplement&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">num&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">num_bits&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">temp&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">num&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">temp&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">temp&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">++&lt;/span>&lt;span class="n">num_bits&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">num_bits&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">31&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">num&lt;/span> &lt;span class="o">^&lt;/span> &lt;span class="n">INT_MAX&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">mask&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span> &lt;span class="o">&amp;lt;&amp;lt;&lt;/span> &lt;span class="n">num_bits&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">num&lt;/span> &lt;span class="o">^&lt;/span> &lt;span class="n">mask&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="reference">Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/number-complement/description" target="_blank" rel="noopener"
>leetcode 476&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>664. Strange Printer</title><link>https://maosong2022.github.io/p/664.-strange-printer/</link><pubDate>Wed, 21 Aug 2024 15:16:41 +0000</pubDate><guid>https://maosong2022.github.io/p/664.-strange-printer/</guid><description>&lt;h1 id="approach">Approach
&lt;/h1>&lt;p>We can use a &lt;code>dp&lt;/code> matrix, where element &lt;code>dp[i][j]&lt;/code> represents the minimum turn to print substring &lt;code>s[i...j]&lt;/code>.&lt;/p>
&lt;p>Then, there are two cases:&lt;/p>
&lt;ul>
&lt;li>case 1: We print &lt;code>s[i]&lt;/code> separately, now we have &lt;code>dp[i][j] = 1 + dp[i + 1][j]&lt;/code>.&lt;/li>
&lt;li>case 2: There is a char &lt;code>s[k] == s[i]&lt;/code>, then the string &lt;code>s[i...k]&lt;/code> can be obtained by printing some new substrings on the range &lt;code>s[i...k]&lt;/code>, now we have &lt;code>dp[i][j] = dp[i][k-1]+dp[k+1][j]&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Combining case 1 and case 2, we can now write the update formula for dynamic programming:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">k&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">k&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>the base case is when &lt;code>i &amp;gt; j&lt;/code>, &lt;code>d[pi][j] = 0&lt;/code> and when &lt;code>i=j&lt;/code>, &lt;code>dp[i][j] = 1&lt;/code>.&lt;/p>
&lt;h1 id="complexity">Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n^3)$$&lt;/li>
&lt;li>
$$O(n^2)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">dp&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">start&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">end&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="k">const&lt;/span> &lt;span class="n">string&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">start&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">end&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">return&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">start&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">end&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">return&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">start&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">return&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">start&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">ans&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dp&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">start&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">end&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">k&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">start&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">k&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">end&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">start&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ans&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ans&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dp&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">start&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dp&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">end&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">start&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ans&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">ans&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="nf">strangePrinter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">string&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dp&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="references">References
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/strange-printer/description/" target="_blank" rel="noopener"
>Leetcode 664&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>264. Ugly Number II</title><link>https://maosong2022.github.io/p/264.-ugly-number-ii/</link><pubDate>Sun, 18 Aug 2024 10:37:20 +0000</pubDate><guid>https://maosong2022.github.io/p/264.-ugly-number-ii/</guid><description>&lt;p>A number is &lt;code>ugly&lt;/code> if its prime factors is a subset of ${2, 3, 5}$. We are required to find the $n$-th &lt;code>ugly&lt;/code> number&lt;/p>
&lt;h1 id="intuition">Intuition
&lt;/h1>&lt;p>Each ugly number is generated from a previous ugly number by multiplying $2$, $3$ or $5$.&lt;/p>
&lt;h1 id="approach">Approach
&lt;/h1>&lt;p>We use three pointers &lt;code>index_2&lt;/code>, &lt;code>index_3&lt;/code> and &lt;code>index_5&lt;/code> to record the index of previous ugly number. For example, $4$ is generated from $2$ by multiplying $2$, so the index &lt;code>index_2&lt;/code> is the index of &lt;code>2&lt;/code>.&lt;/p>
&lt;p>Then, we take the minimum of three generated numbers:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">index_2&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">index_3&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">index_5&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Finally, we need to update the pointers so that there is no duplication.&lt;/p>
&lt;h1 id="complexity">Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n)$$&lt;/li>
&lt;li>
$$O(n)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">nthUglyNumber&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">index_2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">index_3&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">index_5&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">result_2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">index_2&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">result_3&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">index_3&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">result_5&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">index_5&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">current_ugly_num&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">result_2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">result_3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">result_5&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">current_ugly_num&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">result_2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">index_2&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">current_ugly_num&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">result_3&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">index_3&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">current_ugly_num&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">result_5&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">index_5&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="references">References
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/ugly-number-ii/description/" target="_blank" rel="noopener"
>Leetcode 264&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on VITA</title><link>https://maosong2022.github.io/p/notes-on-vita/</link><pubDate>Tue, 13 Aug 2024 14:36:45 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-vita/</guid><description>&lt;h1 id="tldr">TLDR
&lt;/h1>&lt;p>This paper proposes a Multimodal Large Language Model VITA (Video, Image, Text, Audio).
VITA supports non-awakening interaction and audio interruption for better interactive experience.
VITA aims to be an open-sourced version of GPT-4o.&lt;/p>
&lt;h1 id="introduction">Introduction
&lt;/h1>&lt;p>Features of GPT-4o:&lt;/p>
&lt;ol>
&lt;li>a unified framework that processes text, vision, and audio signals in an end-to-end manner,&lt;/li>
&lt;li>the capability to enable natural multimodal human-computer interaction.&lt;/li>
&lt;/ol>
&lt;p>Similar to Mini-GPT4, this paper tries to proposed an open-sourced version of GPT-4o.&lt;/p>
&lt;h1 id="method">Method
&lt;/h1>&lt;h2 id="model">Model
&lt;/h2>&lt;p>The architecture of VITA is shown as follows:
&lt;img src="https://maosong2022.github.io/p/notes-on-vita/VITA_architecture.png"
width="1282"
height="1069"
srcset="https://maosong2022.github.io/p/notes-on-vita/VITA_architecture_hu13166811980198188055.png 480w, https://maosong2022.github.io/p/notes-on-vita/VITA_architecture_hu6710414439966249089.png 1024w"
loading="lazy"
alt="Architecture of VITA"
class="gallery-image"
data-flex-grow="119"
data-flex-basis="287px"
>&lt;/p>
&lt;ul>
&lt;li>LLM: Mixtral $8\times 7$ B&lt;/li>
&lt;li>Visual Encoder: InternViT-300M-448px&lt;/li>
&lt;li>Audio Encoder: Mel Filter Bank block&lt;/li>
&lt;/ul>
&lt;p>To support audio interruption, the author uses two model at the same time, where the generation model is responsible for handling user queries and the other model monitors the environment. The other models starts to work is there is an audio interruption.&lt;/p>
&lt;h2 id="data">Data
&lt;/h2>&lt;h3 id="multimodal-instruction-tuning">multimodal instruction tuning
&lt;/h3>&lt;p>Training data of multimodal instruction tuning is given as follows:&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-vita/VITA_SFT_data.png"
width="1286"
height="672"
srcset="https://maosong2022.github.io/p/notes-on-vita/VITA_SFT_data_hu12706316391905990779.png 480w, https://maosong2022.github.io/p/notes-on-vita/VITA_SFT_data_hu6833967022569500187.png 1024w"
loading="lazy"
alt="Training data of multimodal instruction tuning"
class="gallery-image"
data-flex-grow="191"
data-flex-basis="459px"
>&lt;/p>
&lt;p>Improvements are made:&lt;/p>
&lt;ol>
&lt;li>The questions are randomly (about half) replaced with their audio versions, using TTS technique such as GPT-SoVITS&lt;/li>
&lt;li>Different system prompts are set to avoid conflicts between different types of data&lt;/li>
&lt;/ol>
&lt;p>To support human-AI interaction, the noisy audio data are also constructed. Noisy audio samples are generated from existed QA data. These negative sample texts aim to improve the ability of VITA to not respond to non-query-related content.&lt;/p>
&lt;p>To distinguish three types of queries, the author uses three state tokens:&lt;/p>
&lt;ul>
&lt;li>Token &lt;code>&amp;lt;1&amp;gt;&lt;/code> denotes that the question input is the query audio&lt;/li>
&lt;li>Token &lt;code>&amp;lt;2&amp;gt;&lt;/code> denotes that the question input is the noisy audio.&lt;/li>
&lt;li>Token &lt;code>&amp;lt;3&amp;gt;&lt;/code> signifies the question of pure text.&lt;/li>
&lt;/ul>
&lt;h2 id="training-pipeline">Training pipeline
&lt;/h2>&lt;p>Training pipeline of VITA consists of three stages:&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-vita/VITA_training_pipeline.png"
width="1301"
height="852"
srcset="https://maosong2022.github.io/p/notes-on-vita/VITA_training_pipeline_hu12778043419258304094.png 480w, https://maosong2022.github.io/p/notes-on-vita/VITA_training_pipeline_hu1848643356938254411.png 1024w"
loading="lazy"
alt="Training pipeline of VITA"
class="gallery-image"
data-flex-grow="152"
data-flex-basis="366px"
>&lt;/p>
&lt;h3 id="non-awakening-interaction">Non-awakening Interaction
&lt;/h3>&lt;p>There are following requirements and solutions:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Real-time Tracking of Environmental Sounds. This paper uses SileroVAD to complete the Voice Activity Detection (VAD) task.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Filtering out noisy audio. This is done by making use of token &lt;code>&amp;lt;2&amp;gt;&lt;/code>.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="audio-interrupt-interaction">Audio Interrupt Interaction
&lt;/h3>&lt;p>There are following requirements and solutions:&lt;/p>
&lt;ol>
&lt;li>Real-time Tracking and Filtering of External Queries. This is done by use another VITA model as stated in Model section.&lt;/li>
&lt;/ol>
&lt;h1 id="evaluation">Evaluation
&lt;/h1>&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-vita/VITA_evaluation.png"
width="1297"
height="536"
srcset="https://maosong2022.github.io/p/notes-on-vita/VITA_evaluation_hu3461940985657118198.png 480w, https://maosong2022.github.io/p/notes-on-vita/VITA_evaluation_hu13137771961429217061.png 1024w"
loading="lazy"
alt="Evaluation on image and video understanding"
class="gallery-image"
data-flex-grow="241"
data-flex-basis="580px"
>&lt;/p>
&lt;h1 id="conclusion">Conclusion
&lt;/h1>&lt;p>The paper points out three limitations of VITA:&lt;/p>
&lt;ol>
&lt;li>Enhancement of Foundational Capabilities.&lt;/li>
&lt;li>Refinement of Noisy Audio Construction.&lt;/li>
&lt;li>Building end-to-end TTS in conjunction with LLM.&lt;/li>
&lt;/ol>
&lt;h1 id="reference">Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2408.05211" target="_blank" rel="noopener"
>Arxiv paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://vita-home.github.io" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>78. Subsets</title><link>https://maosong2022.github.io/p/78.-subsets/</link><pubDate>Tue, 21 May 2024 22:16:49 +0800</pubDate><guid>https://maosong2022.github.io/p/78.-subsets/</guid><description>&lt;p>Given an array, find all subsets in the array&lt;/p>
&lt;h1 id="intuition">Intuition
&lt;/h1>&lt;p>Use DFS to iterate over all subsets and record them.&lt;/p>
&lt;h1 id="approach">Approach
&lt;/h1>&lt;p>A subset can be represented as a binary number of &lt;code>n&lt;/code> digits. Each digit is either &lt;code>0&lt;/code> or &lt;code>1&lt;/code>. We can use DFS to iterate over all possible such format of binary numbers.&lt;/p>
&lt;h1 id="complexity">Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(2^n)$$&lt;/li>
&lt;li>
$$O(2^n)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">void&lt;/span> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">index&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">index&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// digit is 1
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nums&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">index&lt;/span>&lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nums&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">index&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">s&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">pop_back&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// digit is 0
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nums&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">index&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">subsets&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nums&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="reference">Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/subsets/description/" target="_blank" rel="noopener"
>Leetcode&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>2812. Find the Safest Path in a Grid</title><link>https://maosong2022.github.io/p/2812.-find-the-safest-path-in-a-grid/</link><pubDate>Wed, 15 May 2024 20:24:56 +0800</pubDate><guid>https://maosong2022.github.io/p/2812.-find-the-safest-path-in-a-grid/</guid><description>&lt;p>Given a matrix, we wish to find a path that from start to end, such that the path is as far as from the dangerous position.&lt;/p>
&lt;h1 id="intuition">Intuition
&lt;/h1>&lt;p>We can convert the problem into finding a path with minimum weights in a graph.&lt;/p>
&lt;h1 id="approach">Approach
&lt;/h1>&lt;p>We first convert the matrix into a graph, with node as position &lt;code>(i, j)&lt;/code> and weight &lt;code>w[i,j]\min_k(|i-thief[k][0]| + |j - thief[k][1]|)&lt;/code>, where &lt;code>thief[k]=(thief[k][0], thief[k][1])&lt;/code> is the position of the thief &lt;code>thief[k]&lt;/code>. To do this, we can use DFS, starting from each thief and iterate through the grids.&lt;/p>
&lt;p>Then, we need to find a path from the start &lt;code>(0, 0)&lt;/code> to target &lt;code>(n - 1, n - 1)&lt;/code>. This can be done via Dijkstra&amp;rsquo;s Algorithm. We use a priority queue to keep track of minimum to-be-visited nodes, this ensures that the newly added nodes are always with the maximum safe factor.&lt;/p>
&lt;h1 id="complexity">Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n^2\log n)$$&lt;/li>
&lt;li>
$$O(n^2)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;span class="lnt">49
&lt;/span>&lt;span class="lnt">50
&lt;/span>&lt;span class="lnt">51
&lt;/span>&lt;span class="lnt">52
&lt;/span>&lt;span class="lnt">53
&lt;/span>&lt;span class="lnt">54
&lt;/span>&lt;span class="lnt">55
&lt;/span>&lt;span class="lnt">56
&lt;/span>&lt;span class="lnt">57
&lt;/span>&lt;span class="lnt">58
&lt;/span>&lt;span class="lnt">59
&lt;/span>&lt;span class="lnt">60
&lt;/span>&lt;span class="lnt">61
&lt;/span>&lt;span class="lnt">62
&lt;/span>&lt;span class="lnt">63
&lt;/span>&lt;span class="lnt">64
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">dirs&lt;/span>&lt;span class="p">{{&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">},&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">},&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">},&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">}};&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">maximumSafenessFactor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="o">||&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">n&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">n&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">scores&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">INT_MAX&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">queue&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">pair&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">q&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">make_pair&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">scores&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="o">!&lt;/span>&lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">empty&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">const&lt;/span> &lt;span class="k">auto&lt;/span> &lt;span class="n">node&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">front&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">first&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">second&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="k">auto&lt;/span>&lt;span class="o">&amp;amp;&lt;/span> &lt;span class="nl">dir&lt;/span> &lt;span class="p">:&lt;/span> &lt;span class="n">dirs&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">new_x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">new_y&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">new_x&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="mi">0&lt;/span> &lt;span class="o">||&lt;/span> &lt;span class="n">new_x&lt;/span> &lt;span class="o">&amp;gt;=&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">||&lt;/span> &lt;span class="n">new_y&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="mi">0&lt;/span> &lt;span class="o">||&lt;/span> &lt;span class="n">new_y&lt;/span> &lt;span class="o">&amp;gt;=&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">scores&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">new_x&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">new_y&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">scores&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">scores&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">new_x&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">new_y&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">scores&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">make_pair&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">new_x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">new_y&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">pop&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">bool&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">visited&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">bool&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">false&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">priority_queue&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">pair&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">pair&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">pq&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pq&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">make_pair&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">scores&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">make_pair&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="o">!&lt;/span>&lt;span class="n">pq&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">empty&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">auto&lt;/span> &lt;span class="n">node&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">pq&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">top&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pq&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">pop&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">safe_factor&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">first&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">auto&lt;/span> &lt;span class="n">pos&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">second&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">pos&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">first&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="n">pos&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">second&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">safe_factor&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">visited&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">pos&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">first&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">pos&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">second&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">true&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="k">auto&lt;/span>&lt;span class="o">&amp;amp;&lt;/span> &lt;span class="nl">dir&lt;/span> &lt;span class="p">:&lt;/span> &lt;span class="n">dirs&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">new_x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">pos&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">first&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">new_y&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">pos&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">second&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">new_x&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="mi">0&lt;/span> &lt;span class="o">||&lt;/span> &lt;span class="n">new_x&lt;/span> &lt;span class="o">&amp;gt;=&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">||&lt;/span> &lt;span class="n">new_y&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="mi">0&lt;/span> &lt;span class="o">||&lt;/span> &lt;span class="n">new_y&lt;/span> &lt;span class="o">&amp;gt;=&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">visited&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">new_x&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">new_y&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">score&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">safe_factor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">scores&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">new_x&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">new_y&lt;/span>&lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pq&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">make_pair&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">score&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">make_pair&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">new_x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">new_y&lt;/span>&lt;span class="p">)));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">visited&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">new_x&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">new_y&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">true&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="references">References
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/find-the-safest-path-in-a-grid/description" target="_blank" rel="noopener"
>Leetcode&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>1219. Path with Maximum Gold</title><link>https://maosong2022.github.io/p/1219.-path-with-maximum-gold/</link><pubDate>Tue, 14 May 2024 20:39:03 +0800</pubDate><guid>https://maosong2022.github.io/p/1219.-path-with-maximum-gold/</guid><description>&lt;p>Given a matrix, whose element representing the number of golds. Find a path such that the sum is maximized and without crossing the grids that has not gold elements.&lt;/p>
&lt;h1 id="intuition">Intuition
&lt;/h1>&lt;p>Use backtracking to find all possible paths, and update the results.&lt;/p>
&lt;h1 id="approach">Approach
&lt;/h1>&lt;p>We use two variables &lt;code>current&lt;/code> and &lt;code>result&lt;/code> to store results, &lt;code>current&lt;/code> stores the sum of golds from start to current position, &lt;code>result&lt;/code> stores the final result.&lt;/p>
&lt;h1 id="complexity">Complexity
&lt;/h1>&lt;ul>
&lt;li>Time complexity: In worst case, each grid contains gold, for each position, there are $\binom{m+n}{m}$ possible paths, so the overall complexity is&lt;/li>
&lt;/ul>
$$O\left(mn\binom{m+n}{m}\right)$$&lt;ul>
&lt;li>Space complexity: No extra spaces needed (without considering recursive stack)&lt;/li>
&lt;/ul>
$$O(1)$$&lt;h1 id="code">Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">dirs&lt;/span>&lt;span class="p">{{&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">},&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">},&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">},&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">}};&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">void&lt;/span> &lt;span class="n">backtracking&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">current&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">result&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">return&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">current&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// update result
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">max&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">result&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">current&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// mark as visited
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">temp&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="k">auto&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="nl">dir&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">dirs&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">row&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">col&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">row&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="mi">0&lt;/span> &lt;span class="o">||&lt;/span> &lt;span class="n">row&lt;/span> &lt;span class="o">&amp;gt;=&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">||&lt;/span> &lt;span class="n">col&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="mi">0&lt;/span> &lt;span class="o">||&lt;/span> &lt;span class="n">col&lt;/span> &lt;span class="o">&amp;gt;=&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">].&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">backtracking&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">grid&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">row&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">col&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">current&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// retrieve the state
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">temp&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">current&lt;/span> &lt;span class="o">-=&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="nf">getMaximumGold&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">].&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">current&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">backtracking&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">grid&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">current&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="reference">Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/path-with-maximum-gold/description/" target="_blank" rel="noopener"
>Leetcode 1219&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>861. Score After Flipping Matrix</title><link>https://maosong2022.github.io/p/861.-score-after-flipping-matrix/</link><pubDate>Mon, 13 May 2024 20:52:02 +0800</pubDate><guid>https://maosong2022.github.io/p/861.-score-after-flipping-matrix/</guid><description>&lt;p>Given a binary matrix, we can flip one column or one row, the goal is to flip zero or more times such that the sum of the number represented by the rows are maximized.&lt;/p>
&lt;h1 id="intuition">Intuition
&lt;/h1>&lt;p>The leading &lt;code>1&lt;/code>s are always important than the trailing &lt;code>1&lt;/code>s. So we make sure that &lt;code>1&lt;/code> appears before &lt;code>0&lt;/code>s.&lt;/p>
&lt;h1 id="approach">Approach
&lt;/h1>&lt;p>The challenge is to determine when to flip the rows and flip the columns. From the intuition, we know that:&lt;/p>
&lt;ol>
&lt;li>a row is flipped only if its first bit is &lt;code>0&lt;/code>, after flipping, the number becomes larger and cannot be flipped again.&lt;/li>
&lt;li>a column is flipped only if the number of &lt;code>1&lt;/code>s are smaller than &lt;code>0&lt;/code>s.&lt;/li>
&lt;/ol>
&lt;p>So we flip the rows first and the columns second.&lt;/p>
&lt;h1 id="complexity">Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$ O(mn) $$&lt;/li>
&lt;li>
$$ O(1) $$&lt;/li>
&lt;/ul>
&lt;h1 id="code">Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">matrixScore&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">m&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">].&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">m&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// flip row
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// check column by column
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">count&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">m&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">count&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">count&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">m&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// flip column
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">k&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">k&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">m&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">m&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span> &lt;span class="o">&amp;lt;&amp;lt;&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="references">References
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/score-after-flipping-matrix/description/" target="_blank" rel="noopener"
>Leetcode&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>786. K-th Smallest Prime Fraction</title><link>https://maosong2022.github.io/p/786.-k-th-smallest-prime-fraction/</link><pubDate>Fri, 10 May 2024 22:39:54 +0800</pubDate><guid>https://maosong2022.github.io/p/786.-k-th-smallest-prime-fraction/</guid><description>&lt;p>Given an integer array of size $n$ containing prime integers, it can form $n(n-1)/2$ fractions, we are required to find the $k$-th smallest prime fraction.&lt;/p>
&lt;h1 id="intuition">Intuition
&lt;/h1>&lt;p>We can use a priority queue to store prime integers, then we maintain the priority queue.&lt;/p>
&lt;h1 id="approach1-brute-force">Approach1: Brute force
&lt;/h1>&lt;h2 id="complexity">Complexity
&lt;/h2>&lt;ul>
&lt;li>
$$O(n^2\log k)$$&lt;/li>
&lt;li>
$$O(k)$$&lt;/li>
&lt;/ul>
&lt;h2 id="code">Code
&lt;/h2>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">class&lt;/span> &lt;span class="nc">Compare&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">bool&lt;/span> &lt;span class="k">operator&lt;/span>&lt;span class="p">()(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">a&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="k">const&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">){&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="mf">1.0&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">a&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">a&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="mf">1.0&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">];&lt;/span> &lt;span class="c1">// the root is the biggest
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">kthSmallestPrimeFraction&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">arr&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">priority_queue&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Compare&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">pq&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">arr&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">r&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">r&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">k&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">r&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">r&lt;/span> &lt;span class="o">&amp;gt;=&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">break&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pq&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="n">arr&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">arr&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">r&lt;/span>&lt;span class="p">]});&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">pq&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pq&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">pop&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">pq&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">top&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="approach2-simplification">Approach2: Simplification
&lt;/h1>&lt;p>Notice that Approach1 requires iterating over all fractions, can we reduce the time complexity?&lt;/p>
&lt;p>The solution is by considering the relative order, if we write a matrix whose element &lt;code>a[i][j]=nums[i]/nums[j]&lt;/code> (&lt;code>i&amp;lt;j&lt;/code>), then we know that &lt;code>a[i][i+1]&amp;gt;...&amp;gt;a[n-1][n]&lt;/code> since the array &lt;code>nums&lt;/code> are increasing. So, the smallest fraction are in &lt;code>a[1][2], ..., a[n-1][n]&lt;/code>. If we take the smallest fraction, and add its successive elements (same column, last row), then we can find the second smallest fraction and so on. This solution requires iterating over $\max(n, k)$ fractions and $O(n)$ spaces.&lt;/p>
&lt;h2 id="complexity-1">Complexity
&lt;/h2>&lt;ul>
&lt;li>
$$O(\max(n, k)\log n)$$&lt;/li>
&lt;li>
$$O(n)$$&lt;/li>
&lt;/ul>
&lt;h2 id="code-1">Code
&lt;/h2>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">kthSmallestPrimeFraction&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">A&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">K&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// (fraction, (i, j))
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="n">priority_queue&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">pair&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">double&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">pair&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">pq&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">A&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pq&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push&lt;/span>&lt;span class="p">({&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mf">1.0&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">A&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">A&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">back&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">A&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">}});&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="o">--&lt;/span>&lt;span class="n">K&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">auto&lt;/span> &lt;span class="n">t&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">pq&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">top&lt;/span>&lt;span class="p">().&lt;/span>&lt;span class="n">second&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">pop&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">--&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">second&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pq&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push&lt;/span>&lt;span class="p">({&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mf">1.0&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">A&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">first&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">A&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">second&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">first&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">t&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">second&lt;/span>&lt;span class="p">}});&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="n">A&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">pq&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">top&lt;/span>&lt;span class="p">().&lt;/span>&lt;span class="n">second&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">first&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">A&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">pq&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">top&lt;/span>&lt;span class="p">().&lt;/span>&lt;span class="n">second&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">second&lt;/span>&lt;span class="p">]};&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="reference">Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/k-th-smallest-prime-fraction/description/" target="_blank" rel="noopener"
>leetcode&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>3068. Find the Maximum Sum of Node Values</title><link>https://maosong2022.github.io/p/3068.-find-the-maximum-sum-of-node-values/</link><pubDate>Thu, 09 May 2024 20:21:07 +0800</pubDate><guid>https://maosong2022.github.io/p/3068.-find-the-maximum-sum-of-node-values/</guid><description>&lt;p>Given a graph and an integer $k$, where the nodes represent values, we can choose an edge, and perform XOR operations on its nodes corresponding to $k$. The goal is the find the maximum sum of the values after 0 or more XOR operations.&lt;/p>
&lt;h1 id="intuition">Intuition
&lt;/h1>&lt;p>Since XOR operations satisfies the property that &lt;code>a XOR b XOR b = a&lt;/code>, we can record the gain after XOR operation on an edge and finally obtain the result.&lt;/p>
&lt;h1 id="approach">Approach
&lt;/h1>&lt;p>We use &lt;code>total_sum&lt;/code> to record the sum of the values of the original trees.&lt;/p>
&lt;p>Then for each node, we perform the XOR operation and record the &lt;code>change&lt;/code> if the &lt;code>change &amp;gt; 0&lt;/code>, and this change requires one operation, which we add to &lt;code>count&lt;/code>. Meanwhile, we use &lt;code>positive_min&lt;/code> and &lt;code>negative_max&lt;/code> to record the minimum absolute change for reverting use.&lt;/p>
&lt;p>Now after all nodes are computed, we need to compute the result.&lt;/p>
&lt;ol>
&lt;li>If &lt;code>count&lt;/code> is even, it means the operations satisfied the requirement that the nodes of an edge changes simultaneously&lt;/li>
&lt;li>If &lt;code>count&lt;/code> is odd, then there is one invalid operation and we need to revert the operation. To make the final sum maximum, we can either subtract the &lt;code>positive_min&lt;/code> or add &lt;code>negative_max&lt;/code>, the result is then obtained by taking the maximum of them.&lt;/li>
&lt;/ol>
&lt;h1 id="complexity">Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n)$$&lt;/li>
&lt;li>
$$O(1)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">long&lt;/span> &lt;span class="kt">long&lt;/span> &lt;span class="n">maximumValueSum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">edges&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">long&lt;/span> &lt;span class="kt">long&lt;/span> &lt;span class="n">total_sum&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">count&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">positive_min&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">INT_MAX&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">negative_max&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">INT_MIN&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="nl">num&lt;/span> &lt;span class="p">:&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">new_num&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">num&lt;/span> &lt;span class="o">^&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">total_sum&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">num&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">change&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">new_num&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">num&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">change&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">positive_min&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">positive_min&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">change&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">total_sum&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">change&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">++&lt;/span>&lt;span class="n">count&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">negative_max&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">max&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">negative_max&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">change&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">count&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">total_sum&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="nf">max&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">total_sum&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">positive_min&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">total_sum&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">negative_max&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="reference">Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/find-the-maximum-sum-of-node-values/description/" target="_blank" rel="noopener"
>Leetcode&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>3075. Maximize Happiness of Selected Children</title><link>https://maosong2022.github.io/p/3075.-maximize-happiness-of-selected-children/</link><pubDate>Thu, 09 May 2024 20:21:07 +0800</pubDate><guid>https://maosong2022.github.io/p/3075.-maximize-happiness-of-selected-children/</guid><description>&lt;p>Given an integer array representing the happiness of the children, select $k$ children such that the sum of their happiness is maximized.&lt;/p>
&lt;h1 id="intuition">Intuition
&lt;/h1>&lt;p>Since happiness of all rest children after choosing one child will decrease by 1, their relative order will still the same. So this problem is actually requiring us to select $k$ most happy children.&lt;/p>
&lt;h1 id="approach">Approach
&lt;/h1>&lt;p>Use sorting.&lt;/p>
&lt;h1 id="complexity">Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n\log n)$$&lt;/li>
&lt;li>
$$O(1)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">long&lt;/span> &lt;span class="kt">long&lt;/span> &lt;span class="n">maximumHappinessSum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">happiness&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sort&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">happiness&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">begin&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">happiness&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">());&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">happiness&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">long&lt;/span> &lt;span class="kt">long&lt;/span> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">max&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">happiness&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">n&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="reference">Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/maximize-happiness-of-selected-children/description/" target="_blank" rel="noopener"
>Leetcode&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>ROUGE (Recall-Oriented Understudy)</title><link>https://maosong2022.github.io/p/rouge-recall-oriented-understudy/</link><pubDate>Thu, 09 May 2024 17:35:20 +0800</pubDate><guid>https://maosong2022.github.io/p/rouge-recall-oriented-understudy/</guid><description>&lt;p>ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes several automatic evaluation methods that measure the similarity between summaries.&lt;/p>
&lt;h1 id="preliminaries">Preliminaries
&lt;/h1>&lt;h1 id="rouge-n-n-gram-co-occurrence-statistics">ROUGE-N: N-gram co-occurrence statistics
&lt;/h1>&lt;h2 id="definitions">Definitions
&lt;/h2>&lt;p>Given any string $y=y_1y_2\cdots y_K$, where $y_i,i\in{1,\dots,K}$ are characters and an integer $n\geq1$, we define the &lt;strong>set of $n$-gram&lt;/strong> to be&lt;/p>
$$ G_n(y) = \{ y_1\cdots y_n, y_2\cdots y_{n+1}, \dots, y_{K-n+1}\cdots y_K\} $$&lt;p>NOTE that this is a set with unique elements, for example, $G_2(abab)={ab, ba}$.&lt;/p>
&lt;p>Given any two strings $s$ and $y$, we define &lt;strong>substring count&lt;/strong> $C(s,y)$ to tbe the number of appearances of $s$ as a substring of $y$. For example, $C(ab, abcbab)=2$ since $ab$ appear in $abcbab$ twice in position $1$ and $5$.&lt;/p>
&lt;h2 id="base-version">Base Version
&lt;/h2>&lt;p>ROUGE-N is an n-gram recall between a candidate summary $\hat{y}$ and a set of reference summaries $S={y_1,\dots,y_n}$. ROUGE-N is defined as follows:&lt;/p>
$$ \text{ROUGE-N}(\hat{y}, S) = \frac{\sum_{i=1}^n\sum_{s\in G_N(y_i)}C(s,\hat{y})}{\sum_{i=1}^n\sum_{s\in G_N(y_i)}C(s, y_i)} $$&lt;p>Features of ROUGE-N:&lt;/p>
&lt;ol>
&lt;li>the denominator increases as we add more references, since there might exists multiple good summaries.&lt;/li>
&lt;li>A candidate summary that contains words shared by more references is favored by the ROUGE-N measure.&lt;/li>
&lt;/ol>
&lt;h2 id="multiple-references">Multiple references
&lt;/h2>&lt;p>When there are multiple references for a candidate summary, it is suggests to use the following formula:&lt;/p>
$$ \text{ROUGE-N}(\hat{y}, S) = \arg\max_{i=1,\dots,n}\text{ROUGE-N}(\hat{y}, \{y_i\}) $$&lt;p>the above formula is favored for the following reasons:&lt;/p>
&lt;ol>
&lt;li>There is no single &amp;ldquo;best&amp;rdquo; reference summary, the multiple reference formula allows ROUGE-N to take into account of all the possible reference summaries and provide a more accurate measure of the quality of the generated summary.&lt;/li>
&lt;li>The multiple reference formula is more robust. If a reference summary contains a typo or a grammatical error, this can affect the ROUGE-N score.&lt;/li>
&lt;li>The multiple reference formula can provide a more comprehensive evaluation of the generated summary, since it can allow ROUGE-N to evaluate the generated summary against a wider range pf possible reference summaries.&lt;/li>
&lt;/ol>
&lt;h1 id="rouge-l">ROUGE-L
&lt;/h1>&lt;p>A sequence $Z=[z_1,\dots,z_m]$ is a subsequence of another sequence $X=[x_1,\dots,x_n]$ if there exists a strict increasing sequence $[i_1,\dots,i_k]$ of indices of $X$ such that for all $j=1,\dots,k$, we have $x_{i_j}=z_j$.&lt;/p>
&lt;p>Given two sequences $X$ and $Y$, the longest common subsequences (LCS) of $X$ and $Y$ is a common subsequences with maximum length.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">longest_common_subsequence&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;compute the length of LCS of x and y
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> Args:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> x (str): a string of length m
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> y (str): a string of length n
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> Return:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> int: the length of LCS of x and y
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">m&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dp&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[[&lt;/span>&lt;span class="mi">0&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">m&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)]&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">m&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">max&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">m&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="sentence-level-lcs">Sentence-level LCS
&lt;/h2>&lt;p>The intuition of sentence-level LCS is that the longer the LCS of two summary sentences is, the more similar the two summaries are.&lt;/p>
&lt;p>Given two summaries $X$ of length $m$ and $Y$ of length $n$, assuming $X$ is a reference summary sentence and $Y$ is a candidate summary sentence, the LCS-based recall, precision and F-measure are defined as follows:&lt;/p>
$$ R_{LCS} = \frac{LCS(X, Y)}{m}, P_{LCS} = \frac{LCS(X, Y)}{n}, R_{LCS} = \frac{(1+\beta^2)R_{LCS}P_{LCS}}{R_{LCS}+\beta^2P_{LCS}} $$&lt;p>the above formula is called ROUGE-L. $\beta$ is a hyperparameter.&lt;/p>
&lt;p>Features of ROUGE-L are listed as follows:&lt;/p>
&lt;ol>
&lt;li>It doesn&amp;rsquo;t require consecutive matches but in-sequence matches, which is more reasonable than n-grams.&lt;/li>
&lt;li>It automatically includes longest in-sequence common n-grams, there fore no predefined n-gram length is necessary.&lt;/li>
&lt;li>It&amp;rsquo;s value is less tan or equal to the minimum of unigram F-measure of $X$ and $Y$.&lt;/li>
&lt;li>The disadvantage of ROUGE-L is that it only counts the main in-sequences words, therefore, other alternative LCSes and shorter sequences are not reflected in the final score.&lt;/li>
&lt;/ol>
&lt;h2 id="summary-level-lcs">Summary-level LCS
&lt;/h2>&lt;p>We can apply sentence-level LCS-based F-measure score to summary level. Given a reference summary of $u$ sentences ${r_1,\dots,r_u}$ containing a total of $m$ words and a candidate summary of $v$ sentences ${c_1,\dots,c_v}$ containing a total of $n$ words, the summary-level LCS-based recall, precision and F-measure are defined as follows:&lt;/p>
$$ R_{LCS} = \frac{\sum_{i=1}^u\max_{j}LCS(r_i, c_j)}{m}, P_{LCS} = \frac{\sum_{i=1}^v \max_{j}LCS(r_i, c_j)}{n}, R_{LCS} = \frac{(1+\beta^2)R_{LCS}P_{LCS}}{R_{LCS}+\beta^2P_{LCS}} $$&lt;h1 id="rouge-w">ROUGE-W
&lt;/h1>&lt;p>The basic LCS has a problem that it doesn&amp;rsquo;t differentiate LCSes of different spatial relations within their embedding sequences.&lt;/p>
&lt;p>To improve the basic LSC method, we can simply remember the length of consecutive matches encountered so fat to a regular two dimensional dynamic program table computing LCS. we call this &lt;em>weighted LCS (WLCS)&lt;/em> and use $k$ to indicate the length of the current consecutive matches ending at words $x_i$ and $y_j$.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">weighted_longest_common_subsequence&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">f&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Callable&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;use dynamic programming to compute WLCS with a weighted function f
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> Args:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> x (str): a candidate summary containing m words
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> y (str): a reference summary containing n words
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> f (Callable): a function satisfies f(x+y) &amp;gt; f(x) + f(y)
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> Return:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> the WLCS score
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> Reference:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> https://aclanthology.org/W04-1013
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">split&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">split&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">m&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">c&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[[&lt;/span>&lt;span class="mi">0&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">m&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)]&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">w&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[[&lt;/span>&lt;span class="mi">0&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">m&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)]&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">m&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># the length of consecutive matches at (i - 1, j - 1)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">k&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">w&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">c&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">f&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">k&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">f&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># remember the length of consecutive matches at (i - 1, j - 1)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">w&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">k&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">c&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">w&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span> &lt;span class="c1"># no match at (i, j)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">elif&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">w&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span> &lt;span class="c1"># no match at (i, j)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">m&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>where &lt;code>c[i][j]&lt;/code> stores the WLCS score ending at word &lt;code>x[i]&lt;/code> of &lt;code>x&lt;/code> and &lt;code>y[i]&lt;/code> of &lt;code>y&lt;/code>. &lt;code>w&lt;/code> stores the length of consecutive matches at &lt;code>c[i][j]&lt;/code>. &lt;code>f&lt;/code> is a function of consecutive matches at &lt;code>c[i][j]&lt;/code>.&lt;/p>
&lt;p>Recall, precision, F-score based on WLCS can be computed as follows:&lt;/p>
$$ R_{WLCS} = f^{-1}\left(\frac{WLCS(X, Y)}{f(m)}\right), P_{WLCS} = f^{-1}\left(\frac{WLCS(X, Y)}{f(n)}\right), R_{LCS} = \frac{(1+\beta^2)R_{WLCS}P_{WLCS}}{R_{WLCS}+\beta^2P_{WLCS}} $$&lt;p>where $f$ is the inverse function of $f$. We call the WLCS-based F-measure as ROUGE-W. Usually, a function $f$ that has a close form inverse is preferred.&lt;/p>
&lt;h1 id="rouge-s">ROUGE-S
&lt;/h1>&lt;p>Skip-bigram is any pair of words in their sentence order, allowing for arbitrary gaps. Skip-bigram co-occurrence statistics measure the overlap of skip-bigrams between a candidate translation and a set of reference translations.
A sentence with $n$ words will have $\binom{n}{2}=n(n-1)/2$ skip-bigrams.&lt;/p>
&lt;p>Recall, precision, F-score based on skip-bigram can be computed as follows:&lt;/p>
$$ R_{\mathrm{SKIP2}} = \frac{\mathrm{SKIP2}(X, Y)}{m}, P_{\mathrm{SKIP2}} = \frac{\mathrm{SKIP2}(X, Y)}{n}, R_{\mathrm{SKIP2}} = \frac{(1+\beta^2)R_{\mathrm{SKIP2}}P_{\mathrm{SKIP2}}}{R_{\mathrm{SKIP2}}+\beta^2P_{\mathrm{SKIP2}}} $$&lt;p>where $\mathrm{SKIP2}(X, Y)$ is the number of skip-bigram matches between $X$ and $Y$. The F-score is called ROUGE-S.&lt;/p>
&lt;h1 id="rouge-su">ROUGE-SU
&lt;/h1>&lt;p>One problem of ROUGE-S is that it doesn&amp;rsquo;t given any credit to a candidate sentence if the sentence doesn&amp;rsquo;t have any word pair co-occurring with its references.&lt;/p>
&lt;p>To fix this problem, we extend ROUGE-S with the addition of unigram as counting unit. The extended version is called ROUGE-SU. We can also obtain ROUGE-SU from ROUGE-S by adding a begin-of-sentence marker at the beginning of candidate and reference sentences.&lt;/p>
&lt;h1 id="reference">Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://aclanthology.org/W04-1013.pdf" target="_blank" rel="noopener"
>ROUGE: A Package for Automatic Evaluation of Summaries&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/Yale-LILY/SummerTime/blob/e49058d928b4bd5b1017b7d774bea984bbdf5006/summertime/model/third_party/HMNet/Evaluation/OldROUGEEval.py" target="_blank" rel="noopener"
>ROUGE Eval&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/google-research/google-research/tree/master/rouge" target="_blank" rel="noopener"
>google-research rouge&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>506. Relative Ranks</title><link>https://maosong2022.github.io/p/506.-relative-ranks/</link><pubDate>Wed, 08 May 2024 20:17:53 +0800</pubDate><guid>https://maosong2022.github.io/p/506.-relative-ranks/</guid><description>&lt;p>Given an array of scores, assign different ranks based on their position in the sorted array&lt;/p>
&lt;h1 id="intuition">Intuition
&lt;/h1>&lt;p>Sort the array and assign based on the sorted array.&lt;/p>
&lt;h1 id="approach">Approach
&lt;/h1>&lt;p>in C++, we can use the property of the container &lt;code>map&lt;/code> to solve this problem, the map is constructed so that the key is the score and the value is the index of the score.&lt;/p>
&lt;h1 id="complexity">Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n\log n)$$&lt;/li>
&lt;li>
$$O(n)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">string&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">findRelativeRanks&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">score&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">map&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">m&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">score&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">m&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">score&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">string&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">score&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">());&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">index&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="k">auto&lt;/span> &lt;span class="n">iter&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">m&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">rbegin&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="n">iter&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="n">m&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">rend&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">iter&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">index&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">index&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">iter&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">second&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;Gold Medal&amp;#34;&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span> &lt;span class="nf">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">index&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">iter&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">second&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;Silver Medal&amp;#34;&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span> &lt;span class="nf">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">index&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">iter&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">second&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;Bronze Medal&amp;#34;&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">iter&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">second&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">to_string&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">index&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="reference">Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/relative-ranks/description" target="_blank" rel="noopener"
>leetcode 506&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>2487. Remove Nodes From Linked List</title><link>https://maosong2022.github.io/p/2487.-remove-nodes-from-linked-list/</link><pubDate>Mon, 06 May 2024 19:47:28 +0800</pubDate><guid>https://maosong2022.github.io/p/2487.-remove-nodes-from-linked-list/</guid><description>&lt;p>Given a linked list, we are required to remove some nodes, such that for each node in the result linked list, the value of the node is the greatest from the node to the end of the linked list.&lt;/p>
&lt;h1 id="intuition">Intuition
&lt;/h1>&lt;p>We construct the result linked list from right to left, that is, the last node in the linked list is kept, then the pointer goes from right to left util there is a node with greater value. This process can be done via post traversal as tree.&lt;/p>
&lt;h2 id="complexity">Complexity
&lt;/h2>&lt;ul>
&lt;li>
$$O(n)$$&lt;/li>
&lt;li>
$$O(n)$$&lt;/li>
&lt;/ul>
&lt;h2 id="code">Code
&lt;/h2>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * Definition for singly-linked list.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * struct ListNode {
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * int val;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * ListNode *next;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * ListNode() : val(0), next(nullptr) {}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * ListNode(int x) : val(x), next(nullptr) {}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * ListNode(int x, ListNode *next) : val(x), next(next) {}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * };
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">void&lt;/span> &lt;span class="n">reverse_traversal&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ListNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">head&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">stack&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="o">!&lt;/span>&lt;span class="n">head&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">return&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">reverse_traversal&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">head&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">next&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">empty&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">||&lt;/span> &lt;span class="n">head&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">val&lt;/span> &lt;span class="o">&amp;gt;=&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">top&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">s&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">head&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">val&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ListNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="nf">removeNodes&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ListNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">head&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">stack&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">reverse_traversal&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">head&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ListNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">dummyhead&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">ListNode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="k">nullptr&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ListNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">cur&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">dummyhead&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="o">!&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">empty&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">cur&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">next&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">ListNode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">top&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="k">nullptr&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">cur&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">cur&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">next&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">s&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">pop&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">dummyhead&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">next&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">``&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cp"># References
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cp">&lt;/span>&lt;span class="o">-&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">Leetcode&lt;/span>&lt;span class="p">](&lt;/span>&lt;span class="nl">https&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="c1">//leetcode.com/problems/remove-nodes-from-linked-list/description/)
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>2816. Double a Number Represented as a Linked List</title><link>https://maosong2022.github.io/p/2816.-double-a-number-represented-as-a-linked-list/</link><pubDate>Mon, 06 May 2024 19:47:28 +0800</pubDate><guid>https://maosong2022.github.io/p/2816.-double-a-number-represented-as-a-linked-list/</guid><description>&lt;p>Given a linked list representing a non-negative integer, we are required to double this integer and convert it back to a linked list.&lt;/p>
&lt;h1 id="intuition">Intuition
&lt;/h1>&lt;p>We can just simulate the process, that is:&lt;/p>
&lt;ol>
&lt;li>Retrieve the integer represented by the linked list&lt;/li>
&lt;li>Double the integer&lt;/li>
&lt;li>Construct the result linked list from the result integer.&lt;/li>
&lt;/ol>
&lt;h1 id="approach">Approach
&lt;/h1>&lt;p>We can just simulate the process as above. However, if the integer is very large, retrieve the integer may cause overflow. To simplify this process, we can use a stack to store the digits of the original integer, then we double the integer by operating on the top of the stack.&lt;/p>
&lt;p>In each step, we pop an element from the stack, doubling it and adding it with the carry digit. Then we construct the result linked list with inserting the new node in the front of the head.&lt;/p>
&lt;h1 id="complexity">Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n)$$&lt;/li>
&lt;li>
$$O(n)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * Definition for singly-linked list.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * struct ListNode {
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * int val;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * ListNode *next;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * ListNode() : val(0), next(nullptr) {}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * ListNode(int x) : val(x), next(nullptr) {}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * ListNode(int x, ListNode *next) : val(x), next(next) {}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * };
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ListNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">doubleIt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ListNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">head&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">stack&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ListNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">cur&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">head&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">cur&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">s&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">cur&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">val&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">cur&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">cur&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">next&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">carry&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="o">!&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">empty&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">num&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">top&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">carry&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ListNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">pre&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">ListNode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="mi">10&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">cur&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">carry&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">num&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="mi">10&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">cur&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">pre&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">s&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">pop&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">carry&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ListNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">pre&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">ListNode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">carry&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">cur&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">cur&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">pre&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">cur&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="references">References
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/double-a-number-represented-as-a-linked-list/description" target="_blank" rel="noopener"
>Leetcode&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>237. Delete Node in a Linked List</title><link>https://maosong2022.github.io/p/237.-delete-node-in-a-linked-list/</link><pubDate>Sun, 05 May 2024 20:13:45 +0800</pubDate><guid>https://maosong2022.github.io/p/237.-delete-node-in-a-linked-list/</guid><description>&lt;p>Given a linked list and the node to be deleted, delete the node without accessing the head of the linked list&lt;/p>
&lt;h1 id="intuition">Intuition
&lt;/h1>&lt;p>This deletion is same as deleting a node from an array.&lt;/p>
&lt;h1 id="approach">Approach
&lt;/h1>&lt;p>We use two pointers, &lt;code>pre&lt;/code> and &lt;code>node&lt;/code> to represent the previous and current node of the linked list, and we update the value of &lt;code>pre&lt;/code> and &lt;code>node&lt;/code> at each iteration. Finally, we delete the last node in the linked list.&lt;/p>
&lt;h1 id="complexity">Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n)$$&lt;/li>
&lt;li>
$$O(1)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * Definition for singly-linked list.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * struct ListNode {
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * int val;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * ListNode *next;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * ListNode(int x) : val(x), next(NULL) {}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * };
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">void&lt;/span> &lt;span class="n">deleteNode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ListNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ListNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">pre&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">nullptr&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">next&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">node&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">val&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">next&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">val&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pre&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">node&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">next&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pre&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">next&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">nullptr&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="references">References
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/delete-node-in-a-linked-list/description/" target="_blank" rel="noopener"
>Leetcode&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>881. Boats to Save People</title><link>https://maosong2022.github.io/p/881.-boats-to-save-people/</link><pubDate>Sat, 04 May 2024 09:06:48 +0800</pubDate><guid>https://maosong2022.github.io/p/881.-boats-to-save-people/</guid><description>&lt;p>Given an array &lt;code>weight&lt;/code> where &lt;code>weight[i]&lt;/code> representing the weight of person &lt;code>i&lt;/code>, now given the capacity of the boat and the constraint that each boat can carry at most two people, find the minimum number of boats to carry all people.&lt;/p>
&lt;h1 id="intuition">Intuition
&lt;/h1>&lt;p>Always pair the lightest person abd the heaviest person to a boat.&lt;/p>
&lt;h1 id="approach">Approach
&lt;/h1>&lt;p>We first sort the array &lt;code>weight&lt;/code> in ascending order. Then we use two pointers &lt;code>left=0&lt;/code> and &lt;code>right=n-1&lt;/code> to iterate through the array. In each step, there are two cases:&lt;/p>
&lt;ol>
&lt;li>If &lt;code>weight[left]+weight[right] &amp;gt; limit&lt;/code>, then we cannot find a peer who can take one boat with &lt;code>right&lt;/code>, in this case, &lt;code>right&lt;/code> occupies a single boat alone.&lt;/li>
&lt;li>If &lt;code>weight[left]+weight[right] &amp;lt;= limit&lt;/code>, then these two people can take one boat.&lt;/li>
&lt;/ol>
&lt;h1 id="complexity">Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$ O(n\log n) $$&lt;/li>
&lt;li>
$$ O(1) $$&lt;/li>
&lt;/ul>
&lt;h1 id="code">Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">numRescueBoats&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">people&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">limit&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sort&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">people&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">begin&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">people&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">());&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">left&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">right&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">people&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">num&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">left&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">right&lt;/span>&lt;span class="p">){&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">people&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">right&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">people&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">left&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">limit&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">++&lt;/span>&lt;span class="n">left&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">--&lt;/span>&lt;span class="n">right&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">++&lt;/span>&lt;span class="n">num&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">num&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="references">References
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/boats-to-save-people/description/" target="_blank" rel="noopener"
>Leetcode&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>165. Compare Version Numbers</title><link>https://maosong2022.github.io/p/165.-compare-version-numbers/</link><pubDate>Fri, 03 May 2024 15:13:23 +0800</pubDate><guid>https://maosong2022.github.io/p/165.-compare-version-numbers/</guid><description>&lt;p>Given two strings containing digits and dot, compare them in the form of a version number.&lt;/p>
&lt;h1 id="intuition">Intuition
&lt;/h1>&lt;p>Since the leading zeros are required to be ignored, we use dot character as separator and compute the integer of each part and compare them individually.&lt;/p>
&lt;h1 id="approach">Approach
&lt;/h1>&lt;p>We use two index &lt;code>i&lt;/code> and &lt;code>j&lt;/code> to iterate over &lt;code>s1&lt;/code> and &lt;code>s2&lt;/code>, we move the index &lt;code>i&lt;/code> to the next dot character and compute the integer &lt;code>num1&lt;/code> we have found. Same operations for &lt;code>j&lt;/code> to obtain the integer &lt;code>num2&lt;/code>. Then &lt;code>num1&lt;/code> and &lt;code>num2&lt;/code> are compared.&lt;/p>
&lt;h1 id="complexity">Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n)$$&lt;/li>
&lt;li>
$$O(1)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">compareVersion&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">string&lt;/span> &lt;span class="n">version1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">string&lt;/span> &lt;span class="n">version2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">m&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">version1&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">version2&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">m&lt;/span> &lt;span class="o">||&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">num1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">m&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="n">version1&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="sc">&amp;#39;.&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">num1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">10&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">num1&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">version1&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">++&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="sc">&amp;#39;0&amp;#39;&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">j&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="n">version2&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="sc">&amp;#39;.&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">num2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">10&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">num2&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">version2&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="o">++&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="sc">&amp;#39;0&amp;#39;&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">num1&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">num2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">return&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">num1&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">num2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">return&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="references">References
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/compare-version-numbers/description/" target="_blank" rel="noopener"
>Leetcode&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Formal Algorithms for Transformer</title><link>https://maosong2022.github.io/p/formal-algorithms-for-transformer/</link><pubDate>Thu, 02 May 2024 13:13:12 +0800</pubDate><guid>https://maosong2022.github.io/p/formal-algorithms-for-transformer/</guid><description>&lt;p>This post is a notes on understanding how transformer works in an algorithm perspective.&lt;/p>
&lt;h1 id="introduction">Introduction
&lt;/h1>&lt;p>Transformer is a neural architecture that is used for neural language processing. Transformer receives an embedding matrix, which represents a sentence as input, and outputs a matrix of the same size as the embedding matrix, then the output can be used for downstream tasks.&lt;/p>
&lt;h2 id="notation">Notation
&lt;/h2>&lt;ol>
&lt;li>We denote $V=[N_V]:={1,\dots,N_V}$ as the &lt;em>vocabulary&lt;/em> of tokens or words or characters.&lt;/li>
&lt;li>We denote $\bm{x}=x[1&amp;hellip;n]:=x[1]&amp;hellip;x[n]\in V^n$ be a sequence of tokens, for example, a sentence or a paragraph.&lt;/li>
&lt;li>Given a matrix $M\in\mathbb{R}^{m\times n}$, $M[i,:]\in\mathbb{R}^n$ is the $i$-th row of $M$, $M[:, j]\in\mathbb{R}^m$ is the $j$-th column of $M$.&lt;/li>
&lt;/ol>
&lt;h1 id="tokenization">Tokenization
&lt;/h1>&lt;p>Tokenization determines how the text are represented. Given a piece of text, for example, &lt;code>&amp;quot;I have an apple&amp;quot;&lt;/code>, we seek to find a proper way to represent this sentence.&lt;/p>
&lt;ol>
&lt;li>Character level tokenization. In this setting, $V$ is the English alphabet plus punctuation. This tends to yield very long sequences (depends on the character contained in the raw text).&lt;/li>
&lt;li>Word level tokenization. In this setting, $V$ is the set of all English words plus punctuation. Word level tokenization is straightforward, but it tends to required a very large vocabulary and cannot handle new vocabulary at test time.&lt;/li>
&lt;li>Subword tokenization. This is the most common way used in nowadays, $V$ is the set containing the commonly used word segments like &amp;ldquo;ing&amp;rdquo;, &amp;ldquo;est&amp;rdquo;. This can be computed via Byte-Pair Encoding (BPE) algorithm.&lt;/li>
&lt;li>We suppose the length of the input text is $L$, if the length input text exceeds $L$, we chunk it.&lt;/li>
&lt;/ol>
&lt;p>After tokenization, each element in the vocabulary is assigned to a unique index $i\in{1,\dots,N_V-3}$, and a number of special tokens are added to the vocabulary. For example:&lt;/p>
&lt;ol>
&lt;li>&lt;code>mask_token&lt;/code>$=N_V-2$, used in masked language modeling&lt;/li>
&lt;li>&lt;code>bos_token&lt;/code>$=N_V-1$ and &lt;code>eos_token&lt;/code>$=N_V$, these two tokens are used to represent the beginning and the end of the sequence.&lt;/li>
&lt;/ol>
&lt;p>Finally, a piece of raw text is represented as a sequence of indices, often called &lt;em>token ID&lt;/em>s corresponding to its subwords, preceded by &lt;code>bos_token&lt;/code> and followed by &lt;code>eos_token&lt;/code>.&lt;/p>
&lt;h1 id="embedding">Embedding
&lt;/h1>&lt;p>The embedding layer is used to represent each token as a vector that contains richer semantic information. The embedding contains two parts:&lt;/p>
&lt;ol>
&lt;li>token embedding, where each token is embedded into a vector space&lt;/li>
&lt;li>positional embedding, where embeds the position information of the tokens.&lt;/li>
&lt;/ol>
&lt;h2 id="token-embedding">Token embedding
&lt;/h2>&lt;p>Given a sequence of token ID, we now need to represent each token as a vector in $\mathbb{R}^d$.&lt;/p>
&lt;p>The simplest way is to use &lt;em>one-hot embedding&lt;/em>, where each token $i$ is represented a vector $[0,\dots,1,\dots,0]\in\mathbb{R}^{N_V}$ whose elements are all $0$ excepts that $i$-th position is equal to $1$. However, the problem is that the vocabulary size $N_V$ is two large.&lt;/p>
&lt;p>To solve this problem, we can train a learnable embedding model, of which parameter is a matrix $W_e\in\mathbb{R}^{d\times N_V}$, its $i$-th row corresponds to vector representation of the token $i$:&lt;/p>
$$ \bm{e} = W_{e}[:, i]\in\mathbb{R}^d $$&lt;h2 id="position-embedding">Position embedding
&lt;/h2>&lt;p>There is a problem in token embedding, that is, it doesn&amp;rsquo;t contain consider the order of tokens. In latter, we show that the self-attention mechanism is equivariant to a permutation matrix $X\Pi$ of data $X$, where $\Pi$ is a permutation matrix, that is,&lt;/p>
$$ \mathrm{Sa}(X\Pi) = \mathrm{Sa}(X)\Pi $$&lt;p>the above equation indicates that the self-attention layer learn no position information at all!&lt;/p>
&lt;p>To solve this problem, we add a positional embedding to token embedding. There are two kinds of embeddings:&lt;/p>
&lt;ol>
&lt;li>Absolute positional embeddings. In this setting, a matrix $W_P\in\mathbb{R}^{d\times N}$ is learned or design to indicate the position of tokens. Mathematically, we have&lt;/li>
&lt;/ol>
$$ \bm{e}_p = W_p[:, \mathrm{index}(i)]\in\mathbb{R}^{d} $$&lt;p>where $\mathrm{index}(i)$ is the index of token $i$ in the input sequence.
2. Relative positional embeddings. We leave this in latter notes. Compared to absolute positional embeddings, relative positional embeddings uses offset information, which performs well when the input sequence is too long.&lt;/p>
&lt;p>The final embedding of a token $i$ is given by&lt;/p>
$$ \bm{e} = W_e[:, i] + W_p[:, \mathrm{index}(i)]\in\mathbb{R}^{d} $$&lt;h1 id="attention">Attention
&lt;/h1>&lt;p>The idea of attention mechanism is: Given a sequence of token, to predict the current token, which token should I pay attention to? For example, &lt;code>I opened the door with my ___&lt;/code>, we may answer &lt;code>key&lt;/code>, &lt;code>password&lt;/code> or &lt;code>fingerprint&lt;/code> etc. This is because we notice that we &lt;code>opened the door&lt;/code>, so to predict the next token, we should make use of the information. What attention mechanism does is to quantify this process and make them parallel and learnable.&lt;/p>
&lt;h2 id="single-query-attention">Single query attention
&lt;/h2>&lt;p>We first consider a simple example. Given the embedding of the current token $\bm{e}\in\mathbb{R}^d$ and the list of context tokens $[\bm{e}_1,\dots,\bm{e}_N]\in\mathbb{R}^{d\times N}$, the attention is given as follows:&lt;/p>
&lt;ol>
&lt;li>compute query vector: $\bm{q}=W_q\bm{e}+b_q\in\mathbb{R}^{d}$&lt;/li>
&lt;li>compute key vectors: for $i=1,\dots,L$, $\bm{k}_i=W_k\bm{e}_i+b_k\in\mathbb{R}^{d}$&lt;/li>
&lt;li>compute value vectors: for $i=1,\dots,L$, $\bm{v}_i=W_v\bm{e}_i+b_v\in\mathbb{R}^{d}$&lt;/li>
&lt;li>compute attention weights: let $\bm{s}=[\bm{q}^T\bm{k}_1,\dots,\bm{q}^T\bm{k}_L]\in\mathbb{R}^{N}$, then:&lt;/li>
&lt;/ol>
$$ \bm\alpha = \mathrm{softmax}\left(\frac{\bm{s}}{\sqrt{d}}\right)\in\mathbb{R}^{N}$$&lt;p>
5. compute vector representation of the token and context combined:&lt;/p>
$$ \bm{v}'= \sum_{i=1}^N\alpha_i\bm{v}_i\in\mathbb{R}^{d} $$&lt;p>where $W_q,W_k,W_v\in\mathbb{R}^{d\times d}$, $b_q,b_k,b_v\in\mathbb{R}$.&lt;/p>
&lt;h2 id="general-attention">General attention
&lt;/h2>&lt;p>To extend the single query attention to general form, we consider the embedding matrix $X\in\mathbb{R}^{D\times N}$, the context matrix $Z\in\mathbb{R}^{d\times C}$ and a mask matrix $M\in\mathbb{R}^{D\times D}$, then the attention is computed as follows:&lt;/p>
&lt;ol>
&lt;li>compute query matrix:&lt;/li>
&lt;/ol>
$$ Q=W_qX+\bm{b}_q\in\mathbb{R}^{D\times N}$$&lt;p>
2. compute key matrix:&lt;/p>
$$ K=W_kZ+\bm{b}_k\in\mathbb{R}^{D\times C}$$&lt;p>
3. compute value vectors:&lt;/p>
$$ V=W_vZ+\bm{b}_v\in\mathbb{R}^{D\times C}$$&lt;p>
4. compute attention weights:&lt;/p>
$$\mathrm{Sa}(X) = \mathrm{softmax}\left(M\odot \frac{K^TQ}{\sqrt{D}}\right) \in\mathbb{R}^{C\times N} $$&lt;p>
where $\odot$ is the element-wise product.
5. output the updated representations of tokens in $X$, folding the information from tokens in $Z$&lt;/p>
$$ \tilde{V} = V\odot \mathrm{Sa}(X)\in\mathbb{R}^{D\times N} $$&lt;p>There are two kinds of mask matrices depending on which attention we are using:&lt;/p>
&lt;ol>
&lt;li>Bidirectional attention, in this case, $M=\bm{1}\bm{1}^T\in\mathbb{R}^{C\times N}$.&lt;/li>
&lt;li>Undirectional attention, in this case, $M[i,j] = \bm{1}&lt;em>{i\leq j}$, where $\bm{1}&lt;/em>{i\leq j}$ is the &lt;em>indicator function&lt;/em>.&lt;/li>
&lt;/ol>
&lt;h2 id="multi-head-attention">Multi-head Attention
&lt;/h2>&lt;p>The previous describes the operation of a &lt;em>single head&lt;/em>. In practice, transformers run multiple attention heads in parallel and combine their outputs, this is called &lt;em>multi-head attention&lt;/em>. The idea behind multi-head attention can be summarized as follows:&lt;/p>
&lt;ol>
&lt;li>In high dimensional space, two vectors are usually far from each other, with multiple attention heads, we can reduce the dimension of the representation.&lt;/li>
&lt;li>With multiple attention heads, each heads may focus on specific semantics of the representation. For example, one head focuses on positiveness, and one another head focuses on noun/verb semantics.&lt;/li>
&lt;/ol>
&lt;p>For simplicity, we denote the single-head attention as $\mathrm{attention}(X, Z\mid M)$, suppose we have $H$ heads, then we compute $\tilde{V}_i$ for each heads:&lt;/p>
$$ \tilde{V}_i = \mathrm{attention}(X, Z\mid M)\in\mathbb{R}^{D\times N},i=1,\dots,H $$&lt;p>
then attention representation are concatenated together:&lt;/p>
$$ V = [\tilde{V}_1^T, \dots, \tilde{V}_H^T]^T\in\mathbb{R}^{HD\times N} $$&lt;p>combined via an output matrix $W_o\in\mathbb{R}^{D\times HD}$:&lt;/p>
$$ \tilde{V} = W_oV+\bm{b}_o\in\mathbb{R}^{D\times N} $$&lt;p>We denote the multi head attention as $\mathrm{MhSa}(X, Z\mid M)$.&lt;/p>
&lt;h2 id="transformer-layer">Transformer layer
&lt;/h2>&lt;p>After computing the multi head attention, we can now construct a transformer layer, which can also be stacked as convolution neural networks. A transformer layer can be constructed by the following operations:&lt;/p>
&lt;ol>
&lt;li>Multi head attention (residual), $X\gets X + \mathrm{MhSa}(X, Z\mid M)$.&lt;/li>
&lt;li>Layer norm, $X \gets \mathrm{LayerNorm}(X)$&lt;/li>
&lt;li>Multi layer perception $\bm{x}_i\gets \bm{x}_i + \mathrm{mlp}(\bm{x}_i), i=1,\dots,N$&lt;/li>
&lt;li>Layer norm, $X \gets \mathrm{LayerNorm}(X)$&lt;/li>
&lt;/ol>
&lt;p>where $\mathrm{LayerNorm}$ is the layer norm operation. $\mathrm{mlp}$ is a multi layer perception, usually it consists of one hidden layer of size $4D$, that is, then umber of neurons in three layers are $D, 4D, D$.&lt;/p>
&lt;p>Usually, a large language model consists of multiple transformer layers.&lt;/p>
&lt;h1 id="unembedding">Unembedding
&lt;/h1>&lt;p>The unembedding learns to convert a vector representation of a token and its context $\bm{e}$ into a distribution over the vocabulary elements.&lt;/p>
$$ \bm{p} = \mathrm{softmax}(W_u\bm{e})\in \Delta(V)\subseteq \mathbb{R}^d $$&lt;p>
where $\Delta(V)$ is a simplex over the set $V$.&lt;/p>
&lt;h1 id="reference">Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://udlbook.github.io/udlbook/" target="_blank" rel="noopener"
>Understanding Deep Learning Chapter 12&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2207.09238" target="_blank" rel="noopener"
>Formal Algorithms for Transformers&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>MiniGPT-4-Enhancing Vision-Language Understanding with Advanced Large Language Models</title><link>https://maosong2022.github.io/p/minigpt-4-enhancing-vision-language-understanding-with-advanced-large-language-models/</link><pubDate>Thu, 02 May 2024 13:13:12 +0800</pubDate><guid>https://maosong2022.github.io/p/minigpt-4-enhancing-vision-language-understanding-with-advanced-large-language-models/</guid><description>&lt;h1 id="tldr">TLDR
&lt;/h1>&lt;p>This paper proposed an open-sourced, GPT-4 liked multimodal language model (MLLM), mini-GPT4, aiming to provide inspiration of how to build an MLLM.&lt;/p>
&lt;h1 id="introduction">Introduction
&lt;/h1>&lt;p>GPT-4 has exhibited remarkable vision language abilities, however, due to it is not open sourced, it is hard to explore the construction and how the GPT-4 is trained.&lt;/p>
&lt;p>To solve this problem, this paper builds mini-GPT4, which utilizes an advanced LLM, Vicuna and BLIP-2, to build an open sourced MLLM. The result show that mini-GPT4 possesses numerous capabilities similar to those demostrated by GPT-4.&lt;/p>
&lt;h1 id="method">Method
&lt;/h1>&lt;p>The architecture of mini-GPT4 is shown as follows:&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/minigpt-4-enhancing-vision-language-understanding-with-advanced-large-language-models/mini-gpt4-architecture.png"
width="907"
height="600"
srcset="https://maosong2022.github.io/p/minigpt-4-enhancing-vision-language-understanding-with-advanced-large-language-models/mini-gpt4-architecture_hu12707907117125568329.png 480w, https://maosong2022.github.io/p/minigpt-4-enhancing-vision-language-understanding-with-advanced-large-language-models/mini-gpt4-architecture_hu14925688768741311301.png 1024w"
loading="lazy"
alt="mini-GPT4 architecture"
class="gallery-image"
data-flex-grow="151"
data-flex-basis="362px"
>&lt;/p>
&lt;p>mini-GPT4 consists of three parts:&lt;/p>
&lt;ol>
&lt;li>Vision Encoder same as used in BLIP-2, which is built upon a ViT backbone and a Q-former network.&lt;/li>
&lt;li>A single projection layer, which aligns the encoded visual features with the Vicuna language model.&lt;/li>
&lt;li>Language decoder: Vicuna.&lt;/li>
&lt;/ol>
&lt;h2 id="training">Training
&lt;/h2>&lt;p>mini-GPT4 only trains the linear projection layer, this includes two stages:&lt;/p>
&lt;ol>
&lt;li>First pre-training stage: in this stage, the language decoder and vision encoder are remain frozen, only the linear projection layer is trained. The training dataset contains Conceptual Caption, SBU and LAION. The problem is that the output may be incoherent like repetitive words or sentences.&lt;/li>
&lt;li>Fine-tune dataset generation: the pre-trained mini-GPT4 is used to generate image-text pair as fine-tune data, to improve the text quality, the generated text is polished with the help of ChatGPT. Finally, the dataset is manually refined.&lt;/li>
&lt;li>Second-stage fine-tuning: Fine-tune the mini-GPT-4 with the high quality fine-tune data.&lt;/li>
&lt;/ol>
&lt;h1 id="reference">Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://github.com/Vision-CAIR/MiniGPT-4/tree/main" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://openreview.net/forum?id=1tZbq88f27" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://minigpt-4.github.io/" target="_blank" rel="noopener"
>Homepage&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on t-SNE</title><link>https://maosong2022.github.io/p/notes-on-t-sne/</link><pubDate>Thu, 02 May 2024 13:13:12 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-t-sne/</guid><description>&lt;p>This post introduces how to understand t-SNE.&lt;/p>
&lt;h1 id="introduction">Introduction
&lt;/h1>&lt;p>t-SNE an dimension reduction algorithm, which projects high-dimensional data into low-dimensional space. Thus the algorithm can be used to visualize the data distribution.&lt;/p>
&lt;p>To understand how t-SNE works, we first review the SNE algorithms, then we introduce the t-SNE algorithm.&lt;/p>
&lt;h1 id="sne">SNE
&lt;/h1>&lt;h2 id="method">Method
&lt;/h2>&lt;p>Stochastic Neighbor Embedding, or SNE, is the previous version of t-SNE.&lt;/p>
&lt;p>The basic idea behind SNE is that: &lt;em>data points that are close in high-dimensional space should be close in lower-dimensional space too&lt;/em>.&lt;/p>
&lt;p>Formally speaking, given a data set $X\in\mathbb{R}^{D\times N}$ consisting of $N$ data points, with each data point lies in $D$ dimensional space. Our goal is to reduce the data points into $d&amp;laquo; D$ dimensional space $Y\in\mathbb{R}^{d\times N}$, that is, we seek to find a map $f:\mathbb{R}^{D\times N}\to \mathbb{R}^{d\times N}$ such that $f(X)=Y$. Usually, $d=2$ or $d=3$ for visualization use.&lt;/p>
&lt;p>SNE measures &amp;ldquo;close&amp;rdquo; in a probabilistic way. The similarity is represented by converting Euclidean distance between data points to condition probabilities:&lt;/p>
$$ p_{j\mid i} = \frac{\exp\left(-\Vert\bm{x}_i-\bm{x}_j\Vert^2/(2\sigma_i^2)\right)}{\sum_{k\neq i}\exp\left(-\Vert\bm{x}_i-\bm{x}_k\Vert^2/(2\sigma_i^2)\right)} $$&lt;p>the above equation can be interpreted as &lt;em>the probability of point $\bm{x}_j$ being a neighbor of point $\bm{i}$ is proportional to the distance between them&lt;/em>. $\sigma_i$ is the variance of the Gaussian distribution that is centered on data point $\bm{x}_i$. We introduce the method for determining $\sigma_i$ later.&lt;/p>
&lt;p>Similarly, we can construct a probability distribution $q$ based on $Y$.&lt;/p>
$$ q_{j\mid i} = \frac{\exp\left(-\Vert\bm{x}_i-\bm{x}_j\Vert^2\right)}{\sum_{k\neq i}\exp\left(-\Vert\bm{x}_i-\bm{x}_k\Vert^2\right)} $$&lt;p>where we set the variance as $1/\sqrt{2}$ following the original paper.&lt;/p>
&lt;p>$p_{i\mid i}$ and $q_{i\mid i}$ are set $0$ since we are only interested in modeling pairwise similarities.&lt;/p>
&lt;p>Now we want $q_{j\mid i}$ are as close as $p_{j\mid i}$, that is, we want two distributions are as close as to each other. This can be measured by &lt;strong>Kullback- Leibler divergence&lt;/strong>, which is written as:&lt;/p>
$$ C(P, Q) = \sum_{i=1}^N\mathrm{KL}(P_i\Vert Q_i)=\sum_{i=1}^N\sum_{j=1}^N p_{j\mid i}\log \frac{p_{i\mid j}}{q_{i\mid j}} $$&lt;p>where $P_i=[p_{1\mid i},\dots,p_{N\mid i}]\in\mathbb{R}^N$ and $Q_i=[q_{1\mid i},\dots,q_{N\mid i}]\in\mathbb{R}^N$.&lt;/p>
&lt;h2 id="choosing-sigma">Choosing $\sigma$
&lt;/h2>&lt;p>Now we introduce how to choose $\sigma$. Note that $\sigma$ determines the distribution of data points, larger $\sigma$ indicates sparser distribution of data points. The original paper uses &lt;em>perplexity&lt;/em> to measure such sparsity. It is defined as&lt;/p>
$$ \mathrm{Perp}(P_i) = 2^{H(P_i)} $$&lt;p>where $H(P_i)$ is the &lt;em>Shannon entropy&lt;/em> of $P_i$ measured in bits:&lt;/p>
$$ H(P_i) = -\sum_{i=1}^N p_{j\mid i}\log p_{j\mid i} $$&lt;p>The perplexity can be interpreted as a smooth measure of the effective number of neighbors. The performance of SNE is fairly robust to changes in the perplexity, and typical values are between 5 and 50.&lt;/p>
&lt;p>Notice that $p_{j\mid i}$, by setting different value on $\mathrm{Perp}(P_i)$, we can obtain different $\sigma_i$ via binary search.&lt;/p>
&lt;h2 id="optimization">Optimization
&lt;/h2>&lt;p>Our goal now becomes minimizing $C(p, q)$ over variables $\bm{y}_1,\dots,\bm{y}_N\in\mathbb{R}^d$, given $p$ and hyperparameter $\sigma_i, i=1,\dots,N$. This can be done via gradient descent methods. The gradient is now given by&lt;/p>
$$ \frac{d C}{d\bm{y}_i}=2\sum_{j=1}^N\left(p_{j\mid i} - q_{j\mid i} + p_{i\mid j}- q_{i\mid j} \right)(\bm{y}_i-\bm{y}_j)\in\mathbb{R}^d, \ i=1,\dots,N $$&lt;p>We can write it in matrix form and add a momentum term:&lt;/p>
$$ Y^{t+1} = Y^t + \beta\frac{dC}{dY} + \alpha_t\left(Y^{t-1}-Y^{t-2}\right) $$&lt;p>where $\beta$ is the step size and $\alpha_t$ is momentum parameter,&lt;/p>
$$ Y^t = [\bm{y}_1^t,\dots, \bm{y}_N^t]\in\mathbb{R}^{d\times N} ,\ \frac{dC}{dY} = \left[\frac{d C}{d\bm{y}_1},\dots,\frac{d C}{d\bm{y}_N}\right]\in\mathbb{R}^{d\times N} $$&lt;h1 id="t-sne">t-SNE
&lt;/h1>&lt;h2 id="symmetric-sne">Symmetric SNE
&lt;/h2>&lt;p>The first difference between t-SNE and SNE is the probability, t-SNE uses symmetric version of SNE to simplify computations.&lt;/p>
&lt;p>Different from SNE, symmetric SNE uses a joint probability instead of a condition probability:&lt;/p>
$$ C(P, Q) = \sum_{i=1}^N\mathrm{KL}(P\Vert Q)=\sum_{i=1}^N\sum_{j=1}^N p_{ij}\log \frac{p_{ij}}{q_{ij}} $$&lt;p>where $p_{ij}$ and $q_{ij}$ are defined as&lt;/p>
$$ p_{ij} = \frac{\exp\left(-\Vert\bm{x}_i-\bm{x}_j\Vert^2/(2\sigma^2)\right)}{\sum_{k\neq r}\exp\left(-\Vert\bm{x}_r-\bm{x}_k\Vert^2/(2\sigma^2)\right)},q_{ij} = \frac{\exp\left(-\Vert\bm{x}_i-\bm{x}_j\Vert^2\right)}{\sum_{k\neq r}\exp\left(-\Vert\bm{x}_r-\bm{x}_k\Vert^2\right)} $$&lt;p>the problem if joint probability $p_{ij}$ is that if there is an outlier $\bm{x}&lt;em>i$, then $p&lt;/em>{ij}$ will be extremely small for all $j$. This problem can be solved by defining $p_{ij}$ from the conditional probability $p_{i\mid j}$ and $p_{j\mid i}$&lt;/p>
$$ p_{ij} = \frac{p_{j\mid i}+p_{i\mid j}}{2N} $$&lt;p>This ensures that&lt;/p>
$$ \sum_{j=1}^N p_{ij} > \frac{1}{2N} $$&lt;p>for all $\bm{x}_i$, in result, each data point makes a significant contribution to the cost function.&lt;/p>
&lt;p>In this case, the gradient of the cost function is now given by&lt;/p>
$$ \frac{d C}{d\bm{y}_i}=4\sum_{j=1}^N\left(p_{ij} - q_{ij}\right)(\bm{y}_i-\bm{y}_j)\in\mathbb{R}^d, \ i=1,\dots,N $$&lt;h2 id="t-sne-1">t-SNE
&lt;/h2>&lt;p>Experiments show that symmetric SNE seems to produce maps that are just as good as asymmetric SNE, and sometimes even a little better.&lt;/p>
&lt;p>However, there is a problem with SNE, that is, the &lt;em>crowding problem&lt;/em>, which manifests as a tendency for points in the low-dimensional space to be clustered too closely together, particularly in high-density regions of the data.&lt;/p>
&lt;p>The causes of the crowding problems are:&lt;/p>
&lt;ol>
&lt;li>Data points in high dimensional space tend to far from each other, which makes the distance information less useful.&lt;/li>
&lt;li>SNE aims to preserve the local structure of the data points, but it can struggle with non-linear relationships. The projected data points will be closed to each other due to this reason.&lt;/li>
&lt;li>The optimization algorithm used by SNE can get stuck in local minimum.&lt;/li>
&lt;/ol>
&lt;p>To alleviate the crowding problem, t-SNE is introduced in the following way:
&lt;em>In the high-dimensional space, we convert distances into probabilities using a Gaussian distribution. In the low-dimensional map, we can use a probability distribution that has much heavier tails than a Gaussian to convert distances into probabilities.&lt;/em>
This allows a moderate distance in the high-dimensional space to be faithfully modeled by a much larger distance in the map and, as a result, it eliminates the unwanted attractive forces between map points that represent moderately dissimilar data points.&lt;/p>
&lt;p>t-SNE uses student t-distribution in low-dimensional map:&lt;/p>
$$ q_{ij} = \frac{\left(1+\Vert\bm{y}_i-\bm{y}_j\Vert^2\right)^{-1}}{\sum_{k\neq r}\left(1+\Vert\bm{y}_i-\bm{y}_j\Vert^2\right)^{-1}} $$&lt;p>A Student t-distribution with a single degree of freedom is used, because it has the particularly nice property that $\left(1+\Vert\bm{y}_i-\bm{y}_j\Vert^2\right)^{-1}$ approaches an inverse square law for large pairwise distances $\Vert\bm{y}_i-\bm{y}_j\Vert$ in the low-dimensional map.&lt;/p>
&lt;p>Compared to Gaussian distribution, t-distribution is heavily tailed。&lt;/p>
&lt;p>A computationally convenient property of t-SNE is that it is much faster to evaluate the density of a point under a Student t-distribution than under a Gaussian because it does not involve an exponential, even though the Student t-distribution is equivalent to an infinite mixture of Gaussians with different variances.&lt;/p>
&lt;p>The gradient of t-SNE is given by&lt;/p>
$$ \frac{d C}{d\bm{y}_i}=4\sum_{j=1}^N\left(p_{ij} - q_{ij}\right)(\bm{y}_i-\bm{y}_j)\left(1+\Vert\bm{y}_i-\bm{y}_j\Vert^2\right)^{-1}\in\mathbb{R}^d, \ i=1,\dots,N $$&lt;p>The advantages of t-SNE gradients over SNE are given by:&lt;/p>
&lt;ol>
&lt;li>The t-SNE gradient strongly repels dissimilar data points that are modeled by a small pairwise distance in the low-dimensional representation.&lt;/li>
&lt;li>Second, although t-SNE introduces strong repulsions between dissimilar data points that are modeled by small pairwise distances, these repulsions do not go to infinity.&lt;/li>
&lt;/ol>
&lt;p>The algorithm is given as follows:&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-t-sne/t-SNE-algorithm.png"
width="1910"
height="1096"
srcset="https://maosong2022.github.io/p/notes-on-t-sne/t-SNE-algorithm_hu528927631688914678.png 480w, https://maosong2022.github.io/p/notes-on-t-sne/t-SNE-algorithm_hu15284604004524732623.png 1024w"
loading="lazy"
alt="algorithm"
class="gallery-image"
data-flex-grow="174"
data-flex-basis="418px"
>&lt;/p>
&lt;h2 id="optimization-1">Optimization
&lt;/h2>&lt;p>There are some optimizations that can be used to improve performance of t-SNE:&lt;/p>
&lt;ol>
&lt;li>Early compression, which is used to force the map points to stay close together at the start of the optimization&lt;/li>
&lt;li>Early exaggeration, which is used to multiply all of the pi j’s by, for example, 4, in the initial stages of the optimization&lt;/li>
&lt;/ol>
&lt;h1 id="implementation">Implementation
&lt;/h1>&lt;h1 id="reference">Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="http://jmlr.org/papers/v9/vandermaaten08a.html" target="_blank" rel="noopener"
>Visualizing Data using t-SNE&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>1915. Number of Wonderful Substrings</title><link>https://maosong2022.github.io/p/1915.-number-of-wonderful-substrings/</link><pubDate>Thu, 02 May 2024 10:51:29 +0800</pubDate><guid>https://maosong2022.github.io/p/1915.-number-of-wonderful-substrings/</guid><description>&lt;p>Given a string ,count the number of wonderful substrings, where a &lt;strong>wonderful string&lt;/strong> is a string that at most one character appears in an odd number of times.&lt;/p>
&lt;h1 id="intuition">Intuition
&lt;/h1>&lt;p>We use the prefix sum and bitwise representation to solve the problem. The idea is that every string can be represented by a state, and we can count the number of wonderful substrings by performing operations among states&lt;/p>
&lt;h1 id="approach">Approach
&lt;/h1>&lt;p>According to the hint and the problem description, we have:&lt;/p>
&lt;ol>
&lt;li>there are $10$ possible variables, which represent &lt;code>a&lt;/code> to &lt;code>j&lt;/code>&lt;/li>
&lt;li>we only care about if the variable appears in odd number of times or in even number of times, which means that each variable has $2$ possible states.&lt;/li>
&lt;/ol>
&lt;p>so the number of overall possible states is $2^10=1024$.
We define the prefix substring as &lt;code>prefix[i] = s[1...i]&lt;/code>.&lt;/p>
&lt;p>For each substring, we can represent the state of the prefix as an index of the array, that is, &lt;code>state(prefix[i])&lt;/code> is a number between 0 and 1023, with each byte representing if the corresponding character appears in odd number of times (&lt;code>1&lt;/code>) or in even number of times (&lt;code>0&lt;/code>). A wonderful string is then defined as &lt;em>a string whose state representation is 0 or a power of 2&lt;/em>.&lt;/p>
&lt;p>now the substring &lt;code>s[i...j]&lt;/code> is defined as &lt;code>prefix[j]-prefix[i]&lt;/code>, in state representations, there are three cases:&lt;/p>
&lt;ol>
&lt;li>if one character appears in odd number of times in both prefix substring, then in the result substring, the character appears in even number of times, which is equivalent to &lt;code>(1, 1) -&amp;gt; 0&lt;/code>&lt;/li>
&lt;li>if one character appears in even number of times in both prefix substring, then in the result substring, the character appears in even number of times, which is equivalent to &lt;code>(0, 0) -&amp;gt; 0&lt;/code>&lt;/li>
&lt;li>If one character appears in even number of times in one prefix substring, and appears in odd number of times in the other prefix substring, then in the result substring, the character appears in even number of times, which is equivalent to &lt;code>(1, 0) -&amp;gt; 1&lt;/code>&lt;/li>
&lt;/ol>
&lt;p>so, according to analysis, the minus operation in string corresponding to the XOR operation in states. That is, the state of &lt;code>s[i...j]&lt;/code> is given by &lt;code>state(s[i...j])=state(prefix[j]) XOR state(prefix[i])&lt;/code>.&lt;/p>
&lt;p>Note that there is also a simplification since if &lt;code>a XOR b = c&lt;/code>, then &lt;code>a XOR c = b&lt;/code>. Instead of using a for loop to compute all substrings that ended with character &lt;code>j&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// substring s[i...j]
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">substr_state&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">state&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">^&lt;/span> &lt;span class="n">state&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// check if substr is a wonderful substring
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>with the property of XOR, wo directly check if &lt;code>state(prefix[j]) XOR 2^k&lt;/code> exists, that is:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">for (int i = 0; i &amp;lt; 10; ++i) {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> int state = state[j] ^ (1 &amp;lt;&amp;lt; i);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> // check if state exists and add them.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="complexity">Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n)$$&lt;/li>
&lt;li>
$$O(1)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">long&lt;/span> &lt;span class="kt">long&lt;/span> &lt;span class="n">wonderfulSubstrings&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">string&lt;/span> &lt;span class="n">word&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">bits&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1024&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">bits&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">long&lt;/span> &lt;span class="kt">long&lt;/span> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">prefix&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">char&lt;/span> &lt;span class="nl">c&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">word&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">prefix&lt;/span> &lt;span class="o">^=&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="o">&amp;lt;&amp;lt;&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">c&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="sc">&amp;#39;a&amp;#39;&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">bits&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">prefix&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="mi">10&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">bits&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">prefix&lt;/span> &lt;span class="o">^&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span> &lt;span class="o">&amp;lt;&amp;lt;&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">)];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">++&lt;/span>&lt;span class="n">bits&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">prefix&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>References:&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/number-of-wonderful-substrings/description/" target="_blank" rel="noopener"
>Leetcode&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>2441. Largest Positive Integer That Exists With Its Negative</title><link>https://maosong2022.github.io/p/2441.-largest-positive-integer-that-exists-with-its-negative/</link><pubDate>Thu, 02 May 2024 10:28:05 +0800</pubDate><guid>https://maosong2022.github.io/p/2441.-largest-positive-integer-that-exists-with-its-negative/</guid><description>&lt;p>Given a list containing integers, find the largest positive integer that its negative also exists in the list.&lt;/p>
&lt;h1 id="intuition">Intuition
&lt;/h1>&lt;p>There are two ways to solve the problem.&lt;/p>
&lt;ul>
&lt;li>One way is to use two sum, we find the all pairs of integers such that their sum is zero and the one is the negative of the other.&lt;/li>
&lt;li>The second way is to use two pointer, we move left and right pointer utils their absolute values are equal.&lt;/li>
&lt;/ul>
&lt;h1 id="approach-1-two-sum">Approach 1: two sum
&lt;/h1>&lt;p>Similar to Two Sum, for each &lt;code>num&lt;/code> in &lt;code>nums&lt;/code>, we store its negative &lt;code>-num&lt;/code> in the hash table,
however, notice that the added term can be determined by &lt;code>num&lt;/code>, we can use a set instead.&lt;/p>
&lt;h2 id="complexity">Complexity
&lt;/h2>&lt;ul>
&lt;li>
$$O(n)$$&lt;/li>
&lt;li>
$$O(n)$$&lt;/li>
&lt;/ul>
&lt;h2 id="code">Code
&lt;/h2>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">findMaxK&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">set&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="nl">num&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">find&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">num&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">max&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">result&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">abs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">s&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">insert&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="approach-2-two-pointer">Approach 2: two pointer
&lt;/h1>&lt;p>We first sort the lists, then we use &lt;code>left=0&lt;/code> and &lt;code>right=length(nums)&lt;/code> pointer to iterate the list, there are three cases:&lt;/p>
&lt;ol>
&lt;li>if &lt;code>nums[left] == -nums[right]&lt;/code>, then we directly return &lt;code>nums[right]&lt;/code> since this is the largest one (note that the list is sorted)&lt;/li>
&lt;li>if &lt;code>nums[left] &amp;lt; -nums[right]&lt;/code>, then we update &lt;code>left&lt;/code> to &lt;code>left + 1&lt;/code> since &lt;code>(nums[left], -nums[left])&lt;/code> cannot be found in the list.&lt;/li>
&lt;li>if &lt;code>nums[left] &amp;gt; -nums[right]&lt;/code>, then we update &lt;code>right&lt;/code> to &lt;code>right - 1&lt;/code> since &lt;code>(-nums[right], nums[right])&lt;/code> cannot be found in the list.&lt;/li>
&lt;/ol>
&lt;h2 id="complexity-1">Complexity
&lt;/h2>&lt;ul>
&lt;li>
$$O(n\log n)$$&lt;/li>
&lt;li>
$$O(1)$$&lt;/li>
&lt;/ul>
&lt;h2 id="code-1">Code
&lt;/h2>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">findMaxK&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sort&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nums&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">begin&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">());&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">left&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">right&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">left&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">right&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">nums&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">left&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="n">nums&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">right&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="k">return&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">right&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span> &lt;span class="nf">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">nums&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">left&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="n">nums&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">right&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">left&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">right&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">``&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cp"># References
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cp">&lt;/span>&lt;span class="o">-&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">Leetcode&lt;/span>&lt;span class="p">](&lt;/span>&lt;span class="nl">https&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="c1">//leetcode.com/problems/largest-positive-integer-that-exists-with-its-negative/description/)
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="o">-&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">Leetcode&lt;/span> &lt;span class="n">Two&lt;/span> &lt;span class="n">Sum&lt;/span>&lt;span class="p">](&lt;/span>&lt;span class="nl">https&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="c1">//leetcode.com/problems/two-sum/)
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>2000. Reverse Prefix of Word</title><link>https://maosong2022.github.io/p/2000.-reverse-prefix-of-word/</link><pubDate>Wed, 01 May 2024 09:01:22 +0800</pubDate><guid>https://maosong2022.github.io/p/2000.-reverse-prefix-of-word/</guid><description>&lt;p>Given a string ans a specified character, reverse the prefix that is ended with the specified character.&lt;/p>
&lt;h1 id="intuition">Intuition
&lt;/h1>&lt;p>Simulate the process.&lt;/p>
&lt;h1 id="approach">Approach
&lt;/h1>&lt;p>find the first occurrence of the specified character, then reverse the prefix.&lt;/p>
&lt;h1 id="complexity">Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n)$$&lt;/li>
&lt;li>
$$O(1)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;span class="lnt">9
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">string&lt;/span> &lt;span class="n">reversePrefix&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">string&lt;/span> &lt;span class="n">word&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">char&lt;/span> &lt;span class="n">ch&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">auto&lt;/span> &lt;span class="n">pos&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">word&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">find&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ch&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">pos&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">word&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">npos&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">return&lt;/span> &lt;span class="n">word&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">reverse&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">word&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">begin&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">word&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">begin&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">pos&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">word&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>834. Sum of Distances in Tree</title><link>https://maosong2022.github.io/p/834.-sum-of-distances-in-tree/</link><pubDate>Mon, 29 Apr 2024 21:24:06 +0800</pubDate><guid>https://maosong2022.github.io/p/834.-sum-of-distances-in-tree/</guid><description>&lt;p>Given a tree, return a vector, with each elements of the vector is the sum of the distances between the corresponding node and all other nodes.&lt;/p>
&lt;h1 id="intuition">Intuition
&lt;/h1>&lt;p>Intuitively, I want to use DFS + memorization to solve the problem, however, such method exceeds te time limit.&lt;/p>
&lt;p>Then, I refer to some solutions, and solve the problem.&lt;/p>
&lt;h1 id="approach">Approach
&lt;/h1>&lt;h2 id="dp---tle">DP -&amp;gt; TLE
&lt;/h2>&lt;p>In the first, I am thinking that we can use &lt;code>dp[i][j]&lt;/code> to represent the distance between the node &lt;code>i&lt;/code> and the node &lt;code>j&lt;/code>. Initially, &lt;code>dp&lt;/code> are initialized as follows:&lt;/p>
&lt;ol>
&lt;li>If there is an edge between &lt;code>i&lt;/code> and &lt;code>j&lt;/code>, then &lt;code>dp[i][j]=dp[j][i]=1&lt;/code>.&lt;/li>
&lt;li>If there is an edge between &lt;code>i&lt;/code> and &lt;code>j&lt;/code>, then &lt;code>dp[i][j]=dp[j][i]=INFINITY&lt;/code>.&lt;/li>
&lt;/ol>
&lt;p>we traversal from node &lt;code>0&lt;/code> to node &lt;code>n-1&lt;/code>, for each node &lt;code>i&lt;/code>, we compute the distance between &lt;code>i&lt;/code> and all other nodes &lt;code>j&lt;/code>. There are two cases:&lt;/p>
&lt;ol>
&lt;li>&lt;code>dp[i][j] != INFINITY&lt;/code>, then we directly returns &lt;code>dp[i][j]&lt;/code>&lt;/li>
&lt;li>&lt;code>dp[i][j] == INFINITY&lt;/code>, then it means the distance between node &lt;code>i&lt;/code> and node &lt;code>j&lt;/code> hasn&amp;rsquo;t been computed, we then update it as follows:&lt;/li>
&lt;/ol>
$$ dp[i][j] = \min_{k\in N(i)} (1 + dp[k][j]) $$&lt;p>where $N(i)$ is the nodes that adjacent to node &lt;code>i&lt;/code>. The &lt;code>min&lt;/code> operation is used here since the node &lt;code>k&lt;/code> and the node &lt;code>j&lt;/code> may not connected (without passing node &lt;code>i&lt;/code>).&lt;/p>
&lt;p>The code is given as follows. However, the time complexity is $O(n^2)$, which exceeds the time limit.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">adjs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">dp&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">bool&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">visited&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">start&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">end&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">start&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="n">INT_MAX&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">return&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">start&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">visited&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">start&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="k">return&lt;/span> &lt;span class="n">INT_MAX&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">visited&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">start&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">true&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">distance&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">INT_MAX&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="nl">next_node&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">adjs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">start&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">distance&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">distance&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">adjs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">visited&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">next_node&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">end&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">visited&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">start&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">false&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">distance&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="n">INT_MAX&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">start&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">distance&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">start&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">sumOfDistancesInTree&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">edges&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">INT_MAX&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">adjs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// initialize the distance matrix
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="k">auto&lt;/span>&lt;span class="o">&amp;amp;&lt;/span> &lt;span class="nl">edge&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">edges&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]][&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]][&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">adjs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]].&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">adjs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]].&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">bool&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">visited&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">false&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">adjs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">visited&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// the graph is undirected
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">j&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="tree--traversal">Tree + Traversal
&lt;/h2>&lt;p>The second way is not easy to figure out. We decompose it into two parts:&lt;/p>
&lt;ol>
&lt;li>compute the distance between the root node and all other nodes.&lt;/li>
&lt;li>Convert the root node from one to another and update the result.&lt;/li>
&lt;/ol>
&lt;h3 id="sum-of-distance-from-root-node-to-all-other-nodes">Sum of distance from root node to all other nodes.
&lt;/h3>&lt;p>Consider one example tree with root set as &lt;code>0&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl"> &lt;span class="mi">0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">/&lt;/span> \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="mi">1&lt;/span> &lt;span class="mi">2&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">/&lt;/span> &lt;span class="o">|&lt;/span> \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="mi">3&lt;/span> &lt;span class="mi">4&lt;/span> &lt;span class="mi">5&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>We first define &lt;code>dist[i]&lt;/code> as the distance of all child nodes of node &lt;code>i&lt;/code> to node &lt;code>i&lt;/code>. Then we have:&lt;/p>
&lt;ol>
&lt;li>$dist[i] = 0$ if &lt;code>i&lt;/code> is a leaf node.&lt;/li>
&lt;li>$dist[i] = \sum_{j\in C(i)}dist[j] + |C(i)|$ if &lt;code>i&lt;/code> is not a leaf node, where $C(i)$ is the offspring of node &lt;code>i&lt;/code>&lt;/li>
&lt;/ol>
&lt;p>We can now compute the sum of distances between root &lt;code>0&lt;/code> to all other nodes, which is &lt;code>result[0]&lt;/code>.&lt;/p>
&lt;p>Now we need to compute all other results. Repeating the above process is too time consuming, we need to reduce the time complexity. We are seeking a way to compute &lt;code>result[i]&lt;/code> from &lt;code>result[j]&lt;/code>, where $j\in C(i)$.&lt;/p>
&lt;p>Note that for $k\in C(j)$, when we compute &lt;code>result[i]&lt;/code>, we are computing distance from $k$ to $i$, so we need to reduce by 1 since their height decreases (the root changes from &lt;code>i&lt;/code> to &lt;code>j&lt;/code>). On the other hand, all other nodes, which are not offspring of node &lt;code>j&lt;/code>, is added by 1 since the height of them increases. Thus, the transformation reads:&lt;/p>
$$ result[j] = result[i] - |C(j)| + n - |C(j)| $$&lt;h1 id="complexity">Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$ O(n) $$&lt;/li>
&lt;li>
$$ O(n) $$&lt;/li>
&lt;/ul>
&lt;h1 id="code">Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">void&lt;/span> &lt;span class="n">post_order_traversal&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">adjs&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">count&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">parent&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="nl">next_node&lt;/span> &lt;span class="p">:&lt;/span> &lt;span class="n">adjs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">next_node&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">parent&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">post_order_traversal&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">adjs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">count&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">next_node&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">count&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">count&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">next_node&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">count&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">next_node&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">next_node&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">++&lt;/span>&lt;span class="n">count&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">void&lt;/span> &lt;span class="nf">pre_order_traversal&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">adjs&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">count&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">parent&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="nl">next_node&lt;/span> &lt;span class="p">:&lt;/span> &lt;span class="n">adjs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">next_node&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">parent&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">next_node&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">count&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">next_node&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">count&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">count&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">next_node&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pre_order_traversal&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">adjs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">count&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">next_node&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">sumOfDistancesInTree&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">edges&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">count&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">adjs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="k">auto&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="nl">edge&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">edges&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">adjs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]].&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">adjs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]].&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// compute result[0]
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="n">post_order_traversal&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">adjs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">count&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// compute other results from result[0]
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="n">pre_order_traversal&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">adjs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">count&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="references">References
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/sum-of-distances-in-tree/description/" target="_blank" rel="noopener"
>Leetcode&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.cnblogs.com/grandyang/p/11520804.html" target="_blank" rel="noopener"
>Solution&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>2997. Minimum Number of Operations to Make Array XOR Equal to K</title><link>https://maosong2022.github.io/p/2997.-minimum-number-of-operations-to-make-array-xor-equal-to-k/</link><pubDate>Mon, 29 Apr 2024 21:16:53 +0800</pubDate><guid>https://maosong2022.github.io/p/2997.-minimum-number-of-operations-to-make-array-xor-equal-to-k/</guid><description>&lt;p>Given an integer array, we can flip one bit of one element of the array at every step, we asked to compute the &lt;em>minimum&lt;/em> flips, such that the XOR of all elements of the array is equal to the given integer $k$.&lt;/p>
&lt;h1 id="intuition">Intuition
&lt;/h1>&lt;p>XOR of multiple bits is always equal to either &lt;code>1&lt;/code> or &lt;code>0&lt;/code>, and changes one of the input bits will cause the result change to the other one.&lt;/p>
&lt;h1 id="approach">Approach
&lt;/h1>&lt;p>We first compute the XOR of all elements of the array, then we compare bit by bit with the XOR result and the given &lt;code>k&lt;/code>, utils all bits becomes the same.&lt;/p>
&lt;h1 id="complexity">Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n)$$&lt;/li>
&lt;li>
$$O(1)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">minOperations&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">^=&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">bits_result&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bits_k&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">bits_k&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">k&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">k&lt;/span> &lt;span class="o">/=&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">result&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">bits_result&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">result&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">/=&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">count&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">bits_result&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">bits_k&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">bits_result&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="n">bits_k&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">++&lt;/span>&lt;span class="n">count&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">bits_result&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">bits_result&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">++&lt;/span>&lt;span class="n">count&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">bits_k&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">bits_k&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">++&lt;/span>&lt;span class="n">count&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">count&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>Regularization methods in deep learning</title><link>https://maosong2022.github.io/p/regularization-methods-in-deep-learning/</link><pubDate>Sat, 27 Apr 2024 18:02:02 +0800</pubDate><guid>https://maosong2022.github.io/p/regularization-methods-in-deep-learning/</guid><description>&lt;p>To reduce the gap of the performance of the model on the training dataset and the test dataset, we need use regularization methods.&lt;/p>
&lt;h1 id="introduction">Introduction
&lt;/h1>&lt;p>Possible reasons for the discrepancy that the model performs worse on the test dataset than on the training dataset are:&lt;/p>
&lt;ol>
&lt;li>the model describes statistical peculiarities of the training dataset that are not representative of the true mapping from the input to the output, that is, overfitting.&lt;/li>
&lt;li>the model is unconstrained in areas with no training areas, leading to suboptimal predictions&lt;/li>
&lt;/ol>
&lt;h1 id="explicit-regularization-methods">Explicit Regularization Methods
&lt;/h1>&lt;p>Consider fitting a model $f(\bm{x}; \phi)$ with parameter $\phi$ using a training dataset ${\bm{x}_i,y_i}$, we seek to minimize the loss function $L(\phi)$:&lt;/p>
$$ \hat{\phi} = \arg\min_{\phi}L(\phi;\{\bm{x}_i,y_i\}) $$&lt;p>Now, to bias the minimization towards certain solutions, we add an additional term:&lt;/p>
$$ \hat{\phi} = \arg\min_{\phi}[L(\phi;\{\bm{x}_i,y_i\}) + \lambda g(\phi)] $$&lt;p>where $g(\phi)$ is called the regularization term and $\lambda&amp;gt;0$ is a hyperparameter.&lt;/p>
&lt;p>In the probabilistic perspective, we can construct the loss function from &lt;em>maximum likelihood estimation&lt;/em>, or MLE, that is&lt;/p>
$$ \hat{\phi} = \arg\max_{\phi}\left[ \prod_{i}\mathrm{Pr}(y_i\mid \bm{x}_i,\phi) \right] $$&lt;p>The regularization term can be considered as a &lt;em>prior&lt;/em> $\mathrm{Pr}(\phi)$， in this way, we are now using &lt;em>maximum a posteriori&lt;/em> criterion:&lt;/p>
$$ \hat{\phi} = \arg\max_{\phi}\left[ \prod_{i}\mathrm{Pr}(y_i\mid \bm{x}_i,\phi)\mathrm{Pr}(\phi) \right] $$&lt;h1 id="implicit-regularization-methods">Implicit Regularization Methods
&lt;/h1>&lt;p>Gradient descent and stochastic gradient descent are commonly used to minimize the loss functions, however, neither of them moves neutrally to the minimum of the loss function, thus the implicit regularization method is proposed to solve the problem.&lt;/p>
&lt;h2 id="implicit-regularization-in-gradient-descent">Implicit Regularization in gradient descent
&lt;/h2>&lt;p>The change of the parameters $\phi$ is defined by the differential equation:&lt;/p>
$$ \frac{d{\phi}}{d t} = -\frac{dL}{d\phi} $$&lt;p>gradient descent uses &lt;a class="link" href="https://en.wikipedia.org/wiki/Difference_quotient" target="_blank" rel="noopener"
>difference quotient&lt;/a> with increment (or learning rate) $\alpha$ to approximate the change of $\phi$:&lt;/p>
$$ \frac{\phi_{t+1}-\phi_{t}}{\alpha}=-\frac{dL}{d\phi} \Rightarrow \phi_{t+1} = \phi_{t} - \alpha\frac{dL}{d\phi} $$&lt;p>However, this discretization causes deviation from the continuous path.&lt;/p>
&lt;p>To fix the problem, and extra item is added to the loss to avoid the deviation caused by discretization:&lt;/p>
$$ \tilde{L}(\phi) = L(\phi) + \frac{\alpha}{4}\left\Vert \frac{dL}{d\phi}\right\Vert^2 $$&lt;h2 id="implicit-regularization-in-stochastic-gradient-descent">Implicit Regularization in stochastic gradient descent
&lt;/h2>&lt;p>A similar approach can be applied to stochastic gradient descent, which reads as&lt;/p>
$$ \tilde{L}(\phi) = L(\phi) + \frac{\alpha}{4|B|}\sum_{i\in B}\left\Vert \frac{dL_{i}}{d\phi}-\frac{dL}{d\phi}\right\Vert^2 $$&lt;p>
where $L_{B}$ is the loss on the batch $B$.&lt;/p>
&lt;h1 id="heuristic-methods">Heuristic Methods
&lt;/h1>&lt;h2 id="early-stopping">Early stopping
&lt;/h2>&lt;p>Early stopping means that we stop the training procedure before the model becomes overfitting. By stopping early, we prevent the model captures the corner features of the training dataset.&lt;/p>
&lt;p>Early stopping has a single hyperparameter, the number of steps after which the training is stopped, this is chosen usually with the help of the validation dataset.&lt;/p>
&lt;h2 id="ensembling">Ensembling
&lt;/h2>&lt;p>Ensembling means we train multiple models on the training dataset, and during the inference time, we take the average inference result of each model. The technique improves the test performance with the sacrifices of training and storing multiple models.&lt;/p>
&lt;p>There are some ensembling methods:&lt;/p>
&lt;ol>
&lt;li>Use different random initializations. This leads the model reaches different local minimum and may help reduce the overfitting.&lt;/li>
&lt;li>Generate several different datasets by re-sampling the training dataset and train model on each of them.This is also known as &lt;em>bootstrap aggregating&lt;/em> or &lt;em>bagging&lt;/em>, this division can smooth out the data, since each model tries to predict the distribution of data that is not included in its training dataset.&lt;/li>
&lt;/ol>
&lt;h2 id="dropout">Dropout
&lt;/h2>&lt;p>Drop out randomly clamps a subset of hidden units of the layer at each iteration of SGD. This makes the model depends on general feature instead of some specific feature, since the specific feature may be masked.&lt;/p>
&lt;p>At test time, we can run the network as usual with all the hidden units active; however, the network now has more hidden units than it was trained with at any given iteration, so we multiply the weights by one minus the dropout probability to compensate. This is known as the &lt;em>weight scaling inference rule&lt;/em>.&lt;/p>
&lt;h2 id="applying-noise">Applying noise
&lt;/h2>&lt;p>Dropout can be interpreted as applying multiplicative Bernoulli noise to the network activations. We can apply noise to other parts of the model during training.&lt;/p>
&lt;ol>
&lt;li>We can add noise to the input data, this smooth out the learned function.&lt;/li>
&lt;li>We can also add noise to model parameters, this encourages the model to be robust to small perturbations of the weights.&lt;/li>
&lt;li>We can also perturb the labels. We can change the label of a portion of the training dataset, this can prevent the model from being overconfident.&lt;/li>
&lt;/ol>
&lt;h2 id="bayesian-inference">Bayesian inference
&lt;/h2>&lt;p>The MLE approach tries to find a function $f(\bm{x};\phi)$ that fit the dataset ${\bm{x}_i,y_i}$, this approach may be overconfident about the task since the bias of the training data construction.&lt;/p>
&lt;p>To overcome such bias, we treats the parameters $\phi$ as unknown variables instead of scalars. Then we find a distribution over the parameters $\phi$ conditioned on the training data ${\bm{x}_i,y_i}$, using &lt;a class="link" href="https://en.wikipedia.org/wiki/Bayes%27_theorem" target="_blank" rel="noopener"
>Bayesian theorem&lt;/a>:&lt;/p>
$$ \mathrm{Pr}(\phi\mid \{\bm{x}_i,y_i\}) = \frac{\prod_{i}\mathrm{Pr}(y_i\mid \bm{x}_i,\phi)\mathrm{Pr}(\phi)}{\int\prod_{i}\mathrm{Pr}(y_i\mid \bm{x}_i,\phi)\mathrm{Pr}(\phi)d\phi} $$&lt;p>where $\mathrm{Pr}(\phi)$ us the prior probability of the parameters, and the denominator is a normalizing term.&lt;/p>
&lt;p>Then the prediction for unseen item ${\bm{x},y}$ is given by the following infinite weighted sum:&lt;/p>
$$ \mathrm{Pr}(y\mid x, \{\bm{x}_i,y_i\}) = \int \mathrm{Pr}(y\mid \bm{x},\phi)\mathrm{Pr}(\phi\mid \{\bm{x}_i,y_i\})d\phi $$&lt;p>This is an infinite weighted ensemble, where the weights depend on&lt;/p>
&lt;ol>
&lt;li>the prior probability of the parameters&lt;/li>
&lt;li>the agreement with the data&lt;/li>
&lt;/ol>
&lt;p>Though Bayesian approach is capable of representing the data more robust, it is hard to implement since there is no way to represent an distribution.
Current implementation simplifies the distribution as Gaussian distribution and each parameter is replaced with the mean $\mu$ and standard deviation $\sigma$ of the Gaussian distribution.&lt;/p>
&lt;h2 id="transfer-learning-and-multi-task-learning">Transfer learning and multi-task learning
&lt;/h2>&lt;p>In transfer learning, the model is first pre-trained before training or fine-tuning on the task we are interested in. The idea is that the model may learn some good representation of the data from the main task. Alternatively, we can think transfer learning as initializing the model parameters in a reasonable area such that the minimum is better compared to the random initialization.&lt;/p>
&lt;p>Multi-task learning is a related technique that the model is trained on multiple related tasks concurrently. In this way, the model can learn from multiple datasets and multiple objectives, this encourages the model to learn the essential part of the tasks.&lt;/p>
&lt;h2 id="self-supervised-learning">Self-supervised learning
&lt;/h2>&lt;p>In some cases, we do not have multiple datasets for pre-training or for multi-tasks. To solve this problem, we can use self-supervised learning to generate large amounts of label-free data. There are two families of self-supervised learning: generative and contrastive.&lt;/p>
&lt;p>In generative self-supervised learning, part of each data example is masked, and the task is to predict the masked part. For example, given a sentence, we can mask the verb and ask for the model to predict the correct verb, the helps the model to learning semantic meaning of a sentence.&lt;/p>
&lt;p>In contrastive self-supervised learning, we try to group related data and separated unrelated data. For example, a cat is more similar to another cat compared with a dog. In this way, the model can learn more robust representations and can be adapted to new tasks easily.&lt;/p>
&lt;h2 id="augmentation">Augmentation
&lt;/h2>&lt;p>Augmentation aims to expand the training dataset, we can perform transformation to each training data without changing the labels, for example we can rotate, flip a image of cat. The augmentation is to teach the model to be invariant to these irrelevant data transformations.&lt;/p>
&lt;h1 id="summary">Summary
&lt;/h1>&lt;p>To summarize the regularization methods, we use the following picture to depict the mechanisms.
&lt;img src="https://maosong2022.github.io/p/regularization-methods-in-deep-learning/regularization_overview.png"
width="1210"
height="718"
srcset="https://maosong2022.github.io/p/regularization-methods-in-deep-learning/regularization_overview_hu13042679083480247267.png 480w, https://maosong2022.github.io/p/regularization-methods-in-deep-learning/regularization_overview_hu17970977390077547624.png 1024w"
loading="lazy"
alt="Regularization methods"
class="gallery-image"
data-flex-grow="168"
data-flex-basis="404px"
>&lt;/p>
&lt;h1 id="reference">Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://udlbook.github.io/udlbook/" target="_blank" rel="noopener"
>Understanding Deep Learning Chapter 9&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>514. Freedom Trail</title><link>https://maosong2022.github.io/p/514.-freedom-trail/</link><pubDate>Sat, 27 Apr 2024 11:55:28 +0800</pubDate><guid>https://maosong2022.github.io/p/514.-freedom-trail/</guid><description>&lt;p>Given a string displaying in the ring format, we can move one character at each step either in clockwise or anticlockwise (or hold still), now we need to retrieve a given string with minimum steps.&lt;/p>
&lt;h1 id="intuition">Intuition
&lt;/h1>&lt;p>We can construct a graph from the string and use DFS to search all possible paths, and find the path with minimum steps.&lt;/p>
&lt;h1 id="approach">Approach
&lt;/h1>&lt;h2 id="dfs---tle">DFS -&amp;gt; TLE
&lt;/h2>&lt;p>In the beginning, I am going to use DFS to find all possible paths and find the path with minimum steps.&lt;/p>
&lt;p>First, we need construct the graph, each character is adajcent to all other characters in &lt;code>ring&lt;/code>, so there are &lt;code>n=length(ring)&lt;/code> nodes and &lt;code>(n-1)^n&lt;/code> edges. each edge has a weight representing the distance between two characters: &lt;code>weight(i,j)=min(abs(i-j), n - abs(i-j))&lt;/code> (here we use index of the character to represent the node).&lt;/p>
&lt;p>Then, we can use DFS to search all possible paths, in each point, we have a state &lt;code>(ring_index, key_index)&lt;/code>, representing the current index on &lt;code>ring&lt;/code> and &lt;code>key&lt;/code>, there are two cases:&lt;/p>
&lt;ol>
&lt;li>&lt;code>ring[ring_index] == key[key_index]&lt;/code>, in this case, we change &lt;code>key_index&lt;/code> to &lt;code>key_index+1&lt;/code> and keeps &lt;code>ring_index&lt;/code> unchanged (hold still).&lt;/li>
&lt;li>&lt;code>ring[ring_index] != key[key_index]&lt;/code>, in this case, we need to rotate the string &lt;code>ring&lt;/code> to make &lt;code>ring[ring_index] == key[key_index]&lt;/code>, this takes step and notice that there may multiple choices, so we need to go over all of them.&lt;/li>
&lt;/ol>
&lt;p>Once &lt;code>key_index == len(key)&lt;/code>, we have found a path and we can now update the result.&lt;/p>
&lt;p>The code is given as follows:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">void&lt;/span> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">unordered_map&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">adjs&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">const&lt;/span> &lt;span class="n">string&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">ring&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="k">const&lt;/span> &lt;span class="n">string&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">key&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">ring_index&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">key_index&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">min_rotates&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">current_rotates&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">key_index&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">min_rotates&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">min_rotates&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">current_rotates&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// character matches
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">ring&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">ring_index&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">key_index&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">adjs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ring&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ring_index&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">key_index&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">min_rotates&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">current_rotates&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// character doesn&amp;#39;t match
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="k">auto&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="nl">next_node&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">adjs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">ring_index&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">next_ring_index&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">next_node&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">rotates&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">next_node&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">ring&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">next_ring_index&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">key_index&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">current_rotates&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">rotates&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">adjs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ring&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">next_ring_index&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">key_index&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">min_rotates&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">current_rotates&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">current_rotates&lt;/span> &lt;span class="o">-=&lt;/span> &lt;span class="n">rotates&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="nf">findRotateSteps&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">string&lt;/span> &lt;span class="n">ring&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">string&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">unordered_map&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">adjs&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ring&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">ring&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">ring&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">j&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">diff&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">abs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">rotates&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">diff&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">diff&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">adjs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">].&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">rotates&lt;/span>&lt;span class="p">});&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">ring_index&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">key_index&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">min_rotates&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">INT_MAX&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">current_rotates&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">adjs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ring&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ring_index&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">key_index&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">min_rotates&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">current_rotates&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">spell&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">min_rotates&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">spell&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="dynamic-programming">Dynamic programming
&lt;/h2>&lt;p>The problem of DFS is that, its time complexity grows exponentially if there have multiple repeat characters, which causes TLE (time limit exceeded) error.&lt;/p>
&lt;p>So, to reduce the complexity, we can construct the solution from bottom to up. That is, we remember the path to go from the current state, and now we now go one step back, util we back to the original state.&lt;/p>
&lt;p>We use &lt;code>dp[i][j]&lt;/code> to represent start from &lt;code>ring[j]&lt;/code>, the minimum rotate steps we need to recover the string &lt;code>key[i...m]&lt;/code>, where &lt;code>m=length(key)&lt;/code>, the target then becomes finding out &lt;code>dp[0][0]&lt;/code>.&lt;/p>
&lt;p>Note that we can easily compute &lt;code>dp[m-1][j]&lt;/code>, &lt;code>j=1,\dots,n&lt;/code>, since there is only one character we need to recover, so we start from &lt;code>ring[j]&lt;/code>, and rotate util we find a character &lt;code>ring[k]&lt;/code> such that &lt;code>ring[k]==key[m-1]&lt;/code>, the minimum steps is then updated. The update formula is then given by&lt;/p>
$$ dp[i][j] = \min_{k=1,\dots,n,\ ring[k]=key[i]}(dp[i][j],\ dp[i + 1][k] + step(j, k)) $$&lt;p>where $step(j,k)=\min(|j-k|,\ n-|j-k|)$.&lt;/p>
&lt;h1 id="complexity">Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(mn^2)$$&lt;/li>
&lt;li>
$$O(mn)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">findRotateSteps&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">string&lt;/span> &lt;span class="n">ring&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">string&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">m&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ring&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">m&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">m&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;gt;=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// start from ring[j]
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">INT_MAX&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// Find the feasible target ring[k] == key[i]
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">k&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">k&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">ring&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">rotates&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">abs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">k&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">abs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">k&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">rotates&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">m&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="reference">Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/freedom-trail/description" target="_blank" rel="noopener"
>leetcode 514&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.cnblogs.com/grandyang/p/6675879.html" target="_blank" rel="noopener"
>Solution&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>1289. Minimum Falling Path Sum II</title><link>https://maosong2022.github.io/p/1289.-minimum-falling-path-sum-ii/</link><pubDate>Fri, 26 Apr 2024 20:44:17 +0800</pubDate><guid>https://maosong2022.github.io/p/1289.-minimum-falling-path-sum-ii/</guid><description>&lt;p>Given a matrix, find the minimum falling path sum from top to bottom, with no two adjacent rows sharing the same column.&lt;/p>
&lt;h1 id="intuition">Intuition
&lt;/h1>&lt;p>Same as the previous version, we only change the update formula.&lt;/p>
&lt;h1 id="approach">Approach
&lt;/h1>&lt;p>We use &lt;code>dp[i][j]&lt;/code> to represent the minimum falling path sum that ends with &lt;code>grid[i][j]&lt;/code>. The update rules is then given by&lt;/p>
$$ dp[i][j] = \min_{k=1,\dots,n,k\neq j}(grid[i][j] + dp[i - 1][k]) $$&lt;h1 id="complexity">Complexity
&lt;/h1>&lt;ul>
&lt;li>Time complexity: We need to iterate all elements of the matrix once, and each iterate requires to iterate its last row once, which is $O(n)$. This can be reduced to $O(n^2\log n)$ by compute the smallest, second smallest elements of the last row.&lt;/li>
&lt;/ul>
$$O(n^3)$$&lt;ul>
&lt;li>Space complexity: the &lt;code>dp&lt;/code> matrix is of size $n\times n$. This can be reduced to $O(n)$ by use a $n\times 2$ matrix since each row only relates to its last row.&lt;/li>
&lt;/ul>
$$O(n^2)$$&lt;h1 id="code">Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">minFallingPathSum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">INT_MAX&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">row&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">row&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">row&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">col&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">col&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">col&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">last_col&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">last_col&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">last_col&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">last_col&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">col&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">row&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">col&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">row&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">col&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">row&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">col&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">row&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">last_col&lt;/span>&lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">INT_MAX&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">result&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">n&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="reference">Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/minimum-falling-path-sum-ii/description/" target="_blank" rel="noopener"
>Leetcode 1289&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>BLEU (Bilingual Evaluation Understudy)</title><link>https://maosong2022.github.io/p/bleu-bilingual-evaluation-understudy/</link><pubDate>Thu, 25 Apr 2024 22:46:53 +0800</pubDate><guid>https://maosong2022.github.io/p/bleu-bilingual-evaluation-understudy/</guid><description>&lt;p>BLEU (Bilingual Evaluation Understudy) is a widely used metric that evaluates the quality of the translated text with respect to the reference translations.&lt;/p>
&lt;h1 id="introduction">Introduction
&lt;/h1>&lt;p>The formula of BLEU is defined as follows:&lt;/p>
$$ \mathrm{BLEU}_ {w_n}(\hat{S}, S) = \mathrm{BP} \cdot \exp\left(\sum_{n=1}^Nw_n\log p_n(\hat{S}, S) \right) $$&lt;p>where&lt;/p>
&lt;ol>
&lt;li>$\mathrm{BP}$ represents the Brevity Penalty to penalize the translations that are shorter than the reference translations.&lt;/li>
&lt;li>$p_n$ represents the modified $n$-gram precision score&lt;/li>
&lt;li>$w_n$ represents the weight for $p_n$, it satisfies $0\leq w_n\leq1$ and $\sum_{n=1}^Nw_n=1$.&lt;/li>
&lt;/ol>
&lt;p>Now we interprets three parts in detail.&lt;/p>
&lt;h1 id="interpretation">Interpretation
&lt;/h1>&lt;h2 id="definitions">Definitions
&lt;/h2>&lt;p>Given any string $y=y_1y_2\cdots y_K$, where $y_i,i\in{1,\dots,K}$ are characters and an integer $n\geq1$, we define the &lt;strong>set of $n$-gram&lt;/strong> to be&lt;/p>
$$ G_n(y) = \{ y_1\cdots y_n, y_2\cdots y_{n+1}, \dots, y_{K-n+1}\cdots y_K\} $$&lt;p>NOTE that this is a set with unique elements, for example, $G_2(abab)={ab, ba}$.&lt;/p>
&lt;p>Given any two strings $s$ and $y$, we define &lt;strong>substring count&lt;/strong> $C(s,y)$ to tbe the number of appearances of $s$ as a substring of $y$. For example, $C(ab, abcbab)=2$ since $ab$ appear in $abcbab$ twice in position $1$ and $5$.&lt;/p>
&lt;h2 id="modified-precision-score">Modified precision score
&lt;/h2>&lt;p>We start from the one candidate translation $\hat{y}$ and one reference translation $y$. The modified $n$-gram is defined as&lt;/p>
$$ p_n(\hat{y}, y) = \frac{\sum_{s\in G_n(\hat{y})}\min (C(s,\hat{y}), C(s,y))}{\sum_{s\in G_n(\hat{y})}C(s,\hat{y})} $$&lt;p>The quantity measures how many $n$-grams of the reference translation $y$ appears in the candidate translation $\hat{y}$.
In case that $\hat{y}$ is too short, we take a minimum between $C(s,\hat{y})$ and $ C(s,y)$. Then we normalize to make $p_n(\hat{y}, y)$ comparable among multiple translation pairs.&lt;/p>
&lt;p>Now, suppose we have a candidate translation corpus, $\hat{S}={\hat{y}^1,\dots,\hat{y}^M}$, and for each candidate translation $\hat{y}^i$, we have a reference translation corpus (there are multiple translations can represent the same meaning) $S_i={y^{i,1},\dots,y^{i,N_i}}$. We define $S={S_1,\dots,S_M}$, then our modified $n$-gram precision is defined as&lt;/p>
$$ p_n(\hat{S}, S) = \frac{\sum_{i=1}^M\sum_{s\in G_n(\hat{y})}\min (C(s,\hat{y}), \max_{y\in S_i}C(s,y))}{\sum_{i=1}^M\sum_{s\in G_n(\hat{y})}C(s,\hat{y})} $$&lt;p>note that we have replaced $\min (C(s,\hat{y}), C(s,y))$ with $\min (C(s,\hat{y}), \max_{y\in S_i}C(s,y))$ since there are multiple reference translation, we use the most similar one. So this is to say: &amp;ldquo;There are multiple answer, go to choose the best one and compute the score.&amp;rdquo;&lt;/p>
&lt;h2 id="bp-brevity-penalty">BP (Brevity Penalty)
&lt;/h2>&lt;p>Candidate translations longer than their references are already penalized by the modified $n$-gram precision measure.
Now, to penalize those translations shorter than the reference translations, we need add an penalty term. This is when brevity penalty comes out:&lt;/p>
$$
\mathrm{BP}=\begin{cases}1&amp;\text{ if }c > r\\
\exp(1-\frac{r}{c})&amp;\text{ if }c \leq r
\end{cases}
$$&lt;p>where&lt;/p>
&lt;ul>
&lt;li>$c$ is the number of words or tokens of the candidate corpus. That is,&lt;/li>
&lt;/ul>
$$ c = \sum_{i=1}^M\mathrm{length}(\hat{y}^i) $$&lt;ul>
&lt;li>$r$ is the number of words or tokens of the effective reference corpus length, where the effective reference is defined as the reference translation whose length is as close to the corresponding candidate translation as possible. That is&lt;/li>
&lt;/ul>
$$ r = \sum_{i=1}^M\mathrm{length}(y^{i,j}),\text{ where } y^{i,j}=\arg\min_{y\in S_i}|\mathrm{length}(\hat{y}^i)-\mathrm{length}(y)| $$&lt;p>with this penalty term, we wish the model to output the translations with the same length as the reference translations.&lt;/p>
&lt;h2 id="weight">Weight
&lt;/h2>&lt;p>The weight measures the importance of different precision score, in the original paper, the uniform weights are adopted, that is&lt;/p>
$$ w_i = \frac{1}{N}, \ \text{ for } i=1,\dots,N $$&lt;h2 id="final-definition">Final definition
&lt;/h2>&lt;p>The final definition of the BLEU is given by&lt;/p>
$$ \mathrm{BLEU}_ {w}(\hat{S}, S) = \mathrm{BP} \cdot \exp\left(\sum_{n=1}^\infty w_n\log p_n(\hat{S}, S) \right) $$&lt;p>usually, the upper-bound of the above summation can be reduced to $\max_{i=1,\dots,M}\mathrm{length}(\hat{y}^i)$.&lt;/p>
&lt;h1 id="analysis">Analysis
&lt;/h1>&lt;p>Disadvantages of BLEU:&lt;/p>
&lt;ul>
&lt;li>BLEU compares overlap in tokens from the predictions and references, instead of comparing meaning. This can lead to discrepancies between BLEU scores and human ratings.&lt;/li>
&lt;li>BLEU scores are not comparable across different datasets, nor are they comparable across different languages.&lt;/li>
&lt;li>BLEU scores can vary greatly depending on which parameters are used to generate the scores, especially when different tokenization and normalization techniques are used.&lt;/li>
&lt;li>BLEU ignores synonym or similar expression, which causes refuses of reasonable translation.&lt;/li>
&lt;li>BLEU is affected by common words.&lt;/li>
&lt;/ul>
&lt;h1 id="python-implementation">Python Implementation
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;span class="lnt">49
&lt;/span>&lt;span class="lnt">50
&lt;/span>&lt;span class="lnt">51
&lt;/span>&lt;span class="lnt">52
&lt;/span>&lt;span class="lnt">53
&lt;/span>&lt;span class="lnt">54
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">math&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">typing&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">Set&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">List&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">compute_n_gram_set&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">Set&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="nb">set&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="p">:&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">modified_n_gram_precision&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">S_hat&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">S&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">]],&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">float&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">S_hat&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="mf">0.0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">numerator&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">denominator&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">index&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y_hat&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">enumerate&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">S_hat&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">n_gram_set&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">compute_n_gram_set&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y_hat&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="n">n_gram_set&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">continue&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># print(n_gram_set)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">n_gram&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">n_gram_set&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">candidate_substr_count&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">y_hat&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">count&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n_gram&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">best_ref_substr_count&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">best_ref_len&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">max&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">[(&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">count&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n_gram&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">S&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">index&lt;/span>&lt;span class="p">]],&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="k">lambda&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">numerator&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="nb">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">candidate_substr_count&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">best_ref_substr_count&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">denominator&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">candidate_substr_count&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">numerator&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">denominator&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">brevity_penalty&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">S_hat&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">S&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">]])&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">float&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">r&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">c&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">index&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y_hat&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">enumerate&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">S_hat&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">c&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y_hat&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">best_match_ref&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">S&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">index&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="k">lambda&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">abs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y_hat&lt;/span>&lt;span class="p">)))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">r&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">best_match_ref&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="mf">1.0&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="n">c&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">r&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="n">math&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">exp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">r&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">compute_bleu_score&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">S_hat&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">S&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">]])&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">float&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">assert&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">S_hat&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">S&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># take N as a sufficiently large integer&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># N = max(len(y_hat) for y_hat in S_hat)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">N&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">sum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y_hat&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">y_hat&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">S_hat&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">bp&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">brevity_penalty&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">S_hat&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">S&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">precisions&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">modified_n_gram_precision&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">S_hat&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">S&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">N&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># bleu_score = bp * exp(p_n)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">bp&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">math&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">exp&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">sum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">math&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">log&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">precision&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">precision&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">precisions&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="n">precision&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="reference">Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://huggingface.co/spaces/evaluate-metric/bleu" target="_blank" rel="noopener"
>Hugging Face space&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://aclanthology.org/P02-1040.pdf" target="_blank" rel="noopener"
>Original Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://en.wikipedia.org/wiki/BLEU" target="_blank" rel="noopener"
>Wikipedia Documentation, Recommended&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>2370. Longest Ideal Subsequence</title><link>https://maosong2022.github.io/p/2370.-longest-ideal-subsequence/</link><pubDate>Thu, 25 Apr 2024 20:34:06 +0800</pubDate><guid>https://maosong2022.github.io/p/2370.-longest-ideal-subsequence/</guid><description>&lt;p>Given a string consisting of lower-case characters, find the longest subsequence such that the distance between adjacent characters in the subsequence are less than a given threshold.&lt;/p>
&lt;h1 id="intuition">Intuition
&lt;/h1>&lt;p>Same as the longest increasing subsequence, we can use dynamic programming to solve this problem.&lt;/p>
&lt;h1 id="approach">Approach
&lt;/h1>$$ dp[i] = \max_{j=1,\dots,i-1, \mathrm{abs}(s[i - 1]-s[j - 1])\leq k}(dp[i],\ dp[j] + 1) ,\ i=1,\dots,n$$&lt;p>However, it turns out that the above solution is of complexity $O(n^2)$, which leads to &lt;em>Time Exceed Limit&lt;/em>, so we need to optimize it.&lt;/p>
&lt;p>Now, consider the property of ideal sequence, we only care about those characters that is within the range &lt;code>(s[i - 1] - k, s[i - 1] + k)&lt;/code>. So, we can use a map &lt;code>record&lt;/code>, whose key is all lowercase characters, to remember the result that is used to update &lt;code>dp[i]&lt;/code>, that is, for given index &lt;code>i&lt;/code>:&lt;/p>
$$ record[l] = \max_{j=1,\dots,i - 1, s[j] - 'a' = l}dp[j] $$$$ dp[i] = \max_{\mathrm{abs}((s[i]-'a') - l)\leq k}(dp[i],\ record[l] + 1),\ i=1,\dots,n $$&lt;p>
notice that &lt;code>len(record)=26&lt;/code>, so the complexity now reduces to $O(n)$.&lt;/p>
&lt;h1 id="complexity">Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n)$$&lt;/li>
&lt;li>
$$O(n)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">longestIdealString&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">string&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">record&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">26&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">index&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="sc">&amp;#39;a&amp;#39;&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="mi">26&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">abs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">index&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">max&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">record&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">record&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">index&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">max&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">record&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">index&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">max&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">result&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>1137. N-th Tribonacci Number</title><link>https://maosong2022.github.io/p/1137.-n-th-tribonacci-number/</link><pubDate>Wed, 24 Apr 2024 18:53:26 +0800</pubDate><guid>https://maosong2022.github.io/p/1137.-n-th-tribonacci-number/</guid><description>&lt;p>Compute the $n$-th tribonacci number.&lt;/p>
&lt;h1 id="intuition">Intuition
&lt;/h1>&lt;p>Same as compute the $n$-th fibonacci number, we use three numbers to remember the state.&lt;/p>
&lt;h1 id="approach">Approach
&lt;/h1>&lt;p>We use three numbers to represent $n-2$, $n-1$ and $n$-th tribonacci number respectively&lt;/p>
&lt;h1 id="complexity">Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n)$$&lt;/li>
&lt;li>
$$O(1)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">tribonacci&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">return&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">temp&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">nums&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">nums&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">nums&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">temp&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>310. Minimum Height Trees</title><link>https://maosong2022.github.io/p/310.-minimum-height-trees/</link><pubDate>Tue, 23 Apr 2024 21:02:07 +0800</pubDate><guid>https://maosong2022.github.io/p/310.-minimum-height-trees/</guid><description>&lt;p>Given a tree, reorganize the tree such that the height of the tree is minimized.&lt;/p>
&lt;h1 id="intuition">Intuition
&lt;/h1>&lt;p>We construct the tree from bottom to top, util we find the root of the tree&lt;/p>
&lt;h1 id="approach">Approach
&lt;/h1>&lt;p>We use topological sort to order the nodes of the tree, then we iteratively construct the tree from bottom to top with BFS.&lt;/p>
&lt;p>We use a different stop criteria to avoid missing possible solutions.&lt;/p>
&lt;blockquote>
&lt;p>The result contains at most two possible roots, since if there are three, then the degree of one node must be lower than the other two nodes.&lt;/p>
&lt;/blockquote>
&lt;h1 id="complexity">Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n)$$&lt;/li>
&lt;li>
$$O(n)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">findMinHeightTrees&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">edges&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// boundary check
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">return&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">adjs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">in_degrees&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="k">auto&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="nl">edge&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">edges&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">adjs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]].&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">adjs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]].&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">++&lt;/span>&lt;span class="n">in_degrees&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">++&lt;/span>&lt;span class="n">in_degrees&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// leaf nodes
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="n">queue&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">q&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">in_degrees&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// construct next layer
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">n&lt;/span> &lt;span class="o">-=&lt;/span> &lt;span class="n">size&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">size&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">node&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">front&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="nl">next_node&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">adjs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="o">--&lt;/span>&lt;span class="n">in_degrees&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">next_node&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">next_node&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">pop&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="o">!&lt;/span>&lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">empty&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">front&lt;/span>&lt;span class="p">());&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">pop&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>752. Open the Lock</title><link>https://maosong2022.github.io/p/752.-open-the-lock/</link><pubDate>Mon, 22 Apr 2024 19:01:23 +0800</pubDate><guid>https://maosong2022.github.io/p/752.-open-the-lock/</guid><description>&lt;p>Given a four-digit string, change one digit (plus or minus 1) at a time, find the minimum number of steps to go from the source to target without passing through invalid states.&lt;/p>
&lt;h1 id="intuition">Intuition
&lt;/h1>&lt;p>We can image this as a graph path finding problem, where we need to find a path from the &lt;code>source&lt;/code> to the &lt;code>target&lt;/code> with the minimum number of steps.&lt;/p>
&lt;h1 id="approach">Approach
&lt;/h1>&lt;p>We use BFS to solve this problem. The graph is constructed as follows: each possible state is a node of the graph, such as &lt;code>&amp;quot;1234&amp;quot;&lt;/code>, &lt;code>&amp;quot;4567&amp;quot;&lt;/code>, each operation defines an edge between two nodes, for example, we can rotate third digit of &lt;code>&amp;quot;1234&amp;quot;&lt;/code> to obtain &lt;code>&amp;quot;1244&amp;quot;&lt;/code>, since there are two possible directions and four digits, each node has $2^4=16$ adjacent nodes. We keep track of visited nodes and add them to &lt;code>deadends&lt;/code> since there are no difference between them.&lt;/p>
&lt;h1 id="complexity">Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(1)$$&lt;/li>
&lt;li>
$$O(1)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;span class="lnt">49
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">string&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">get_ajacent_nodes&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="n">string&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">string&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">string&lt;/span> &lt;span class="n">s1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">string&lt;/span> &lt;span class="n">s2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="sc">&amp;#39;1&amp;#39;&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="sc">&amp;#39;9&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">s1&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">replace&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">string&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">s2&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">replace&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">string&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="nf">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="sc">&amp;#39;9&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">s1&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">replace&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#34;0&amp;#34;&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">s2&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">replace&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#34;8&amp;#34;&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">s1&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">replace&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#34;1&amp;#34;&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">s2&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">replace&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#34;9&amp;#34;&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">s1&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">s2&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="nf">openLock&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">string&lt;/span>&lt;span class="o">&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">deadends&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">string&lt;/span> &lt;span class="n">target&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">unordered_set&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">string&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">set_deadends&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">deadends&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">begin&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">deadends&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">());&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">queue&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">string&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">q&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#34;0000&amp;#34;&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="o">!&lt;/span>&lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">empty&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">size&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">string&lt;/span> &lt;span class="n">node&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">front&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">pop&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// dead end check
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">set_deadends&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">find&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="n">set_deadends&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// target check
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">target&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// mark as visited
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="n">set_deadends&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">insert&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">const&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">string&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">adjacent_nodes&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">get_ajacent_nodes&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="n">string&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="nl">adjacent_node&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">adjacent_nodes&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">adjacent_node&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">++&lt;/span>&lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>Notes on Llama3</title><link>https://maosong2022.github.io/p/notes-on-llama3/</link><pubDate>Mon, 22 Apr 2024 16:22:19 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-llama3/</guid><description>&lt;p>Meta released Llama3 at April 18, which is evaluated on several benchmarks and achieves the SOTA on open-sourced LLMs.&lt;/p>
&lt;h1 id="introduction">Introduction
&lt;/h1>&lt;h2 id="instruct-model-performance">Instruct model performance
&lt;/h2>&lt;p>The performance of Llama3 8B compared with Gemma and Mistral:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Llama3 8B&lt;/th>
&lt;th>Gemma 7B - It&lt;/th>
&lt;th>Mistral &amp;amp;B Instruct&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;em>MMLU&lt;/em> (5 shot)&lt;/td>
&lt;td>&lt;strong>68.4&lt;/strong>&lt;/td>
&lt;td>53.3&lt;/td>
&lt;td>58.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;em>GPQA&lt;/em> (0 shot)&lt;/td>
&lt;td>&lt;strong>34.2&lt;/strong>&lt;/td>
&lt;td>21.4&lt;/td>
&lt;td>26.3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;em>HumanEval&lt;/em> (0 shot)&lt;/td>
&lt;td>&lt;strong>62.2&lt;/strong>&lt;/td>
&lt;td>30.5&lt;/td>
&lt;td>36.6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;em>GSM-8K&lt;/em> (8 shot, CoT)&lt;/td>
&lt;td>&lt;strong>79.6&lt;/strong>&lt;/td>
&lt;td>30.6&lt;/td>
&lt;td>39.9&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;em>MATH&lt;/em> (4 shot, CoT)&lt;/td>
&lt;td>&lt;strong>30.0&lt;/strong>&lt;/td>
&lt;td>12.2&lt;/td>
&lt;td>11.0&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>performance of Llama3 70B compared with Gemini Pro 1.5 and Claude Sonnet:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Llama3 70B&lt;/th>
&lt;th>Gemini Pro 1.5 (Published)&lt;/th>
&lt;th>Claude 3 Sonnet (Published)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;em>MMLU&lt;/em> (5 shot)&lt;/td>
&lt;td>&lt;strong>82.0&lt;/strong>&lt;/td>
&lt;td>81.9&lt;/td>
&lt;td>79.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;em>GPQA&lt;/em> (0 shot)&lt;/td>
&lt;td>39.5&lt;/td>
&lt;td>&lt;strong>41.5&lt;/strong> (CoT)&lt;/td>
&lt;td>38.5 (CoT)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;em>HumanEval&lt;/em> (0 shot)&lt;/td>
&lt;td>&lt;strong>81.7&lt;/strong>&lt;/td>
&lt;td>71.9&lt;/td>
&lt;td>73.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;em>GSM-8K&lt;/em> (8 shot, CoT)&lt;/td>
&lt;td>&lt;strong>93.0&lt;/strong>&lt;/td>
&lt;td>91.7 (11 shot)&lt;/td>
&lt;td>92.3 (0 shot)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;em>MATH&lt;/em> (4 shot, CoT)&lt;/td>
&lt;td>50.4&lt;/td>
&lt;td>&lt;strong>58.5&lt;/strong> (Minerva prompt)&lt;/td>
&lt;td>40.5&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="pre-trained-model-performance">Pre-trained model performance
&lt;/h2>&lt;p>The performance of Llama3 8B compared with Gemma and Mistral:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Llama3 8B&lt;/th>
&lt;th>Gemma 7B (Published)&lt;/th>
&lt;th>Gemma 7B (Measured)&lt;/th>
&lt;th>Mistral 7B (Published)&lt;/th>
&lt;th>Mistral 7B (Measured)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;em>MMLU&lt;/em> (5 shot)&lt;/td>
&lt;td>&lt;strong>66.6&lt;/strong>&lt;/td>
&lt;td>64.3&lt;/td>
&lt;td>64.4&lt;/td>
&lt;td>62.5&lt;/td>
&lt;td>63.9&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;em>AGIEval English&lt;/em> (3-5 shot)&lt;/td>
&lt;td>&lt;strong>45.9&lt;/strong>&lt;/td>
&lt;td>41.7&lt;/td>
&lt;td>44.9&lt;/td>
&lt;td>-&lt;/td>
&lt;td>44.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;em>BIG-Bench Hard&lt;/em> (3 shot, CoT)&lt;/td>
&lt;td>&lt;strong>61.1&lt;/strong>&lt;/td>
&lt;td>55.1&lt;/td>
&lt;td>59.0&lt;/td>
&lt;td>-&lt;/td>
&lt;td>56.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;em>ARC-Challenge&lt;/em> (25 shot)&lt;/td>
&lt;td>78.6&lt;/td>
&lt;td>53.2(0 shot)&lt;/td>
&lt;td>&lt;strong>79.1&lt;/strong>&lt;/td>
&lt;td>78.1&lt;/td>
&lt;td>78.7&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;em>DROP&lt;/em> (3 shot, F1)&lt;/td>
&lt;td>&lt;strong>58.4&lt;/strong>&lt;/td>
&lt;td>-&lt;/td>
&lt;td>56.3&lt;/td>
&lt;td>-&lt;/td>
&lt;td>54.4&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>performance of Llama3 70B compared with Gemini Pro 1.5 and Claude Sonnet:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Llama3 70B&lt;/th>
&lt;th>Gemini Pro 1.0 (Published)&lt;/th>
&lt;th>Mixtral 8 $\times$ 22B (Measured)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;em>MMLU&lt;/em> (5-shot)&lt;/td>
&lt;td>&lt;strong>79.5&lt;/strong>&lt;/td>
&lt;td>71.8&lt;/td>
&lt;td>77.7&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;em>AGIEval English&lt;/em> (3-5 shot)&lt;/td>
&lt;td>&lt;strong>63.0&lt;/strong>&lt;/td>
&lt;td>-&lt;/td>
&lt;td>61.2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;em>BIG-Bench Hard&lt;/em> (3 shot, CoT)&lt;/td>
&lt;td>&lt;strong>81.3&lt;/strong>&lt;/td>
&lt;td>75.0&lt;/td>
&lt;td>79.2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;em>ARC-Challenge&lt;/em> (25 shot)&lt;/td>
&lt;td>&lt;strong>93.0&lt;/strong>&lt;/td>
&lt;td>-&lt;/td>
&lt;td>90.7&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;em>DROP&lt;/em> (3 shot, F1)&lt;/td>
&lt;td>&lt;strong>79.7&lt;/strong>&lt;/td>
&lt;td>74.1 (variable shot)&lt;/td>
&lt;td>77.6&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h1 id="model-architecture">Model Architecture
&lt;/h1>&lt;p>Several improvements are made on Llama3 compared to llama2:&lt;/p>
&lt;ol>
&lt;li>Llama3 uses a tokenizer with a vocabulary of 128K tokens.&lt;/li>
&lt;li>Llama3 adopts &lt;a class="link" href="https://maosong.website/p/notes-on-gqa/" target="_blank" rel="noopener"
>grouped query attention (GQA)&lt;/a> across both the 8B and 70B sizes.&lt;/li>
&lt;li>Llama3 uses to context window of size 8192 tokens&lt;/li>
&lt;/ol>
&lt;h1 id="traning">Traning
&lt;/h1>&lt;p>Llama3 uses 15T tokens for pre-training. Compares to Llama2, it is seven times larger and includes four times more code.&lt;/p>
&lt;p>5% data of the training dataset are non-English to support multi-lingual use cases.&lt;/p>
&lt;p>Data processing includes:&lt;/p>
&lt;ol>
&lt;li>Heuristic filters&lt;/li>
&lt;li>NSFW filters&lt;/li>
&lt;li>Semantic deduplication approaches&lt;/li>
&lt;li>Text classifiers to predict data quality. Llama2 is used to generate training data for the text classifiers.&lt;/li>
&lt;/ol>
&lt;p>Data mixing strategy is explored to improve the performance of Llama3.&lt;/p>
&lt;h1 id="scaling-up-pretraining">Scaling up pretraining
&lt;/h1>&lt;p>Llama3 developed a series of scaling laws for downstream benchmark evaluations.&lt;/p>
&lt;p>Scaling laws help:&lt;/p>
&lt;ol>
&lt;li>Select an optimal data mix and to make informed decisions on how to best use training compute.&lt;/li>
&lt;li>Scaling laws allow Llama3 to predict the performance of the largest models on key tasks without training the models.&lt;/li>
&lt;/ol>
&lt;p>The authors finds our that the performance of the model continues to improve log-linearly as the training tokens increase. It is seen that Larger models can match the performance of these smaller models with less training compute, but smaller models are generally preferred because they are much more efficient during inference.&lt;/p>
&lt;p>The authors combine three types of parallelization:&lt;/p>
&lt;ol>
&lt;li>Data parallelization&lt;/li>
&lt;li>Model parallelization&lt;/li>
&lt;li>Pipeline parallelization&lt;/li>
&lt;/ol>
&lt;h1 id="instruction-fine-tuning">Instruction fine-tuning
&lt;/h1>&lt;p>The fine-tuning of Llama3 contains:&lt;/p>
&lt;ol>
&lt;li>Supervised fine-tuning&lt;/li>
&lt;li>Rejection sampling&lt;/li>
&lt;li>Proximal Policy Optimization&lt;/li>
&lt;li>Direct Preference Optimization&lt;/li>
&lt;/ol>
&lt;p>Learning from perference rankings via PPO and DPO also greatly improved the performance of LLma3 on reasoning and coding tasks. Since perference ranking helps the model to select answer when it is in a dilemma.&lt;/p>
&lt;h1 id="reference">Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://ai.meta.com/blog/meta-llama-3/" target="_blank" rel="noopener"
>Llam3 blog&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/meta-llama/llama3/blob/main/eval_details.md" target="_blank" rel="noopener"
>Evaluation details&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md" target="_blank" rel="noopener"
>Model Card&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>1971. Find if Path Exists in Graph</title><link>https://maosong2022.github.io/p/1971.-find-if-path-exists-in-graph/</link><pubDate>Sun, 21 Apr 2024 10:51:46 +0800</pubDate><guid>https://maosong2022.github.io/p/1971.-find-if-path-exists-in-graph/</guid><description>&lt;p>Find a available path from a given source to a given destination in a given graph.&lt;/p>
&lt;h1 id="intuition">Intuition
&lt;/h1>&lt;p>Use DFS to find all reachable nodes and check if the destination lie within those nodes.&lt;/p>
&lt;h1 id="approach">Approach
&lt;/h1>&lt;p>We first transform the adjacency matrix to adjacency list to make BFS easier, then we use
a queue to maintain the reachable nodes, to prevent from cycling, we also use a set to keep track of visited nodes.
If at any point, we reach the &lt;code>destination&lt;/code>, we return directly.&lt;/p>
&lt;h1 id="complexity">Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n)$$&lt;/li>
&lt;li>
$$O(n)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">bool&lt;/span> &lt;span class="n">validPath&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">edges&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">source&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">destination&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">unordered_map&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">adjs&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="k">auto&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="nl">edge&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">edges&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">adjs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]].&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">adjs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]].&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">unordered_set&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">visited&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">queue&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">q&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">source&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="o">!&lt;/span>&lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">empty&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">node&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">front&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">pop&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">destination&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">return&lt;/span> &lt;span class="nb">true&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="nl">next_node&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">adjs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">visited&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">find&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">next_node&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="n">visited&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">next_node&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">visited&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">insert&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">next_node&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="nb">false&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>1992. Find All Groups of Farmland</title><link>https://maosong2022.github.io/p/1992.-find-all-groups-of-farmland/</link><pubDate>Sat, 20 Apr 2024 15:31:49 +0800</pubDate><guid>https://maosong2022.github.io/p/1992.-find-all-groups-of-farmland/</guid><description>&lt;p>Given a matrix where its grid component representing islands and forests, count the number of farmlands.&lt;/p>
&lt;h1 id="intuition">Intuition
&lt;/h1>&lt;p>Start from the top left coordinate of the farmland, Use DFS to find the bottom right coordinate of the farmland.&lt;/p>
&lt;h1 id="approach">Approach
&lt;/h1>&lt;p>We define the possible directions as &lt;code>go_right&lt;/code> and &lt;code>go_down&lt;/code> respectively, we iterate all grids, if it is an grid of the farmland,
then we use DFS to find the bottom right coordinate of the current farmland, and then we mark the farmland as visited and store the coordinates.&lt;/p>
&lt;h1 id="complexity">Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(mn)$$&lt;/li>
&lt;li>
$$O(mn)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">dirs&lt;/span>&lt;span class="p">{{&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">},&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">}};&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">void&lt;/span> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">land&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">bottom_right&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">land&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">bool&lt;/span> &lt;span class="n">reach_end&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">true&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="k">auto&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="nl">dir&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">dirs&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">row&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">col&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">row&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="n">row&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">land&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="mi">0&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">col&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="n">col&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">land&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">].&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">land&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">row&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">col&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">reach_end&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">false&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">land&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">row&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">col&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bottom_right&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">reach_end&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="n">bottom_right&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">empty&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">bottom_right&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">findFarmland&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">land&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">land&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">land&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">].&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">land&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">bottom_right&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">land&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bottom_right&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bottom_right&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">bottom_right&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]});&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>200. Number of Islands</title><link>https://maosong2022.github.io/p/200.-number-of-islands/</link><pubDate>Fri, 19 Apr 2024 20:25:03 +0800</pubDate><guid>https://maosong2022.github.io/p/200.-number-of-islands/</guid><description>&lt;p>Given a matrix where its grid component representing islands and waters, count the number of Islands.&lt;/p>
&lt;h1 id="intuition">Intuition
&lt;/h1>&lt;p>Use DFS to find all connected components of the island, then count the number of islands.&lt;/p>
&lt;h1 id="approach">Approach
&lt;/h1>&lt;p>We iterate all grid component, when we meet the land, we use DFS to find all connected components of the island, and mark those connected components as visited.&lt;/p>
&lt;h1 id="complexity">Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(mn)$$&lt;/li>
&lt;li>
$$O(1)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">dirs&lt;/span>&lt;span class="p">{{&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">},&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">},&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">},&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">}};&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">void&lt;/span> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">char&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="sc">&amp;#39;0&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">return&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="sc">&amp;#39;0&amp;#39;&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="k">auto&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="nl">dir&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">dirs&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="mi">0&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">].&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">grid&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="nf">numIslands&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">char&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">count&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">].&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="sc">&amp;#39;0&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">grid&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">++&lt;/span>&lt;span class="n">count&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">count&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>463. Island Perimeter</title><link>https://maosong2022.github.io/p/463.-island-perimeter/</link><pubDate>Thu, 18 Apr 2024 20:49:51 +0800</pubDate><guid>https://maosong2022.github.io/p/463.-island-perimeter/</guid><description>&lt;p>Given a matrix, find the perimeter of the connected grid land&lt;/p>
&lt;h1 id="intuition">Intuition
&lt;/h1>&lt;p>Use DFS to find the island, then compute the perimeter.&lt;/p>
&lt;h1 id="approach">Approach
&lt;/h1>&lt;p>We use &lt;code>result&lt;/code> to store the result and find all connected components of the island with DFS, to compute the perimeter,
to update &lt;code>result&lt;/code>, we need to compute how many components that are connected the current component.&lt;/p>
&lt;p>Now, note that to prevent infinite recursion, we set &lt;code>grid[i][j] = -1&lt;/code> to mark it visited, then for each component, it may be in three states:&lt;/p>
&lt;ol>
&lt;li>&lt;code>grid[i][j] = 0&lt;/code>, it is water&lt;/li>
&lt;li>&lt;code>grid[i][j] = 1&lt;/code>, it is a component of the island and being unvisited&lt;/li>
&lt;li>&lt;code>grid[i][j] = -1&lt;/code>, it is a component of the island and has been visited.&lt;/li>
&lt;/ol>
&lt;h1 id="complexity">Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(mn)$$&lt;/li>
&lt;li>
$$O(1)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">dirs&lt;/span>&lt;span class="p">{{&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">},&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">},&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">},&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">}};&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">void&lt;/span> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">result&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">return&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">num_edges&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">4&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="k">auto&lt;/span>&lt;span class="o">&amp;amp;&lt;/span>&lt;span class="nl">dir&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">dirs&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="mi">0&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">].&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]][&lt;/span>&lt;span class="n">j&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">--&lt;/span>&lt;span class="n">num_edges&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">grid&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">num_edges&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="nf">islandPerimeter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">].&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">grid&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>Practical advice for analysis of large, complex data sets</title><link>https://maosong2022.github.io/p/practical-advice-for-analysis-of-large-complex-data-sets/</link><pubDate>Wed, 17 Apr 2024 22:40:11 +0800</pubDate><guid>https://maosong2022.github.io/p/practical-advice-for-analysis-of-large-complex-data-sets/</guid><description>&lt;h1 id="introduction">Introduction
&lt;/h1>&lt;p>These advises are given by Patrick Riley in 2016, though it has been years util now, I think some of them are still useful.&lt;/p>
&lt;p>The advice is organized into three general areas:&lt;/p>
&lt;ul>
&lt;li>Technical: Ideas and techniques for how to manipulate and examine your data.&lt;/li>
&lt;li>Process: Recommendation on how you approach your data, what questions to ask, and what things to check.&lt;/li>
&lt;li>Social: How to work with others and communicate about your data and insights.&lt;/li>
&lt;/ul>
&lt;h1 id="technical">Technical
&lt;/h1>&lt;h2 id="look-at-your-distribution">Look at your distribution
&lt;/h2>&lt;p>Besides the typically used summary metrics, we should looking at a much richer representation of the distribution, such as histograms, CDFs, Q-Q plots, etc. This allows us to see some interesting features.&lt;/p>
&lt;h2 id="consider-the-outliers">Consider the outliers
&lt;/h2>&lt;p>We should look at the outliers in our data. It&amp;rsquo;s fine to exclude them from our data or to lump them together into an unusual category, but we should make sure we know why.&lt;/p>
&lt;h2 id="report-noise-confidence">Report noise/ confidence
&lt;/h2>&lt;p>Every estimator that you produce should have a notion of your confidence in this estimate attached to it.&lt;/p>
&lt;h2 id="look-at-examples">Look at examples
&lt;/h2>&lt;p>Anytime you are producing new analysis code, you need to look at examples of the underlying data and how your code is interpreting those examples.&lt;/p>
&lt;h2 id="slice-your-data">Slice your data
&lt;/h2>&lt;p>slicing help us obtain underlying features of the data subgroups easier. However, when we use slicing, we need to care about the mix shift.&lt;/p>
&lt;h2 id="consider-practical-significance">Consider practical significance
&lt;/h2>&lt;p>Don&amp;rsquo;t be blind by statistics, watch out those may have an impact on deploying or ethical problems.&lt;/p>
&lt;h2 id="check-for-consistency-over-time">Check for consistency over time
&lt;/h2>&lt;p>One particular slicing you should almost always employ is to slice by units of time.
This is because many disturbances to underlying data happen as our systems evolve over time.&lt;/p>
&lt;h1 id="process">Process
&lt;/h1>&lt;h2 id="separate-validation-description-and-evaluation">Separate Validation, description, and evaluation
&lt;/h2>&lt;ul>
&lt;li>Description should be things that everyone can agree on from the data.&lt;/li>
&lt;li>Evaluation is likely to have much more debate because you imbuing meaning and value to the data.&lt;/li>
&lt;/ul>
&lt;h2 id="confirm-exptdata-collection-setup">Confirm expt/data collection setup
&lt;/h2>&lt;p>Before looking at any data, make sure you understand the experiment and data collection setup&lt;/p>
&lt;h2 id="check-vital-signs">Check vital signs
&lt;/h2>&lt;p>Before actually answering the question you are interested in you need to check for a lot of other things that may not be related to what you are interested in but may be useful in later analysis or indicate problems in the data&lt;/p>
&lt;h2 id="standard-first-custom-second">Standard first, custom second
&lt;/h2>&lt;p>When we use metric, we should always look at standard metrics first, even if we expect them to change.&lt;/p>
&lt;h2 id="measure-twice-or-more">Measure twice, or more
&lt;/h2>&lt;p>If you are trying to capture a new phenomenon, try to measure the same underlying thing in multiple ways.
Then, check to see if these multiple measurements are consistent&lt;/p>
&lt;h2 id="check-for-reproducibility">Check for reproducibility
&lt;/h2>&lt;p>Both slicing and consistency over time are particular examples of checking for reproducibility.
If a phenomenon is important and meaningful, you should see it across different user populations and time.&lt;/p>
&lt;h2 id="check-for-consistency-with-past-measurements">Check for consistency with past measurements
&lt;/h2>&lt;p>You should compare your metrics to metrics reported in the past, even if these measurements are on different user populations.&lt;/p>
&lt;p>New metrics should be applied to old data/features first&lt;/p>
&lt;h2 id="make-hypothesis-and-look-for-evidence">Make hypothesis and look for evidence
&lt;/h2>&lt;p>Typically, exploratory data analysis for a complex problem is iterative. You will discover anomalies, trends, or other features of the data. Naturally, you will make hypotheses to explain this data. It’s essential that you don’t just make a hypothesis and proclaim it to be true. Look for evidence (inside or outside the data) to confirm/deny this theory.&lt;/p>
&lt;h2 id="exploratory-analysis-benefits-from-end-to-end-iteration">Exploratory analysis benefits from end to end iteration
&lt;/h2>&lt;p>When doing exploratory analysis, you should strive to get as many iterations of the whole analysis as possible.&lt;/p>
&lt;h1 id="social">Social
&lt;/h1>&lt;h2 id="data-analysis-starts-with-questions-not-data-or-a-technique">Data analysis starts with questions, not data or a technique
&lt;/h2>&lt;p>Ask question first and use tools to answer the questions.&lt;/p>
&lt;h2 id="acknowledge-and-count-your-filtering">Acknowledge and count your filtering
&lt;/h2>&lt;ul>
&lt;li>Acknowledge and clearly specify what filtering you are doing&lt;/li>
&lt;li>Count how much is being filtered at each of your steps
the best way to do the latter is to actually compute all your metrics even for the population you are excluding&lt;/li>
&lt;/ul>
&lt;h2 id="ratios-should-have-clear-numerator-and-denominators">Ratios should have clear numerator and denominators
&lt;/h2>&lt;p>When you communicate results containing ratios, you must be clear about the numerator and denominator.&lt;/p>
&lt;h2 id="educate-your-consumers">Educate your consumers
&lt;/h2>&lt;p>You are responsible for providing the context and a full picture of the data and not just the number a consumer asked for.&lt;/p>
&lt;h2 id="be-both-skeptic-and-champion">Be both skeptic and champion
&lt;/h2>&lt;p>As you work with data, you must be both the champion of the insights you are gaining as well as a skeptic.&lt;/p>
&lt;h2 id="share-with-peers-first-external-consumers-second">Share with peers first, external consumers second
&lt;/h2>&lt;p>A skilled peer reviewer can provide qualitatively different feedback and sanity-checking than the consumers of your data can, especially since consumers generally have an outcome they want to get&lt;/p>
&lt;h1 id="reference">Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://www.unofficialgoogledatascience.com/2016/10/practical-advice-for-analysis-of-large.html" target="_blank" rel="noopener"
>Practical advice for analysis of large, complex data sets&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>988. Smallest String Starting From Leaf</title><link>https://maosong2022.github.io/p/988.-smallest-string-starting-from-leaf/</link><pubDate>Wed, 17 Apr 2024 21:15:03 +0800</pubDate><guid>https://maosong2022.github.io/p/988.-smallest-string-starting-from-leaf/</guid><description>&lt;p>Given a binary tree with value on each node representing a lowercase letter, find the lexicographically smallest string starting from the leaf to root node&lt;/p>
&lt;h1 id="intuition">Intuition
&lt;/h1>&lt;p>We use DFS to find all strings from the root node to the leaf nodes, then reverse the string and compare it withe the largest string.&lt;/p>
&lt;h1 id="approach">Approach
&lt;/h1>&lt;p>We use &lt;code>current&lt;/code> to represent the string starting from the root node to the current node and we use &lt;code>result&lt;/code> to store the currently best result. When we reach the leaf node, we compare the &lt;code>current&lt;/code> with &lt;code>result&lt;/code> and update &lt;code>result&lt;/code>.&lt;/p>
&lt;h1 id="complexity">Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n)$$&lt;/li>
&lt;li>
$$O(\log n)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * Definition for a binary tree node.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * struct TreeNode {
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * int val;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode *left;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode *right;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode() : val(0), left(nullptr), right(nullptr) {}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode(int x) : val(x), left(nullptr), right(nullptr) {}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right) {}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * };
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">void&lt;/span> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">TreeNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">root&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">string&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">current&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">string&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">result&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">current&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sc">&amp;#39;a&amp;#39;&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">root&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">val&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// leaf node
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="o">!&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">left&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="o">!&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">right&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// update result
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="n">reverse&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">current&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">begin&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">current&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">());&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">current&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">current&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">reverse&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">current&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">begin&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">current&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">());&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">left&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">left&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">current&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">right&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">right&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">current&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">current&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">pop_back&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">string&lt;/span> &lt;span class="nf">smallestFromLeaf&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">TreeNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">root&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">string&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">8501&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="sc">&amp;#39;z&amp;#39;&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">string&lt;/span> &lt;span class="n">current&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">current&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>129. Sum Root to Leaf Numbers</title><link>https://maosong2022.github.io/p/129.-sum-root-to-leaf-numbers/</link><pubDate>Sun, 14 Apr 2024 21:06:34 +0800</pubDate><guid>https://maosong2022.github.io/p/129.-sum-root-to-leaf-numbers/</guid><description>&lt;p>concatenate the digit of path from the root to the leaf, and sum over the concatenated numbers.&lt;/p>
&lt;h1 id="intuition">Intuition
&lt;/h1>&lt;p>Use DFS to find all the paths, use a &lt;code>num&lt;/code> variable to store the number concatenated, then sum over &lt;code>num&lt;/code>.&lt;/p>
&lt;h1 id="approach">Approach
&lt;/h1>&lt;p>We use &lt;code>num&lt;/code> to represent the number from the root to the current node, if the current node is a leaf node,
we add &lt;code>num&lt;/code> to &lt;code>sum&lt;/code>. Finally, we return &lt;code>sum&lt;/code> as the result.&lt;/p>
&lt;h1 id="complexity">Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n)$$&lt;/li>
&lt;li>
$$O(1)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * Definition for a binary tree node.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * struct TreeNode {
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * int val;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode *left;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode *right;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode() : val(0), left(nullptr), right(nullptr) {}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode(int x) : val(x), left(nullptr), right(nullptr) {}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right) {}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * };
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">void&lt;/span> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">TreeNode&lt;/span> &lt;span class="o">*&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">sum&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">num&lt;/span>&lt;span class="p">){&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="o">!&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">num&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">num&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">10&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">root&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">val&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="o">!&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">left&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="o">!&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">right&lt;/span>&lt;span class="p">){&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sum&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">num&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>&lt;span class="k">else&lt;/span>&lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">left&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sum&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">right&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sum&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">num&lt;/span> &lt;span class="o">/=&lt;/span> &lt;span class="mi">10&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="nf">sumNumbers&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">TreeNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">root&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">sum&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">num&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sum&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">sum&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>404. Sum of Left Leaves</title><link>https://maosong2022.github.io/p/404.-sum-of-left-leaves/</link><pubDate>Sun, 14 Apr 2024 21:06:34 +0800</pubDate><guid>https://maosong2022.github.io/p/404.-sum-of-left-leaves/</guid><description>&lt;p>Given the &lt;code>root&lt;/code> of a binary tree, return the sum of all left leaves.&lt;/p>
&lt;h2 id="intuition">Intuition
&lt;/h2>&lt;p>Use DFS to iterate over all nodes, if it is a left leaf, sum it to the result.&lt;/p>
&lt;h2 id="approach">Approach
&lt;/h2>&lt;p>For every node, we care about one thing: whether its left child is a leaf node or not. If it is, then we add it.&lt;/p>
&lt;h2 id="complexity">Complexity
&lt;/h2>&lt;ul>
&lt;li>
$$ O(n) $$&lt;/li>
&lt;li>
$$ O(1) $$&lt;/li>
&lt;/ul>
&lt;h2 id="code">Code
&lt;/h2>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * Definition for a binary tree node.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * struct TreeNode {
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * int val;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode *left;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode *right;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode() : val(0), left(nullptr), right(nullptr) {}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode(int x) : val(x), left(nullptr), right(nullptr) {}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right) {}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * };
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">bool&lt;/span> &lt;span class="n">is_leaf_node&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">TreeNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="o">!&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">return&lt;/span> &lt;span class="nb">false&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="o">!&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">left&lt;/span> &lt;span class="o">||&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">right&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="nf">sumOfLeftLeaves&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">TreeNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">root&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="o">!&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">return&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">is_leaf_node&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">left&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">root&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">left&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">val&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">sumOfLeftLeaves&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">right&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">sumOfLeftLeaves&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">left&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>405. Convert a Number to Hexadecimal</title><link>https://maosong2022.github.io/p/405.-convert-a-number-to-hexadecimal/</link><pubDate>Sun, 14 Apr 2024 21:06:34 +0800</pubDate><guid>https://maosong2022.github.io/p/405.-convert-a-number-to-hexadecimal/</guid><description>&lt;h1 id="intuition">Intuition
&lt;/h1>&lt;p>Just use the transformation algorithm from decimal to hexadecimal&lt;/p>
&lt;h1 id="approach">Approach
&lt;/h1>&lt;p>Simulation by doing the following:&lt;/p>
&lt;ol>
&lt;li>compute &lt;code>remain = num % 16&lt;/code>, add &lt;code>remain&lt;/code> to the result (&lt;code>push_back&lt;/code>)&lt;/li>
&lt;li>update &lt;code>num = (num - remainder) / 16&lt;/code>&lt;/li>
&lt;li>repeat step 1 and step 2 until &lt;code>num&lt;/code> is 0&lt;/li>
&lt;/ol>
&lt;p>Notice that when &lt;code>num &amp;lt; 0&lt;/code>, we need use its complement.&lt;/p>
&lt;h1 id="complexity">Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(\log n)$$&lt;/li>
&lt;li>
$$ O(1) $$&lt;/li>
&lt;/ul>
&lt;h1 id="code">Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">string&lt;/span> &lt;span class="n">toHex&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">long&lt;/span> &lt;span class="n">num&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">num&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">return&lt;/span> &lt;span class="s">&amp;#34;0&amp;#34;&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">num&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="n">num&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">INT_MAX&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">num&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">INT_MAX&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">string&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">num&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">divide&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">num&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="mi">16&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">num&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">num&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">divide&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="mi">16&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">divide&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="mi">10&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="n">result&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="sc">&amp;#39;0&amp;#39;&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">divide&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span> &lt;span class="n">result&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="sc">&amp;#39;a&amp;#39;&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">divide&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">10&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">reverse&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">result&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">begin&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">());&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>623. Add One Row to Tree</title><link>https://maosong2022.github.io/p/623.-add-one-row-to-tree/</link><pubDate>Sun, 14 Apr 2024 21:06:34 +0800</pubDate><guid>https://maosong2022.github.io/p/623.-add-one-row-to-tree/</guid><description>&lt;h1 id="intuition">Intuition
&lt;/h1>&lt;p>Just list all nodes with height &lt;code>height - 1&lt;/code> and insert a new layer with the given rules.&lt;/p>
&lt;h1 id="approach">Approach
&lt;/h1>&lt;p>Use a &lt;code>queue&lt;/code> to store all nodes with the same height, and use BFS to update the nodes,
once we reach the height &lt;code>height - 1&lt;/code>, we add a new layer with the given &lt;code>val&lt;/code> for each &lt;code>node&lt;/code>:&lt;/p>
&lt;ol>
&lt;li>Create a new left child with the given &lt;code>(val, node-&amp;gt;left, nullptr)&lt;/code>&lt;/li>
&lt;li>Create a new right child with the given &lt;code>(val, nullptr, node-&amp;gt;right)&lt;/code>&lt;/li>
&lt;/ol>
&lt;h1 id="complexity">Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n)$$&lt;/li>
&lt;li>
$$O(n)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * Definition for a binary tree node.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * struct TreeNode {
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * int val;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode *left;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode *right;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode() : val(0), left(nullptr), right(nullptr) {}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode(int x) : val(x), left(nullptr), right(nullptr) {}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right) {}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * };
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">TreeNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">addOneRow&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">TreeNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">root&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">val&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">depth&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">depth&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">TreeNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">new_root&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">TreeNode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">val&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">root&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="k">nullptr&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">new_root&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">queue&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">TreeNode&lt;/span>&lt;span class="o">*&amp;gt;&lt;/span> &lt;span class="n">q&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">--&lt;/span>&lt;span class="n">depth&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="o">--&lt;/span>&lt;span class="n">depth&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">size&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">auto&lt;/span> &lt;span class="n">node&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">front&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">left&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">left&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">right&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">right&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">pop&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="o">!&lt;/span>&lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">empty&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">auto&lt;/span> &lt;span class="n">node&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">front&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">node&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">left&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">TreeNode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">val&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">left&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="k">nullptr&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">node&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">right&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">TreeNode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">val&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="k">nullptr&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">right&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">pop&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">root&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>Notes on RAG</title><link>https://maosong2022.github.io/p/notes-on-rag/</link><pubDate>Sun, 14 Apr 2024 12:38:04 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-rag/</guid><description>&lt;img src="https://maosong2022.github.io/agent_performance.png" alt="Featured image of post Notes on RAG" />&lt;h1 id="problems-of-llm">Problems of LLM
&lt;/h1>&lt;ul>
&lt;li>Out of date knowledge: the model cannot gain knowledge after training&lt;/li>
&lt;li>Humiliation: the model may generate nonsense output&lt;/li>
&lt;li>Specific domain: the generalized model is difficult to adapt to specific domain&lt;/li>
&lt;li>Enthetic problems: the model may encounter&lt;/li>
&lt;/ul>
&lt;h1 id="fine-tuning">Fine-tuning
&lt;/h1>&lt;p>Fine-tuning is used to improve performance of foundation model on specific tasks with the help with some supervised data&lt;/p>
&lt;p>Fine-tuning methods can be classified into:&lt;/p>
&lt;ol>
&lt;li>Based on range of updated parameters:
&lt;ul>
&lt;li>Full Model fine-tuning: update the parameters of the whole model&lt;/li>
&lt;li>Partial fine-tuning: freeze the top layer; freeze the bottom layer&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Based on special technology:
&lt;ul>
&lt;li>Adapter tuning&lt;/li>
&lt;li>LoRA&lt;/li>
&lt;li>Continual Learning fine-tuning&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Based on input:
&lt;ul>
&lt;li>Instruction tuning&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Based on objective
&lt;ul>
&lt;li>Multi-task fine-tuning&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>Problems of fine-tuning:&lt;/p>
&lt;ol>
&lt;li>Requires task-specific labeled data, may cause overfitting and catastrophic forgetting。&lt;/li>
&lt;li>The generalization ability is limited, and fine-tuning are required when adapting to new tasks&lt;/li>
&lt;li>The performance may be destroyed after fine-tuning, for example, safety.&lt;/li>
&lt;/ol>
&lt;h1 id="rag">RAG
&lt;/h1>&lt;p>RAG consists of three major processes of &lt;em>retrieval&lt;/em>, &lt;em>augmentation&lt;/em>, and &lt;em>generation&lt;/em>. The framework of RAG in LLM can be described as follows:&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/RAG-framework.png"
loading="lazy"
alt="RAG-framework"
>&lt;/p>
&lt;h2 id="retrieval">Retrieval
&lt;/h2>&lt;h3 id="retriever-type">Retriever type
&lt;/h3>&lt;p>Retrieval methods can be generally categorized into two types: sparse and dense, based on the information encoding methods.&lt;/p>
&lt;ol>
&lt;li>sparse retrieval usually relies on inverted index matching along with the raw data input, for example TF-IDF and BM25. The limitation of sparse retrieval in RAG is
&lt;ol>
&lt;li>its no-training nature, which makes the retrieval performance heavily rely on the quality of database construction and query generation.&lt;/li>
&lt;li>Moreover, such fixed term-based methods only support similarity retrieval, while cannot be adapted for other retrieval considerations demanding in LLM applications, such as the diversity&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>dense retrieval, on the contrary, embeds the query and documents into continuous vector space with certain criteria, for example, semantic similarity. Examples include BERT, Dense Passage Retriever (DPR), etc.&lt;/li>
&lt;/ol>
&lt;h3 id="retrieval-granularity">Retrieval Granularity
&lt;/h3>&lt;p>Retrieval granularity denotes the retrieval unit in which the corpus is indexed, e.g., document, passage, token, or other levels like entity.&lt;/p>
&lt;ol>
&lt;li>Chunk retrieval.&lt;/li>
&lt;li>Token retrieval.&lt;/li>
&lt;li>Entity retrieval.&lt;/li>
&lt;/ol>
&lt;h3 id="pre-retrieval-and-post-retrieval-enhancement">Pre-retrieval and Post-retrieval Enhancement
&lt;/h3>&lt;p>Pre-retrieval and post retrieval strategies can be added to improve the quality of the retriever.&lt;/p>
&lt;p>Pre-retrieval methods include:&lt;/p>
&lt;ol>
&lt;li>Query rewrite. This method aims to close the gaps between the input text and the needed knowledge in retrieval, to reformulate the original question into a more conducive version to retrieve.&lt;/li>
&lt;li>Query augmentation. This method aims to combine the original query and the preliminary generated outputs as a new query, which is further used to retrieve relevant information from the external database&lt;/li>
&lt;/ol>
&lt;p>Post-retrieval enhancement denotes the procedure to process the extracted top-k documents from the retriever before feeding them to the generator for the sake of better alignment between the retrieval and generation stages.&lt;/p>
&lt;h3 id="database">Database
&lt;/h3>&lt;ol>
&lt;li>Wikipedia&lt;/li>
&lt;li>Domain specific database&lt;/li>
&lt;li>search engine&lt;/li>
&lt;/ol>
&lt;h2 id="generation">Generation
&lt;/h2>&lt;ol>
&lt;li>Parameter-Accessible Generators (White-box). Allow parameter optimization.&lt;/li>
&lt;li>Parameter-Inaccessible Generators (Black-box). Focus more on retrieval and augmentation processes, trying to enhance the generator by augmenting the input with better knowledge, guidances or examples for the generation.&lt;/li>
&lt;/ol>
&lt;h2 id="augmentation">Augmentation
&lt;/h2>&lt;ol>
&lt;li>Input layer integration&lt;/li>
&lt;li>Output layer integration&lt;/li>
&lt;li>Intermediate layer integration&lt;/li>
&lt;/ol>
&lt;h2 id="retrieval-frequency">Retrieval Frequency
&lt;/h2>&lt;p>If it is necessary to retrieve? Self-RAG&lt;/p>
&lt;p>retrieval frequency:&lt;/p>
&lt;ol>
&lt;li>One-time.&lt;/li>
&lt;li>Every-n-token&lt;/li>
&lt;li>Every token&lt;/li>
&lt;/ol>
&lt;h1 id="rag-training">RAG training
&lt;/h1>&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-rag/RAG-training.png"
width="1585"
height="957"
srcset="https://maosong2022.github.io/p/notes-on-rag/RAG-training_hu15820208252790534899.png 480w, https://maosong2022.github.io/p/notes-on-rag/RAG-training_hu16765200802062362635.png 1024w"
loading="lazy"
alt="RAG-training"
class="gallery-image"
data-flex-grow="165"
data-flex-basis="397px"
>&lt;/p>
&lt;ol>
&lt;li>Training Free&lt;/li>
&lt;li>Independent training&lt;/li>
&lt;li>Sequential training&lt;/li>
&lt;li>Joint training&lt;/li>
&lt;/ol>
&lt;h1 id="advance-rag">Advance RAG
&lt;/h1>&lt;h1 id="module-rag">Module RAG
&lt;/h1>&lt;h1 id="applications">Applications
&lt;/h1>&lt;ol>
&lt;li>NLP applications
&lt;ul>
&lt;li>QA systems: REALM&lt;/li>
&lt;li>Chatbot:&lt;/li>
&lt;li>Fact Verification: self-RAG&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Downstream tasks:
&lt;ul>
&lt;li>Recommendations&lt;/li>
&lt;li>Software engineering&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Domain-specific Applications
&lt;ul>
&lt;li>AI for science&lt;/li>
&lt;li>Finance: ChatDOC&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h2 id="limitations-of-rag">Limitations of RAG
&lt;/h2>&lt;h1 id="long-context-window">Long Context Window
&lt;/h1>&lt;p>Advantages of Long Context Window:&lt;/p>
&lt;ol>
&lt;li>Improve the understanding and relativity: long context window allows model to refer to more context information when generating answers.&lt;/li>
&lt;li>Handling complex tasks: long context window makes handling complex tasks such as writing a long article, coding&lt;/li>
&lt;li>Improve users&amp;rsquo; experience: the user expects the model remember the chat history and use them to interact with the user.&lt;/li>
&lt;/ol>
&lt;p>Disadvantages of long context window:&lt;/p>
&lt;ol>
&lt;li>Only uses context once, Requires refeeding the data to use long context window.&lt;/li>
&lt;li>Cost expensive due to input price.&lt;/li>
&lt;li>Time expensive due to limit of tokens per second.&lt;/li>
&lt;li>Needle in HayStack experiment show that there are problems with long context window.&lt;/li>
&lt;/ol>
&lt;p>Advantages of RAG:&lt;/p>
&lt;ol>
&lt;li>Privacy projection.&lt;/li>
&lt;li>Allow chunking the texts and retrieve the related information more accurately&lt;/li>
&lt;li>Adaptive to the size of data.&lt;/li>
&lt;li>Accepts multiple type of data source (multimodality).&lt;/li>
&lt;li>Only uses a small part of the total data, which is cheaper compared with long context window .&lt;/li>
&lt;/ol>
&lt;p>problems of RAG&lt;/p>
&lt;ol>
&lt;li>The quality of retrieval
&lt;ul>
&lt;li>The retrieved text cannot be aligned with the queried text.&lt;/li>
&lt;li>The queried text are not retrieved all.&lt;/li>
&lt;li>Redundancy or out-dated data may cause inaccuracy.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>the quality of response generation
&lt;ul>
&lt;li>Model Humiliation&lt;/li>
&lt;li>Irrelevance&lt;/li>
&lt;li>Organize the output to make it reasonable&lt;/li>
&lt;li>Depends on the external information&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>Futures:&lt;/p>
&lt;ol>
&lt;li>Trustworthy RA-LLMs&lt;/li>
&lt;li>Multi-lingual RA-LLMs&lt;/li>
&lt;li>Multi-modal RA-LLMs&lt;/li>
&lt;li>Quality of External Knowledge&lt;/li>
&lt;/ol>
&lt;h1 id="other-technologies">Other technologies
&lt;/h1>&lt;ol>
&lt;li>Query transformations&lt;/li>
&lt;li>Sentence window retrieval&lt;/li>
&lt;li>Fusion retrieval/ hybrid search&lt;/li>
&lt;li>multi-document agents&lt;/li>
&lt;/ol>
&lt;h1 id="reference">Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://www.semanticscholar.org/paper/A-Survey-on-RAG-Meets-LLMs%3A-Towards-Large-Language-Ding-Fan/0576e9ab604ba4bf20cd5947f3c4a2c609ac2705" target="_blank" rel="noopener"
>A Survey on RAG Meets LLMs: Towards Retrieval-Augmented Large Language Models&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>What's next for AI agentic workflows</title><link>https://maosong2022.github.io/p/whats-next-for-ai-agentic-workflows/</link><pubDate>Sun, 14 Apr 2024 12:38:04 +0800</pubDate><guid>https://maosong2022.github.io/p/whats-next-for-ai-agentic-workflows/</guid><description>&lt;img src="https://maosong2022.github.io/p/whats-next-for-ai-agentic-workflows/agent_performance.png" alt="Featured image of post What's next for AI agentic workflows" />&lt;p>In AI Ascent, Andrew Ng shared his opinion about the agentic workflow, specifically, his talk focused on designs and future works on agentic workflows.&lt;/p>
&lt;h1 id="what-is-agent">What is Agent
&lt;/h1>&lt;p>LLM based agents can be classified into two categories: non-agentic and agentic.&lt;/p>
&lt;ul>
&lt;li>Non-agentic workflow requires LLM do something in a sequence of steps, usually this requires the &amp;ldquo;zero-shot&amp;rdquo; learning ability of LLM.&lt;/li>
&lt;li>Agentic workflow requires LLM do something step by step, and revise its action by thinking or reflection.&lt;/li>
&lt;/ul>
&lt;p>Andrew then posts an image shows the agentic workflow can improve the performance of LLM on HumanEval benchmark.&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/whats-next-for-ai-agentic-workflows/agent_performance.png"
width="3706"
height="1730"
srcset="https://maosong2022.github.io/p/whats-next-for-ai-agentic-workflows/agent_performance_hu9429534153792096942.png 480w, https://maosong2022.github.io/p/whats-next-for-ai-agentic-workflows/agent_performance_hu5054130303720915832.png 1024w"
loading="lazy"
alt="Comparisons of agent performance"
class="gallery-image"
data-flex-grow="214"
data-flex-basis="514px"
>&lt;/p>
&lt;h1 id="agentic-design-reasoning-design-pattern">Agentic Design Reasoning Design Pattern
&lt;/h1>&lt;p>Agentic design pattern can be divided into following kinds. Where the first two design patterns are robust and the latter two are emerging.&lt;/p>
&lt;h3 id="reflection">Reflection
&lt;/h3>&lt;ul>
&lt;li>Self-Refine: Iterative Refinement with Self-Feedback&lt;/li>
&lt;li>Reflexion: Language Agents with Verbal Reinforcement Learning&lt;/li>
&lt;/ul>
&lt;h2 id="tool-use">Tool Use
&lt;/h2>&lt;ul>
&lt;li>Gorilla: Large Language Model Connected with Massive APIs&lt;/li>
&lt;li>MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action&lt;/li>
&lt;/ul>
&lt;h2 id="planning">Planning
&lt;/h2>&lt;ul>
&lt;li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models&lt;/li>
&lt;li>HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face&lt;/li>
&lt;/ul>
&lt;h2 id="multi-agent-collaboration">Multi-agent collaboration
&lt;/h2>&lt;ul>
&lt;li>Communicative Agents for Software Development&lt;/li>
&lt;li>AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation&lt;/li>
&lt;/ul>
&lt;h1 id="future-perspective">Future Perspective
&lt;/h1>&lt;ul>
&lt;li>The set of tasks that AI can do expand dramatically because of agentic workflows.&lt;/li>
&lt;li>We have to get used to delegating tasks to AI agent and wait patiently for response.&lt;/li>
&lt;li>Fast token generation is important. Generating more tokens even from a a lower quality LLM can achieve a good result.&lt;/li>
&lt;/ul>
&lt;h1 id="reference">Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://www.youtube.com/watch?v=sal78ACtGTc&amp;amp;list=PLOhHNjZItNnOoPxOF3dmq30UxYqFuxXKn" target="_blank" rel="noopener"
>What&amp;rsquo;s next for AI agentic workflows ft. Andrew Ng of AI Fund&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Rules of Machine Learning</title><link>https://maosong2022.github.io/p/rules-of-machine-learning/</link><pubDate>Sat, 13 Apr 2024 19:57:47 +0800</pubDate><guid>https://maosong2022.github.io/p/rules-of-machine-learning/</guid><description>&lt;p>Best practice for machine learning.&lt;/p>
&lt;h1 id="introduction">Introduction
&lt;/h1>&lt;p>Google posts a guide on how to uses machine learning in practice.
It represents a style for machine learning, similar to Google C++ Style Guide.&lt;/p>
&lt;p>In overview, to make great products:&lt;/p>
&lt;blockquote>
&lt;p>Do machine learning like the great engineer your are, not like the great machine learning expert you are.&lt;/p>
&lt;/blockquote>
&lt;p>Most algorithms we are facing are engineering problems instead of machine learning algorithms. A basic approach Google recommends is:&lt;/p>
&lt;ol>
&lt;li>Make sure your pipeline is solid end to end&lt;/li>
&lt;li>Start with a reasonable objective&lt;/li>
&lt;li>Add a common-sense features in a simple way&lt;/li>
&lt;li>Make sure that your pipeline stays solid.&lt;/li>
&lt;/ol>
&lt;p>Google separates rules with respect to different stages.&lt;/p>
&lt;h1 id="before-machine-learning">Before machine learning
&lt;/h1>&lt;p>These rules help us understand whether the time is right for building a machine learning system.&lt;/p>
&lt;blockquote>
&lt;p>Rule #1: Do not be afraid of lunch a product without machine learning.&lt;/p>
&lt;/blockquote>
&lt;p>This rule tells us that is not absolutely necessary, if rule-based methods work well, there is no need to develop a machine learning algorithm.&lt;/p>
&lt;blockquote>
&lt;p>Rule #2: First, design and implement metrics.&lt;/p>
&lt;/blockquote>
&lt;p>This rule tells us that tracking as much as possible before we formalize what our machine learning system will do. This step helps us construct the goal of our system, that is, a metric.&lt;/p>
&lt;blockquote>
&lt;p>Rule #3: Choose machine learning over a complex heuristic.&lt;/p>
&lt;/blockquote>
&lt;p>Considering using machine learning algorithms only if the heuristic algorithm doesn&amp;rsquo;t work well, since a complex heuristic is not maintainable. Meanwhile, machine-learned models are easier to update and maintain.&lt;/p>
&lt;h1 id="ml-phase-1-your-first-pipeline">ML phase 1: Your First Pipeline
&lt;/h1>&lt;p>When creating our first pipeline, we should focus on our system infrastructure&lt;/p>
&lt;blockquote>
&lt;p>Rule #4: Keep the first model simple and get the infrastructure right.&lt;/p>
&lt;/blockquote>
&lt;p>Remember infrastructure issues are many more than model problems when we creating the first machine learning model. We have to determine:&lt;/p>
&lt;ol>
&lt;li>How to obtain data for our model&lt;/li>
&lt;li>How to evaluate the performance of our model&lt;/li>
&lt;li>How to integrate our model into our application&lt;/li>
&lt;/ol>
&lt;p>Moreover, we should choose simple features to ensure that:&lt;/p>
&lt;ul>
&lt;li>The features reach our learning algorithm correctly&lt;/li>
&lt;li>The model learns reasonably weights&lt;/li>
&lt;li>The features reach our model in the sever correctly&lt;/li>
&lt;/ul>
&lt;p>Once we have a system that does above things reliably, we have done most of the work.&lt;/p>
&lt;blockquote>
&lt;p>Rule #5: Test the infrastructure independently from the machine learning.&lt;/p>
&lt;/blockquote>
&lt;p>This rule tells us split the infrastructure and the machine learning model and test them separately to avoid dependency issue occurs. In this way, our model can be developed without worrying about environment. Specifically:&lt;/p>
&lt;ol>
&lt;li>Test getting data into algorithm. Check the data that are feed into the models and do some statistics before using the data&lt;/li>
&lt;li>Testing getting models out of the training algorithm. Make sure our algorithms work in the same way in our serving environment as the training environment.&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>Rule #6: Be careful about dropped data.&lt;/p>
&lt;/blockquote>
&lt;p>Do not drop data without thoroughly test, this may data loss.&lt;/p>
&lt;blockquote>
&lt;p>Rule #7: Turn heuristics into features, or handle them externally.&lt;/p>
&lt;/blockquote>
&lt;p>Before using machine learning models, if we have tried some heuristic algorithms, then these algorithms may help us to improve the overall performance. Some ways we can use an existing heuristic algorithm:&lt;/p>
&lt;ol>
&lt;li>Preprocessing using the heuristic. If the feature is incredibly awesome, then do not try to relearn the feature. Just use the heuristic way to pre-process the data.&lt;/li>
&lt;li>Create a feature. We can use the heuristic way to create a new feature to help improve the machine learning performance.&lt;/li>
&lt;li>Mine the raw inputs of the heuristic. We can use the inputs of heuristic as features to learn the heuristic implicitly.&lt;/li>
&lt;li>Modify the label.&lt;/li>
&lt;/ol>
&lt;h2 id="monitoring">Monitoring
&lt;/h2>&lt;p>In general, such as making alerts and having a dashboard page.&lt;/p>
&lt;blockquote>
&lt;p>Rule #8: Know the freshness requirements of our system.&lt;/p>
&lt;/blockquote>
&lt;p>It is important for us to know the freshness of our model, for example, how much does performance degrade if we have a model that is a day old. The freshness helps us monitor and improve the performance.&lt;/p>
&lt;blockquote>
&lt;p>Rule #9: Detect problems before exporting models.&lt;/p>
&lt;/blockquote>
&lt;p>This rule tells us that evaluating the performance of the model before serving. The evaluation includes the testing on hold-out data, check AUC metric.&lt;/p>
&lt;blockquote>
&lt;p>Rule #10: Watch for silent failures.&lt;/p>
&lt;/blockquote>
&lt;p>Since the continuos change of data, silent failures may occur, so keep tracking statistics of the data as well as manually inspect the data on occasion help us reduce these kind of issues.&lt;/p>
&lt;blockquote>
&lt;p>Rule #11: Give feature owners and documentation.&lt;/p>
&lt;/blockquote>
&lt;p>Knowing who created the feature helps us gain information about data. A detailed documentation helps user understand how it works.&lt;/p>
&lt;h2 id="your-first-objective">Your first objective
&lt;/h2>&lt;blockquote>
&lt;p>Rule #12: Don&amp;rsquo;t overthink which objective you choose to optimize.&lt;/p>
&lt;/blockquote>
&lt;p>There are many metrics to optimize according to Rule #2. However, it turns out in early stage, some metrics are optimized even though we not directly optimizing them.&lt;/p>
&lt;blockquote>
&lt;p>Rule #13: Choose simple, observable and attributable metric for your first objective.&lt;/p>
&lt;/blockquote>
&lt;p>This rule tells us the strategy of choosing metric in the beginning. In principal, The ML objective should be something that is easy to measure and is a proxy for the &amp;ldquo;true&amp;rdquo; objective. In fact however, there is no such &amp;ldquo;true&amp;rdquo; objective, so we should keep the objective as simple as possible, it&amp;rsquo;s better if the objective is observable. Then, we can modify the objective based on the performance of the model.&lt;/p>
&lt;blockquote>
&lt;p>Rule #14: Starts with an interpretable model makes debugging easier.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Rule #15: Separate spam filtering and quality ranking in a policy layer.&lt;/p>
&lt;/blockquote>
&lt;p>Sometimes, spam filtering confuses quality ranking, when we do quality ranking, we should clean the data.&lt;/p>
&lt;h1 id="ml-phase-2-feature-engineering">ML phase 2: Feature engineering
&lt;/h1>&lt;p>After we have a working end to end system with unit and system tests instrumented, Phase II begins.
In this phase, we should make use of features.&lt;/p>
&lt;blockquote>
&lt;p>Rule #16: Plan to lunch and iterate.&lt;/p>
&lt;/blockquote>
&lt;p>There are three reasons to lunch new models:&lt;/p>
&lt;ol>
&lt;li>You are coming up with new features&lt;/li>
&lt;li>You are tuning regularization and combining old features in new ways&lt;/li>
&lt;li>You are tuning the objectives&lt;/li>
&lt;/ol>
&lt;p>When lunch a new model, we should think about:&lt;/p>
&lt;ol>
&lt;li>How easy is it to add or remove or recombine features&lt;/li>
&lt;li>How easy is it to create a fresh copy of the pipeline and verify its correctness.&lt;/li>
&lt;li>Is it possible to have two or three copies running in parallel.&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>Rule #17: Start with directly observed and reported features as opposed to learned features.&lt;/p>
&lt;/blockquote>
&lt;p>a learned feature is a feature generated by an external system or by the learner itself.
If the learned feature comes from an external system, then bias or being out-of-date may affect the model. If the learned feature comes from the learner itself, then it is hard to tell the impact of the feature.&lt;/p>
&lt;blockquote>
&lt;p>Rule #18: Explore with features of content that generalize across contexts.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Rule #19: Use very specific features when you can.&lt;/p>
&lt;/blockquote>
&lt;p>It is simpler to learn millions of simple features than a few complex features.&lt;/p>
&lt;blockquote>
&lt;p>Rule #20: Combine and modify existing features to create new features in human-understanding ways.&lt;/p>
&lt;/blockquote>
&lt;p>Combine features may causing overfitting problems&lt;/p>
&lt;blockquote>
&lt;p>Rule #21: The number of feature weights we can learn in a linear model is roughly proportional to the number of data you have.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Rule #22: Clean up features you are no longer using.&lt;/p>
&lt;/blockquote>
&lt;p>If you find that you are not using a feature, and that combining it with other features is not working, then drop it out of your infrastructure.&lt;/p>
&lt;h2 id="human-analysis-of-the-system">Human analysis of the system
&lt;/h2>&lt;p>This subsection teaches us how to look at an existing model and improve it.&lt;/p>
&lt;blockquote>
&lt;p>Rule #23: You are not a typical end user.&lt;/p>
&lt;/blockquote>
&lt;p>Check carefully before we deploying the model.&lt;/p>
&lt;blockquote>
&lt;p>Rule #24: Measure the delta between models&lt;/p>
&lt;/blockquote>
&lt;p>Make sure the system is stable when making small changes. Make sure that a model when compared with itself has a low (ideally zero) symmetric difference.&lt;/p>
&lt;blockquote>
&lt;p>Rule #25: When choosing models, utilitarian performance trumps predictive power.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Rule #26: Look for patterns in the measured errors, and create new features.&lt;/p>
&lt;/blockquote>
&lt;p>Once you have examples that the model got wrong, look for trends that are outside your current feature set.&lt;/p>
&lt;blockquote>
&lt;p>Rule #27: Try to quantify observed undesired behavior&lt;/p>
&lt;/blockquote>
&lt;p>If your issues are measurable, then you can start using them as features, objectives, or metrics. The general rule is &amp;ldquo;&lt;strong>measure first, optimize second&lt;/strong>&amp;rdquo;.&lt;/p>
&lt;blockquote>
&lt;p>Rule #28: Be aware that identical short-term behavior does not imply identical long-term behavior.&lt;/p>
&lt;/blockquote>
&lt;h2 id="training-serving-skew">Training-Serving skew
&lt;/h2>&lt;p>Training-serving skew is a difference between performance during training and performance during serving.
This skew can be caused by:&lt;/p>
&lt;ol>
&lt;li>A discrepancy between how you handle data in the training and serving pipelines&lt;/li>
&lt;li>A change in the data between when you train and when you serve&lt;/li>
&lt;li>A feedback loop between your model and your algorithm.&lt;/li>
&lt;/ol>
&lt;p>The best solution is to explicitly monitor it so that system and data changes don&amp;rsquo;t introduce skew unnoticed.&lt;/p>
&lt;blockquote>
&lt;p>Rule #29: The best way to make sure that you train like you serve is to save the set of features used at the serving time, and then pipe those features to a log to use them at training time.&lt;/p>
&lt;/blockquote>
&lt;p>This can help verify the consistency between the training and serving.&lt;/p>
&lt;blockquote>
&lt;p>Rule #30: Importance-weight sampled data, don&amp;rsquo;t arbitrarily drop it.&lt;/p>
&lt;/blockquote>
&lt;p>Importance weighting means that if you decide that you are going to sample example X with a 30% probability, then give it a weight of 10/3. With importance weighting, all of the calibration properties discussed in Rule #14 still hold.&lt;/p>
&lt;blockquote>
&lt;p>Rule #31: Beware that if your join data from a table at training and serving time, the data in the table may change.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Rule #32: Re-use code between your training pipeline and your serving pipeline whenever possible.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Rule #33: If you produce a model based on the data until January 5th, test the model on the data from January 6th and after.&lt;/p>
&lt;/blockquote>
&lt;p>In general, measure performance of a model on the data gathered after the data you trained the model on, as this better reflects what your system will do in production&lt;/p>
&lt;blockquote>
&lt;p>Rule #37: Measure Training-Serving Skew.&lt;/p>
&lt;/blockquote>
&lt;p>We can divide causes of Training-Serving Skew into several parts:&lt;/p>
&lt;ol>
&lt;li>The difference between the performance on the training data and the holdout data. In general, this will always exist, and it is not always bad.&lt;/li>
&lt;li>The difference between the performance on the holdout data and the &amp;ldquo;next­day&amp;rdquo; data. Again, this will always exist&lt;/li>
&lt;li>The difference between the performance on the &amp;ldquo;next-day&amp;rdquo; data and the live data.&lt;/li>
&lt;/ol>
&lt;h1 id="ml-phase-3-slowed-growth-optimization-refinement-and-complex-models">ML phase 3: Slowed growth, Optimization refinement, and complex models
&lt;/h1>&lt;blockquote>
&lt;p>Rule #38: Don’t waste time on new features if unaligned objectives have become the issue.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Rule #39: Launch decisions are a proxy for long-term product goals.&lt;/p>
&lt;/blockquote>
&lt;p>The only easy launch decisions are when all metrics get better (or at least do not get worse).&lt;/p>
&lt;p>Individuals, on the other hand, tend to favor one objective that they can directly optimize.&lt;/p>
&lt;blockquote>
&lt;p>Rule #40: Keep ensembles simple.&lt;/p>
&lt;/blockquote>
&lt;p>To keep things simple, each model should either be an ensemble only taking the input of other models, or a base model taking many features, but not both.&lt;/p>
&lt;blockquote>
&lt;p>Rule #41: When performance plateaus, look for qualitatively new sources of information to add rather than refining existing signals.&lt;/p>
&lt;/blockquote>
&lt;p>As in any engineering project, you have to weigh the benefit of adding new features against the cost of increased complexity.&lt;/p>
&lt;blockquote>
&lt;p>Rule #42: Don’t expect diversity, personalization, or relevance to be as correlated with popularity as you think they are.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Rule #43: Your friends tend to be the same across different products. Your interests tend not to be.&lt;/p>
&lt;/blockquote>
&lt;h1 id="conclusion">Conclusion
&lt;/h1>&lt;p>In early stage, make sure that the infrastructure is well constructed, the used model can be simple.&lt;/p>
&lt;p>In main stage, focusing on the utilitarian performance and the gap between training data and test data.&lt;/p>
&lt;p>When utilizing features, use simple, observable features.&lt;/p>
&lt;p>When deploying models, watch out training-serving skew.&lt;/p>
&lt;h1 id="reference">Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://developers.google.com/machine-learning/guides/rules-of-ml" target="_blank" rel="noopener"
>Rules of Machine Learning&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>How to fix http error when creating a new environment.</title><link>https://maosong2022.github.io/p/how-to-fix-http-error-when-creating-a-new-environment./</link><pubDate>Fri, 25 Aug 2023 00:00:00 +0000</pubDate><guid>https://maosong2022.github.io/p/how-to-fix-http-error-when-creating-a-new-environment./</guid><description>&lt;h1 id="problem-description">Problem description
&lt;/h1>&lt;p>When we use &lt;code>conda create --env myenv&lt;/code> to create a new environment, it may throw an exception&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">CondaHTTPError: HTTP &lt;span class="m">000&lt;/span> CONNECTION FAILED &lt;span class="k">for&lt;/span> url &amp;lt;https://repo.anaconda.com/pkgs/main/osx-64/repodata.json&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>usually, this occurs when the source channels are not domestic. In this case, we need the change the configuration.&lt;/p>
&lt;h1 id="solution">Solution
&lt;/h1>&lt;ol>
&lt;li>use &lt;code>conda info&lt;/code> to view the configuration file:&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">conda info
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>focus on the &lt;code>user config file&lt;/code> and open it.
2. use the following configuration to overwrite the file:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">ssl_verify: &lt;span class="nb">false&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">channels:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> - defaults
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">show_channel_urls: &lt;span class="nb">true&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">default_channels:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">custom_channels:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> pytorch-lts: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> deepmodeling: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>Archives</title><link>https://maosong2022.github.io/archives/</link><pubDate>Sun, 06 Mar 2022 00:00:00 +0000</pubDate><guid>https://maosong2022.github.io/archives/</guid><description/></item><item><title>Tags</title><link>https://maosong2022.github.io/tags/</link><pubDate>Sun, 06 Mar 2022 00:00:00 +0000</pubDate><guid>https://maosong2022.github.io/tags/</guid><description>&lt;p>This page displays a tag cloud of all tags used across the blog posts. The size of each tag indicates its frequency. Click on any tag to see all posts associated with that tag.&lt;/p></description></item><item><title>About</title><link>https://maosong2022.github.io/about/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://maosong2022.github.io/about/</guid><description>&lt;p>Hi, my name is Mao Song (毛松). I am currently a researcher in &lt;a class="link" href="https://www.shlab.org.cn/" target="_blank" rel="noopener"
>Shanghai Artificial Intelligence Laboratory&lt;/a> or Shanghai AI Lab. I am working on multimodal large language models (MLLMs) and their applications. SEE my personal CV at &lt;a class="link" href="https://github.com/MaoSong2022/MaoSong2022.github.io/blob/master/assets/%E6%AF%9B%E6%9D%BE_%E7%AE%80%E5%8E%86.pdf" target="_blank" rel="noopener"
>Resume&lt;/a>&lt;/p>
&lt;p>Before joining Shanghai AI Lab, I received my Master&amp;rsquo;s degree from ShanghaiTech University, under the supervision of &lt;a class="link" href="https://sist.shanghaitech.edu.cn/wanghao1/main.htm" target="_blank" rel="noopener"
>Hao Wang&lt;/a> and Bachelor&amp;rsquo;s degree from Beijing Institute of Technology.&lt;/p>
&lt;p>Though I am working on MLLMs, I am also interested in other topics, such as large language models, reasoning models and math-related topics.&lt;/p>
&lt;p>If you are interested in my work, please feel free to contact me. My email is &lt;a class="link" href="mailto:maosong@pjlab.org.cn" >maosong AT pjlab.org.cn&lt;/a>.&lt;/p>
&lt;p>Occasionally, I will upload some of my learning videos on &lt;a class="link" href="https://b23.tv/a0Bb9Z1" target="_blank" rel="noopener"
>Bilibili&lt;/a>, feel free to check it out and leave comments.&lt;/p></description></item><item><title>Search</title><link>https://maosong2022.github.io/search/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://maosong2022.github.io/search/</guid><description/></item></channel></rss>