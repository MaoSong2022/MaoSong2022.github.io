<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Mao Song(毛松)'s Homepage</title><link>https://maosong.website/</link><description>Recent content on Mao Song(毛松)'s Homepage</description><generator>Hugo -- gohugo.io</generator><language>en</language><atom:link href="https://maosong.website/index.xml" rel="self" type="application/rss+xml"/><item><title>Notes on Step3-VL 10B</title><link>https://maosong.website/p/notes-on-step3-vl-10b/</link><pubDate>Fri, 13 Feb 2026 18:05:47 +0800</pubDate><guid>https://maosong.website/p/notes-on-step3-vl-10b/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>Step3-VL-10B 基于 &lt;a class="link" href="Perception%20Encoder.md" >Perception Encoder&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a> 开发得到，模型在预训练阶段使用了 1.2T 多模态 token, 在 post training 阶段使用了 &lt;a class="link" href="PaCoRe.md" >PaCoRe&lt;/a> 来提高模型的表现。作者强调关键改进在于高质量的预训练数据以及 RL 阶段的提升。&lt;/p>
&lt;p>总的来说，感觉这是阶跃在多模态大模型领域的一次初步尝试，使用的技术路线都比较成熟。&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;p>Step3-VL-10B 包含 3 个模块：&lt;/p>
&lt;ul>
&lt;li>ViT: 基于 &lt;a class="link" href="Perception%20Encoder.md" >Perception Encoder&lt;/a>, 大小为 1.8B&lt;/li>
&lt;li>MLP: 基于 &lt;a class="link" href="DeepSeek-OCR.md" >DeepSeek-OCR&lt;/a> 构建了一个两层的卷积层，将视觉 token 个数压缩为原来的 16 倍，对应代码如下&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># vision_encoder.py StepRoboticsVisionEncoder&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">vit_downsampler1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Conv2d&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">kernel_size&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">stride&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">padding&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">vit_downsampler2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Conv2d&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">4&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">kernel_size&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">stride&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">padding&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># modeling_step_vl.py _process_image_features&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">image_features&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">vision_model&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">vit_downsampler1&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">image_features&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">image_features&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">vision_model&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">vit_downsampler2&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">image_features&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>LLM: 基于 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a> 的 Qwen3-8B&lt;/li>
&lt;/ul>
&lt;p>对于输入的图片，Step3-VL-10B 采用了 LLaVA-OneVision 的做法，即将图片分为 $728\times 728$ 的 global thumnail 和 $504\times 504$ 的 local crop.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># modeling_step_vl.py _process_image_input&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">image_features&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_process_image_features&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">image_features&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">patch_image_features&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_process_image_features&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">patch_image_features&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="n">patch_image_features&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="kc">None&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="kc">None&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h3>&lt;p>预训练的数据包括：&lt;/p>
&lt;ul>
&lt;li>knowledge: knowledge 数据又包括图文交错数据，image-text pairs 数据&lt;/li>
&lt;li>education: 15M K12, university, adult education 数据&lt;/li>
&lt;li>OCR: 又分为以下几类
&lt;ul>
&lt;li>image to text: 10M (real-world) + 30M (synthetic) 数据&lt;/li>
&lt;li>image to code: 10M markup-based code (latex, matplotlib 等) 数据，15M 合成的 infographics 数据，5M reconstruction (tikz) 数据&lt;/li>
&lt;li>document to text: 80M full-page 数据&lt;/li>
&lt;li>document to code: HTML, markdown, latex 等数据，共 4M tables 和 100M formulas&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>grounding and counting: 400M 数据&lt;/li>
&lt;li>VQA: 10M 数据&lt;/li>
&lt;li>GUI: 23M 数据&lt;/li>
&lt;/ul>
&lt;p>训练使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-adamw/" target="_blank" rel="noopener"
>AdamW&lt;/a> 优化器，一共训练了 1.2T token, batch size 为 8192, 上下文长度为 4096.&lt;/p>
&lt;h3 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h3>&lt;p>post-training 包括 SFT 和 RL 两个阶段&lt;/p>
&lt;p>SFT 阶段又包括了两个小的 stage, 第一个 stage 用于提高模型的纯文本推理能力，纯文本数据和多模态数据的比例为 9:1,训练使用了 190B token; 第二个 stage 用于提高模型的多模态推理能力，纯文本数据和多模态数据的比例为 1:1. 训练使用了 36B token. SFT 训练时 batch size 为 32, 上下文长度为 128K&lt;/p>
&lt;p>RL 阶段作者使用了 &lt;a class="link" href="PPO.md" >PPO&lt;/a> 算法进行训练。reward function 也是分为 rule-based 和 model-based&lt;/p>
&lt;p>在 RLVR 之后，作者还进行了 RLHF 来提高模型的对齐能力。&lt;/p>
&lt;p>作者进一步使用了 &lt;a class="link" href="PaCoRe.md" >PaCoRe&lt;/a> 来提高模型的并行推理能力。&lt;/p>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>Step3-VL-10B 的表现如下所示，可以看到在 10B 模型下面，Step3-VL-10B 的表现达到了 SOTA.&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-step3-vl-10b/Step3-VL-performance.png"
width="1210"
height="691"
loading="lazy"
alt="Performance of Step3-VL"
class="gallery-image"
data-flex-grow="175"
data-flex-basis="420px"
>&lt;/p>
&lt;h3 id="ablation-study">&lt;a href="#ablation-study" class="header-anchor">&lt;/a>Ablation Study
&lt;/h3>&lt;p>作者首先对比了 perception encoder 和 &lt;a class="link" href="DINOv3.md" >DINOv3&lt;/a> 作为 vision encoder 的表现，结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Vision Encoder&lt;/th>
&lt;th>Perception BLINK&lt;/th>
&lt;th>Perception Omni.&lt;/th>
&lt;th>Perception MMVP&lt;/th>
&lt;th>Perception OCRBench&lt;/th>
&lt;th>General MMStar&lt;/th>
&lt;th>General SVQA&lt;/th>
&lt;th>General CCBench&lt;/th>
&lt;th>General V*&lt;/th>
&lt;th>General MMMU&lt;/th>
&lt;th>General ReMI&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>DINOv3&lt;/td>
&lt;td>42.35&lt;/td>
&lt;td>43.31&lt;/td>
&lt;td>28.00&lt;/td>
&lt;td>57.60&lt;/td>
&lt;td>41.43&lt;/td>
&lt;td>22.18&lt;/td>
&lt;td>56.32&lt;/td>
&lt;td>34.55&lt;/td>
&lt;td>46.56&lt;/td>
&lt;td>24.50&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>PE-lang (Ours)&lt;/td>
&lt;td>41.19&lt;/td>
&lt;td>43.57&lt;/td>
&lt;td>32.00&lt;/td>
&lt;td>70.10&lt;/td>
&lt;td>42.10&lt;/td>
&lt;td>21.15&lt;/td>
&lt;td>59.39&lt;/td>
&lt;td>37.17&lt;/td>
&lt;td>47.67&lt;/td>
&lt;td>26.08&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Δ&lt;/td>
&lt;td>-1.16&lt;/td>
&lt;td>+0.26&lt;/td>
&lt;td>+4.00&lt;/td>
&lt;td>+12.50&lt;/td>
&lt;td>+0.67&lt;/td>
&lt;td>-1.03&lt;/td>
&lt;td>+3.07&lt;/td>
&lt;td>+2.62&lt;/td>
&lt;td>+1.11&lt;/td>
&lt;td>+1.58&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>作者分析认为由于 DINOv3 在纯视觉任务上进行训练，其表现不如使用语言监督信号的 perception encoder&lt;/p>
&lt;p>作者还对比了 &lt;a class="link" href="https://maosong.website/p/notes-on-adamw/" target="_blank" rel="noopener"
>AdamW&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/notes-on-moonlight/" target="_blank" rel="noopener"
>Muon&lt;/a> 两种优化器，结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Optimizer&lt;/th>
&lt;th>Perception BLINK&lt;/th>
&lt;th>Perception Omni.&lt;/th>
&lt;th>Perception MMVP&lt;/th>
&lt;th>Perception OCRBench&lt;/th>
&lt;th>General MMStar&lt;/th>
&lt;th>General SVQA&lt;/th>
&lt;th>General CCBench&lt;/th>
&lt;th>General V*&lt;/th>
&lt;th>General MMMU&lt;/th>
&lt;th>General ReMI&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Muon&lt;/td>
&lt;td>41.14&lt;/td>
&lt;td>42.73&lt;/td>
&lt;td>32.00&lt;/td>
&lt;td>67.70&lt;/td>
&lt;td>44.58&lt;/td>
&lt;td>27.08&lt;/td>
&lt;td>60.72&lt;/td>
&lt;td>36.65&lt;/td>
&lt;td>47.56&lt;/td>
&lt;td>22.23&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Adam (Ours)&lt;/td>
&lt;td>40.72&lt;/td>
&lt;td>44.94&lt;/td>
&lt;td>29.33&lt;/td>
&lt;td>71.10&lt;/td>
&lt;td>41.77&lt;/td>
&lt;td>20.60&lt;/td>
&lt;td>60.13&lt;/td>
&lt;td>39.27&lt;/td>
&lt;td>46.11&lt;/td>
&lt;td>25.00&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Δ&lt;/td>
&lt;td>-0.42&lt;/td>
&lt;td>+2.21&lt;/td>
&lt;td>-2.67&lt;/td>
&lt;td>+3.40&lt;/td>
&lt;td>-2.81&lt;/td>
&lt;td>-6.48&lt;/td>
&lt;td>-0.59&lt;/td>
&lt;td>+2.62&lt;/td>
&lt;td>-1.45&lt;/td>
&lt;td>+2.77&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>结果发现 Muon 可以解决大规模数据的噪声和不平衡问题，但是作者没有使用 Muon, 原因是 Muon 对参数的初始化比较敏感&lt;/p>
&lt;p>作者还探究了 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3-vl/" target="_blank" rel="noopener"
>Qwen3-VL&lt;/a> 使用的 Deepstack 的有效性，结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Technique&lt;/th>
&lt;th>Perception BLINK&lt;/th>
&lt;th>Perception Omni.&lt;/th>
&lt;th>Perception MMVP&lt;/th>
&lt;th>Perception OCRBench&lt;/th>
&lt;th>General MMStar&lt;/th>
&lt;th>General SVQA&lt;/th>
&lt;th>General CCBench&lt;/th>
&lt;th>General V*&lt;/th>
&lt;th>General MMMU&lt;/th>
&lt;th>General ReMI&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>w/ DeepStack&lt;/td>
&lt;td>40.72&lt;/td>
&lt;td>42.92&lt;/td>
&lt;td>26.00&lt;/td>
&lt;td>71.20&lt;/td>
&lt;td>43.31&lt;/td>
&lt;td>28.66&lt;/td>
&lt;td>63.94&lt;/td>
&lt;td>36.65&lt;/td>
&lt;td>47.44&lt;/td>
&lt;td>26.96&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>w/o DeepStack (Ours)&lt;/td>
&lt;td>40.61&lt;/td>
&lt;td>43.57&lt;/td>
&lt;td>31.33&lt;/td>
&lt;td>69.30&lt;/td>
&lt;td>42.44&lt;/td>
&lt;td>25.20&lt;/td>
&lt;td>62.80&lt;/td>
&lt;td>38.22&lt;/td>
&lt;td>47.78&lt;/td>
&lt;td>26.96&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Δ&lt;/td>
&lt;td>-0.11&lt;/td>
&lt;td>+0.65&lt;/td>
&lt;td>+5.33&lt;/td>
&lt;td>-1.90&lt;/td>
&lt;td>-0.87&lt;/td>
&lt;td>-3.46&lt;/td>
&lt;td>-1.14&lt;/td>
&lt;td>+1.57&lt;/td>
&lt;td>+0.34&lt;/td>
&lt;td>+0.00&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>结果发现，尽管 DeepStack 可以加速训练，但是对于下游任务的提升非常有限，因此作者没有使用这个策略。&lt;/p>
&lt;p>对于 RL 训练，作者发现随着训练进行，模型的 reward 稳步提升。但是其输出长度并不是单调提升的，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-step3-vl-10b/Step3-VL-RL-dynamics.png"
width="1271"
height="407"
loading="lazy"
alt="RLVR dynamics"
class="gallery-image"
data-flex-grow="312"
data-flex-basis="749px"
>&lt;/p>
&lt;p>作者分析认为这是由于模型在 reasoning 任务和 perception 任务上模型使用的模式不同导致的，reasoning 任务上模型使用更长的输出长度来解决问题，而对于 perception 任务，由于答案唯一且确定，因此模型通过多次探索后会逐渐收敛到唯一的确定性模式，直接给出对应的答案。因此，其输出长度会越来越短。作者针对这种现象给出了一个假设，即针对 perception 任务，我们的训练数据并不包含思考的过程，而这种数据则让模型只能选择直接回答或者瞎猜，而不是先思考再回答。为了解决这个问题，作者使用了 &lt;a class="link" href="PaCoRe.md" >PaCoRe&lt;/a> 来让模型通过 proposal-then-refinement 策略来提高模型的思考长度&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者介绍了 Step3-VL，一个 10B 的多模态大模型，作者详细介绍了模型的架构，训练以及数据。&lt;/p>
&lt;p>作者认为后续工作有：&lt;/p>
&lt;ol>
&lt;li>通过 universal RL Scaling 提高 token efficiency:
&lt;ol>
&lt;li>将算力重心从 pre-training 迁移到 RL, 这一点与 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v3.2/" target="_blank" rel="noopener"
>DeepSeek-V3.2&lt;/a> 一致&lt;/li>
&lt;li>消除过度思考，提高模型的 reasoning efficiency&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>提高模型理解物理世界的能力
&lt;ol>
&lt;li>构建 world model 帮助模型理解世界&lt;/li>
&lt;li>使用 high-fidelity environment 来提高模型对于物理定律的理解能力&lt;/li>
&lt;li>具身智能&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2601.09668" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Kimi-k2.5</title><link>https://maosong.website/p/notes-on-kimi-k2.5/</link><pubDate>Thu, 12 Feb 2026 11:13:13 +0800</pubDate><guid>https://maosong.website/p/notes-on-kimi-k2.5/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>Kimi-K2.5 的核心有亮点：&lt;/p>
&lt;ol>
&lt;li>native multi-modal: 通过在预训练，SFT, RL 阶段使用多模态数据来提高模型的多模态能力&lt;/li>
&lt;li>agent: 通过并行 multi-agent 的方式来提高模型解决复杂问题的效率和能力&lt;/li>
&lt;/ol>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;p>Kimi K2.5 是一个标准的 ViT-MLP-LLM 架构，其中&lt;/p>
&lt;ol>
&lt;li>ViT, 基于 &lt;a class="link" href="Kimi-VL.md" >Kimi-VL&lt;/a> 提出的 MoonViT, 并进行了改进, 参数量为 400M&lt;/li>
&lt;li>MLP, 基于 patch merger,&lt;/li>
&lt;li>LLM, 基于 &lt;a class="link" href="https://maosong.website/p/notes-on-kimi-k2/" target="_blank" rel="noopener"
>Kimi-k2&lt;/a>, 参数量为 1.02T-A32B&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>ViT&lt;/strong>
作者使用了 &lt;a class="link" href="Kimi-VL.md" >Kimi-VL&lt;/a> 提出的 MoonViT, MoonViT 基于 &lt;a class="link" href="SigLIP.md" >SigLIP&lt;/a> 提出的 SigLIP-SO-400M 开发得到，MoonViT 使用了 &lt;a class="link" href="NaViT.md" >NaViT&lt;/a> 来避免切分图片和使用不同精度图片进行训练。&lt;/p>
&lt;p>在 MoonViT 的基础上，Kimi-K2.5 还进一步提出了 MoonViT-3D, 将 &lt;a class="link" href="NaViT.md" >NaViT&lt;/a> 的思想扩展到了 3D 用于提高模型的视频理解能力，具体做法为将连续 4 帧的视频展开为 1D sequence, 这样在图像上的注意力机制就可以无缝衔接到视频上了。并且，通过这种方式，我们可以让模型关注跨帧的信息（注意力在 4 帧的 token 之间进行），简化代码如下所示&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># config.json temporal_merge_kernel_size&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># kimi_k25_vision_processing.py split_video_chunks&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">video_chunk&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">frames&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mi">4&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">patches&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">frame&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">video_chunk&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">patches&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">extend&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">split_into_patches&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">frame&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">tokens&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">patches&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># modeling_kimi_k25.py Learnable2DInterpPosEmbDivided_fixed&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">positions&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">spatial_embedding&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">temporal_embedding&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># modeling_kimi_k25.py MoonViT3dEncoder&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">transformer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tokens&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">positions&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>最后，在进入 MLP 之前，作者还对每个 temporal chunk 内的特征进行 pooling 操作，将时序长度压缩到了原来的 1/4, 进而提高模型可处理的视频长度。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># modeling_kimi_k25.py tpool_patch_merger&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">tpool_patch_merger&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">grid_thws&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">merge_kernel_size&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">tuple&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">list&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">d_model&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">outputs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pre_sum&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">t&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">h&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">w&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">grid_thws&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tolist&lt;/span>&lt;span class="p">():&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Get the current sequence&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">seq&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">pre_sum&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">pre_sum&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">t&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">h&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">w&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Reshape along self.merge_kernel_size and concat to the last dimension&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">kernel_height&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">kernel_width&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">merge_kernel_size&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">new_height&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">new_width&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">h&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="n">kernel_height&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">w&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="n">kernel_width&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">reshaped_seq&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">seq&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">new_height&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">kernel_height&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">new_width&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">kernel_width&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d_model&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">reshaped_seq&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">reshaped_seq&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">permute&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">4&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">contiguous&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mean&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># temporal pooling&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">padded_seq&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">reshaped_seq&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">new_height&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">new_width&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">kernel_height&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">kernel_width&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">outputs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">padded_seq&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pre_sum&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">t&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">h&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">w&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">outputs&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;strong>MLP&lt;/strong>
MLP 使用了 PatchMerger, 用于减少视觉 token 个数，这个方案在之前的 Qwen-VL 系列里已经得到了应用。&lt;/p>
&lt;p>&lt;strong>LLM&lt;/strong>
LLM 基于 &lt;a class="link" href="https://maosong.website/p/notes-on-kimi-k2/" target="_blank" rel="noopener"
>Kimi-k2&lt;/a> 的 MoE 模型，总参数为 1T, 激活参数为 32B&lt;/p>
&lt;h3 id="data">&lt;a href="#data" class="header-anchor">&lt;/a>Data
&lt;/h3>&lt;h3 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h3>&lt;p>预训练阶段一共使用了 15T token, 分为了三个阶段：&lt;/p>
&lt;ol>
&lt;li>ViT-training: 单独训练 ViT, 实际用了 image caption, grounding, ocr, video 等数据进行训练，训练方式采用了类似 InternVL 的方式，即通过 cross entropy loss 来与一个清凉话的 LLM 进行对齐，这个阶段训练使用了 1T token, 然后作者使用了一个非常短的 stage 来更新 MLP 用于对齐 ViT 和 Kimi-K2&lt;/li>
&lt;li>Joint pre-training: 训练所有参数，长下文长度为 4K, 使用了 15T token. 这里主要强调了提升代码数据的比例&lt;/li>
&lt;li>Long context mid-training: 使用 &lt;a class="link" href="https://maosong.website/p/notes-on-yarn/" target="_blank" rel="noopener"
>YARN&lt;/a> 来提高模型的上下文长度&lt;/li>
&lt;/ol>
&lt;p>最终预训练阶段 recipe 如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2.5/Kimi-k2.5-pre-training.png"
width="1430"
height="415"
loading="lazy"
alt="Pre-training recipe of Kimi-K2.5"
class="gallery-image"
data-flex-grow="344"
data-flex-basis="826px"
>&lt;/p>
&lt;p>&lt;strong>Native Multimodal pre-training&lt;/strong>
在 Joint pre-training stage, Kimi-K2.5 还采用了一个与 &lt;a class="link" href="InternVL3.md" >InternVL3&lt;/a> 类似的策略，即在预训练一开始直接使用多模态数据进行预训练。&lt;/p>
&lt;p>传统的多模态大模型往往基于一个比较成熟的 LLM backbone 来完成多模态大模型的训练，但是其问题在于成熟的 LLM 其表示空间会收敛到语言模态上，多模态信息的迁移能力比较差。&lt;a class="link" href="InternVL3.md" >InternVL3&lt;/a> 虽然也是 native multimodal pre-training, 但是其仍然依赖于成熟的 LLM. Kimi K2.5 则是使用预训练阶段的 Kimi K2 作为 backbone 来避免表示空间的塌缩，在训练一开始即直接加入少量多模态数据来保持模型的多模态能力。&lt;/p>
&lt;p>作者探究了预训练阶段不同的数据对比，试验结果如下图所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Vision Injection Timing&lt;/th>
&lt;th>Vision-Text Ratio&lt;/th>
&lt;th>Vision Knowledge&lt;/th>
&lt;th>Vision Reasoning&lt;/th>
&lt;th>OCR&lt;/th>
&lt;th>Text Knowledge&lt;/th>
&lt;th>Text Reasoning&lt;/th>
&lt;th>Code&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Early&lt;/td>
&lt;td>0%&lt;/td>
&lt;td>10%:90%&lt;/td>
&lt;td>25.8&lt;/td>
&lt;td>43.8&lt;/td>
&lt;td>65.7&lt;/td>
&lt;td>45.5&lt;/td>
&lt;td>58.5&lt;/td>
&lt;td>24.8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Mid&lt;/td>
&lt;td>50%&lt;/td>
&lt;td>20%:80%&lt;/td>
&lt;td>25.0&lt;/td>
&lt;td>40.7&lt;/td>
&lt;td>64.1&lt;/td>
&lt;td>43.9&lt;/td>
&lt;td>58.6&lt;/td>
&lt;td>24.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Late&lt;/td>
&lt;td>80%&lt;/td>
&lt;td>50%:50%&lt;/td>
&lt;td>24.2&lt;/td>
&lt;td>39.0&lt;/td>
&lt;td>61.5&lt;/td>
&lt;td>43.1&lt;/td>
&lt;td>57.8&lt;/td>
&lt;td>24.0&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>结果显示，在训练早期加入少部分的多模态数据可以有效提高模型的表现。&lt;/p>
&lt;h3 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h3>&lt;p>Post-training 分为了 SFT 和 RL, SFT 阶段作者使用了合成的高质量数据，主要提升模型的交互式推理能力以及工具调用能力。为了解决传统 VLM 工具调用能力比较差且扩展性差的问题，Kimi-k2.5 提出了 Zero-Vision SFT, 其核心思想模型在预训练阶段已经完成了多模态对齐，因此我们可以仅使用纯文本 SFT 数据来激活 VLM 的视觉 agent 能力，具体做法就是将所有图像操作通过 IPython 的代码进行代理操作，这样视觉工具的调用就编程了程序化的图像处理指令。&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2.5/Kimi-k2.5-zero-vision-sft.png"
width="1340"
height="796"
loading="lazy"
alt="Performance of Vision RL on zero-vision SFT"
class="gallery-image"
data-flex-grow="168"
data-flex-basis="404px"
>&lt;/p>
&lt;p>在 RL 阶段，作者基于 &lt;a class="link" href="https://maosong.website/p/notes-on-kimi-k1.5/" target="_blank" rel="noopener"
>Kimi-k1.5&lt;/a> 提出的策略优化算法加入了一个 token-level clipping 机制来减少 off-policy divergence, 目标函数如下所示&lt;/p>
$$
\mathcal{L}(\theta)=\mathbb{E}_{x\sim\mathcal{D}}\left[\frac{1}{N}\sum_{j=1}^k\sum_{i=1}^{|y_i|}\mathrm{clip}\left(\log\frac{\pi_{\theta}(y_j^i\mid x, y_{j}^{0:i})}{\pi_{\mathrm{old}}(y_j^i\mid x, y_{j}^{0:i})},\alpha,\beta\right)(r(x, y_j) - \bar{r}(x)) - \tau\left(\log\frac{\pi_{\theta}(y_j^i\mid x, y_{j}^{0:i})}{\pi_{\mathrm{old}}(y_j^i\mid x, y_{j}^{0:i})}\right)^2\right]
$$&lt;p>其中 $k$ 是针对每个回答 $x$ 的采样次数，$N=\sum_{j=1}^k|y_j|$ 是一个 batch 里总的 token 个数， $\alpha,\beta,\tau$ 为超参数，$\bar{r}(x)$ 是对 normalization 的估计，这里采用了 Kimi-K1.5 的 mean reward, 即 $\bar{r}(x)=1/k\sum_{j=1}^Kr(x,y_j)$. 这里的 clipping 机制与 PPO 不同的地方在于针对 log-ratio 进行 clipping, 而不依赖于 advantage 的计算。最终训练时使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-moonlight/" target="_blank" rel="noopener"
>Moonlight&lt;/a> 的 MuonClip 算法&lt;/p>
&lt;p>对于 reward 的设计，Kimi-k2.5 也使用了基于规则和基于 reward model 的方式，前者针对答案可验证的任务，后者针对开放式的任务。&lt;/p>
&lt;p>作者还构建了 length penalty 来提高模型的推理效率，作者发现 &lt;a class="link" href="https://maosong.website/p/notes-on-kimi-k1.5/" target="_blank" rel="noopener"
>Kimi-k1.5&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/notes-on-kimi-k2/" target="_blank" rel="noopener"
>Kimi-k2&lt;/a> 中的 length penalty 虽然可以生成更准确的 reasoning chain, 但是其很难泛化到更高的算力. 为了解决这个问题，作者提出了 &lt;strong>Toggle&lt;/strong> 策略，即在 inference-time scaling 和 budget-constrained optimization 两种模式之间进行切换优化，对应的 reward 定义为&lt;/p>
$$
\tilde{r}(x,y) = \begin{cases}
r(x,y)*\mathbb{I}\{1/k\sum_{j=1}^kr(x,y_i&lt;\lambda \text{ or }|y_j|\leq \text{budget}(x)\},&amp;\text{if }\lfloor t/m\rfloor\mod 2 == 0\ (\text{Phase }0)\\
r(x,y),&amp;\text{otherwise } (\text{Phase }1)
\end{cases}
$$&lt;p>其中 $\lambda, m$ 都是超参数。budget 基于正确回答的长度的 p 分位得到：&lt;/p>
$$
\text{budget}(x) = \text{Percentile}(\{|y_j| \mid r(x,y_j)=1, j\in[k]\},\rho)
$$&lt;p>两种模式每隔 $m$ 个 iteration 切换一次：&lt;/p>
&lt;ul>
&lt;li>phase 0: budget limited phase, 训练模型在给定 token budget 下解决问题，减少 reasoning chain 长度&lt;/li>
&lt;li>phase 1: scaling phase, 训练模型使用更多的算力来解决更复杂的问题，提高模型的智能程度&lt;/li>
&lt;/ul>
&lt;p>作者评估 Toogle 策略得到的结果如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2.5/Kimi-k2.5-toggle.png"
width="1200"
height="689"
loading="lazy"
alt="Performance of toggle strategy"
class="gallery-image"
data-flex-grow="174"
data-flex-basis="417px"
>&lt;/p>
&lt;p>结果现实，使用 toggle 策略之后，模型的输出长度减少了 30% 左右，且模型的表现并没有明显下降。作者还发现，一些重复的 pattern 也随之降低，且 toggle 策略的泛化程度更高。&lt;/p>
&lt;p>在 Zero-Vision SFT 的基础上，Kimi-k2.5 使用了 Joint multimodal RL 训练策略。现有的多模态 RL 存在的问题为：模型很容易忽略视觉输入而过度依赖于纯文本进行推理。为了解决这个问题，作者构建了需要视觉理解才能得到答案的任务来提高模型对于视觉信息的利用程度，这些任务覆盖三个 domain:&lt;/p>
&lt;ol>
&lt;li>visual grounding and counting: 定位和计数&lt;/li>
&lt;li>chart and document understanding: 图表文档理解&lt;/li>
&lt;li>vision-critical STEM problems: 需要图片来完成求解的数学物理问题&lt;/li>
&lt;/ol>
&lt;p>作者在 visual RL 之后评估了模型的表现，发现模型在 MMLU-Pro, GPQA-Diamond 等任务上的表现都有了提升，作者认为 visual RL 可以在不损害模型纯文本能力的情况下提高模型跨模态的泛化性&lt;/p>
&lt;h3 id="agent-swarm">&lt;a href="#agent-swarm" class="header-anchor">&lt;/a>Agent Swarm
&lt;/h3>&lt;p>Kimi-k2.5 的另一个重大改进为使用并行机制来提高模型的 agent 能力。传统的 agent 往往序列执行 reasoning, tool-use, 这限制了模型处理复杂任务的能力，Kimi-k2.5 通过 Agent Swarm 和 Parallel Agent Reinforcement Learning (PARL) 来解决这个问题，其核心思想就是并行，框架图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2.5/Kimi-k2.5-agent-swarm.png"
width="1354"
height="729"
loading="lazy"
alt="Framework of Agent Swarm"
class="gallery-image"
data-flex-grow="185"
data-flex-basis="445px"
>&lt;/p>
&lt;p>agent swarm 架构包含了一个 orchestrator 和若干个 subagent, 为了解决 agent swarm 的 reward 比较难以设置的问题，PARL 构建了三个不同 level 的 reward&lt;/p>
$$
r_{PARL}(x,y) = \lambda_1 r_{parallel} + \lambda_2r_{finish} + \lambda_3r_{perf}(x,y)
$$&lt;p>其中 $r_{perf}$ 评估了 solution $y$ 的质量， $r_{parallel}$ 则是避免并行模式崩塌，从 multi-agent 崩塌为 single agent, $r_{finish}$ 则是评估模型的完成性。超参数 $\lambda_1,\lambda_2$ 随训练逐渐降为 0 来提高模型整体的表现。&lt;/p>
&lt;p>作者还提出了使用 critical steps 来评估 parallel agent 的计算时间消耗，其计算公式如下&lt;/p>
$$
CriticalSteps = \sum_{i=1}^T\left(S_{main}^{(t)}+\max_iS_{sub,i}^{(t)}\right)
$$&lt;p>其中 $T$ 为一个 episode 的时间，$S_{main}^{(t)}$ 为 orchestrator 在第 $t$ 步的运行时间， $S_{sub,i}^{(t)}$ 为第 i 个 subagent 的运行时间。&lt;/p>
&lt;p>为了提高模型的并行能力，作者构建了一批广度优先搜索和深度优先搜索的数据，通过这些数据的构建，我们可以提高 orchestrator 的并行调用能力。&lt;/p>
&lt;p>最终，PARL 的表现如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2.5/Kimi-k2.5-PARL.png"
width="1501"
height="441"
loading="lazy"
alt="Performance of PARL"
class="gallery-image"
data-flex-grow="340"
data-flex-basis="816px"
>&lt;/p>
&lt;h3 id="infra">&lt;a href="#infra" class="header-anchor">&lt;/a>Infra
&lt;/h3>&lt;p>Kimi-k2.5 的 infra 基于 &lt;a class="link" href="https://maosong.website/p/notes-on-kimi-k2/" target="_blank" rel="noopener"
>Kimi-k2&lt;/a>, 作者主要强调了 decouple encoder process (DEP) 这一改进。之前的工作将 vision encoder 和 text embedding 都做为 PP 的第一个 stage, 但是由于 vision encoder 对不同输入的处理时间不同，这个 stage 的算历和内存分配随输入变化比较大。为了解决这个问题，作者提出了 DEP, 包含三个 stage 来提高训练效率:&lt;/p>
&lt;ol>
&lt;li>balanced vision forward: 由于 vision encoder 比较小 (400M), 因此作者将 vision encoder 复制到所有 GPU 上，然后根据负载来将 visual data 分配到不同的 GPU 上进行处理，这个阶段不保存中间激活值，处理完毕之后所有的结果作为 PP Stage0 的输入&lt;/li>
&lt;li>backbone training: 正常进行训练，与 LLM 的训练优化一致&lt;/li>
&lt;li>vision recomputation &amp;amp; backward: 这个阶段，我们重新计算 vision encoder 的 forward pass, 然后再对 vision encoder 进行反向传播&lt;/li>
&lt;/ol>
&lt;p>通过 DEP, Kimi-k2.5 的训练效率达到了 Kimi-k2 的 90%.&lt;/p>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>首先是 Kimi-k2.5 在 general &amp;amp; reasoning 类任务上的表现，可以看到，Kimi-k2.5 超过了 DeeoSeek-V3.2 的表现，&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Benchmark&lt;/th>
&lt;th>Kimi K2.5&lt;/th>
&lt;th>Claude Opus 4.5&lt;/th>
&lt;th>GPT-5.2 (xhigh)&lt;/th>
&lt;th>Gemini 3 Pro&lt;/th>
&lt;th>DeepSeek-V3.2&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>HLE-Full&lt;/td>
&lt;td>30.1&lt;/td>
&lt;td>30.8&lt;/td>
&lt;td>34.5&lt;/td>
&lt;td>37.5&lt;/td>
&lt;td>25.1†&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>HLE-Full w/ tools&lt;/td>
&lt;td>50.2&lt;/td>
&lt;td>43.2&lt;/td>
&lt;td>45.5&lt;/td>
&lt;td>45.8&lt;/td>
&lt;td>40.8†&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>AIME 2025&lt;/td>
&lt;td>96.1&lt;/td>
&lt;td>92.8&lt;/td>
&lt;td>100&lt;/td>
&lt;td>95.0&lt;/td>
&lt;td>93.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>HMMT 2025 (Feb)&lt;/td>
&lt;td>95.4&lt;/td>
&lt;td>92.9*&lt;/td>
&lt;td>99.4&lt;/td>
&lt;td>97.3*&lt;/td>
&lt;td>92.5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>IMO-AnswerBench&lt;/td>
&lt;td>81.8&lt;/td>
&lt;td>78.5*&lt;/td>
&lt;td>86.3&lt;/td>
&lt;td>83.1*&lt;/td>
&lt;td>78.3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GPQA-Diamond&lt;/td>
&lt;td>87.6&lt;/td>
&lt;td>87.0&lt;/td>
&lt;td>92.4&lt;/td>
&lt;td>91.9&lt;/td>
&lt;td>82.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MMLU-Pro&lt;/td>
&lt;td>87.1&lt;/td>
&lt;td>89.3*&lt;/td>
&lt;td>86.7*&lt;/td>
&lt;td>90.1&lt;/td>
&lt;td>85.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SimpleQA Verified&lt;/td>
&lt;td>36.9&lt;/td>
&lt;td>44.1&lt;/td>
&lt;td>38.9&lt;/td>
&lt;td>72.1&lt;/td>
&lt;td>27.5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>AdvancedIF&lt;/td>
&lt;td>75.6&lt;/td>
&lt;td>63.1&lt;/td>
&lt;td>81.1&lt;/td>
&lt;td>74.7&lt;/td>
&lt;td>58.8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LongBench v2&lt;/td>
&lt;td>61.0&lt;/td>
&lt;td>64.4*&lt;/td>
&lt;td>54.5*&lt;/td>
&lt;td>68.2*&lt;/td>
&lt;td>59.8*&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>接下来是模型在 coding 任务上的表现&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Benchmark&lt;/th>
&lt;th>Kimi K2.5&lt;/th>
&lt;th>Claude Opus 4.5&lt;/th>
&lt;th>GPT-5.2 (xhigh)&lt;/th>
&lt;th>Gemini 3 Pro&lt;/th>
&lt;th>DeepSeek-V3.2&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>SWE-Bench Verified&lt;/td>
&lt;td>76.8&lt;/td>
&lt;td>80.9&lt;/td>
&lt;td>80.0&lt;/td>
&lt;td>76.2&lt;/td>
&lt;td>73.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SWE-Bench Pro (public)&lt;/td>
&lt;td>50.7&lt;/td>
&lt;td>55.4*&lt;/td>
&lt;td>55.6&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SWE-Bench Multilingual&lt;/td>
&lt;td>73.0&lt;/td>
&lt;td>77.5&lt;/td>
&lt;td>72.0&lt;/td>
&lt;td>65.0&lt;/td>
&lt;td>70.2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Terminal Bench 2.0&lt;/td>
&lt;td>50.8&lt;/td>
&lt;td>59.3&lt;/td>
&lt;td>54.0&lt;/td>
&lt;td>54.2&lt;/td>
&lt;td>46.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>PaperBench (CodeDev)&lt;/td>
&lt;td>63.5&lt;/td>
&lt;td>72.9*&lt;/td>
&lt;td>63.7*&lt;/td>
&lt;td>-&lt;/td>
&lt;td>47.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CyberGym&lt;/td>
&lt;td>41.3&lt;/td>
&lt;td>50.6&lt;/td>
&lt;td>-&lt;/td>
&lt;td>39.9*&lt;/td>
&lt;td>17.3*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SciCode&lt;/td>
&lt;td>48.7&lt;/td>
&lt;td>49.5&lt;/td>
&lt;td>52.1&lt;/td>
&lt;td>56.1&lt;/td>
&lt;td>38.9&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OIBench (cpp)&lt;/td>
&lt;td>57.4&lt;/td>
&lt;td>54.6*&lt;/td>
&lt;td>-&lt;/td>
&lt;td>68.5*&lt;/td>
&lt;td>54.7*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LiveCodeBench (v6)&lt;/td>
&lt;td>85.0&lt;/td>
&lt;td>82.2*&lt;/td>
&lt;td>-&lt;/td>
&lt;td>87.4*&lt;/td>
&lt;td>83.3&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>在 agent 任务上的表现&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Benchmark&lt;/th>
&lt;th>Kimi K2.5&lt;/th>
&lt;th>Claude Opus 4.5&lt;/th>
&lt;th>GPT-5.2 (xhigh)&lt;/th>
&lt;th>Gemini 3 Pro&lt;/th>
&lt;th>DeepSeek-V3.2&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>BrowseComp&lt;/td>
&lt;td>60.6&lt;/td>
&lt;td>37.0&lt;/td>
&lt;td>65.8&lt;/td>
&lt;td>37.8&lt;/td>
&lt;td>51.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>BrowseComp (w/ ctx manage)&lt;/td>
&lt;td>74.9&lt;/td>
&lt;td>57.8&lt;/td>
&lt;td>-&lt;/td>
&lt;td>59.2&lt;/td>
&lt;td>67.6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>BrowseComp (Agent Swarm)&lt;/td>
&lt;td>78.4&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>WideSearch&lt;/td>
&lt;td>72.7&lt;/td>
&lt;td>76.2*&lt;/td>
&lt;td>-&lt;/td>
&lt;td>57.0&lt;/td>
&lt;td>32.5*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>WideSearch (Agent Swarm)&lt;/td>
&lt;td>79.0&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DeepSearchQA&lt;/td>
&lt;td>77.1&lt;/td>
&lt;td>76.1*&lt;/td>
&lt;td>71.3*&lt;/td>
&lt;td>63.2*&lt;/td>
&lt;td>60.9*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FinSearchCompT2&amp;amp;T3&lt;/td>
&lt;td>67.8&lt;/td>
&lt;td>66.2*&lt;/td>
&lt;td>-&lt;/td>
&lt;td>49.9&lt;/td>
&lt;td>59.1*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Seal-0&lt;/td>
&lt;td>57.4&lt;/td>
&lt;td>47.7*&lt;/td>
&lt;td>45.0&lt;/td>
&lt;td>45.5*&lt;/td>
&lt;td>49.5*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GDPVal-AA&lt;/td>
&lt;td>41.0&lt;/td>
&lt;td>45.0&lt;/td>
&lt;td>48.0&lt;/td>
&lt;td>35.0&lt;/td>
&lt;td>34.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OSWorld-Verified&lt;/td>
&lt;td>63.3&lt;/td>
&lt;td>66.3&lt;/td>
&lt;td>8.6&lt;/td>
&lt;td>20.7&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>WebArena&lt;/td>
&lt;td>58.9&lt;/td>
&lt;td>63.4&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>多模态表现&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Benchmark&lt;/th>
&lt;th>Kimi K2.5&lt;/th>
&lt;th>Claude Opus 4.5&lt;/th>
&lt;th>GPT-5.2 (xhigh)&lt;/th>
&lt;th>Gemini 3 Pro&lt;/th>
&lt;th>Qwen3-VL-235B-A22B&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;strong>Image&lt;/strong>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MMMU-Pro&lt;/td>
&lt;td>78.5&lt;/td>
&lt;td>74.0&lt;/td>
&lt;td>79.5*&lt;/td>
&lt;td>81.0&lt;/td>
&lt;td>69.3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MMMU (val)&lt;/td>
&lt;td>84.3&lt;/td>
&lt;td>80.7&lt;/td>
&lt;td>86.7*&lt;/td>
&lt;td>87.5*&lt;/td>
&lt;td>80.6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CharXiv (RQ)&lt;/td>
&lt;td>77.5&lt;/td>
&lt;td>67.2*&lt;/td>
&lt;td>82.1&lt;/td>
&lt;td>81.4&lt;/td>
&lt;td>66.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MathVision&lt;/td>
&lt;td>84.2&lt;/td>
&lt;td>77.1*&lt;/td>
&lt;td>83.0&lt;/td>
&lt;td>86.1*&lt;/td>
&lt;td>74.6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MathVista (mini)&lt;/td>
&lt;td>90.1&lt;/td>
&lt;td>80.2*&lt;/td>
&lt;td>82.8*&lt;/td>
&lt;td>89.8*&lt;/td>
&lt;td>85.8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SimpleVQA&lt;/td>
&lt;td>71.2&lt;/td>
&lt;td>69.7*&lt;/td>
&lt;td>55.8*&lt;/td>
&lt;td>69.7*&lt;/td>
&lt;td>56.8*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>WorldVQA&lt;/td>
&lt;td>46.3&lt;/td>
&lt;td>36.8&lt;/td>
&lt;td>28.0&lt;/td>
&lt;td>47.4&lt;/td>
&lt;td>23.5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ZeroBench&lt;/td>
&lt;td>9&lt;/td>
&lt;td>3*&lt;/td>
&lt;td>9*&lt;/td>
&lt;td>8*&lt;/td>
&lt;td>4*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ZeroBench w/ tools&lt;/td>
&lt;td>11&lt;/td>
&lt;td>9*&lt;/td>
&lt;td>7*&lt;/td>
&lt;td>12*&lt;/td>
&lt;td>3*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>BabyVision&lt;/td>
&lt;td>36.5&lt;/td>
&lt;td>14.2&lt;/td>
&lt;td>34.4&lt;/td>
&lt;td>49.7&lt;/td>
&lt;td>22.2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>BLINK&lt;/td>
&lt;td>78.9&lt;/td>
&lt;td>68.8*&lt;/td>
&lt;td>-&lt;/td>
&lt;td>78.7*&lt;/td>
&lt;td>68.9&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MMVP&lt;/td>
&lt;td>87.0&lt;/td>
&lt;td>80.0*&lt;/td>
&lt;td>83.0*&lt;/td>
&lt;td>90.0*&lt;/td>
&lt;td>84.3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OmniDocBench 1.5&lt;/td>
&lt;td>88.8&lt;/td>
&lt;td>87.7*&lt;/td>
&lt;td>85.7&lt;/td>
&lt;td>88.5&lt;/td>
&lt;td>82.0*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OCRBench&lt;/td>
&lt;td>92.3&lt;/td>
&lt;td>86.5*&lt;/td>
&lt;td>80.7*&lt;/td>
&lt;td>90.3*&lt;/td>
&lt;td>87.5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>InfoVQA (test)&lt;/td>
&lt;td>92.6&lt;/td>
&lt;td>76.9*&lt;/td>
&lt;td>84*&lt;/td>
&lt;td>57.2*&lt;/td>
&lt;td>89.5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>Video&lt;/strong>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>VideoMMMU&lt;/td>
&lt;td>86.6&lt;/td>
&lt;td>84.4*&lt;/td>
&lt;td>85.9&lt;/td>
&lt;td>87.6&lt;/td>
&lt;td>80.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MMVU&lt;/td>
&lt;td>80.4&lt;/td>
&lt;td>77.3*&lt;/td>
&lt;td>80.8*&lt;/td>
&lt;td>77.5*&lt;/td>
&lt;td>71.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MotionBench&lt;/td>
&lt;td>70.4&lt;/td>
&lt;td>60.3&lt;/td>
&lt;td>64.8&lt;/td>
&lt;td>70.3&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Video-MME&lt;/td>
&lt;td>87.4&lt;/td>
&lt;td>66.0*&lt;/td>
&lt;td>86.0*&lt;/td>
&lt;td>88.4*&lt;/td>
&lt;td>79.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LongVideoBench&lt;/td>
&lt;td>79.8&lt;/td>
&lt;td>67.2*&lt;/td>
&lt;td>76.5*&lt;/td>
&lt;td>77.7*&lt;/td>
&lt;td>65.6*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LVBench&lt;/td>
&lt;td>75.9&lt;/td>
&lt;td>57.3&lt;/td>
&lt;td>-&lt;/td>
&lt;td>73.5*&lt;/td>
&lt;td>63.6&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>我们这里基于模型在不同类别任务上的排名来进行可视化，结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2.5/Kimi-k2.5-rank-frequency.png"
width="1590"
height="790"
loading="lazy"
alt="Rank performance of difference models"
class="gallery-image"
data-flex-grow="201"
data-flex-basis="483px"
>&lt;/p>
&lt;p>从结果可以看出，Kimi-K2.5 的 agent 能力达到了 SOTA 级别，其多模态能力也比较强。&lt;/p>
&lt;p>与 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v3.2/" target="_blank" rel="noopener"
>DeepSeek-V3.2&lt;/a> 一样，作者也对比了不同模型的推理效率，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2.5/Kimi-k2.5-reasoning-efficiency.png"
width="783"
height="262"
loading="lazy"
alt="Reasoning efficiency of Kimi K2.5"
class="gallery-image"
data-flex-grow="298"
data-flex-basis="717px"
>&lt;/p>
&lt;p>可以看到，相比与 Kimi-K2, Kimi-K2.5 通过在 RL 层面进行优化，降低了输出长度，但是相比与 DeepSeel-V3.2 和 Gemini3.0 Pro 之间还存在一定差距。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>在本文中，作者提出了 Kimi-k2.5， 一个多模态的 agent model, Kimi-k2.5 集成了 Kimi-k2 和 Kimi-VL 的能力，扩展了模型的 agent 能力。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://huggingface.co/moonshotai/Kimi-K2.5" target="_blank" rel="noopener"
>huggingface&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2602.02276" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on KL divergence</title><link>https://maosong.website/p/notes-on-kl-divergence/</link><pubDate>Sat, 24 Jan 2026 16:32:14 +0800</pubDate><guid>https://maosong.website/p/notes-on-kl-divergence/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>在本节中，我们先介绍 KL divergence 的基本定义，然后我们介绍 KL divergence 的一般形式，即 f-divergence.&lt;/p>
&lt;h3 id="kl-divergence">&lt;a href="#kl-divergence" class="header-anchor">&lt;/a>KL-divergence
&lt;/h3>&lt;p>KL divergence 用于衡量近似概率分布 $Q(x)$ 到真实概率分布 $P(x)$ 的误差，我们可以将其理解为：如果我们用 $Q(x)$ 来替换 $P(x)$, 会有多大的信息损失？&lt;/p>
&lt;p>连续概率分布的 KL divergence 的定义如下&lt;/p>
$$
D_{KL}(P\parallel Q) =\mathbb{E}_{x\sim P}\left[\log \frac{P(x)}{Q(x)}\right]=\int P(x)\log\left(\frac{P(x)}{Q(x)}\right)dx
$$&lt;p>离散概率分布的 KL divergence 定义如下&lt;/p>
$$
D_{KL}(P\parallel Q) = \sum_{x} P(x)\log\left(\frac{P(x)}{Q(x)}\right)
$$&lt;p>KL divergence 有两几个关键性质：&lt;/p>
&lt;ol>
&lt;li>非负性：$D_{KL}(P\parallel Q)\geq0$, 且 $D_{KL}(P\parallel Q)=0$ 当且仅当 $P(x)=Q(x)$ 对任意 $x$ 成立&lt;/li>
&lt;li>非对称性： 一般情况下，$D_{KL}(P\parallel Q)\neq D_{KL}(Q\parallel P)$.&lt;/li>
&lt;li>有限性：如果存在 $x$ 使得 $P(x)>0$ 但是 $Q(x)=0$, 则 $D_{\mathrm{KL}}(P\parallel Q)=\infty$.&lt;/li>
&lt;/ol>
&lt;p>一般我们称 $D_{KL}(P\parallel Q)$ 为 &lt;strong>forward KL&lt;/strong> (相对于 $Q$), 对应的还有 &lt;strong>reverse KL&lt;/strong> $D_{KL}(Q\parallel P)$ (相对于 $Q$).&lt;/p>
&lt;h3 id="f-divergence">&lt;a href="#f-divergence" class="header-anchor">&lt;/a>F-divergence
&lt;/h3>&lt;p>KL divergence 是 f-divergence 的一种特殊情况。 f-divergence 是一类衡量不同概率分布 $P$ 和 $Q$ 的函数 $D_f(P\parallel Q)$.&lt;/p>
&lt;p>假设函数 $f:(0,\infty)\to\mathbb{R}$ 是一个凸函数，且 $f(1)=0$. $P$ 和 $Q$ 是两个概率分布，则 f-divergence 定义如下&lt;/p>
$$
D_f(P\parallel Q) = \mathbb{E}_{x\sim Q}\left[ f\left(\frac{P(x)}{Q(x)}\right)\right]=\int Q(x)f\left(\frac{P(x)}{Q(x)}\right)dx
$$&lt;p>我们称 $f$ 为 $D_f$ 的 &lt;strong>generator&lt;/strong>.&lt;/p>
&lt;p>以下是几种常见的 f-divergence:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Name&lt;/th>
&lt;th>generator&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>forward KL divergence&lt;/td>
&lt;td>$f(x)=x\log x$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>reverse KL divergence&lt;/td>
&lt;td>$f(x)=-\log x$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Total variation&lt;/td>
&lt;td>$f(x)=1/2\vert x-1\vert$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\chi^2$-divergence&lt;/td>
&lt;td>$f(x)=(x-1)^2$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>JS-divergence&lt;/td>
&lt;td>$f(x)=x\log\frac{2x}{x+1}+\log\frac{2}{x+1}$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>我们这里推导一下 KL divergence 对应的 generator.&lt;/p>
&lt;p>对于 forward KL, 注意到&lt;/p>
$$
D_f(P \parallel Q) = \int Q(x) \left( \frac{P(x)}{Q(x)} \log \frac{P(x)}{Q(x)} \right) dx = \int P(x) \log \frac{P(x)}{Q(x)} dx = D_{KL}(P \parallel Q)
$$&lt;p>因此 forward KL 对应的 generator 为 $f=x\log x$.&lt;/p>
&lt;p>对于 reverse KL, 注意到&lt;/p>
$$
D_f(P \parallel Q) = \int Q(x) \left( -\log \frac{P(x)}{Q(x)} \right) dx = \int Q(x) \log \frac{Q(x)}{P(x)} dx = D_{KL}(Q \parallel P)
$$&lt;p>因此 forward KL 对应的 generator 为 $f=-\log x$.&lt;/p>
&lt;h4 id="properties-of-f-divergence">&lt;a href="#properties-of-f-divergence" class="header-anchor">&lt;/a>Properties of F-divergence
&lt;/h4>&lt;p>f-divergence 性质如下&lt;/p>
&lt;ol>
&lt;li>linearity: $D_{a_1f_1+a_2f_2}=a_1D_{f_1}+a_2D_{f_2}$.&lt;/li>
&lt;li>$D_f=D_g$ 当且仅当存在 $c\in\mathbb{R}$ 使得 $f(x)=g(x)+c(x-1)$.&lt;/li>
&lt;li>non-negativity. $D_f(P\parallel Q)\geq0$ 且 $D_f(P\parallel Q)$ 当且仅当 $P=Q$.&lt;/li>
&lt;/ol>
&lt;p>性质 2 证明如下：&lt;/p>
&lt;p>如果 $f(x)=g(x)+c(x-1)$, 则通过定义，我们可以验证得到 $D_f=D_g$.&lt;/p>
&lt;p>反之，如果 $D_f=D_g$, 令 $h=f-g$, 对任意两个在集合 $\{0, 1\}$ 上的概率分布 $P,Q$, 由于 $D_f(P\parallel Q) - D_g(P\parallel Q)=0$, 我们有&lt;/p>
$$
h\left(\frac{P(1)}{Q(1)}\right) = -\frac{Q(0)}{Q(1)}h\left(\frac{P(0)}{Q(0)}\right)
$$&lt;p>我们不妨假设 $P(0)=aQ(0)$, $P(1)=bQ(1)$, 结合 $P(0)+P(1)=1$ 和 $Q(0)+Q(1)=1$ 我们有&lt;/p>
$$
Q(0) = \frac{1-a}{b-a}, Q(1) = \frac{b-1}{b-a}
$$&lt;p>从而&lt;/p>
$$
\frac{h(b)}{b-1}=\frac{h(a)}{a-1}
$$&lt;p>由于我们可以任意选定 $P$ 和 $Q$, 因此 $h$ 是一个线性函数，形式为 $h(x)=c(x-1)$. $\blacksquare$&lt;/p>
&lt;h2 id="approximation">&lt;a href="#approximation" class="header-anchor">&lt;/a>Approximation
&lt;/h2>&lt;p>本节中，我们将介绍针对 KL divergence 的三种近似形式。&lt;/p>
&lt;p>在实际计算 KL divergence 时，由于：&lt;/p>
&lt;ol>
&lt;li>完整计算 KL divergence 需要的算力或内存过高&lt;/li>
&lt;li>没有闭式解&lt;/li>
&lt;li>我们可以仅保存 log-probability, 而不是整个概率分布&lt;/li>
&lt;/ol>
&lt;p>因此，我们假设我们只能计算输入 $x$ 对应的概率 $P(x)$ 和 $Q(x)$. 一般来说，我们会通过 Monte Carlo estimate 来进行近似。即我们先对 $P$ 进行采样得到 $x_1,\dots,x_N\sim P$, 然后我们构建估计量。&lt;/p>
&lt;p>一个高的估计量应该是无偏 (unbiased) 并且方差低 (low variance) 的。John Schulman 给出了三种 estimator. 我们分别针对 forward KL 和 reverse KL 进行介绍。这里我们定义&lt;/p>
$$
r = \frac{P(x)}{Q(x)}
$$&lt;h3 id="forward-kl-estimation">&lt;a href="#forward-kl-estimation" class="header-anchor">&lt;/a>Forward KL Estimation
&lt;/h3>&lt;p>对于 forward KL $D_{KL}(P\parallel Q)$, 其对应的 generator 为 $f(x)=x\log x$, 注意到 $\mathbb{E}_{x\sim Q}[r]=1$, 且 $f$ 是一个凸函数，因此我们有 $f(r)-f'(1)(r-1)\geq0$, 从而我们可以得到一个新的估计为 $\boxed{k=r\log r - (r-1)}$.&lt;/p>
&lt;h3 id="reverse-kl-estimation">&lt;a href="#reverse-kl-estimation" class="header-anchor">&lt;/a>Reverse KL Estimation
&lt;/h3>&lt;p>对于 reverse KL $D_{KL}(Q\parallel P)$, 其对应的 generator 为 $f(x)=-\log x$, 由概率性质，$\boxed{k_1=-\log r}$ 是 $D_{KL}(Q\parallel P)$ 的一个无偏估计。但是 $k_1$ 的问题在于 当 $r$ 非常小时，$k_1$ 会变得非常大。也就是说，$k_1$ 的 variance 比较高。&lt;/p>
&lt;p>John Schulman 基于 f-divergence 泰勒展开给出了一个新的估计 $k_2$, 其定义为&lt;/p>
$$
\boxed{k_2 = \frac12(\log r)^2}
$$&lt;p>其期望为&lt;/p>
$$
\mathbb{E}_Q[k_2] = \mathbb{E}_Q\left[\frac12(\log r)^2\right]
$$&lt;p>这是一个 f-divergence, 对应的 generator 为 $f_{k_2}(x)=1/2(\log x)^2$, 而 $D_{KL}(Q\parallel P)$ 对应的 generator 为 $f_{k_1}(x)=-\log x$.&lt;/p>
&lt;p>当 $P$ 和 $Q$ 比较靠近时，我们记 $\theta=r-1$， 对 $D_{f}(P\parallel Q)$ 在 $x=1$ 处进行展开得到&lt;/p>
$$
\begin{aligned}
D_f(P\parallel Q) &amp;= \mathbb{E}_{x\sim Q}\left[ f(r)\right]\\
&amp;= \mathbb{E}_{x\sim Q}\left[ f(1) + f'(1)\theta + \frac{f''(1)}{2}f(1+\lambda)\theta^2+O(\theta^3)\right]\\
&amp;= \frac{f''(1)}{2}F\theta^2+O(\theta^3)
\end{aligned}
$$&lt;p>这里我们应用了 $f(1)=0$, $\mathbb{E}[\theta]=0$, $F=\mathbb{E}[f(1+\lambda\theta)$ 是 Fisher information matrix.&lt;/p>
&lt;p>我们分别带入 $f_{k_1}(x)$ 和 $f_{k_2}(x)$ 得到 $f_{k_1}''(1)=f_{k_2}''(1)=1$, 即 $k_1$ 和 $k_2$ 在 $P$ 和 $Q$ 比较靠近时二阶近似是相同的。因此，**$k_2$ 表面上是一个二阶近似，在分布接近时有效，但本质上是在优化 &lt;em>另一个 f-divergence&lt;/em>*&lt;/p>
&lt;p>John Schulman 还构造了第三种估计。回顾前面 f-divergence 的性质 2，即当 $f(x)=g(x)+c(x-1)$ 时，我们有 $D_f=D_g$, 因此我们可以选取合适的 $c$ 来降低估计的 variance. 注意到 $k_1$ 的主要问题在于存在负数的可能性，因此我们就构建一个对应的估计量来解决这个问题。注意到 $\log x \leq x -1$, 因此我们可以令 $c=1$, 此时就得到了新的估计&lt;/p>
$$
\boxed{k_3 =(r-1)- \log r }
$$&lt;p>$k_3$ 继承了 $k_1$ 的无偏性，并且 $k_3$ 通过 f-divergence 等价类消除了负值，兼顾无偏与低方差，解决了 $k_1$ variance 过大的问题&lt;/p>
&lt;h3 id="experiments-on-approximation">&lt;a href="#experiments-on-approximation" class="header-anchor">&lt;/a>Experiments on Approximation
&lt;/h3>&lt;p>对于分布 $P=\mathcal{N}(0,1)$ 以及 $Q=\mathcal{N}(0.1, 1)$, 真实的 KV divergence 为 0.005, 三个 estimator 的误差如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Method&lt;/th>
&lt;th>Bias&lt;/th>
&lt;th>Std Dev&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$k_1$&lt;/td>
&lt;td>0.0001&lt;/td>
&lt;td>20.0005&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$k_2$&lt;/td>
&lt;td>0.0025&lt;/td>
&lt;td>1.4175&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$k_3$&lt;/td>
&lt;td>0.0000&lt;/td>
&lt;td>1.4163&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>当 $P=\mathcal{N}(1,1)$, $Q=\mathcal{N}(0.1, 1)$ 时， 真实的 KV divergence 为 0.405, 三个 estimator 的误差如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Method&lt;/th>
&lt;th>Bias&lt;/th>
&lt;th>Std Dev&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$k_1$&lt;/td>
&lt;td>-0.0000&lt;/td>
&lt;td>2.2223&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$k_2$&lt;/td>
&lt;td>0.2025&lt;/td>
&lt;td>1.6762&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$k_3$&lt;/td>
&lt;td>0.0000&lt;/td>
&lt;td>1.6342&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到 $k_1$ 的 variance 非常大，$k_2$ 是一个有偏估计，$k_3$ 既满足了无偏又满足了 low variance.&lt;/p>
&lt;h3 id="summary">&lt;a href="#summary" class="header-anchor">&lt;/a>Summary
&lt;/h3>&lt;p>我们接下来总结 reverse KL $D_{KL}(Q\parallel P)$ 的近似 $k_1$, $k_2$ 和 $k_3$ 的性质如下 ($r=P(x)/Q(x)$)&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>estimation&lt;/th>
&lt;th>definition&lt;/th>
&lt;th>motivation&lt;/th>
&lt;th>bias&lt;/th>
&lt;th>variance&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$k_1$&lt;/td>
&lt;td>$-\log r$&lt;/td>
&lt;td>naive estimation&lt;/td>
&lt;td>unbiased&lt;/td>
&lt;td>high&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$k_2$&lt;/td>
&lt;td>$\frac12(\log r)^2$&lt;/td>
&lt;td>f-divergence, taylor expansion&lt;/td>
&lt;td>biased&lt;/td>
&lt;td>low&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$k_3$&lt;/td>
&lt;td>$(r-1)- \log r$&lt;/td>
&lt;td>f-divergence, non-negativity&lt;/td>
&lt;td>unbiased&lt;/td>
&lt;td>low&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="applications-to-ml">&lt;a href="#applications-to-ml" class="header-anchor">&lt;/a>Applications to ML
&lt;/h2>&lt;blockquote>
&lt;p>Remark
本节内容主要参考了 &lt;a class="link" href="https://dibyaghosh.com/blog/probability/kldivergence/" target="_blank" rel="noopener"
>KL Divergence for Machine Learning&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>我们假设真实目标分布和近似的目标分布分别记为 $p_{data}(x)$ 和 $p_\theta(x)$. 由于 KL divergence 的非对称性，因此我们需要考虑两种目标函数：&lt;/p>
&lt;ol>
&lt;li>forward KL: $\arg\min_\theta D_{KL}(p_{data}\parallel p_\theta)$&lt;/li>
&lt;li>reverse KL: $\arg\min_\theta D_{KL}(p_\theta \parallel p_{data})$&lt;/li>
&lt;/ol>
&lt;p>我们将会看到，这两种不同的目标函数导致的结果也不尽相同&lt;/p>
&lt;h3 id="forward-kl">&lt;a href="#forward-kl" class="header-anchor">&lt;/a>Forward KL
&lt;/h3>&lt;p>对目标函数进行简化得到&lt;/p>
$$
\arg\min_\theta D_{KL}(p_{data}\parallel p_\theta) = \arg\max_\theta \mathbb{E}_{x\sim p_{data}}\left[\log p_\theta(x)\right]
$$&lt;p>实际在计算时，我们会使用 Monte Carlo 的方式对真实分布进行采样然后进行估计。&lt;/p>
&lt;p>Forward KL 其代表的含义为，我们从分布 $p_{data}$ 中进行采样，然后求 $p_\theta$ 的最大似然估计。最终的结果满足：&lt;strong>当 $p_{data}(x)$ 概率很高时，$p_\theta(x)$ 的概率也需要很高&lt;/strong>. 这是一种 &lt;strong>mean-seeking&lt;/strong> behavior, 因为 $p_\theta$ 必须覆盖 $p_{data}$ 的所有 modes.&lt;/p>
&lt;p>一般来说，supervised learning 对应的就是 forward KL. 我们可以证明 forward KL divergence 和 MLE 是等价的。也就是说，最大似然估计得到的分布就是 KL divergence 最小的近似分布。我们将 $p_{data}(x)$ 和 $p_\theta(x)$ 对应的 KL divergence 进行展开得到&lt;/p>
$$
\begin{aligned}
\theta_{KL}^* &amp;= \arg\min_{\theta}D_{KL}(p_{data}(x)\parallel p_\theta(x))\\
&amp;= \arg\min_{\theta} \int p_{data}(x)\frac{p_{data}(x)}{p_\theta(x)} dx\\
&amp;= \arg\min_{\theta}\int p_{data}(x)\log p_{data}(x) dx - \int p_{data}(x)\log p_\theta(x)dx \\
&amp;= \arg\min_{\theta} - \int p_{data}(x)\log p_\theta(x)dx \\
&amp;= \arg\max_{\theta} \int p_{data}(x)\log p_\theta(x)dx
\end{aligned}
$$&lt;p>实际上，真实的数据分布 $p_{data}(x)$ 是未知的，我们只有从 $p_{data}(x)$ 采样得到的一批数据 $X=\{x_1,\dots,x_n\}\sim p_{data}(x)$. 基于大数定律，我们有&lt;/p>
$$
\frac{1}{n}\sum_{i=1}^n\log p(\theta_i\mid \theta)=\mathbb{E}_{x\sim p_{data}}[\log p_\theta(x)] = \int p_{data}(x)\log p_\theta(x)dx, n\to \infty
$$&lt;p>这样，最大似然估计就与最小化 KL divergence 构建起了联系：&lt;/p>
$$
\begin{aligned}
\theta_{MLE}^*&amp;=\arg\max_{\theta} \sum_{i=1}^n \log p(x_i\mid \theta)\\
&amp;= \arg\max_{\theta} \int p_{data}(x)\log p_\theta(x)dx\\
&amp;= \theta_{KL}^*, n\to\infty.
\end{aligned}
$$&lt;p>也就是说，当采样样本足够多的时候，最大似然估计和最小 KL divergence 是等价的。监督学习中，我们先从真实分布 $p_{data}(x,y)$ 中收集一个数据集 $\mathcal{D}=\{(x_i,y_i)\}$, 然后我们会基于模型 $f_\theta:\mathcal{X}\to\mathcal{Y}$ 和损失函数 $\mathcal{L}:\mathcal{Y}\times\mathcal{Y}\to\mathbb{R}$ 来优化模型参数 $\theta$:&lt;/p>
$$
\arg\min_\theta \mathbb{E}_{(x_i,y_i)\sim\mathcal{D}}[\mathcal{L}(f_\theta(x_i), y_i)]
$$&lt;p>对于使用 cross-entropy loss 的分类问题以及 MSE loss 的回归问题，其目标函数实际上都是最小化 KL divergence.&lt;/p>
&lt;h3 id="reverse-kl">&lt;a href="#reverse-kl" class="header-anchor">&lt;/a>Reverse KL
&lt;/h3>&lt;p>对目标函数进行简化，得到&lt;/p>
$$
\arg\min_\theta D_{KL}(Q_\theta\parallel p_{data}) = \arg\max_\theta \mathbb{E}_{x\sim Q_\theta}\left[\log p_{data}(x)\right] - \mathbb{E}_{x\sim Q_\theta}\left[\log Q_\theta(x)\right]
$$&lt;p>实际在计算时，我们需要知道真实概率分布在采样点上的概率值 $p_{data}(x)$.&lt;/p>
&lt;p>Reverse KL 代表的含义为，我们从分布 $p_\theta(x)$ 中进行采样，然后最大化采样点在 $p_{data}(x)$ 中的概率分布。entropy item 鼓励 $p_\theta$ 尽可能均匀分布（覆盖广），从而最终结果满足：&lt;strong>当 $p_\theta(x)$ 概率很高时，$p_{data}(x)$ 的概率也需要很高&lt;/strong>。注意到与 forward KL 不同，Reverse KL 中包含 entropy 项，其避免了 $p_\theta$ 收缩到 $p_{data}$ 的某一个 非常窄的 mode 上，最终结果是 $p_\theta$ 会找到 $p_{data}$ 的一个 &lt;strong>high probability&lt;/strong> 以及 &lt;strong>wide support&lt;/strong> 的 mode, 然后进行覆盖。&lt;/p>
&lt;p>一般来说，reinforcement learning 对应的就是 reverse KL, 这是因为我们希望 policy model 不要离 reference model 太远，并不一定要 cover 所有的 mode.&lt;/p>
&lt;h3 id="experiments-on-forward-and-reverse-kl">&lt;a href="#experiments-on-forward-and-reverse-kl" class="header-anchor">&lt;/a>Experiments on forward and Reverse KL
&lt;/h3>&lt;p>我们通过概率分布来可视化 forward RL 与 reverse RL 的区别，验证 forward KL 与 reverse KL 不同的模式。&lt;/p>
&lt;p>我们假设 $p_{data}=w_1\mathcal{N}(\mu_1, \sigma_1^2)+w_2\mathcal{N}(\mu_2, \sigma_2^2)$, 然后我们用一个 normal distribution $p_\theta=\mathcal{N}(\mu, \sigma^2)$ 来近似 $p_{data}$, 这里 $\theta=(\mu, \sigma^2)$. 对于 forward KL, 我们可以从理论上得出最优解，对应的 $\mu=w_1\mu_1+w_2\mu_2$, 而 reverse KL 则只能通过优化的方式进行求解，并且解与初始化条件相关，下面是相关的实验结果&lt;/p>
&lt;p>首先我们令 $w_1=w_2=0.5$, $\mu_1=\mu_2=4.0$, $\sigma_1=\sigma_2=1$, reverse KL 的初始化条件为 $\theta_0=(2,1)$, 对应的结果为&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kl-divergence/KL-divergence-reverse-kl-vis1.png"
width="1010"
height="549"
loading="lazy"
alt="visualization of forward KL v.s. reverse KL (1)"
class="gallery-image"
data-flex-grow="183"
data-flex-basis="441px"
>&lt;/p>
&lt;p>接下来我们改变 reverse KL 的初始化条件为 $\theta_0=(-2,1)$, 对应的结果为&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kl-divergence/KL-divergence-reverse-kl-vis2.png"
width="1010"
height="549"
loading="lazy"
alt="visualization of forward KL v.s. reverse KL (2)"
class="gallery-image"
data-flex-grow="183"
data-flex-basis="441px"
>&lt;/p>
&lt;p>可以看到，与前面分析一致，使用 forward KL 时，最终得到的 $p_\theta$ 会倾向于拟合分布的中心 (mean seeking), 即 $\mu(p_\theta)=\mu(p_{data})$, 而使用 reverse KL 时，最终得到的 $P$ 会倾向于拟合分布的 mode (mode seeking).&lt;/p>
&lt;h2 id="applications-to-rl">&lt;a href="#applications-to-rl" class="header-anchor">&lt;/a>Applications to RL
&lt;/h2>&lt;blockquote>
&lt;p>Remark
本节内容主要参考了 &lt;a class="link" href="https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-en.html" target="_blank" rel="noopener"
>Understanding KL Divergence Estimators in RL: From Value Approximation to Gradient Estimation&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>在本节中，我们将基于 RL 来推导 KL 的相关性质。为了统一，这里我们使用 RL 中常见的 notation 来进行计算&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>notation&lt;/th>
&lt;th>description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$\pi_\theta$&lt;/td>
&lt;td>policy model with parameter $\theta$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\pi_{ref}$&lt;/td>
&lt;td>reference model&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\pi_{old}$&lt;/td>
&lt;td>behavior model to sample from&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$s_\theta(x)=\nabla_\theta \log \pi_\theta(x)$&lt;/td>
&lt;td>score function&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\rho(x)=\pi_\theta(x)/\pi_{old}(x)$&lt;/td>
&lt;td>importance weight&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\mathrm{sg}(\cdot)$&lt;/td>
&lt;td>stop gradient operation&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>首先 score function 有一个期望为 0 的性质：&lt;/p>
$$
\mathbb{E}_{x\sim\pi_\theta}[s_\theta(x)]=\int_x \pi_\theta(x)\nabla_\theta \log \pi_\theta(x)dx = \int_x\nabla_\theta \pi_\theta(x)dx= \nabla_\theta\int_x \pi_\theta(x)dx =\nabla_\theta1 = 0
$$&lt;p>接下来，我们分别推导 forward KL 和 reverse KL 的梯度。对于 forward KL, 我们有&lt;/p>
$$
\nabla_\theta D_{KL}(\pi_{ref}\parallel \pi_\theta) = -\int \pi_{ref}\nabla_\theta \log \pi_\theta dx=-\mathbb{E}_{\pi_{ref}}[s_\theta] = \boxed{-\mathbb{E}_{\pi_\theta}\left[\frac{\pi_{ref}}{\pi_\theta}s_\theta\right]}
$$&lt;p>对于 reverse KL,我们有&lt;/p>
$$
\begin{aligned}
\nabla_\theta D_{KL}(\pi_\theta\parallel \pi_{ref})&amp; = \int\left[\nabla_\theta \pi_\theta\cdot\log\frac{\pi_\theta}{\pi_{ref}} + \pi_\theta \nabla_\theta\log \frac{\pi_\theta}{\pi_{ref}}\right]dx\\
&amp;= \int \pi_\theta s_\theta\log \frac{\pi_\theta}{\pi_{ref}}dx + \int \pi_\theta s_\theta dx\\
&amp;= \mathbb{E}_{\pi_\theta}\left[s_\theta\log \frac{\pi_\theta}{\pi_{ref}}\right]+\mathbb{E}_{\pi_\theta}[s_\theta]\\
&amp;= \boxed{\mathbb{E}_{\pi_\theta}\left[s_\theta\log \frac{\pi_\theta}{\pi_{ref}}\right]}
\end{aligned}
$$&lt;p>这里我们使用了 $\nabla_\theta\pi_\theta=\pi_\theta s_\theta$ , $\nabla_\theta\log\pi_\theta=s_\theta$ 以及 前面推导的 $\mathbb{E}_{\pi_\theta}[s_\theta]=0$ 的结论.&lt;/p>
&lt;p>RL 的目标函数如下&lt;/p>
$$
\mathcal{J}(\theta) = \mathbb{E}_{\tau\sim \pi_\theta}\left[\sum_{t=0}^T\gamma^tr(s_t,a_t)\right] - \beta D_{KL}(\pi_\theta\parallel \pi_{ref})
$$&lt;h3 id="ki-as-loss">&lt;a href="#ki-as-loss" class="header-anchor">&lt;/a>Ki as Loss
&lt;/h3>&lt;p>由于 KL divergcne 不能直接计算（或者计算难度较大），因此，基于前面对 KL divergence estimation 的分析，我们可以使用如下代理损失函数来优化我们的模型：&lt;/p>
$$
\mathcal{J}_1(\theta) = \mathbb{E}_{\tau\sim \pi_\theta}\left[\sum_{t=0}^T\gamma^tr(s_t,a_t)\right] - \beta k_i(\pi_\theta, \pi_{ref})
$$&lt;p>这里 $i\in\{1,2,3\}$ 代表了我们使用的估计。从直觉上来说，这样做是没问题的，但是我们将从数学分析上说明，$k_1,k_3$ 作为损失函数都存在问题。其核心问题在于&lt;/p>
$$
\mathbb{E}[\widehat{D_{KL}}]=D_{KL} \nRightarrow \mathbb{E}[\nabla_\theta \widehat{D_{KL}}] =\nabla_\theta D_{KL}
$$&lt;p>也就是说，&lt;strong>KL divergence estimation 的无偏性不能推导出 KL divergence estimation gradient 的无偏性，这是因为我们在求期望时，对应的概率分布可能也与参数相关&lt;/strong>。实际上，我们有&lt;/p>
$$
\begin{aligned}
\nabla_\theta D_{KL}(\pi_\theta\parallel \pi_{ref}) &amp;= \nabla_\theta \mathbb{E}_{x\sim\pi_\theta}[\widehat{D_{KL}}(\pi_\theta\parallel \pi_{ref})]\\
&amp;= \mathbb{E}_{x\sim\pi_\theta}[\nabla_\theta \widehat{D_{KL}}(\pi_\theta\parallel \pi_{ref})] + \mathbb{E}_{x\sim\pi_\theta}[\widehat{D_{KL}}(\pi_\theta\parallel \pi_{ref})\nabla_\theta \pi_\theta(x)]\\
&amp;\neq \mathbb{E}_{x\sim\pi_\theta}[\nabla_\theta \widehat{D_{KL}}(\pi_\theta\parallel \pi_{ref})]
\end{aligned}
$$&lt;p>因此 $\nabla_\theta \widehat{D_{KL}}$ 是 $\nabla_\theta D_{KL}$ 的一个有偏估计。&lt;/p>
&lt;p>我们分别来分析一下 $k_1,k_2,k_3$ 梯度，&lt;/p>
$$
\begin{aligned}
\nabla_\theta k_1 &amp;= \nabla_\theta\left[-\log \frac{\pi_{ref}}{\pi_\theta}\right] = s_\theta\\
\nabla_\theta k_2 &amp;= \nabla_\theta\left[\frac12\left(\log \frac{\pi_{ref}}{\pi_\theta}\right)^2\right] = -\log \frac{\pi_{ref}}{\pi_\theta}s_\theta\\
\nabla_\theta k_3 &amp;= \nabla_\theta\left[\frac{\pi_{ref}}{\pi_\theta}-1- \log \frac{\pi_{ref}}{\pi_\theta}\right] = \left(1 - \frac{\pi_{ref}}{\pi_\theta}\right)s_\theta
\end{aligned}
$$&lt;p>此时对应的梯度的期望为&lt;/p>
$$
\begin{aligned}
\mathbb{E}_{\pi_{\theta}}[\nabla_\theta k_1] &amp;= \mathbb{E}_{\pi_{\theta}}[s_\theta]=0\\
\mathbb{E}_{\pi_{\theta}}[\nabla_\theta k_2] &amp;= \mathbb{E}_{\pi_{\theta}}\left[-\log \frac{\pi_{ref}}{\pi_\theta}s_\theta\right]=\nabla_\theta D_{KL}(\pi_\theta\parallel \pi_{ref})\\
\mathbb{E}_{\pi_{\theta}}[\nabla_\theta k_3] &amp;= \mathbb{E}_{\pi_{\theta}}\left[\left(1 - \frac{\pi_{ref}}{\pi_\theta}\right)s_\theta\right]=\nabla_\theta D_{KL}(\pi_{ref}\parallel \pi_\theta)\\
\end{aligned}
$$&lt;p>也就是说，$k_1$ 估计的梯度的期望为 0，对整体训练没有任何帮助，$k_3$ 估计的梯度的期望等价于优化 forward KL, **只有 $k_2$ 估计的梯度的期望等价于优化 reverse KL.&lt;/p>
&lt;hr>
&lt;p>在实际代码实现的时候，KL divergence 有两种不同的实现形式：&lt;/p>
&lt;p>第一种是根据定义将 KL divergence 作为损失函数的一部分，此时我们的 KL divergence 参与反向传播，对应的实现方式如下&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">loss = -advantage * log_prob + beta * kl
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>第二种是只调整 reward, 而不参与反向传播（通过 $\mathrm{sg}(\cdot)$ 实现），对应的实现方式如下所示&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">shaped_reward = reward - beta * kl.detach()
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>这两者对于模型的训练影响很大，下面我们分别来进行介绍&lt;/p>
&lt;h3 id="kl-as-loss">&lt;a href="#kl-as-loss" class="header-anchor">&lt;/a>KL as Loss
&lt;/h3>&lt;p>为了统一 on-policy 和 off-policy 两种形式，我们使用一个统一的表达形式，即&lt;/p>
$$
L=\rho k_i
$$&lt;p>此时对应的 RL 目标函数为&lt;/p>
$$
\mathcal{J}_2(\theta) = \mathbb{E}_{\tau\sim \pi_\theta}\left[\sum_{t=0}^T\gamma^tr(s_t,a_t)\right] - \beta\rho k_i(\pi_\theta, \pi_{ref})
$$&lt;p>这里&lt;/p>
$$
\rho = \frac{\pi_\theta}{\mathrm{sg}(\pi_{old})}
$$&lt;p>是 importance weight,&lt;/p>
&lt;ol>
&lt;li>当算法为 on-policy 时，$\pi_\theta=\pi_{old}$, $\rho\equiv1$.&lt;/li>
&lt;li>当算法为 off-policy 时，$\rho=\pi_\theta/\pi_{old}$, $\nabla_\theta \rho=\rho s_\theta$.&lt;/li>
&lt;/ol>
&lt;p>通过这种方式，我们使得参数分布本身不会对梯度计算产生影响，从而使得对期望进行求导和对导数求期望相等，即&lt;/p>
$$
\nabla_\theta\mathbb{E}_{\pi_{old}}[k] = \int \pi_{old}(x)\nabla_\theta kdx= \mathbb{E}_{\pi_{old}}[\nabla_\theta k]
$$&lt;p>接下来我们来计算对应估计的梯度的期望，即 $\mathbb{E}[\nabla_\theta(\rho k_i)]$, 首先我们计算对应的梯度&lt;/p>
$$
\begin{aligned}
\nabla_\theta (\rho k_1) &amp;= \rho s_\theta k_1+r\rho_\theta=\rho s_\theta(k_1+1)\\
\nabla_\theta (\rho k_2) &amp;= \rho s_\theta k_2+\rho\left(-\log \frac{\pi_{ref}}{\pi_\theta}s_\theta\right)=\rho s_\theta(k_1+k_2)\\
\nabla_\theta (\rho k_3) &amp;= \rho s_\theta k_3+\rho\left(1 - \frac{\pi_{ref}}{\pi_\theta}\right)s_\theta=\rho s_\theta\left(k_3+1-\frac{\pi_{ref}}{\pi_\theta}\right)=\rho s_\theta k_1
\end{aligned}
$$&lt;p>注意到 $\mathbb{E}_{\pi_{old}} [\rho k_i]=\mathbb{E}_{\pi_{\theta}}[k_i]$ 以及 $\mathbb{E}_{\pi_{\theta}}[s_\theta]=0$, 我们对上述梯度求期望得到&lt;/p>
$$
\begin{aligned}
\mathbb{E}_{\pi_{old}}[\nabla_\theta (\rho k_1)] &amp;= \mathbb{E}_{\pi_{old}}[\rho s_\theta(k_1+1)]=\mathbb{E}_{\pi_{\theta}}[s_\theta k_1]=\nabla_\theta D_{KL}(\pi_\theta\parallel \pi_{ref})\\
\mathbb{E}_{\pi_{old}}[\nabla_\theta (\rho k_2)] &amp;= \mathbb{E}_{\pi_{old}}[\rho s_\theta(k_1+k_2)]=\nabla_\theta \mathbb{E}_{\pi_\theta}[k_2]\\
\mathbb{E}_{\pi_{old}}[\nabla_\theta (\rho k_3)] &amp;= \mathbb{E}_{\pi_{old}}[\rho s_\theta k_1]=\nabla_\theta D_{KL}(\pi_\theta\parallel \pi_{ref})
\end{aligned}
$$&lt;p>这里在计算 $\mathbb{E}_{\pi_{old}}[\nabla_\theta (\rho k_2)]$ 时，我们使用了 Leibniz 乘法法则：&lt;/p>
$$
\mathbb{E}_{\pi_{old}}[\rho s_\theta(k_1+k_2)]= \mathbb{E}_{\pi_{\theta}}[s_\theta k_2]+\mathbb{E}_{\pi_{\theta}}[\nabla_\theta k_2]=\nabla_\theta\mathbb{E}_{\pi_{\theta}}[k_2]
$$&lt;p>可以看到，$\rho k_1$ 和 $\rho k_3$ 都满足梯度与期望的可交换性，而 $\rho k_2$ 不满足，为了解决这个问题，我们可以使用 stop gradient, 即 $\mathrm{sg}(\rho)l_2$, 此时，我们有&lt;/p>
$$
\nabla_\theta(\mathrm{sg}(\rho) k_2) = \mathrm{sg}(\rho)\nabla_\theta k_2 = \rho s_\theta k_1
$$&lt;p>对其求期望有&lt;/p>
$$
\mathbb{E}_{\pi_{old}}[\nabla_\theta(\mathrm{sg}(\rho) k_2)] = \mathbb{E}_{\pi_{old}}[\rho s_\theta k_1] = \mathbb{E}_{\pi_{\theta}}[s_\theta k_1]=\nabla_\theta D_{KL}(\pi_\theta\parallel \pi_{ref})
$$&lt;p>我们将如上结果总结为下表&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Loss&lt;/th>
&lt;th>gradient&lt;/th>
&lt;th>expected gradient&lt;/th>
&lt;th>objective&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$\rho k_1$&lt;/td>
&lt;td>$\rho s_\theta (k_1+1)$&lt;/td>
&lt;td>$\nabla_\theta D_{KL}(\pi_\theta\parallel \pi_{ref})$&lt;/td>
&lt;td>reverse KL&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\rho k_2$&lt;/td>
&lt;td>$\rho s_\theta (k_1+k_2)$&lt;/td>
&lt;td>$\nabla_\theta\mathbb{E}_{\pi_{\theta}}[k_2]$&lt;/td>
&lt;td>f-divergence&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\mathrm{sg}(\rho) k_2$&lt;/td>
&lt;td>$\rho s_\theta k_1$&lt;/td>
&lt;td>$\nabla_\theta D_{KL}(\pi_\theta\parallel \pi_{ref})$&lt;/td>
&lt;td>reverse KL&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\rho k_3$&lt;/td>
&lt;td>$\rho s_\theta k_1$&lt;/td>
&lt;td>$\nabla_\theta D_{KL}(\pi_\theta\parallel \pi_{ref})$&lt;/td>
&lt;td>reverse KL&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>接下来，我们就可以分析在 on-policy 和 off-policy 场景下分析不同 estimator 的性质了。&lt;/p>
&lt;p>如果说，我们显式加入 $\rho$, 则根据上表我们可以使用上表的 $\rho k_1$, $\mathrm{sg}(\rho) k_2$ 以及 $\rho k_3$ 都可以作为损失函数的代替。&lt;/p>
&lt;blockquote>
&lt;p>注
实际上 on-policy 场景下使用 $k_2$ 也有用的原因在于 $\nabla_\theta k_2=s_\theta k_1$, 也就是 $k_2$ 和 $\rho k_3$ 的梯度相同，其本质上是一个等效梯度。但是其收敛得到的 policy 与 target optimal policy 不同&lt;/p>
&lt;/blockquote>
&lt;p>接下来，我们来分析一下 $\rho k_1, \mathrm{sg}(\rho)k_2, \rho k_3$ 这三种估计的梯度的 variance, 为了避免混淆，【2】使用了 &amp;ldquo;projection variance in any direction&amp;rdquo; 的概念，即任意取一个向量 $u$, 然后计算 $\rho k_1$ 和后两者之间对应的 variance 的差（由于 $\mathrm{sg}(\rho)k_2$ 的梯度与 $\rho k_3$ 相同，因此这里我们仅计算 $\rho k_3$），得到:&lt;/p>
$$
\begin{aligned}
\mathrm{var}[\nabla_\theta (\rho k_1)^Tu] - \mathrm{var}[\nabla_\theta (\rho k_3)^Tu] &amp;= (\mathbb{E}_{\pi_{old}}[(\nabla_\theta (\rho k_1)^Tu)^2] -\mathbb{E}_{\pi_{old}}^2[\nabla_\theta (\rho k_1)^Tu] ) - (\mathbb{E}_{\pi_{old}}[(\nabla_\theta (\rho k_3)^Tu)^2] -\mathbb{E}_{\pi_{old}}^2[\nabla_\theta (\rho k_3)^Tu] ) \\
&amp;= \mathbb{E}_{\pi_{old}}[(\nabla_\theta (\rho k_1)^Tu)^2] - \mathbb{E}_{\pi_{old}}[(\nabla_\theta (\rho k_3)^Tu)^2]\\
&amp;= \mathbb{E}_{\pi_{old}}[\rho(x)^2(s(\theta)(x)^Tu)^2(2k_1(x)+1)]
\end{aligned}
$$&lt;p>当 $\pi_\theta$ 和 $\pi_{ref}$ 比较接近时，我们有&lt;/p>
$$
\frac{\pi_{ref}(x)}{\pi_\theta(x)} = 1+\epsilon(x), \text{ where } |\epsilon(x)| &lt;&lt; 1
$$&lt;p>此时&lt;/p>
$$
2k_1(x) + 1 = 1-2\log(1+\epsilon(x))\approx 1-2\epsilon(x) \geq 0
$$&lt;p>从而我们有&lt;/p>
$$
\boxed{\mathrm{var}[\nabla_\theta (\rho k_1)]\geq \mathrm{var}[\nabla_\theta (\rho k_3)]=\mathrm{var}[\nabla_\theta (\mathrm{sg}(\rho)k_2)]}
$$&lt;p>即当 $\pi_\theta$ 和 $\pi_{ref}$ 比较接近时，$\rho k_3$ 的 variance 比 $\rho k_1$ 更小，这是由于 $\rho s_\theta (k_1+1)$ 额外包含了一个 期望为零的项，这导致了其 variance 比较高。在 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v3.2/" target="_blank" rel="noopener"
>DeepSeek-V3.2&lt;/a> 中，作者就使用了 $\rho k_3$ 来降低梯度的 variance, 提高训练的稳定性。&lt;/p>
&lt;p>【3】将相关的估计总结为了下表的形式&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Type&lt;/th>
&lt;th>Loss&lt;/th>
&lt;th>Gradient&lt;/th>
&lt;th>Expected gradient&lt;/th>
&lt;th>Objective&lt;/th>
&lt;th>Biased&lt;/th>
&lt;th>Variance&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>on/off-policy&lt;/td>
&lt;td>$\rho k_1$&lt;/td>
&lt;td>$\rho s_\theta (k_1+1)$&lt;/td>
&lt;td>$\nabla_\theta D_{KL}(\pi_\theta\parallel \pi_{ref})$&lt;/td>
&lt;td>reverse KL&lt;/td>
&lt;td>unbiased&lt;/td>
&lt;td>high&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>on/off-policy&lt;/td>
&lt;td>$\rho k_2$&lt;/td>
&lt;td>$\rho s_\theta (k_1+k_2)$&lt;/td>
&lt;td>$\nabla_\theta\mathbb{E}_{\pi_{\theta}}[k_2]$&lt;/td>
&lt;td>f-divergence&lt;/td>
&lt;td>biased&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>on/off-policy&lt;/td>
&lt;td>$\mathrm{sg}(\rho) k_2$&lt;/td>
&lt;td>$\rho s_\theta k_1$&lt;/td>
&lt;td>$\nabla_\theta D_{KL}(\pi_\theta\parallel \pi_{ref})$&lt;/td>
&lt;td>reverse KL&lt;/td>
&lt;td>unbiased&lt;/td>
&lt;td>low&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>on/off-policy&lt;/td>
&lt;td>$\rho k_3$&lt;/td>
&lt;td>$\rho s_\theta k_1$&lt;/td>
&lt;td>$\nabla_\theta D_{KL}(\pi_\theta\parallel \pi_{ref})$&lt;/td>
&lt;td>reverse KL&lt;/td>
&lt;td>unbiased&lt;/td>
&lt;td>low&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>【3】还强调了一点就是我们的损失函数必须显式包含 $\rho$, 在 on-policy 场景下，虽然 $\rho\equiv1$, 但是在反向传播时我们通过 $\nabla_\theta \rho=s_\theta$ 保留了采样信息从而避免了梯度估计期望的错配问题。&lt;/p>
&lt;p>对于 $\rho k_1$ variance 比较高的特点，我们还可以采用 variance reduction 的方法来降低不同估计的 variance. 【TODO】&lt;/p>
&lt;p>&lt;strong>analytic gradient&lt;/strong>
当 action space 有限时，我们还可以使用解析梯度【TODO】&lt;/p>
&lt;h3 id="as-a-reward-reshaping-item">&lt;a href="#as-a-reward-reshaping-item" class="header-anchor">&lt;/a>As a Reward Reshaping Item
&lt;/h3>&lt;p>接下来我们来探究一下第二种形式，即 KL divergence 只影响最终的 reward, 而不参与反向传播。对应的代理目标函数形式为&lt;/p>
$$
\mathcal{J}_3(\theta) = \mathbb{E}_{\tau\sim \pi_\theta}\left[R\right] - \beta\ \mathrm{sg}(k_i(\pi_\theta, \pi_{ref}))
$$&lt;p>这里 $R=\sum_{t=0}^T\gamma^tr(s_t,a_t)$ 为 accumulative reward&lt;/p>
&lt;p>首先，基于前面分析，我们可以得到原始目标函数的梯度为&lt;/p>
$$
\begin{aligned}
\nabla_\theta \mathcal{J}(\theta) &amp;= \nabla_\theta\mathbb{E}_{\pi_\theta}\left[R\right] - \beta \nabla_\theta D_{KL}(\pi_\theta, \pi_{ref})\\
&amp;= \mathbb{E}_{\pi_\theta}\left[s_\theta R\right]-\beta \mathbb{E}_{\pi_\theta}\left[s_\theta\log \frac{\pi_\theta}{\pi_{ref}}\right]\\
&amp;= \mathbb{E}_{\pi_\theta}\left[s_\theta(R-\beta k_1) \right]
\end{aligned}
$$&lt;p>代理目标函数的梯度为&lt;/p>
$$
\nabla_\theta \mathcal{J}_3(\theta) = \mathbb{E}_{\pi_\theta}\left[s_\theta(R-\beta k_i) \right]
$$&lt;p>显然，当我们使用 $k_1$ 时，我们有 $\nabla_\theta \mathcal{J}(\theta)=\nabla_\theta \mathcal{J}_3(\theta)$.&lt;/p>
&lt;p>当我们使用 $k_2$ 时，带入 $k_2$ 表达式易知 $\nabla_\theta \mathcal{J}_3(\theta)\neq \nabla_\theta \mathcal{J}(\theta)$,&lt;/p>
&lt;p>当我们使用 $k_3$ 时，&lt;/p>
$$
\begin{aligned}
\mathbb{E}_{\pi_\theta}\left[s_\theta k_i \right] &amp;= \mathbb{E}_{\pi_\theta}\left[s_\theta \left(\frac{\pi_{ref}}{\pi_\theta}-1- \log \frac{\pi_{ref}}{\pi_\theta} \right)\right]\\
&amp;= \mathbb{E}_{\pi_\theta}\left[s_\theta \frac{\pi_{ref}}{\pi_\theta}\right] - \mathbb{E}_{\pi_\theta}\left[s_\theta \right] - \mathbb{E}_{\pi_\theta}\left[s_\theta \log \frac{\pi_{ref}}{\pi_\theta} \right]\\
&amp;=s_\theta k_1 -\nabla_\theta D_{KL}(\pi_{ref}\parallel \pi_\theta)
\end{aligned}
$$&lt;p>此时，$\nabla_\theta \mathcal{J}_3(\theta)\neq \nabla_\theta \mathcal{J}(\theta)$. 因此，&lt;strong>在 on-policy 场景下，只有 $k_1$ 对应的梯度是无偏的&lt;/strong>&lt;/p>
&lt;p>在 off-policy 场景下，由于 Off-policy 只影响 $R$ 的计算，因此原始目标函数和代理目标函数的梯度仍然保持不变，on-policy 场景的结论也适用。&lt;/p>
&lt;p>总之，&lt;strong>当我们将 KL divergence 作为 reward reshaping item 时，只有 $k_1$ 产生的梯度是无偏的。&lt;/strong>&lt;/p>
&lt;h3 id="comparison-of-two-paradigms">&lt;a href="#comparison-of-two-paradigms" class="header-anchor">&lt;/a>Comparison of Two Paradigms
&lt;/h3>&lt;p>接下来我们来比较一下 KL divergence 作为 loss 和 reward shaping item 的异同之处。首先，两者对于梯度的贡献分别为&lt;/p>
$$
\begin{align}
&amp;\rho s_\theta k_1\tag{loss}\\
&amp; \mathbb{E}_{\pi_{old}}[\rho s_\theta k_1]\tag{reward shaping}
\end{align}
$$&lt;p>即两者在期望上时一致的。但是两者也存在不一致的地方，即 KL divergence 作为 loss 时不会影响 $R$, 而作为 reward shaping item 时会影响。因此这就导致两者的优化方向不一致。&lt;/p>
&lt;h3 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h3>&lt;p>首先，我们来验证前面的结论，我们构造一个包含 $100$ 个 arms 的 multi-arm bandits, 然后令&lt;/p>
$$
\pi_{ref}=\epsilon_1, \pi= \epsilon_1+\epsilon_2
$$&lt;p>其中 $\epsilon_1,\epsilon_2\sim\mathcal{N}(0,1)$, 我们实验 100 次然后取平均值，然后分别计算 estimator 与真实 KL divergence 之间的 MSE 和 estimator gradient 与真实 kl divergence gradient 的 RMSE, 结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kl-divergence/KL-divergence-estimator-gradient-bias.png"
width="1389"
height="489"
loading="lazy"
alt="bias of KL divergence estimators and their gradients"
class="gallery-image"
data-flex-grow="284"
data-flex-basis="681px"
>&lt;/p>
&lt;p>可以看到，这验证了我们之前分析的结论，即 $k_1$ 和 $k_3$ 是无偏估计，而在计算梯度时，只有 $k_2$ 梯度的期望与真实 KL divergence 的梯度相同。&lt;/p>
&lt;h3 id="overview">&lt;a href="#overview" class="header-anchor">&lt;/a>Overview
&lt;/h3>&lt;p>我们在本节总结前面的分析，如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Type&lt;/th>
&lt;th>Loss&lt;/th>
&lt;th>Gradient&lt;/th>
&lt;th>Expected gradient&lt;/th>
&lt;th>Objective&lt;/th>
&lt;th>Biased&lt;/th>
&lt;th>Variance&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>on-policy&lt;/td>
&lt;td>$k_1$&lt;/td>
&lt;td>$s_\theta$&lt;/td>
&lt;td>$0$&lt;/td>
&lt;td>constants&lt;/td>
&lt;td>biased&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>on-policy&lt;/td>
&lt;td>$k_2$&lt;/td>
&lt;td>$-\log r s_\theta$&lt;/td>
&lt;td>$\nabla_\theta D_{KL}(\pi_\theta\parallel \pi_{ref})$&lt;/td>
&lt;td>reverse KL&lt;/td>
&lt;td>&lt;strong>unbiased&lt;/strong>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>on-policy&lt;/td>
&lt;td>$k_3$&lt;/td>
&lt;td>$(1-r)s_\theta$&lt;/td>
&lt;td>$\nabla_\theta D_{KL}(\pi_{ref}\parallel \pi_\theta)$&lt;/td>
&lt;td>forward KL&lt;/td>
&lt;td>biased&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>on/off-policy&lt;/td>
&lt;td>$\rho k_1$&lt;/td>
&lt;td>$\rho s_\theta (k_1+1)$&lt;/td>
&lt;td>$\nabla_\theta D_{KL}(\pi_\theta\parallel \pi_{ref})$&lt;/td>
&lt;td>reverse KL&lt;/td>
&lt;td>unbiased&lt;/td>
&lt;td>high&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>on/off-policy&lt;/td>
&lt;td>$\rho k_2$&lt;/td>
&lt;td>$\rho s_\theta (k_1+k_2)$&lt;/td>
&lt;td>$\nabla_\theta\mathbb{E}_{\pi_{\theta}}[k_2]$&lt;/td>
&lt;td>f-divergence&lt;/td>
&lt;td>biased&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>on/off-policy&lt;/td>
&lt;td>$\mathrm{sg}(\rho) k_2$&lt;/td>
&lt;td>$\rho s_\theta k_1$&lt;/td>
&lt;td>$\nabla_\theta D_{KL}(\pi_\theta\parallel \pi_{ref})$&lt;/td>
&lt;td>reverse KL&lt;/td>
&lt;td>&lt;strong>unbiased&lt;/strong>&lt;/td>
&lt;td>low&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>on/off-policy&lt;/td>
&lt;td>$\rho k_3$&lt;/td>
&lt;td>$\rho s_\theta k_1$&lt;/td>
&lt;td>$\nabla_\theta D_{KL}(\pi_\theta\parallel \pi_{ref})$&lt;/td>
&lt;td>reverse KL&lt;/td>
&lt;td>&lt;strong>unbiased&lt;/strong>&lt;/td>
&lt;td>low&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>on/off-policy&lt;/td>
&lt;td>$\rho\mathrm{sg}(k_1)$&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>&lt;strong>unbiased&lt;/strong>&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>on/off-policy&lt;/td>
&lt;td>$\rho \mathrm{sg}(k_2)$&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>biased&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>on/off-policy&lt;/td>
&lt;td>$\rho \mathrm{sg}(k_3)$&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>biased&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>在本文中，我们详细介绍了 KL-divergence 的基本性质，相关估计方法以及在机器学习特别是 RL 领域中的应用。最终结论为：&lt;/p>
&lt;ol>
&lt;li>如果希望稳定可控，则将 KL divergence 作为 loss item; 如果希望更灵活，与奖励信号结合的话，则将其作为 reward shaping item.&lt;/li>
&lt;li>使用 KL divergence 作为 loss item 时，on-policy 场景下使用 $k_2$ 近似 KL divergence 效果最好；off-policy 场景下，使用 $\mathrm{sg}(\rho)k_2, \rho k_3$ 效果最好&lt;/li>
&lt;li>使用 KL divergence 作为 reward shaping item 时，$k_1$ 的效果最好&lt;/li>
&lt;/ol>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://joschu.net/blog/kl-approx.html" target="_blank" rel="noopener"
>Approximating KL Divergence&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://dibyaghosh.com/blog/probability/kldivergence/" target="_blank" rel="noopener"
>KL Divergence for Machine Learning&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-en.html" target="_blank" rel="noopener"
>Understanding KL Divergence Estimators in RL: From Value Approximation to Gradient Estimation&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2506.09477" target="_blank" rel="noopener"
>On a few pitfalls in KL divergence gradient estimation for RL&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Qwen3-Next</title><link>https://maosong.website/p/notes-on-qwen3-next/</link><pubDate>Fri, 23 Jan 2026 10:29:56 +0800</pubDate><guid>https://maosong.website/p/notes-on-qwen3-next/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>当前大语言模型在性能与效率上面临双重挑战：纯 Softmax 注意力计算成本高，而纯线性注意力则性能不足。Qwen3-Next 尝试通过&lt;strong>混合注意力机制&lt;/strong>解决这一矛盾，同时结合 MoE 架构与多项训练优化策略，实现在保持高性能的同时大幅提升训练与推理效率。&lt;/p>
&lt;p>Qwen3-Next 包含三个模型：&lt;/p>
&lt;ol>
&lt;li>Qwen3-Next-80B-A3B-Base&lt;/li>
&lt;li>Qwen3-Next-80B-A3B-Instruct&lt;/li>
&lt;li>Qwen3-Next-80B-A3B-Thinking&lt;/li>
&lt;/ol>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;p>模型架构如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen3-next/Qwen3-Next-architecture.png"
width="2204"
height="2348"
loading="lazy"
alt="architecture of Qwen3-Next"
class="gallery-image"
data-flex-grow="93"
data-flex-basis="225px"
>&lt;/p>
&lt;h3 id="hybrid-attention">&lt;a href="#hybrid-attention" class="header-anchor">&lt;/a>Hybrid Attention
&lt;/h3>&lt;p>作者首先总结了 linear attention 和 softmax attention 各自的优缺点。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>pros&lt;/th>
&lt;th>cons&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>linear attention&lt;/td>
&lt;td>fast&lt;/td>
&lt;td>low performance&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>softmax attention&lt;/td>
&lt;td>slow&lt;/td>
&lt;td>high performance&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>因此，作者的动机就是是结合 linear attention 与 softmax attention, 在局部利用 linear attention 的高效性来提高训练和推理效率，在关键部分使用 softmax attention 来提高模型的能力。 这种混合注意力机制之前也有很多模型采用，比如 &lt;a class="link" href="https://maosong.website/p/notes-on-minimax-01/" target="_blank" rel="noopener"
>MiniMax-01&lt;/a> 等。最终 Qwen3-Next 使用了 Gated DeltaNet+Gated Attention 的混合注意力机制，模型的 transformer layers 按照 4 个为一组，前三层使用 Gated DeltaNet, 第四层使用 Gated Attention.&lt;/p>
&lt;p>下面是一些细节：&lt;/p>
&lt;ol>
&lt;li>Gated DeltaNet 相比于 &lt;a class="link" href="https://maosong.website/p/notes-on-rnope-swa/" target="_blank" rel="noopener"
>SWA&lt;/a> 和 Mamba2, 其 in-context learning 能力更强&lt;/li>
&lt;li>对于 softmax attention:
&lt;ol>
&lt;li>使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-gated-attention/" target="_blank" rel="noopener"
>Gated Attention&lt;/a> 提出的 gating 机制来解决 massive activation 和 attention sink 问题&lt;/li>
&lt;li>将 attention head 的 dimension 从 128 提高到 256&lt;/li>
&lt;li>使用了和 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v3/" target="_blank" rel="noopener"
>DeepSeek-V3&lt;/a> 类似的 partial RoPE 机制，仅对前 $25\%$ 的元素进行旋转&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;h3 id="moe">&lt;a href="#moe" class="header-anchor">&lt;/a>MoE
&lt;/h3>&lt;ul>
&lt;li>1 个共享专家，512 个路由专家，其中激活专家个数为 10 个。&lt;/li>
&lt;li>对于 MoE router 的参数，作者还进行了 normalization 来保证每个专家被选择的概率相同。&lt;/li>
&lt;li>与 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a> 一致，Qwen3-Next 也是用了 &lt;a class="link" href="https://maosong.website/p/notes-on-global-batch-load-balancing/" target="_blank" rel="noopener"
>Global-batch load balancing&lt;/a> 策略，在保持激活专家数不变的情况下，通过提高总专家个数来降低训练损失。&lt;/li>
&lt;/ul>
&lt;h3 id="normalization-and-training">&lt;a href="#normalization-and-training" class="header-anchor">&lt;/a>Normalization and Training
&lt;/h3>&lt;ul>
&lt;li>使用 Gemma 提出的 Zero-Centered RMSNorm 以及 weight decay 来避免过大的权重出现&lt;/li>
&lt;li>为了提高数据使用效率，作者还使用了 MTP 策略来提高训练效率，模型表现以及 Speculative decoding 的接受率。&lt;/li>
&lt;li>预训练时，Qwen3-Next 使用了&lt;strong>15T&lt;/strong> token 进行训练，训练时间相比于 Qwen3-30B-A3B 有了大幅度的提升&lt;/li>
&lt;/ul>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;h3 id="efficiency">&lt;a href="#efficiency" class="header-anchor">&lt;/a>Efficiency
&lt;/h3>&lt;p>下图是 Qwen3-Next 与 Qwen3-32B 模型的训练效率对比&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen3-next/Qwen3-Next-pre-training-efficiency.png"
width="2860"
height="1114"
loading="lazy"
alt="Pre-training efficiency of Qwen3-Next"
class="gallery-image"
data-flex-grow="256"
data-flex-basis="616px"
>&lt;/p>
&lt;p>从结果可以看出，相比于 Qwen3-32B, Qwen3-Next 只用了 $9.3\%$ 的算力就达到了更强的表现。&lt;/p>
&lt;p>并且，在 inference 阶段，由于使用了 linear attention, Qwen3-Next 的效率也更高，下面是 Qwen3-Next 相比于 Qwen3-32B 的效率提升&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>4K&lt;/th>
&lt;th>32K&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Prefilling&lt;/td>
&lt;td>$7\times$&lt;/td>
&lt;td>$10\times$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Decoding&lt;/td>
&lt;td>$4\times$&lt;/td>
&lt;td>$10\times$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="performance">&lt;a href="#performance" class="header-anchor">&lt;/a>Performance
&lt;/h3>&lt;p>下面是 Qwen3-Next-Base 的表现&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen3-next/Qwen3-Next-base-performance.png"
width="1288"
height="844"
loading="lazy"
alt="Performance of Qwen3-Next-Base"
class="gallery-image"
data-flex-grow="152"
data-flex-basis="366px"
>&lt;/p>
&lt;p>可以看到，Qwen3-Next-Base 在多个 Benchmark 上的表现仅次于 Qwen3-235B-A22B&lt;/p>
&lt;p>Qwen3-Next-Instruct 的表现如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Benchmark&lt;/th>
&lt;th>Qwen3-Next-80B-A3B-Instruct&lt;/th>
&lt;th>Qwen3-235B-A22B-Instruct-2507&lt;/th>
&lt;th>Qwen3-32B Non-thinking&lt;/th>
&lt;th>Qwen3-30B-A3B-Instruct-2507&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>SuperGPQA&lt;/td>
&lt;td>58.8&lt;/td>
&lt;td>&lt;strong>62.6&lt;/strong>&lt;/td>
&lt;td>43.2&lt;/td>
&lt;td>53.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>AIME25&lt;/td>
&lt;td>69.5&lt;/td>
&lt;td>&lt;strong>70.3&lt;/strong>&lt;/td>
&lt;td>20.2&lt;/td>
&lt;td>61.3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LiveCodeBench v6&lt;/td>
&lt;td>&lt;strong>56.6&lt;/strong>&lt;/td>
&lt;td>51.8&lt;/td>
&lt;td>29.1&lt;/td>
&lt;td>43.2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Arena-Hard v2&lt;/td>
&lt;td>&lt;strong>82.7&lt;/strong>&lt;/td>
&lt;td>79.2&lt;/td>
&lt;td>34.1&lt;/td>
&lt;td>69.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LiveBench&lt;/td>
&lt;td>&lt;strong>75.8&lt;/strong>&lt;/td>
&lt;td>75.4&lt;/td>
&lt;td>59.8&lt;/td>
&lt;td>69.0&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Qwen3-Next-Instruct 的长文本表现（RULER Benchmark）如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Avg.&lt;/th>
&lt;th>4K&lt;/th>
&lt;th>8K&lt;/th>
&lt;th>16K&lt;/th>
&lt;th>32K&lt;/th>
&lt;th>64K&lt;/th>
&lt;th>96k&lt;/th>
&lt;th>128K&lt;/th>
&lt;th>192k&lt;/th>
&lt;th>256k&lt;/th>
&lt;th>384k&lt;/th>
&lt;th>512k&lt;/th>
&lt;th>640k&lt;/th>
&lt;th>768k&lt;/th>
&lt;th>896k&lt;/th>
&lt;th>1M&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Qwen3-30B-A3B-Instruct-2507&lt;/td>
&lt;td>86.8&lt;/td>
&lt;td>98.0&lt;/td>
&lt;td>96.7&lt;/td>
&lt;td>96.9&lt;/td>
&lt;td>97.2&lt;/td>
&lt;td>93.4&lt;/td>
&lt;td>91.0&lt;/td>
&lt;td>89.1&lt;/td>
&lt;td>89.8&lt;/td>
&lt;td>82.5&lt;/td>
&lt;td>83.6&lt;/td>
&lt;td>78.4&lt;/td>
&lt;td>79.7&lt;/td>
&lt;td>77.6&lt;/td>
&lt;td>75.7&lt;/td>
&lt;td>72.8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen3-235B-A22B-Instruct-2507&lt;/td>
&lt;td>92.5&lt;/td>
&lt;td>98.5&lt;/td>
&lt;td>97.6&lt;/td>
&lt;td>96.9&lt;/td>
&lt;td>97.3&lt;/td>
&lt;td>95.8&lt;/td>
&lt;td>94.9&lt;/td>
&lt;td>93.9&lt;/td>
&lt;td>94.5&lt;/td>
&lt;td>91.0&lt;/td>
&lt;td>92.2&lt;/td>
&lt;td>90.9&lt;/td>
&lt;td>87.8&lt;/td>
&lt;td>84.8&lt;/td>
&lt;td>86.5&lt;/td>
&lt;td>84.5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen3-Next-80B-A3B-Instruct&lt;/td>
&lt;td>91.8&lt;/td>
&lt;td>98.5&lt;/td>
&lt;td>99.0&lt;/td>
&lt;td>98.0&lt;/td>
&lt;td>98.7&lt;/td>
&lt;td>97.6&lt;/td>
&lt;td>95.0&lt;/td>
&lt;td>96.0&lt;/td>
&lt;td>94.0&lt;/td>
&lt;td>93.5&lt;/td>
&lt;td>91.7&lt;/td>
&lt;td>86.9&lt;/td>
&lt;td>85.5&lt;/td>
&lt;td>81.7&lt;/td>
&lt;td>80.3&lt;/td>
&lt;td>80.3&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到， Qwen3-Next-Instruct 在 1M 长度范围内保持稳定性能，整体平均得分 91.8，接近 Qwen3-235B（92.5）。&lt;/p>
&lt;p>Qwen3-Next-Thinking 的表现如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;strong>Benchmark&lt;/strong>&lt;/th>
&lt;th>&lt;strong>Qwen3-Next-80B-A3B-Thinking&lt;/strong>&lt;/th>
&lt;th>&lt;strong>Gemini-2.5-Flash Thinking&lt;/strong>&lt;/th>
&lt;th>&lt;strong>Qwen3-32B Thinking&lt;/strong>&lt;/th>
&lt;th>&lt;strong>Qwen3-30B-A3B-Thinking2507&lt;/strong>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>SuperGPQA&lt;/td>
&lt;td>60.8&lt;/td>
&lt;td>57.8&lt;/td>
&lt;td>54.1&lt;/td>
&lt;td>56.8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>AIME25&lt;/td>
&lt;td>87.8&lt;/td>
&lt;td>72.0&lt;/td>
&lt;td>72.9&lt;/td>
&lt;td>85.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LiveCodeBench v6&lt;/td>
&lt;td>68.7&lt;/td>
&lt;td>61.2&lt;/td>
&lt;td>60.6&lt;/td>
&lt;td>66.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Arena-Hard v2&lt;/td>
&lt;td>62.3&lt;/td>
&lt;td>56.7&lt;/td>
&lt;td>48.4&lt;/td>
&lt;td>56.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LiveBench&lt;/td>
&lt;td>76.6&lt;/td>
&lt;td>74.3&lt;/td>
&lt;td>74.9&lt;/td>
&lt;td>76.8&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，Qwen3-Next-Thinking 的表现在除了 Livebench 之外的三个 Benchmark 均达到了 SOTA&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>Qwen3-Next 通过&lt;strong>混合注意力架构&lt;/strong>与&lt;strong>精细化 MoE 设计&lt;/strong>，在训练与推理效率上实现突破性提升。其仅以较小计算代价达到接近超大模型性能的表现，为下一代高效大语言模型的设计提供了重要参考。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&amp;amp;from=research.latest-advancements-list" target="_blank" rel="noopener"
>Qwen3-Next: Towards Ultimate Training &amp;amp; Inference Efficiency&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>megatron-lm</title><link>https://maosong.website/p/megatron-lm/</link><pubDate>Wed, 21 Jan 2026 18:04:12 +0800</pubDate><guid>https://maosong.website/p/megatron-lm/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>随着模型参数变大，现有的 GPU 已经很难使用单一 GPU 来训练模型。对于多 GPU 训练场景，目前主要采用了 pipeline parallelism, 比如 &lt;a class="link" href="https://maosong.website/p/gpipe/" target="_blank" rel="noopener"
>GPipe&lt;/a> 等，但是，这些策略需要我们对代码进行比较大的改动，这提高了开发成本。&lt;/p>
&lt;p>为了解决多 GPU 训练大规模 LLM 的效率，降低开发成本，目前主要使用了 model parallelism 策略，即对模型进行切分部署在多个 GPU 上。model parallelism 有两种范式：&lt;/p>
&lt;ol>
&lt;li>pipeline parallelism (PP): 将模型按照 layer 进行切分，如 &lt;a class="link" href="https://maosong.website/p/gpipe/" target="_blank" rel="noopener"
>GPipe&lt;/a> 等，这种方法的问题是需要额外的逻辑来处理通信以及存在 pipeline bubbles&lt;/li>
&lt;li>tensor parallelism (TP): 将模型的按照权重进行切分，部署在不同的 GPU 上。&lt;/li>
&lt;/ol>
&lt;p>作者在本文中基于 TP 策略来对 attention, FFN layer 进行简单改动来实现训练效率的提升。&lt;/p>
&lt;p>作者通过实现验证了 tensor parallelism 的有效性和高效率，结果发现在 512 张 GPU 的场景下，TP 可以达到 $76\%$ 的 scaling efficiency (相比于 1 张 GPU 带来的性能提升)&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;p>作者使用的 transformer 架构如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/megatron-lm/megatron-lm-transformer-architecture.png"
width="210"
height="501"
loading="lazy"
alt="transformer architecture"
class="gallery-image"
data-flex-grow="41"
data-flex-basis="100px"
>&lt;/p>
&lt;p>本文中，作者探究了 BERT 和 GPT-2 两种架构。&lt;/p>
&lt;p>首先，我们假设 transformer layer 输入为 $X\in\mathbb{R}^{bs\times d}$, 这里 $b, s$ 分别为 batch size, sequence length, 接下来我们介绍如何针对 FFN, attention 以及 embedding 构建 TP 策略&lt;/p>
&lt;h3 id="ffn">&lt;a href="#ffn" class="header-anchor">&lt;/a>FFN
&lt;/h3>&lt;p>论文中使用的 FFN 为 &lt;code>Linear-GeLU-Linear&lt;/code> 的结构，对应第一层权重为 $W_1\in\mathbb{R}^{d\times d_{ff}}$, 第二层权重为 $W_2\in\mathbb{R}^{d_{ff}\times d}$, 对应数学表达式为&lt;/p>
$$
Y = \mathrm{GeLU}(XW_1)W_2\in\mathbb{R}^{bs\times d}
$$&lt;p>我们首先对 $W_1$ 按照 column 进行切分，得到&lt;/p>
$$
W_1 = [W_{11}, W_{12}]\in\mathbb{R}^{d\times d_{ff}}, \text{ where } W_{11}\in\mathbb{R}^{d\times d_1}, W_{12}\in\mathbb{R}^{d\times d_2}, d_1+d_2=d_{ff}
$$&lt;p>这里 $d_1, d_2$ 与我们并行的 GPU 数 (x-way TP) 相关，这样，我们就有&lt;/p>
$$
\mathrm{GeLU}(XW_1) = \mathrm{GeLU}(X[W_{11}, W_{12}]) = \mathrm{GeLU}([XW_{11}, XW_{22}]) = [\mathrm{GeLU}(XW_{11}), \mathrm{GeLU}(XW_{12})]
$$&lt;p>从而我们可以分别将 $W_{11}$ 和 $W_{12}$ 部署在两个 GPU 上，然后并行计算。&lt;/p>
&lt;p>论文中还介绍如果我们对 $W_1$ 按照 row 进行切分，则最终由于 $\mathrm{GeLU}(A+B)\neq \mathrm{GeLU}(A)+\mathrm{GeLU}(B)$ 计算时会产生一次额外的同步。&lt;/p>
&lt;p>接下来，对于 $W_2$, 我们按照 row 进行切分得到&lt;/p>
$$
W_2 = \begin{bmatrix}
W_{21}\\
W_{22}
\end{bmatrix}\in\mathbb{R}^{d_{ff}\times d}, \text{ where }W_{21}\in\mathbb{R}^{d_1\times d}, W_{22}\in\mathbb{R}^{d_2\times d}, d_1+d_2=d_{ff}
$$&lt;p>计算时，我们有&lt;/p>
$$
\mathrm{GeLU}(XW_1)W_2 = [\mathrm{GeLU}(XW_{11}), \mathrm{GeLU}(XW_{12})]\begin{bmatrix}
W_{21}\\
W_{22}
\end{bmatrix} = \mathrm{GeLU}(XW_{11})W_{21} + \mathrm{GeLU}(XW_{12})W_{22}
$$&lt;p>可以看到，通过按照 row 进行切分，我们可以将 $W_{11}, W_{21}$ 部署在一个 GPU 上，将 $W_{12}, W_{22}$ 部署在另一个 GPU 上，分别计算出 $\mathrm{GeLU}(XW_{11})W_{21}$ 和 $\mathrm{GeLU}(XW_{12})W_{22}$ 之后，再通过一此 &lt;code>all-reduce&lt;/code> 操作得到最终的输出结果。计算图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/megatron-lm/megatron-lm-MLP-tp.png"
width="580"
height="264"
loading="lazy"
alt="Tensor Parallelism for MLP in transformer block"
class="gallery-image"
data-flex-grow="219"
data-flex-basis="527px"
>&lt;/p>
&lt;p>这里 $f$ 和 $g$ 是两个对偶算子，代表了 TP 产生的额外通信开销&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>operator&lt;/th>
&lt;th>forward&lt;/th>
&lt;th>backward&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$f$&lt;/td>
&lt;td>identity&lt;/td>
&lt;td>&lt;code>all-reduce&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$g$&lt;/td>
&lt;td>&lt;code>all-reduce&lt;/code>&lt;/td>
&lt;td>identity&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>如果说我们使用的是 SwiGLU FFN, 即&lt;/p>
$$
Y = (XW_3\odot \mathrm{Swish}(XW_1))W_2
$$&lt;p>我们按照 column 对 $W_1, W_3$ 进行切分，按照 row 对 $W_2$ 进行切分（假设我们有 2 个 GPU），得到&lt;/p>
$$
\begin{aligned}
W_1 &amp;= [W_{11}, W_{12}]\in\mathbb{R}^{d\times d_{ff}}, \text{ where } W_{11}\in\mathbb{R}^{d\times d_1}, W_{12}\in\mathbb{R}^{d\times d_2}, d_1+d_2=d_{ff}\\
W_3 &amp;= [W_{31}, W_{32}]\in\mathbb{R}^{d\times d_{ff}}, \text{ where } W_{31}\in\mathbb{R}^{d\times d_1}, W_{32}\in\mathbb{R}^{d\times d_2}, d_1+d_2=d_{ff}\\
W_2 &amp;= \begin{bmatrix}
W_{21}\\
W_{22}
\end{bmatrix}\in\mathbb{R}^{d_{ff}\times d}, \text{ where }W_{21}\in\mathbb{R}^{d_1\times d}, W_{22}\in\mathbb{R}^{d_2\times d}, d_1+d_2=d_{ff}
\end{aligned}
$$&lt;p>然后我们将 $W_{11}, W_{31}, W_{21}$ 放在第一个 GPU 上，将 $W_{12}, W_{32}, W_{22}$ 放在第二个 GPU 上，此时，&lt;/p>
$$
\begin{aligned}
\mathrm{Swish}(XW_1) &amp;= \mathrm{Swish}(X[W_{11}, W_{12}])
= \mathrm{Swish}([XW_{11}, XW_{12}])=[\mathrm{Swish}(XW_{11}, \mathrm{Swish}(XW_{12}]\\
XW_3\odot \mathrm{Swish}(XW_1) &amp;= [XW_{31}, XW_{32}]\mathrm{Swish}(XW_1) = [XW_{31}\mathrm{Swish}(XW_{11}), XW_{32}\mathrm{Swish}(XW_{12})]\\
Y = (XW_3\odot \mathrm{Swish}(XW_1))W_2&amp;=(XW_3\odot \mathrm{Swish}(XW_1))\begin{bmatrix}
W_{21}\\
W_{22}
\end{bmatrix} = XW_{31}\mathrm{Swish}(XW_{11})W_{21}+ XW_{32}\mathrm{Swish}(XW_{12})W_{22}
\end{aligned}
$$&lt;p>这样我们通过一次 &lt;code>all-reduce&lt;/code> 也可以完成 SwiGLU FFN 的 tensor parallelism, 示意图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/megatron-lm/megatron-lm-SwiGLU-TP.png"
width="771"
height="449"
loading="lazy"
alt="Tensor Parallelism for SwiGLU MLP in transformer block"
class="gallery-image"
data-flex-grow="171"
data-flex-basis="412px"
>&lt;/p>
&lt;h3 id="attention">&lt;a href="#attention" class="header-anchor">&lt;/a>Attention
&lt;/h3>&lt;p>Attention 的处理与 MLP 非常相似，论文中的做法就是将不同 head 部署到不同 gpu 上分别进行计算，最后在计算 output projection 时再通过一次 &lt;code>all-reduce&lt;/code> 来合并输出，这里我们假设有 $h$ 个 heads, 每个 head 的 dimension 为 $d_h$, 我们先对 query, key, value layer 的 weight $W_Q, W_K, W_V\in\mathbb{R}^{d\times hd_h}$ 进行切分&lt;/p>
$$
W_Q = [W_{Q1}, \dots, W_{Qh}], W_K = [W_{K1}, \dots, W_{kh}], W_V = [W_{V1}, \dots, W_{Vh}]
$$&lt;p>其中 $W_{Qi}, W_{Ki}, W_{Vi}\in\mathbb{R}^{d\times d_h}$ 为每个 head 对应的 query, key, value weight. 我们将切分后的 $W_{Qi}, W_{Ki}, W_{Vi}$ 部署在一个 GPU 上（也可以将若干个 head 部署在一个 GPU 上），然后分别计算出每个 GPU 的 attention 结果，最后再进行汇总，如下所示&lt;/p>
$$
\begin{aligned}
o_i &amp;= \mathrm{softmax}\left(\frac{(XW_{Qi})(XW_{Ki})^T}{d_h}\right) XW_{Vi}, i=1,\dots,h\\
O &amp;= [o_1,\dots,o_h]W_O
\end{aligned}
$$&lt;p>下面是 multi-head attention 对应的 TP 示意图&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/megatron-lm/megatron-lm-mha-tp.png"
width="596"
height="324"
loading="lazy"
alt="Tensor Parallelism for multi-head attention"
class="gallery-image"
data-flex-grow="183"
data-flex-basis="441px"
>&lt;/p>
&lt;h3 id="embedding">&lt;a href="#embedding" class="header-anchor">&lt;/a>Embedding
&lt;/h3>&lt;p>对于 Input embedding, 作者将 embedding matrix $E\in\mathbb{E}^{V\times d}$ 按照 row 进行切分（论文中使用了转置，因此是按照 column 进行切分），得到 $E=[E_1,E_2]^T$, 这里 $E_i\in\mathbb{R}^{d\times V_i}$, $V_1+V_2=V$, 接下来我们把切分后的 embedding matrix 部署在不同的 GPU 上，由于每个 GPU 只有部分结果，因此我们还需要进行 &lt;code>all-reduce&lt;/code> 来进行汇总。&lt;/p>
&lt;p>而对于 output embedding, 我们也可以使用类似的做法进行切分，每个 GPU 上计算完结果之后我们还需要一个 &lt;code>all-gather&lt;/code> 来汇总结果。&lt;/p>
&lt;p>作者在这里还额外介绍了针对 output embedding 的优化方法，由于 embedding 的输出大小为 $[bs, V]$, 而 $V$ 通常比较大，因此，为了降低通信开销，作者将 cross-entropy-loss 与 output embedding kernel 进行融合，这样我们传输的数据量就减少到了 $bs$.&lt;/p>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>作者首先对 GPT-2 模型进行了修正，首先将 &lt;code>vocab_size&lt;/code> 从 50257 提升到 128 的倍数，即 51200. 对于 model+data parallelism, 作者固定 global batch size 为 512. (64-way DP)&lt;/p>
&lt;p>配置如下表所示（head size 为 96）&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Hidden size&lt;/th>
&lt;th>attention heads&lt;/th>
&lt;th>layers&lt;/th>
&lt;th>parameters (B)&lt;/th>
&lt;th>TP&lt;/th>
&lt;th>TP+DP&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>1536&lt;/td>
&lt;td>16&lt;/td>
&lt;td>40&lt;/td>
&lt;td>1.2&lt;/td>
&lt;td>1&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1920&lt;/td>
&lt;td>20&lt;/td>
&lt;td>54&lt;/td>
&lt;td>2.5&lt;/td>
&lt;td>2&lt;/td>
&lt;td>128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2304&lt;/td>
&lt;td>24&lt;/td>
&lt;td>64&lt;/td>
&lt;td>4.2&lt;/td>
&lt;td>4&lt;/td>
&lt;td>256&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>3072&lt;/td>
&lt;td>32&lt;/td>
&lt;td>72&lt;/td>
&lt;td>8.3&lt;/td>
&lt;td>8&lt;/td>
&lt;td>512&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>对应的 scaling （使用多卡训练后，每个 GPU 相对于单卡训练的利用率）如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>parallelism&lt;/th>
&lt;th>TP-1&lt;/th>
&lt;th>TP-2&lt;/th>
&lt;th>TP-4&lt;/th>
&lt;th>TP-8&lt;/th>
&lt;th>TP-1+DP-64&lt;/th>
&lt;th>TP-2+DP-64&lt;/th>
&lt;th>TP-4+DP-64&lt;/th>
&lt;th>TP-8+DP-64&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>scaling&lt;/td>
&lt;td>100%&lt;/td>
&lt;td>95%&lt;/td>
&lt;td>82%&lt;/td>
&lt;td>77%&lt;/td>
&lt;td>96%&lt;/td>
&lt;td>83%&lt;/td>
&lt;td>79%&lt;/td>
&lt;td>74%&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="implementation">&lt;a href="#implementation" class="header-anchor">&lt;/a>Implementation
&lt;/h2>&lt;p>首先是 linear layer 的 TP 版本，如下所示&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">torch&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">torch.nn&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">nn&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">torch.distributed&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">dist&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">ColumnParallelLinear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">in_features&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">out_features&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="fm">__init__&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">rank&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">world_size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_world_size&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">local_out&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">out_features&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">world_size&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">weight&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Parameter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">empty&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">in_features&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">local_out&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">out&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="o">@&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">weight&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">gather_list&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">empty_like&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">out&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">world_size&lt;/span>&lt;span class="p">)]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">all_gather&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">gather_list&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">out&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cat&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">gather_list&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">RowParallelLinear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">in_features&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">out_features&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="fm">__init__&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">rank&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">world_size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_world_size&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">local_in&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">in_features&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">world_size&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">weight&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Parameter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">empty&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">local_in&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">out_features&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x_local&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">chunk&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">world_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)[&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">rank&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">out&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x_local&lt;/span> &lt;span class="o">@&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">weight&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">all_reduce&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">out&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">op&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ReduceOp&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">SUM&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">out&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>接下来是针对 LLM 中使用的 SwiGLU FFN 进行的优化，基于前面的介绍，我们不需要对基于 column linear 进行 all-reduce, 代码如下所示&lt;/p>
&lt;p>SwiGLU&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">torch&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">torch&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">nn&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">torch.nn.functional&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">F&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">torch.distributed&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">dist&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">world_size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">rank&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">ColumnParallelLinear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">in_features&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">out_features&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">assert&lt;/span> &lt;span class="n">out_features&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="n">world_size&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;Output features must be divisible by world size (world_size=&lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">world_size&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">)&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">part_out_features&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">out_features&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="n">world_size&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">weight&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Parameter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">empty&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">part_out_features&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">part_in_features&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">dtype&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">y&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="o">@&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">weight&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">y&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">RowParallelLinear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">in_features&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">out_features&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">assert&lt;/span> &lt;span class="n">in_features&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="n">world_size&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;Input features must be divisible by world size (world_size=&lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">world_size&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">)&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">part_in_features&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">in_features&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="n">world_size&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">weight&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Parameter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">empty&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">out_features&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">part_in_features&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">dtype&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">y&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="o">@&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">weight&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">world_size&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">all_reduce&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">y&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">MLP&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">inter_dim&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="fm">__init__&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">w1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ColumnParallelLinear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">inter_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">w2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">RowParallelLinear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">inter_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">w3&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ColumnParallelLinear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">inter_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">w2&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">F&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">silu&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">w1&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">w3&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>attention&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;span class="lnt">49
&lt;/span>&lt;span class="lnt">50
&lt;/span>&lt;span class="lnt">51
&lt;/span>&lt;span class="lnt">52
&lt;/span>&lt;span class="lnt">53
&lt;/span>&lt;span class="lnt">54
&lt;/span>&lt;span class="lnt">55
&lt;/span>&lt;span class="lnt">56
&lt;/span>&lt;span class="lnt">57
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">TPMultiHeadAttention&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d_model&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_heads&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">head_dim&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">d_model&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">d_model&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_heads&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">num_heads&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">head_dim&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="n">head_dim&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="kc">None&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="n">d_model&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="n">num_heads&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">assert&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_heads&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="n">world_size&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;num_heads must be divisible by world size&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">assert&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">d_model&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_heads&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;d_model must equal num_heads * head_dim&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># heads of different GPU&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">local_num_heads&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_heads&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="n">world_size&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">local_qkv_dim&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">local_num_heads&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">qkv_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ColumnParallelLinear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">in_features&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">d_model&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">out_features&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">3&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">local_qkv_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">out_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">RowParallelLinear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">in_features&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">d_model&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">out_features&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">d_model&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">scale&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">1.0&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">_split_heads&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">batch_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">seq_len&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># [batch, seq_len, local_num_heads, head_dim]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reshape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">batch_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">seq_len&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">local_num_heads&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># [batch, local_num_heads, seq_len, head_dim]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transpose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">attn_mask&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">batch_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">seq_len&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># [batch, seq_len, local_qkv_dim * 3]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">qkv&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">qkv_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># [batch, seq_len, local_qkv_dim]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">v&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">split&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">qkv&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">local_qkv_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># [batch, local_num_heads, seq_len, head_dim]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_split_heads&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">q&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">k&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_split_heads&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">v&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_split_heads&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">v&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attn_scores&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">q&lt;/span> &lt;span class="o">@&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transpose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">scale&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">attn_mask&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attn_scores&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">attn_scores&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">attn_mask&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attn_weights&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">F&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">softmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">attn_scores&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># [batch, local_num_heads, seq_len, head_dim]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attn_output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">attn_weights&lt;/span> &lt;span class="o">@&lt;/span> &lt;span class="n">v&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># [batch, seq_len, local_num_heads, head_dim]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attn_output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">attn_output&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transpose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># [batch, seq_len, local_qkv_dim]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attn_output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">attn_output&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reshape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">batch_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">seq_len&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">local_qkv_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">gather_list&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">empty_like&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">attn_output&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">world_size&lt;/span>&lt;span class="p">)]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">all_gather&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">gather_list&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">attn_output&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># [batch, seq_len, d_model]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">full_attn_output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cat&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">gather_list&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">out&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">out_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">full_attn_output&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">out&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Embedding&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">ParallelEmbedding&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vocab_size&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="fm">__init__&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">vocab_size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">vocab_size&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dim&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">dim&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">assert&lt;/span> &lt;span class="n">vocab_size&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="n">world_size&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;Vocabulary size must be divisible by world size (world_size=&lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">world_size&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">)&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">part_vocab_size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">vocab_size&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="n">world_size&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">vocab_start_idx&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">rank&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">part_vocab_size&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">vocab_end_idx&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">vocab_start_idx&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">part_vocab_size&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">weight&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Parameter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">empty&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">part_vocab_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">world_size&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">mask&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">vocab_start_idx&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">|&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span> &lt;span class="o">&amp;gt;=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">vocab_end_idx&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">vocab_start_idx&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">mask&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">y&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">F&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">embedding&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">weight&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">world_size&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">y&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">mask&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">all_reduce&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">y&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者提出了针对 transformer 架构的 tensor parallelism 策略来提高整体的训练效率，通过在训练过程加入四次 all-reduce 通信我们就可以训练更大规模的模型。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://github.com/NVIDIA/Megatron-LM/tree/main#megatron-lm--megatron-core" target="_blank" rel="noopener"
>Megatron-LM &amp;amp; Megatron Core Github&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/1909.08053" target="_blank" rel="noopener"
>Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/inference/model.py" target="_blank" rel="noopener"
>DeepSeek-V3&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Gated Attention</title><link>https://maosong.website/p/notes-on-gated-attention/</link><pubDate>Tue, 20 Jan 2026 15:41:52 +0800</pubDate><guid>https://maosong.website/p/notes-on-gated-attention/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>现有的大部分模型都基于 Transformer 提出的 softmax attention (SDPA), 虽然也有相关的改进工作，但是主要集中于降低 attention 计算复杂度，提高 attention 在推理时的内存使用效率等。之前的工作提出了关于 attention 的两个问题：&lt;/p>
&lt;ol>
&lt;li>attention sink, 即模型的注意力会放在初始几个 token 上, 这限制了模型的上下文扩展能力&lt;/li>
&lt;li>massive activation, 少部分 token 的 hidden states 会非常大，这限制了模型的训练稳定性&lt;/li>
&lt;/ol>
&lt;p>在本文中，作者通过在 attention 中加入 gating 机制来探索 gating 对模型表现和训练稳定性的影响。尽管 gating 并没有降低 attention 计算复杂度，但是 gating 提出了一个新的视角，即 sparity 与 attention sink 和 massive activation 息息相关，这为后面 sparse attention 的研究提供了 Insight.&lt;/p>
&lt;p>作者发现，对 Multi head attention 的输出进行 head-specific gating 的效果最好，并且这种方式还可以提高训练稳定性，模型的表达能力和长上下文能力。作者还进一步分析了这种 gating 方式更好的原因，发现有两点：&lt;/p>
&lt;ol>
&lt;li>non-linearity: 通过 gating 可以有效提高 output projection layer 输入的秩，进而提高表达能力&lt;/li>
&lt;li>sparsity: gating 可以降低 massive activation 和 attention sink 的影响&lt;/li>
&lt;/ol>
&lt;p>作者最终推荐使用 element-wise SDPA gating 方式来进行训练&lt;/p>
&lt;h2 id="related-work">&lt;a href="#related-work" class="header-anchor">&lt;/a>Related Work
&lt;/h2>&lt;p>作者主要介绍了 gating 和 attention sink 这两部分的工作。&lt;/p>
&lt;p>gating 早在 LSTM 和 GRU 使其就得到了广泛的运用，在 transformer 之后，相关的现行注意力也有应用，比如 &lt;a class="link" href="https://maosong.website/p/notes-on-minimax-01/" target="_blank" rel="noopener"
>MiniMax-01&lt;/a> 所使用的 Lightning Attention 等，但是这些工作没有系统性探究 gating 背后的机制。&lt;/p>
&lt;p>第二部分是 attention sink, attention sink 现象由 &lt;a class="link" href="https://maosong.website/p/notes-on-streamingllm/" target="_blank" rel="noopener"
>StreamingLLM&lt;/a> 提出， 即模型会将相当一部分注意力权重方开始开始的几个 token 上。而本文提出的 gating 机制可以缓解 attention sink 现象。&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;p>首先是标准 MHA 定义：&lt;/p>
$$
\begin{aligned}
Q &amp;= XW_Q, K=XW_K, V=XW_V\\
\mathrm{Attn}_i(Q,K,V) &amp;= \mathrm{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V, i=1,\dots,h\\
\mathrm{MHA}(Q, K, V) &amp;= \mathrm{Concat}([\mathrm{Attn}_1,\dots,\mathrm{Attn}_h])\\
O &amp;= \mathrm{MHA}(Q, K, V) W_O
\end{aligned}
$$&lt;p>这里 $X\in\mathbb{R}^{n\times d}$ 是 transformer layer pre-normalization 的输出（或者 attention block 的输入）, $n$ 是 sequence length, $d$ 是 hidden size, $h$ 是 number of heads, $d_k$ 是 head dimension.&lt;/p>
&lt;p>接下来，作者介绍了不同的 gating 策略。这里作者用同一的公式来进行表示&lt;/p>
$$
Y' = g(Y,X,W_\theta, \sigma) = Y\odot \sigma(XW_\theta)
$$&lt;p>这里 $Y$ 是输入， $X$ 是 attention 的输入，$W_\theta$ 是可学习权重&lt;/p>
&lt;p>&lt;strong>Position&lt;/strong>
首先是位置，作者考虑了如下几种变体：&lt;/p>
$$
\begin{align}
\mathrm{MHA}(Q, K, V)' &amp;= \mathrm{MHA}(Q, K, V)\odot \sigma\left(X W_\theta)\right) \tag{G1}\\
Q' &amp;= Q\odot \sigma\left(XW_\theta\right) \tag{G2}\\
K' &amp;= K\odot \sigma\left(XW_\theta\right) \tag{G3}\\
V' &amp;= V\odot \sigma\left(XW_\theta\right) \tag{G4}\\
O' &amp;= O\odot \sigma\left(XW_\theta\right) \tag{G5}\\
\end{align}
$$&lt;p>这里 $\sigma$ 是激活函数，$W_\theta$ 是激活函数的可学习参数，我们可以将其理解为一个 linear layer, 即当前模块的输出取决于输入 hidden sates 经过一个线性层和激活层之后的结果，相似的做法还有 &lt;a class="link" href="https://maosong.website/p/moe-tutorial/" target="_blank" rel="noopener"
>MoE&lt;/a> 中的 gating layer, &lt;a class="link" href="https://maosong.website/p/notes-on-nsa/" target="_blank" rel="noopener"
>NSA&lt;/a> 中的 gating layer 等。对应的示意图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gated-attention/Gated-attention-position.png"
width="517"
height="565"
loading="lazy"
alt="Positions of different gating methods"
class="gallery-image"
data-flex-grow="91"
data-flex-basis="219px"
>&lt;/p>
&lt;p>&lt;strong>granularity&lt;/strong>
作者设计了不同粒度的 gating（假设输入为 $X\in\mathbb{R}^{n\times h\times d_k}$）：&lt;/p>
&lt;ol>
&lt;li>head-shared: 不同 head 共享 gating score, &lt;code>Y'[i,h,k]=gate[i,k]*Y[i,h,k]&lt;/code>&lt;/li>
&lt;li>head-wise: 同一个 head 共享 gating score, &lt;code>Y'[i,h,:]=gate[i,h]*Y[i,h,:]&lt;/code>&lt;/li>
&lt;li>element-wise: 不同元素不共享 gating score, &lt;code>Y'[i,h,k]=gate[i,h,k]*Y[i,h,k]&lt;/code>&lt;/li>
&lt;/ol>
&lt;p>从 attention 的角度看，不同 head 本身就承担不同的语义子空间，如果强行共享 gating，会破坏这种分工。&lt;/p>
&lt;p>&lt;strong>format&lt;/strong>
作者还构建了 multiplication 和 addition 两种形式：&lt;/p>
&lt;ol>
&lt;li>multiplication: $Y'=Y\odot \sigma(XW_\theta)$&lt;/li>
&lt;li>addition: $Y'=Y+\sigma(XW_\theta)$&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>activation function&lt;/strong>
本文中作者使用了 SiLU 和 sigmoid 两种形式，即&lt;/p>
$$
\sigma_{\mathrm{sigmoid}}(x) = \frac{1}{1+e^{-x}},\quad \sigma_{\mathrm{SiLU}} = x*\sigma_{\mathrm{sigmoid}}(x)=\frac{x}{1+e^{-x}}
$$&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>作者构建了三个模型进行实验，模型配置如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>1.7B-28 layers&lt;/th>
&lt;th>1.7B-48 layers&lt;/th>
&lt;th>15B-A2.4B MoE&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Layers&lt;/td>
&lt;td>28&lt;/td>
&lt;td>48&lt;/td>
&lt;td>24&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>query heads&lt;/td>
&lt;td>16&lt;/td>
&lt;td>16&lt;/td>
&lt;td>32&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>key/value heads&lt;/td>
&lt;td>8&lt;/td>
&lt;td>8&lt;/td>
&lt;td>4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>head dim&lt;/td>
&lt;td>128&lt;/td>
&lt;td>128&lt;/td>
&lt;td>128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>tie embedding&lt;/td>
&lt;td>yes&lt;/td>
&lt;td>yes&lt;/td>
&lt;td>no&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>QK normalization&lt;/td>
&lt;td>yes&lt;/td>
&lt;td>yes&lt;/td>
&lt;td>yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>hidden size&lt;/td>
&lt;td>2048&lt;/td>
&lt;td>1536&lt;/td>
&lt;td>2048&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ffn hidden size&lt;/td>
&lt;td>6144&lt;/td>
&lt;td>4608&lt;/td>
&lt;td>768&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>experts&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>top-K&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>首先是不同 gating 方法对 MoE model 影响，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gated-attention/Gated-attention-gating-variant-performance.png"
width="1292"
height="847"
loading="lazy"
alt="Performance of different gating variants"
class="gallery-image"
data-flex-grow="152"
data-flex-basis="366px"
>&lt;/p>
&lt;p>结论如下：&lt;/p>
&lt;ol>
&lt;li>对 SDPA 的输出 (G1) 或者 value (G2) 进行 gating 效果最好&lt;/li>
&lt;li>head-specific gating 效果更好&lt;/li>
&lt;li>multiplication 效果比 addition 效果更好&lt;/li>
&lt;li>sigmoid 效果比 SiLU 效果更好&lt;/li>
&lt;/ol>
&lt;p>总的来说，position 对最终结果提升最明显，其次是 granularity 和 activation function.&lt;/p>
&lt;p>接下来是不同 gating 方法对 dense model 的影响，作者构建了两个 dense 模型，参数都是 1.7B, 这两个模型的 layers 和 FFN hidden size 不同（通过调整保持总参数一致）。作者对比了 G1 和 baseline 的表现， 结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gated-attention/Gated-attention-dense-model-performance.png"
width="1290"
height="750"
loading="lazy"
alt="Performance of dense models with Gated attention"
class="gallery-image"
data-flex-grow="172"
data-flex-basis="412px"
>&lt;/p>
&lt;p>结论验证了 gating 机制可以有效提高模型的表现。作者还发现使用 gating 之后，模型的训练也更加稳定，训练的损失变化曲线如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gated-attention/Gated-attention-training-loss-curve.png"
width="387"
height="515"
loading="lazy"
alt="training loss curve of gated attention"
class="gallery-image"
data-flex-grow="75"
data-flex-basis="180px"
>&lt;/p>
&lt;h2 id="analysis">&lt;a href="#analysis" class="header-anchor">&lt;/a>Analysis
&lt;/h2>&lt;p>首先，作者对 multi head attention 进行了重写，得到如下形式&lt;/p>
$$
o_i^k = \sum_{j=1}^i\left(S_{ij}^k X_jW_V^k\right)W_O^k = \sum_{j=1}^i S_{ij}^k X_j(W_V^kW_O^k)
$$&lt;p>也就是说，$W_K$ 和 $W_O$ 可以吸收到一起，由于 $W_V^j\in\mathbb{R}^{d\times d_k}$, $W_O^k\in\mathbb{R}^{d_k\times d}$, 从而 $\mathrm{rank}(W_V^jW_O^k)\leq \max(\mathrm{rank}(W_V^j), \mathrm{rank}(W_O^k))\leq d_k$. 对于 &lt;a class="link" href="https://maosong.website/p/notes-on-gqa/" target="_blank" rel="noopener"
>GQA&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/notes-on-mqa/" target="_blank" rel="noopener"
>MQA&lt;/a>, 最终的有效秩会进一步降低。&lt;/p>
&lt;p>而使用本文提到的 G1 和 G2 gating 策略之后，我们相当于是通过非线性机制提高了上面的秩，进而解决了 softmax attention 表达能力不足的问题, 实际上，StepFun 的 &lt;a class="link" href="https://maosong.website/p/notes-on-mfa/" target="_blank" rel="noopener"
>MFA&lt;/a> 也是类似的思想。下面是 G1 和 G2 做的改进：&lt;/p>
$$
\begin{align}
o_i^k &amp;= \sum_{j=1}^i\left(S_{ij}^k \mathrm{gating}(X_jW_V^k)\right)W_O^k\tag{G1}\\
o_i^k &amp;= \mathrm{gating}\left(\sum_{j=1}^iS_{ij}^k X_jW_V^k\right)W_O^k \tag{G2}
\end{align}
$$&lt;p>通过 gating 的非线性机制，我们提高的矩阵的秩，进而提高了模型的表达能力，而 G5 提升有限的原因也在于此。实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gated-attention/Gated-attention-non-linearity-performance.png"
width="957"
height="261"
loading="lazy"
alt="Performance of different non-linearity variants"
class="gallery-image"
data-flex-grow="366"
data-flex-basis="880px"
>&lt;/p>
&lt;p>可以看到，不同的 non-linearity 方法对模型表现都有提升，这验证了矩阵秩会影响模型表达能力的分析。&lt;/p>
&lt;p>接下来，作者探究了 gating 机制对 attention score distribution 的影响，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gated-attention/Gated-attention-score-distribution.png"
width="1283"
height="408"
loading="lazy"
alt="attention score distribution of different methods"
class="gallery-image"
data-flex-grow="314"
data-flex-basis="754px"
>&lt;/p>
&lt;p>实验结果说明：&lt;/p>
&lt;ol>
&lt;li>有效的 gating 机制对应的 attention score 是非常稀疏的&lt;/li>
&lt;li>head-specific sparsity 非常重要，当在不同的 head 共享 gating 时，模型表现会有所下降&lt;/li>
&lt;li>gating 必须与 query 相关，与 G2 先比，G1 的表现更好，这说明 gating score 更依赖于 query. 作者认为基于当前 query token 构建 gating, 可以有效过滤历史 token 的噪音信息&lt;/li>
&lt;li>non-sparse gating 效果比较差，作者构建了一个 non-sparse 版本的 sigmoid, 结果发现模型表现非常差，这说明了 attention score 应该是一个稀疏形式&lt;/li>
&lt;/ol>
&lt;p>通过前面的分析和实验结果，作者认为 gating 机制还可以缓解 attention sink 现象，作者对 baseline 以及 G1 两种方法的 attention 分布进行了可视化，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gated-attention/Gated-attention-sink-visualization.png"
width="1263"
height="721"
loading="lazy"
alt="Visualization of attention sink"
class="gallery-image"
data-flex-grow="175"
data-flex-basis="420px"
>&lt;/p>
&lt;p>实验结果整理如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>method&lt;/th>
&lt;th>massive activation&lt;/th>
&lt;th>attention sink&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>baseline&lt;/td>
&lt;td>high&lt;/td>
&lt;td>high&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>input-independence&lt;/td>
&lt;td>high&lt;/td>
&lt;td>high&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>head-shared gating&lt;/td>
&lt;td>low&lt;/td>
&lt;td>high&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>head-specific gating&lt;/td>
&lt;td>low&lt;/td>
&lt;td>low&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>因此，作者的结论为，input-dependent, head-specific gating 可以提高 attention score distribution 的 sparsity, 进而减缓 attention sink. 并且引入 spaisity 之后，我们还可以避免 massive activation, 进而使用更低的精度进行训练。&lt;/p>
&lt;p>最后，作者探究了以下 gating 机制的上下文扩展能力，作者在已有的模型上基于 32k 上下文长度使用了 80B token 进行 continue pre-training, 然后使用 &lt;a class="link" href="https://maosong.website/p/notes-on-yarn/" target="_blank" rel="noopener"
>YARN&lt;/a> 将模型上下文长度扩展到了 128K。 测试的结果如下图所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Method&lt;/th>
&lt;th>4k&lt;/th>
&lt;th>8k&lt;/th>
&lt;th>16k&lt;/th>
&lt;th>32k&lt;/th>
&lt;th>64k&lt;/th>
&lt;th>128k&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Baseline&lt;/td>
&lt;td>88.89&lt;/td>
&lt;td>85.88&lt;/td>
&lt;td>83.15&lt;/td>
&lt;td>79.50&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SDPA-Gate&lt;/td>
&lt;td>90.56&lt;/td>
&lt;td>87.11&lt;/td>
&lt;td>84.61&lt;/td>
&lt;td>79.77&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>YaRN Extended&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Baseline&lt;/td>
&lt;td>82.90 (-6.0)&lt;/td>
&lt;td>71.52 (-14.4)&lt;/td>
&lt;td>61.23 (-21.9)&lt;/td>
&lt;td>37.94 (-41.56)&lt;/td>
&lt;td>37.51&lt;/td>
&lt;td>31.65&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SDPA-Gate&lt;/td>
&lt;td>88.13 (-2.4)&lt;/td>
&lt;td>80.01 (-7.1)&lt;/td>
&lt;td>76.74 (-7.87)&lt;/td>
&lt;td>72.88 (-6.89)&lt;/td>
&lt;td>66.60&lt;/td>
&lt;td>58.82&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，对于短上下文，虽然两者表现都有所下降，但是本文提出的 gating 表现下降程度较小。而对于长上下文，本文提出的 gating 机制效果明显更好。作者分析原因认为这是由于 softmax attention 倾向于退化为对少数 token 的依赖， 而 gating 通过引入 token-level sparsity，避免了这种路径依赖。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>在本文中，作者系统性探究了 attention 中的 gating 机制，包括 gating 对模型表现，训练稳定性以及训练动态的影响。作者发现，通过提高 non-linearity 和 sparsity 我们可以有效提高模型的上下文能力以及减缓 attention sink 现象。&lt;/p>
&lt;p>从更高层次看，本文的结果可以总结为一点：&lt;/p>
&lt;blockquote>
&lt;p>attention 的问题不在于 softmax 本身，而在于线性 aggregation 的表达上限与缺乏选择性。而 gating 提供了一种几乎零成本、却极其有效的方式来引入非线性与稀疏性。&lt;/p>
&lt;/blockquote>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://openreview.net/forum?id=1b7whO4SfY" target="_blank" rel="noopener"
>Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="appendix">&lt;a href="#appendix" class="header-anchor">&lt;/a>Appendix
&lt;/h2>&lt;p>作者在附录中还进一步分析了 massive activation 以及 attention sink.&lt;/p>
&lt;ol>
&lt;li>massive activation 并不是 attention sink 产生的必要原因，并且 sparsity 可以减缓这一现象&lt;/li>
&lt;li>head-specific gating 会提升 gating score 的值，因此不同的 head 需要安排不同的 sparsity&lt;/li>
&lt;li>并不能通过 clipping 的方式来提高训练稳定性&lt;/li>
&lt;li>在 continue pre-training 阶段加入 gating 机制并不能提高模型的表现&lt;/li>
&lt;/ol></description></item><item><title>NextFlow 基于single-branch的统一理解与生成多模态大模型</title><link>https://maosong.website/p/nextflow-%E5%9F%BA%E4%BA%8Esingle-branch%E7%9A%84%E7%BB%9F%E4%B8%80%E7%90%86%E8%A7%A3%E4%B8%8E%E7%94%9F%E6%88%90%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B/</link><pubDate>Sat, 17 Jan 2026 17:31:53 +0800</pubDate><guid>https://maosong.website/p/nextflow-%E5%9F%BA%E4%BA%8Esingle-branch%E7%9A%84%E7%BB%9F%E4%B8%80%E7%90%86%E8%A7%A3%E4%B8%8E%E7%94%9F%E6%88%90%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>现在的统一理解与生成多模态大模型面临的主要问题是难以解决语义特征和图像特征之间不匹配性。diffusion model 可以较好学习图像特征，但是缺乏了对于高阶语义特征的理解和推理能力，反之 LLM 可以比较好利用高阶语义特征，而代价则是难以处理图片细节。&lt;/p>
&lt;p>为了解决这个问题，目前的主流做法是分流，分流策略有两种：&lt;/p>
&lt;ol>
&lt;li>类似 BAGEL 在 transformer 架构上进行分流，分别处理图像模态以及文本模态特征，这也是自 BAGEL 以来比较常用的一种做法&lt;/li>
&lt;li>输入输出端进行分流，代表性工作有 Transfusion, 这种策略使用 encoder 将图片映射到文本空间，然后对输出进行解码，通过使用图片生成的目标函数，我们可以保证图片生成的质量&lt;/li>
&lt;/ol>
&lt;p>尽管基于 transformer 架构分流策略的效果比较好，但是对应地，其增加了整体训练的代价，而且并没有完成深度模态统一的目标；而基于输入输出分流的方法则因为目标函数不同很容易导致训练不稳定或者损害模型本身的表现。&lt;/p>
&lt;p>在本文中，作者采取的策略为不使用分流策略，避免对模型架构产生比较大的修改。但是，这样就引入了一个新的问题，自回归生成模型的低效率性，目前主流的生成方式为 raster scan next-token prediction, 当图片非常大时，我们要生成的 token 非常多，从而整体的推理效率非常低。作者在这里举例提到 Emu3 生成一张 $1024\times 1024$ 的图片需要 10 分钟。并且，自回归生成模型对应的 tokenizer 往往基于 reconstruction 的目标进行训练，而这种训练目标产生的 token 更关注图片细节，这与 LLM 更关注语义特征并不一致，因此其效果也更差。&lt;/p>
&lt;p>为了解决已有自回归生成模型的问题，作者提出了使用 VAR 提出的 next-scale prediction 策略，通过 next-scale prediction, 我们可以极大程度提高图片生成的效率，作者这里强调了 NextFlow 生成一张 $1024\times 1024$ 的图片仅需要 5 秒，这个效率是 Emu3 的 120 倍。&lt;/p>
&lt;p>接下来就是作者针对提出的架构和方法进行的改进，主要包括：&lt;/p>
&lt;ol>
&lt;li>使用了 6T 数据进行训练&lt;/li>
&lt;li>使用了基于 prefix-tuning 策略的 GRPO 方法来提高模型的 reasoning 能力&lt;/li>
&lt;li>构建了一个基于 diffusion 的 decoder 来对输出图片进行优化&lt;/li>
&lt;/ol>
&lt;p>最后，作者通过实验验证了 NextFlow 的有效性，并且在效率上，相比于基于 MMDiT 架构的模型，NextFlow 使用了更少的算力进行推理。&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;p>NextFlow 的架构图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/nextflow-%E5%9F%BA%E4%BA%8Esingle-branch%E7%9A%84%E7%BB%9F%E4%B8%80%E7%90%86%E8%A7%A3%E4%B8%8E%E7%94%9F%E6%88%90%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B/NextFlow-Architecture.png"
width="1017"
height="979"
loading="lazy"
alt="Architecture of NextFlow"
class="gallery-image"
data-flex-grow="103"
data-flex-basis="249px"
>&lt;/p>
&lt;p>模型架构包括 3 个部分：&lt;/p>
&lt;ol>
&lt;li>tokenizer: NextFlow 的 tokenizer 基于 TokenFlow, TokenFlow 通过使用两个 codebook 来分别提取对应的语义特征和图片特征进而提高 tokenizer 的表达能力&lt;/li>
&lt;li>transformer: NextFlow 使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5-vl/" target="_blank" rel="noopener"
>Qwen2.5-VL&lt;/a> 7B 模型作为 base model, 作者将其 Vision Encoder 替换为了 TokenFlow tokenizer 用于提取视觉特征&lt;/li>
&lt;li>optinal diffusion decoder: 用于进一步优化图片的细节。作者使用 TokenFLow tokenizer 的 token 表示，外接了一个 diffusion model 来优化最终的输出。&lt;/li>
&lt;/ol>
&lt;p>在数据格式上，作者使用了 &lt;code>&amp;lt;boi&amp;gt;&lt;/code>, &lt;code>&amp;lt;eoi&amp;gt;&lt;/code> 来标记图片 token, 然后每个图片通过 TokenFLow 表示为不同 scale 的 token, 如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/nextflow-%E5%9F%BA%E4%BA%8Esingle-branch%E7%9A%84%E7%BB%9F%E4%B8%80%E7%90%86%E8%A7%A3%E4%B8%8E%E7%94%9F%E6%88%90%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B/NextFlow-data-fotmat.png"
width="1394"
height="185"
loading="lazy"
alt="data format of NextFlow"
class="gallery-image"
data-flex-grow="753"
data-flex-basis="1808px"
>&lt;/p>
&lt;p>在位置编码上，作者采用了 Multi-Scale 3D RoPE 策略，第 $t$ 个 token 如果是文本 token, 则表示为 $(t,t,t)$, 如果是图片 token, 则表示为如下形式&lt;/p>
$$
(p_x, p_y, p_s) = \left(\frac{C}{\sqrt{HW}}(i+0.5),\frac{C}{\sqrt{HW}}(j+0.5),s\right)
$$&lt;p>这里 $H, W, C, s$ 分别是 grid 对应的 size, constant range factor 以及 scale. 为了保持数据的一致性，作者还对所有的空间位置进行了归一化，避免在 SFT 时进行外推。&lt;/p>
&lt;p>与 VAR 一致，作者使用了 scale length positional embedding, 即在 scale 层面使用了 Transformer 提出的 Sinusoidal encoding, 作者认为通过显示编码 scale 可以提高模型对于图片精度的认知能力。&lt;/p>
&lt;p>在训练上，作者提出了两个优化策略。&lt;/p>
&lt;p>首先，作者发现，不同 scale 的 token 数量不一致，small scale 对应的 token 数量较少，随着生成 token 数增加，由于 attention 存在局部依赖性（见 &lt;a class="link" href="https://maosong.website/p/notes-on-nsa/" target="_blank" rel="noopener"
>NSA&lt;/a>）模型对于早期 token 关注度较低，为了解决这个问题，作者的做法就是使用 scale-dependent weight, 不同 scale 对应 token 的损失函数权重为&lt;/p>
$$
k_s =\frac{1}{(h_s\times w_s)^\alpha}
$$&lt;p>这里 $\alpha$ 是超参数，通过这种方式，作者保证了模型对于不同 scale token 能够一视同仁。&lt;/p>
&lt;p>接下来，就是训练与推理的不一致性，训练时，模型的上下文是无损的，但是在推理时，由于模型需要基于自己的输出来预测下一个 token, 因此很容易出现误差累积。为了解决这个问题，作者的做法是使用 [[infinity]] 提出的 self-correction 机制，具体做法就是，在 encode 当前 token 时，随机加入一些噪声，制造出有损的上下文然后让模型预测下一个 token, 也就是提高模型对于误差的稳健性。但是，使用这种策略并没有带来理想中的提升，作者分析原因发现是 VAR 的 feature 与输入 feature 存在较大的不一致性，作者的解决方法就是直接使用 residual feature. 通过这种方式模型的能力有了极大的提升&lt;/p>
&lt;h3 id="ablation-study">&lt;a href="#ablation-study" class="header-anchor">&lt;/a>Ablation Study
&lt;/h3>&lt;p>作者在本节探讨了架构设计上的一些细节。&lt;/p>
&lt;p>首先，作者探究了 tokenizer 的训练，原始 TokenFLow tokenizer 使用了衣蛾预训练的 semantic encoder 以及一个从零开始训练的 pixel encoder, 作者发现这种不一致性会损害模型的表现。为了解决这个问题，作者分别从零开始训练 semantic encoder 和 pixel encoder, 确保两者都在一个水平上，然后再进行 TokenFlow 的训练，通过这种多阶段训练方式，我们可以有效提高训练效率以及模型的表现。训练时，作者还随机丢弃了 $50\%$ 的 VAR scale 来提高模型的稳健性。作者进一步对比了 TokenFlow 这种 dual-codebook 和 single-codebook 之间的表现，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/nextflow-%E5%9F%BA%E4%BA%8Esingle-branch%E7%9A%84%E7%BB%9F%E4%B8%80%E7%90%86%E8%A7%A3%E4%B8%8E%E7%94%9F%E6%88%90%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B/NextFlow-tokenizer-design.png"
width="1383"
height="382"
loading="lazy"
alt="Tokenizer comparison"
class="gallery-image"
data-flex-grow="362"
data-flex-basis="868px"
>&lt;/p>
&lt;p>结果显示，尽管 single-codebook 的重建效果更好，但是在下游任务上其表现不如 TokenFlow.&lt;/p>
&lt;p>接下来，作者探讨了 output head, 也就是我们应该分别针对不同的模态设计不同的 output head 还是使用统一的 output head, 作者对比了两种架构，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/nextflow-%E5%9F%BA%E4%BA%8Esingle-branch%E7%9A%84%E7%BB%9F%E4%B8%80%E7%90%86%E8%A7%A3%E4%B8%8E%E7%94%9F%E6%88%90%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B/NextFlow-ablation-output-head.png"
width="1381"
height="410"
loading="lazy"
alt="Ablation study on output head design"
class="gallery-image"
data-flex-grow="336"
data-flex-basis="808px"
>&lt;/p>
&lt;p>实验结果显示，single head 的效果更好。因此作者使用了 single head 设计&lt;/p>
&lt;h3 id="training">&lt;a href="#training" class="header-anchor">&lt;/a>Training
&lt;/h3>&lt;p>训练流程图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/nextflow-%E5%9F%BA%E4%BA%8Esingle-branch%E7%9A%84%E7%BB%9F%E4%B8%80%E7%90%86%E8%A7%A3%E4%B8%8E%E7%94%9F%E6%88%90%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B/NextFlow-training-pipeline.png"
width="1385"
height="600"
loading="lazy"
alt="Training Pipeline of NextFlow"
class="gallery-image"
data-flex-grow="230"
data-flex-basis="554px"
>&lt;/p>
&lt;p>在 alignment 时，作者使用 TokenFlow tokenizer 替换 Qwen2.5-VL 7B 的 ViT, 作者同时训练 connector (MLP) 和 output head, 冻结其他参数，这个阶段使用了 &lt;strong>10M&lt;/strong> image-text pairs.&lt;/p>
&lt;p>pre-training 时，作者采用了三阶段训练策略，图片数据的精度分别为 256, 512, 1024. 共使用了 6T token.&lt;/p>
&lt;ul>
&lt;li>256-level: 2B 文生图样本，纯文本数据，以及 147M 图文交错数据&lt;/li>
&lt;li>512-level: 提高模型的细节生成能力，使用了 scale-dependent weight 策略&lt;/li>
&lt;li>1024-level: 使用了 40M 高质量样本&lt;/li>
&lt;/ul>
&lt;p>continue pre-training 和 SFT 阶段，作者分别使用了美学相关的数据和对话数据来提高模型图片生成的质量和指令跟随能力。&lt;/p>
&lt;p>RL 阶段，由于早期的 token 对最终图片生成更加重要，作者使用了一个 prefix-tuning 策略，也就是我们只计算 image token sequence 中前 $m$ 个 scale 对应的 token, 通过这种方式我们提高训练速度以及保证模型生成的质量。训练目标函数如下所示&lt;/p>
$$
L_{GRPO}(\theta) = \mathbb{E}_{\mathbf{c} \sim \mathcal{C},\{\mathbf{s}_t^i\}_{i=1}^G \sim \pi_\theta} \frac{1}{G} \sum_{t=1}^m k_t \min\left( \frac{p_\theta(\mathbf{s}_{t+1}^i|\mathbf{s}_t^i,\mathbf{c})}{p_{\theta_{old}}(\mathbf{s}_{t+1}^i|\mathbf{s}_t^i,\mathbf{c})} A_i, \, \text{clip}\left( \frac{p_\theta(\mathbf{s}_{t+1}^i|\mathbf{s}_t^i,\mathbf{c})}{p_{\theta_{old}}(\mathbf{s}_{t+1}^i|\mathbf{s}_t^i,\mathbf{c})}, \, 1-\epsilon, \, 1+\epsilon \right) A_i \right) - \beta D_{KL}(\pi_\theta, \pi_{ref})
$$&lt;p>最终，training recipe 如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/nextflow-%E5%9F%BA%E4%BA%8Esingle-branch%E7%9A%84%E7%BB%9F%E4%B8%80%E7%90%86%E8%A7%A3%E4%B8%8E%E7%94%9F%E6%88%90%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B/NextFlow-training-recipe.png"
width="1390"
height="331"
loading="lazy"
alt="Training Recipe of NextFlow"
class="gallery-image"
data-flex-grow="419"
data-flex-basis="1007px"
>&lt;/p>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>我们主要关注 NextFlow 和 Qwen-Image, Seedream 3.0 的表现对比情况，结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Benchmark&lt;/th>
&lt;th>NextFlow&lt;/th>
&lt;th>NextFlow-RL&lt;/th>
&lt;th>Qwen-Image&lt;/th>
&lt;th>Seedream 3.0&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>DPG&lt;/td>
&lt;td>86.00&lt;/td>
&lt;td>&lt;strong>88.32&lt;/strong>&lt;/td>
&lt;td>88.32&lt;/td>
&lt;td>88.27&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GenEval&lt;/td>
&lt;td>0.83&lt;/td>
&lt;td>0.84&lt;/td>
&lt;td>&lt;strong>0.87&lt;/strong>&lt;/td>
&lt;td>0.84&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>WISE&lt;/td>
&lt;td>0.59&lt;/td>
&lt;td>&lt;strong>0.62&lt;/strong>&lt;/td>
&lt;td>0.62&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>PRISM-Bench&lt;/td>
&lt;td>74.7&lt;/td>
&lt;td>78.8&lt;/td>
&lt;td>&lt;strong>79.9&lt;/strong>&lt;/td>
&lt;td>79.6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ImgEdit&lt;/td>
&lt;td>4.44&lt;/td>
&lt;td>4.49&lt;/td>
&lt;td>4.27&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 NextFlow, 一个基于自回归的统一理解与生成模型，NextFlow 使用了 TokenFlow 解决了单一 tokenizer 不能同时提取语义信息和像素信息的缺点，使用了 VAR 解决了自回归图片生成范式效率低的问题。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2601.02204" target="_blank" rel="noopener"
>NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>State of AI--从OpenRouter 100T token使用情况了解AI 大模型能力分层竞争逻辑</title><link>https://maosong.website/p/state-of-ai--%E4%BB%8Eopenrouter-100t-token%E4%BD%BF%E7%94%A8%E6%83%85%E5%86%B5%E4%BA%86%E8%A7%A3ai-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%83%BD%E5%8A%9B%E5%88%86%E5%B1%82%E7%AB%9E%E4%BA%89%E9%80%BB%E8%BE%91/</link><pubDate>Sat, 17 Jan 2026 17:04:07 +0800</pubDate><guid>https://maosong.website/p/state-of-ai--%E4%BB%8Eopenrouter-100t-token%E4%BD%BF%E7%94%A8%E6%83%85%E5%86%B5%E4%BA%86%E8%A7%A3ai-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%83%BD%E5%8A%9B%E5%88%86%E5%B1%82%E7%AB%9E%E4%BA%89%E9%80%BB%E8%BE%91/</guid><description>&lt;h2 id="介绍">&lt;a href="#%e4%bb%8b%e7%bb%8d" class="header-anchor">&lt;/a>介绍
&lt;/h2>&lt;p>本报告基于 OpenRouter 2024 年 11 月 - 2025 年 11 月 100T token 调用数据，从模型、任务、用户维度分析 AI 大模型使用特征，核心结论如下：&lt;/p>
&lt;p>&lt;strong>模型维度&lt;/strong>&lt;/p>
&lt;ol>
&lt;li>&lt;strong>市场格局&lt;/strong>：闭源模型占 70% token 使用量，主导高价值、高稳定性场景；开源模型占 30%，聚焦低成本、场景化需求，其中中国开源模型占比持续上升，但开源市场因竞争呈现碎片化（2025 年底无单一开源模型占比超 25%）。&lt;/li>
&lt;li>&lt;strong>模型偏好&lt;/strong>：不同闭源模型形成差异化优势（Anthropic 擅长复杂推理 / 代码、Google 偏向通用翻译 / 知识问答、xAI/Qwen 专注编程等）。&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>任务维度&lt;/strong>&lt;/p>
&lt;ol>
&lt;li>&lt;strong>核心需求&lt;/strong>：编程类任务 token 占比超 50%，Anthropic 占该领域 60% 以上份额，且编程任务输入长度是其他类别 3 倍以上；但 90% 编程需求依赖闭源模型，开源模型存在能力短板。&lt;/li>
&lt;li>&lt;strong>场景特征&lt;/strong>：角色扮演是开源模型第一大任务（占比超 50%），其在该场景 token 占比（43%）接近闭源模型（42%）且持续上升；agentic inference（工具调用推理）场景 token 占比超 60%，Claude 系列主导该领域。&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>用户维度&lt;/strong>&lt;/p>
&lt;ol>
&lt;li>&lt;strong>核心逻辑&lt;/strong>：能力优于成本，企业愿为强能力模型支付溢价（价格弹性低，降价 10% 仅带来 0.5~0.7% 使用率增长）；免费开源模型仅达 “可用” 水平，因无法落地到实际工作流程难以形成竞争力；小模型数量占比下降，反映市场对模型能力要求提升。&lt;/li>
&lt;li>&lt;strong>留存逻辑（水晶鞋效应）&lt;/strong>：用户留存由 “能力拐点”（首次解决未被满足的长尾需求）驱动，如 Gemini 2.5 Pro、Claude 4 Sonnet 实现能力突破后 5 个月留存率仍达 40%；缺乏能力突破的模型留存率极差，且 “水晶鞋时刻” 窗口狭窄。&lt;/li>
&lt;/ol>
&lt;h2 id="background">&lt;a href="#background" class="header-anchor">&lt;/a>Background
&lt;/h2>&lt;p>分析使用的 100T token 基于 OpenRouter 从&lt;strong>2024 年 11 月到 2025 年 11 月&lt;/strong>的模型使用情况。&lt;/p>
&lt;p>对于模型，作者将模型分为三类， 分别是 Proprietary, Chinese Open Sourced (Chinese OSS), Rest-of-World (RoW) open source models&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>category&lt;/th>
&lt;th>examples&lt;/th>
&lt;th>description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Proprietary&lt;/td>
&lt;td>Claude, GPT-5&lt;/td>
&lt;td>闭源商业模型&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Chinese OSS&lt;/td>
&lt;td>DeepSeek-V3, Qwen&lt;/td>
&lt;td>中文开源模型&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RoW OSS&lt;/td>
&lt;td>mistral, LLaMA&lt;/td>
&lt;td>其他国家开源模型&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>作者基于 metadata 和 GoogleTagClassifier 将任务分为以下类别&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>category&lt;/th>
&lt;th>sub-category&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Programming&lt;/td>
&lt;td>- Computers &amp;amp; Electronics&lt;br>- Programming&lt;br>- Science&lt;br>- Computer Science&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Roleplay&lt;/td>
&lt;td>- Games&lt;br>- Roleplaying Games&lt;br>- Adult&lt;br>- Arts &amp;amp; Entertainment&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Translation&lt;/td>
&lt;td>- Reference&lt;br>- Language Resources&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>General Q&amp;amp;A / Knowledge&lt;/td>
&lt;td>- Reference&lt;br>- General Reference&lt;br>- News&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Productivity/Writing&lt;/td>
&lt;td>- Computers &amp;amp; Electronics&lt;br>- Software&lt;br>- Business &amp;amp; Productivity Software&lt;br>- Business &amp;amp; Industrial&lt;br>- Business Services&lt;br>- Writing &amp;amp; Editing Services&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Education&lt;/td>
&lt;td>- Jobs &amp;amp; Education&lt;br>- Education&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Literature/Creative Writing&lt;/td>
&lt;td>- Books &amp;amp; Literature&lt;br>- narrative leaves under &lt;br>- Arts &amp;amp; Entertainment&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="model">&lt;a href="#model" class="header-anchor">&lt;/a>Model
&lt;/h2>&lt;p>首先是开源模型与闭源模型的对比，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/state-of-ai--%E4%BB%8Eopenrouter-100t-token%E4%BD%BF%E7%94%A8%E6%83%85%E5%86%B5%E4%BA%86%E8%A7%A3ai-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%83%BD%E5%8A%9B%E5%88%86%E5%B1%82%E7%AB%9E%E4%BA%89%E9%80%BB%E8%BE%91/State-AI-open-vs-closed.png"
width="1358"
height="654"
loading="lazy"
alt="Open v.s. Closed source model split"
class="gallery-image"
data-flex-grow="207"
data-flex-basis="498px"
>&lt;/p>
&lt;p>结果显示，开源模型的 token 占比在 $30\%$ 左右，并且中文开源模型 token 占比在持续上升。作者分析认为，闭源模型有着更好的表现以及稳定性，而开源模型的透明程度，成本控制以及可定制化更好。&lt;/p>
&lt;p>在闭源模型中，作者对比了不同模型在不同任务上的使用情况，得出了模型偏好如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Provider&lt;/th>
&lt;th>preference&lt;/th>
&lt;th>description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Anthropic&lt;/td>
&lt;td>- Programming&lt;br>- Technology&lt;/td>
&lt;td>擅长推理，代码，复杂任务，领域专家&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Google&lt;/td>
&lt;td>- Translation&lt;br>- Science&lt;br>- Technology&lt;br>- General Knowledge&lt;/td>
&lt;td>各个任务都没有短板，通用信息引擎&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>xAI&lt;/td>
&lt;td>programming&lt;/td>
&lt;td>专注编程，程序员专用&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenAI&lt;/td>
&lt;td>- Science&lt;br>- Programming&lt;br>- Technology&lt;/td>
&lt;td>介于 Anthropic 和 Google 之间，更人性化&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DeepSeek&lt;/td>
&lt;td>- roleplay&lt;/td>
&lt;td>日常对话，面向消费端&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen&lt;/td>
&lt;td>- programming&lt;/td>
&lt;td>专注编程，适用范围比 Anthropic 广&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>在开源模型中，DeepSeek 占了 $42.51\%$，模型 token 使用情况对比如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Provider&lt;/th>
&lt;th># Tokens (T)&lt;/th>
&lt;th>ratio (%)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>DeepSeek&lt;/td>
&lt;td>14.37&lt;/td>
&lt;td>42.51&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen&lt;/td>
&lt;td>5.59&lt;/td>
&lt;td>16.54&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Meta LLaMA&lt;/td>
&lt;td>3.96&lt;/td>
&lt;td>11.72&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Mistral AI&lt;/td>
&lt;td>2.92&lt;/td>
&lt;td>8.64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenAI&lt;/td>
&lt;td>1.65&lt;/td>
&lt;td>4.88&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Minimax&lt;/td>
&lt;td>1.26&lt;/td>
&lt;td>3.73&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Z-AI&lt;/td>
&lt;td>1.18&lt;/td>
&lt;td>3.49&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TNGTech&lt;/td>
&lt;td>1.13&lt;/td>
&lt;td>3.34&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MoonshotAI&lt;/td>
&lt;td>0.92&lt;/td>
&lt;td>2.72&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Google&lt;/td>
&lt;td>0.82&lt;/td>
&lt;td>2.43&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>但是到 25 年底，因为竞争太强，已经不存在单一模型占比超过 $25\%$, 下面是开源模型 token 使用随时间变化情况&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/state-of-ai--%E4%BB%8Eopenrouter-100t-token%E4%BD%BF%E7%94%A8%E6%83%85%E5%86%B5%E4%BA%86%E8%A7%A3ai-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%83%BD%E5%8A%9B%E5%88%86%E5%B1%82%E7%AB%9E%E4%BA%89%E9%80%BB%E8%BE%91/State-AI-token-usage-over-time.png"
width="1352"
height="516"
loading="lazy"
alt="Token usage of OSS model over time"
class="gallery-image"
data-flex-grow="262"
data-flex-basis="628px"
>&lt;/p>
&lt;p>接下来，作者将模型分为 Large (&amp;gt; 70B), Medium (&amp;gt; 15B, &amp;lt; 70B) 以及 Small (&amp;lt; 15B) 三个区间，分析了各自的使用情况。结果发现，整体趋势为，Small 区间的模型数量占比正在减少，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/state-of-ai--%E4%BB%8Eopenrouter-100t-token%E4%BD%BF%E7%94%A8%E6%83%85%E5%86%B5%E4%BA%86%E8%A7%A3ai-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%83%BD%E5%8A%9B%E5%88%86%E5%B1%82%E7%AB%9E%E4%BA%89%E9%80%BB%E8%BE%91/State-AI-num-oss-models.png"
width="1350"
height="529"
loading="lazy"
alt="evolution of the number of OSS models"
class="gallery-image"
data-flex-grow="255"
data-flex-basis="612px"
>&lt;/p>
&lt;p>token 使用这方面，由于小模型本地部署较多，因此报告结果存在一定偏差性。&lt;/p>
&lt;p>作者还对比了不同模型的价格与 token 使用情况，作者根据中位数 $0.73\$$ per 1M tokens 来将模型分为了四类：&lt;/p>
&lt;ol>
&lt;li>Premium Workloads (high-cost, high-usage): technology, science 等任务&lt;/li>
&lt;li>Mass-Market Volum Drivers (low-cost, high-usage): programming, roleplay 等任务&lt;/li>
&lt;li>Specialized Experts (high-cost, low-usage): finance, academia, health 等任务&lt;/li>
&lt;li>Niche Utilities (low-cost, low-usage): translation, legal 等任务&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://maosong.website/p/state-of-ai--%E4%BB%8Eopenrouter-100t-token%E4%BD%BF%E7%94%A8%E6%83%85%E5%86%B5%E4%BA%86%E8%A7%A3ai-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%83%BD%E5%8A%9B%E5%88%86%E5%B1%82%E7%AB%9E%E4%BA%89%E9%80%BB%E8%BE%91/State-AI-classification-models.png"
width="1493"
height="875"
loading="lazy"
alt="model landscape: cost v.s. usage"
class="gallery-image"
data-flex-grow="170"
data-flex-basis="409px"
>&lt;/p>
&lt;p>图中红线是拟合出来的结果，红线说明，降低 $10\%$ 的价格只会带来 $0.5\sim0.7\%$ 的 token 使用率增长。上面这幅图说明了闭源模型主要解决高价值的任务，而开原模型则是解决 low-cost 的任务&lt;/p>
&lt;p>作者对比不同模型，给出了一些例子，如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/state-of-ai--%E4%BB%8Eopenrouter-100t-token%E4%BD%BF%E7%94%A8%E6%83%85%E5%86%B5%E4%BA%86%E8%A7%A3ai-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%83%BD%E5%8A%9B%E5%88%86%E5%B1%82%E7%AB%9E%E4%BA%89%E9%80%BB%E8%BE%91/State-AI-example-models.png"
width="1550"
height="890"
loading="lazy"
alt="Example models by segment"
class="gallery-image"
data-flex-grow="174"
data-flex-basis="417px"
>&lt;/p>
&lt;p>这里的关键结论有几点：&lt;/p>
&lt;ol>
&lt;li>宏观需求无弹性，微观行为分化：企业愿意支付成本使用更强模型，而个人用户则对成本比较敏感&lt;/li>
&lt;li>Jevons Paradox: 模型成本下降之后，使用率反而会上升&lt;/li>
&lt;li>&lt;strong>能力优于成本&lt;/strong>：用户愿意为更强的模型付出更高的成本&lt;/li>
&lt;li>低价不能成为竞争力：免费开源模型不能落地的原因是仅仅达到“可用”水平，无法部署到实际工作流程中&lt;/li>
&lt;/ol>
&lt;h2 id="task">&lt;a href="#task" class="header-anchor">&lt;/a>Task
&lt;/h2>&lt;p>首先是所有模型在不同任务上的 token 使用情况，可以看到，programming 的占比最近已经升到了 $50\%$ 以上&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/state-of-ai--%E4%BB%8Eopenrouter-100t-token%E4%BD%BF%E7%94%A8%E6%83%85%E5%86%B5%E4%BA%86%E8%A7%A3ai-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%83%BD%E5%8A%9B%E5%88%86%E5%B1%82%E7%AB%9E%E4%BA%89%E9%80%BB%E8%BE%91/State-AI-category-close-model.png"
width="1353"
height="509"
loading="lazy"
alt="Category trend of tokens"
class="gallery-image"
data-flex-grow="265"
data-flex-basis="637px"
>&lt;/p>
&lt;p>并且对于 programming, Anthropic 拥有 $60\%$ 以上的份额，如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/state-of-ai--%E4%BB%8Eopenrouter-100t-token%E4%BD%BF%E7%94%A8%E6%83%85%E5%86%B5%E4%BA%86%E8%A7%A3ai-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%83%BD%E5%8A%9B%E5%88%86%E5%B1%82%E7%AB%9E%E4%BA%89%E9%80%BB%E8%BE%91/State-AI-anthropic-share-progamming.png"
width="1349"
height="535"
loading="lazy"
alt="Share of models on programming"
class="gallery-image"
data-flex-grow="252"
data-flex-basis="605px"
>&lt;/p>
&lt;p>接下来是开源模型在不同任务上的 token 使用情况。&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/state-of-ai--%E4%BB%8Eopenrouter-100t-token%E4%BD%BF%E7%94%A8%E6%83%85%E5%86%B5%E4%BA%86%E8%A7%A3ai-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%83%BD%E5%8A%9B%E5%88%86%E5%B1%82%E7%AB%9E%E4%BA%89%E9%80%BB%E8%BE%91/State-AI-category-usage.png"
width="1348"
height="577"
loading="lazy"
alt="Category Trends of OSS models over time"
class="gallery-image"
data-flex-grow="233"
data-flex-basis="560px"
>&lt;/p>
&lt;p>排名前二的任务分别为 roleplay 以及 programming, 前者占比 $50\%$ 以上， 而后者占比在 $15\sim20\%$ 。中文的 OSS model 主要也集中在这两个任务上，但是 roleplay 占比下降到了 $33\%$. 而 programming+technology 的占比为 $39\%$.&lt;/p>
&lt;p>对于 programming，目前 $90\%$ 的 token 都基于闭源商业模型，对于开源模型，目前中文开源模型的占比已经超过了其他开源模型&lt;/p>
&lt;p>对于 roleplay, 其他开源模型与闭源商业模型的占比分别为 $43\%$, $42\%$ , 且开源模型占比持续上升&lt;/p>
&lt;p>开源模型的几个关键用途为：&lt;/p>
&lt;ol>
&lt;li>roleplay and creative dialogue: 写作，虚拟人物等&lt;/li>
&lt;li>programming: 编程，开发&lt;/li>
&lt;li>translation: 多语种任务&lt;/li>
&lt;li>general QA: 日常问答&lt;/li>
&lt;/ol>
&lt;p>作者还对 token 长度进行了分析，结果显示，目前输入长度区间为 $[1.5K, 6K]$, 输出长度区间为 $[150, 400]$, 这说明不同于早期简单问答，现在用户倾向于输入更丰富的上下文或者材料来让模型解决相应问题。而且，&lt;strong>programming 相关的输入长度大约是其他 category 输入的 3 倍以上&lt;/strong>，下图是不同 category 输入长度随时间的变化情况&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/state-of-ai--%E4%BB%8Eopenrouter-100t-token%E4%BD%BF%E7%94%A8%E6%83%85%E5%86%B5%E4%BA%86%E8%A7%A3ai-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%83%BD%E5%8A%9B%E5%88%86%E5%B1%82%E7%AB%9E%E4%BA%89%E9%80%BB%E8%BE%91/State-AI-prompt-token-length.png"
width="1355"
height="512"
loading="lazy"
alt="Length of prompt token length trends"
class="gallery-image"
data-flex-grow="264"
data-flex-basis="635px"
>&lt;/p>
&lt;p>作者还展示了不同子任务的占比情况（只列出大于 $10\%$ 的部分）&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>category&lt;/th>
&lt;th>sub-category&lt;/th>
&lt;th>ratio (%)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Roleplay&lt;/td>
&lt;td>Games/Roleplaying Games&lt;/td>
&lt;td>57.9&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Books &amp;amp; Literature/Wrters Resources&lt;/td>
&lt;td>16.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Adult&lt;/td>
&lt;td>15.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Programming&lt;/td>
&lt;td>general programming&lt;/td>
&lt;td>66.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>developing tools&lt;/td>
&lt;td>26.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Translation&lt;/td>
&lt;td>general&lt;/td>
&lt;td>51&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Foreign Language Resources&lt;/td>
&lt;td>49&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Science&lt;/td>
&lt;td>Machine Learning &amp;amp; AI&lt;/td>
&lt;td>80.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Technology&lt;/td>
&lt;td>personal assistences&lt;/td>
&lt;td>31.3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>software&lt;/td>
&lt;td>10.9&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>health&lt;/td>
&lt;td>general&lt;/td>
&lt;td>25.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>research&lt;/td>
&lt;td>11.6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>finance&lt;/td>
&lt;td>currencies&lt;/td>
&lt;td>19.2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>stocks&lt;/td>
&lt;td>15.9&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>investing&lt;/td>
&lt;td>15.5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>accounting&lt;/td>
&lt;td>13.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>academia&lt;/td>
&lt;td>educational&lt;/td>
&lt;td>42.7&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>writing&lt;/td>
&lt;td>36.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>management&lt;/td>
&lt;td>14.8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>legal&lt;/td>
&lt;td>Government&lt;/td>
&lt;td>42.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Legal&lt;/td>
&lt;td>18.9&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>trivia&lt;/td>
&lt;td>-&lt;/td>
&lt;td>96.2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>marketing&lt;/td>
&lt;td>marketing&lt;/td>
&lt;td>66.3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>sales&lt;/td>
&lt;td>16.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>seo&lt;/td>
&lt;td>seo&lt;/td>
&lt;td>100&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>接下来是模型在 agentic inference 场景下的 token 使用情况，从下图可以看出，Reasoning token 占比已经超过了 $60\%$. 这代表了用户对于使用工具解决复杂能力的需求&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/state-of-ai--%E4%BB%8Eopenrouter-100t-token%E4%BD%BF%E7%94%A8%E6%83%85%E5%86%B5%E4%BA%86%E8%A7%A3ai-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%83%BD%E5%8A%9B%E5%88%86%E5%B1%82%E7%AB%9E%E4%BA%89%E9%80%BB%E8%BE%91/State-AI-reasoning-token.png"
width="1361"
height="507"
loading="lazy"
alt="Reasoning token Trend over time"
class="gallery-image"
data-flex-grow="268"
data-flex-basis="644px"
>&lt;/p>
&lt;p>这 agentic inference 场景下, 使用最多的模型有 Grok Code Fast1, Gemini 2.5 Pro/flash 等，下图是不同模型的占比情况&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/state-of-ai--%E4%BB%8Eopenrouter-100t-token%E4%BD%BF%E7%94%A8%E6%83%85%E5%86%B5%E4%BA%86%E8%A7%A3ai-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%83%BD%E5%8A%9B%E5%88%86%E5%B1%82%E7%AB%9E%E4%BA%89%E9%80%BB%E8%BE%91/State-AI-tool-use-trend.png"
width="1351"
height="525"
loading="lazy"
alt="tool-use calls trend"
class="gallery-image"
data-flex-grow="257"
data-flex-basis="617px"
>&lt;/p>
&lt;p>可以看到，Claude 系列占了大部分份额&lt;/p>
&lt;h2 id="user">&lt;a href="#user" class="header-anchor">&lt;/a>User
&lt;/h2>&lt;p>用户这方面，中国和北美的使用占了近 $80\%$, 其中中文用户最近占 $31\%$&lt;/p>
&lt;p>语言上，token 的语言占比情况如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Language&lt;/th>
&lt;th>Token Share (%)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>English&lt;/td>
&lt;td>82.87&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Chinese (Simplified)&lt;/td>
&lt;td>4.95&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Russian&lt;/td>
&lt;td>2.47&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Spanish&lt;/td>
&lt;td>1.43&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Thai&lt;/td>
&lt;td>1.03&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Other (combined)&lt;/td>
&lt;td>7.25&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>作者还分了用户留存行为，发现一小部分用户的留存率非常高，作者将这个现象称之为水晶鞋效应 (Glass Slipper effect). 作者分析原因有以下几点：&lt;/p>
&lt;ol>
&lt;li>市场存在未被满足的需求：尽管 AI 模型层出不穷，但是始终有些任务连续多代模型都无法解决，也就是 &amp;quot; 水晶鞋“”&lt;/li>
&lt;li>新模型发布是一个“试穿水晶鞋的过程”：每一代模型都会被用于测试是否能解决这些未解决需求&lt;/li>
&lt;li>“灰姑娘时刻”：一旦某一个模型解决这个未解决的需求，这个模型就会吸引相当一部分用户从维持比较高的留存率&lt;/li>
&lt;/ol>
&lt;p>作者举例说明，Gemini 2.5 Pro 和 Claude 4 Sonnet 在 5 个月以后用户留存率还有 $40\%$, 而 Gemini 2.0 Flash 和 LLaMA 4 Maverick 的用户留存率非常差。作者因此得出三个关键结论：&lt;/p>
&lt;ol>
&lt;li>首次解决是持久竞争的核心优势：也就是先发制人&lt;/li>
&lt;li>用户留存率是能力拐点的信号：因为模型实现了从不可能到可能得跨越，才能保留一批早期用户&lt;/li>
&lt;li>“水晶鞋时刻”的窗口很窄：一旦抓不住机会，很可能就会丢掉一大批用户群体&lt;/li>
&lt;/ol>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://openrouter.ai/state-of-ai" target="_blank" rel="noopener"
>state of AI&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>LLM Memory Computation</title><link>https://maosong.website/p/llm-memory-computation/</link><pubDate>Sat, 17 Jan 2026 10:04:32 +0800</pubDate><guid>https://maosong.website/p/llm-memory-computation/</guid><description>&lt;p>本文中，我们将介绍如何计算 LLM 在训练和推理过程中的内存需求以及简要介绍对应的优化方法。&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>我们在本文中回答的核心问题为：&lt;/p>
&lt;blockquote>
&lt;p>在训练和推理时 LLM 所需要的内存是多少？如何进行优化内存占用？&lt;/p>
&lt;/blockquote>
&lt;p>为了回答这两个问题，我们需要回答以下问题：&lt;/p>
&lt;ol>
&lt;li>训练和推理时的内存由哪几部分组成？&lt;/li>
&lt;li>训练和推理过程中哪个阶段是 memory-bound? 哪个阶段是 compute bound?&lt;/li>
&lt;li>训练和推理过程中如何进行优化？&lt;/li>
&lt;/ol>
&lt;p>我们将首先介绍如何计算 LLM 在训练阶段和推理阶段的内存。接下来，我们针对可优化部分进行分析以及介绍相应的优化算法。后续，我们将针对每部分的优化进行详细介绍&lt;/p>
&lt;h2 id="background">&lt;a href="#background" class="header-anchor">&lt;/a>Background
&lt;/h2>&lt;p>首先我们介绍一下使用的 notation, 这与之前参数量，FLOPs 计算使用的 notation 基本一致。需要注意的是，我们直接使用参数量 $P$ 这个记号，这部分在 &lt;a class="link" href="https://maosong.website/p/llm-parameter-computation/" target="_blank" rel="noopener"
>LLM parameter analysis&lt;/a> 中已经进行了详细介绍，因此我们略过这部分。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>variable&lt;/th>
&lt;th>description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$P$&lt;/td>
&lt;td>number of parameters&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$L$&lt;/td>
&lt;td>layers&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$V$&lt;/td>
&lt;td>vocabulary size&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$d$&lt;/td>
&lt;td>hidden size&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$d_{ff}$&lt;/td>
&lt;td>FFN hidden size&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$s$&lt;/td>
&lt;td>sequence length&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$b$&lt;/td>
&lt;td>batch size&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$h$&lt;/td>
&lt;td>number of attention heads&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$d_h$&lt;/td>
&lt;td>attention head dimension&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="assumption">&lt;a href="#assumption" class="header-anchor">&lt;/a>Assumption
&lt;/h3>&lt;ol>
&lt;li>没有特别说明的话，我们使用 BF16/FP16 作为精度，此时每个参数需要 $2$ byte 来表示&lt;/li>
&lt;li>不使用 dropout (现代大模型普遍没有 dropout)&lt;/li>
&lt;/ol>
&lt;h2 id="computation">&lt;a href="#computation" class="header-anchor">&lt;/a>Computation
&lt;/h2>&lt;h3 id="overview">&lt;a href="#overview" class="header-anchor">&lt;/a>Overview
&lt;/h3>&lt;p>我们首先给出训练和推理阶段各部分的内存需求，然后我们给出详细的计算公式&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>component&lt;/th>
&lt;th>训练&lt;/th>
&lt;th>推理&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>weights&lt;/td>
&lt;td>Fixed&lt;/td>
&lt;td>Fixed&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>optimizer states&lt;/td>
&lt;td>Fixed and massive&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>gradients&lt;/td>
&lt;td>Fixed&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>activations&lt;/td>
&lt;td>Large (stored for backprop)&lt;/td>
&lt;td>Tiny (discarded after use)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>KV cache&lt;/td>
&lt;td>0&lt;/td>
&lt;td>Large (grows with sequence)&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="training">&lt;a href="#training" class="header-anchor">&lt;/a>Training
&lt;/h3>&lt;p>LLM 训练阶段对的内存开销包含三部分&lt;/p>
$$
\text{Memory}_{\text{train}} = \text{Memory}(\text{weight}) + \text{Memory}(\text{activation}) + \text{Memory}(\text{optimizer})+\text{Memory}(\text{gradient})
$$&lt;h4 id="weights">&lt;a href="#weights" class="header-anchor">&lt;/a>Weights
&lt;/h4>&lt;p>我们在前面已经介绍了如何计算大语言模型的参数量，这里我们就直接记为 $P$, 由于我们使用单精度，因此所需要的内存为 $2P$.&lt;/p>
&lt;h4 id="activation">&lt;a href="#activation" class="header-anchor">&lt;/a>Activation
&lt;/h4>&lt;p>激活值（activation）是前向传播过程中产生的中间张量，反向传播计算梯度时需复用这些张量，因此训练阶段需全程存储。我们用一个简单的例子来进行说明，假设我们有一层神经网络，定义为&lt;/p>
$$
\begin{aligned}
\mathbf{z}_l &amp;= W_l\mathbf{a}_{l-1}+b_l\\
\mathbf{a}_{l} &amp;= \phi(\mathbf{z}_l)
\end{aligned}
$$&lt;p>那么在反向传播过程中，我们有&lt;/p>
$$
\frac{\partial \mathcal{L}}{\partial W_l} = \frac{\partial \mathcal{L}}{\partial \mathbf{z}_l}\frac{\partial \mathbf{z}_l}{\partial W_l}=\frac{\partial \mathcal{L}}{\partial \mathbf{z}_l} \mathbf{a}_{l-1}
$$&lt;p>也就是说，在计算第 $l$ 层的参数对应的梯度时，我们需要知道对应的输入 $\mathbf{a}_{l-1}$.&lt;/p>
&lt;p>接下来，我们通过计算图来分析 LLM 所需要的 activation&lt;/p>
&lt;p>&lt;strong>Attention&lt;/strong>
Attention 的计算图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/llm-memory-computation/Attention-computation-graph.png"
width="841"
height="776"
loading="lazy"
alt="Computation graph of attention"
class="gallery-image"
data-flex-grow="108"
data-flex-basis="260px"
>&lt;/p>
&lt;p>根据计算图，对应的 activation 为（注：这里我们不做任何优化，仅此理论上进行分析）：&lt;/p>
&lt;ol>
&lt;li>query, key, value projection: 共享输入，对应的 activation 大小为 $2bsd$.&lt;/li>
&lt;li>$Q^TK$ : $Q$, $K$ 都需要保存，大小为 $4bsd$.&lt;/li>
&lt;li>softmax: 需要保存 $2bhs^2$ 大小的输入&lt;/li>
&lt;li>weighted sum of values: 两者都需要保存，前者大小为 $2bhs^2$, 后者大小为 $2bsd$&lt;/li>
&lt;li>output projection layer: 需要保存输入，大小为 $2bsd$.&lt;/li>
&lt;/ol>
&lt;p>因此 attention 部分总共需要 $\boxed{10sbd+4bhs^2}$.&lt;/p>
&lt;p>&lt;strong>FFN&lt;/strong>
FFN 计算图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/llm-memory-computation/FFN-computation-graph.png"
width="559"
height="742"
loading="lazy"
alt="FFN computation graph"
class="gallery-image"
data-flex-grow="75"
data-flex-basis="180px"
>&lt;/p>
&lt;p>根据计算图，对应的 activation （我们假设 MLP 是一个基于 SwiGLU 的 dense MLP, 其 hidden size $d_{ff}=8/3d$,）：&lt;/p>
&lt;ol>
&lt;li>MLP 的第一层输入大小为 $2sbd$,&lt;/li>
&lt;li>MLP 的第二层输入大小为 $16/3sbd$,&lt;/li>
&lt;li>SwiGLU 的输入为 $16/3sbd$&lt;/li>
&lt;/ol>
&lt;p>因此总的 activation 大小为 $\boxed{18sbd}$.&lt;/p>
&lt;p>&lt;strong>LayerNorm&lt;/strong>
LayerNorm 需要保存输入，大小为 $\boxed{2bsd}$.&lt;/p>
&lt;p>以上三部分相加，我们就得到单一 transformer layer 所需要的 activation:&lt;/p>
$$
\begin{aligned}
\mathrm{activation}(\mathrm{transformer}\_{\mathrm{block}})&amp;=\mathrm{activation}(\mathrm{PerNorm})+\mathrm{activation}(\mathrm{Attention})+\mathrm{activation}(\mathrm{PostNorm})+\mathrm{activation}(\mathrm{FFN})\\
&amp;= 2bsd + (10bsd+4bhs^2) + 2bsd + 18bsd\\
&amp;= \boxed{bs(32d+4hs)}
\end{aligned}
$$&lt;p>&lt;strong>output&lt;/strong>
output 部分的计算图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/llm-memory-computation/output-computation-graph.png"
width="381"
height="545"
loading="lazy"
alt="Output computation graph"
class="gallery-image"
data-flex-grow="69"
data-flex-basis="167px"
>&lt;/p>
&lt;p>根据计算图，对应的 activation 为：&lt;/p>
&lt;ol>
&lt;li>normalization 的输入大小为大小为 $2sbd$&lt;/li>
&lt;li>&lt;code>lm_head&lt;/code> 的输入大小为 $2sbd$&lt;/li>
&lt;li>loss 的输入大小为 $2bsV$&lt;/li>
&lt;/ol>
&lt;p>从而输出部分的 activation 大小为&lt;/p>
$$
\mathrm{activation}(\mathrm{output}) = \mathrm{activation}(\mathrm{FinalNorm})+\mathrm{activation}(\mathrm{lm\ head})+\mathrm{activation}(\mathrm{Loss}) = \boxed{4bsd+2bsV}
$$&lt;p>因此，总的 activation 为&lt;/p>
$$
\begin{aligned}
\text{Memory}(\text{activation}) &amp;= L*(\mathrm{transformer}\_{\mathrm{block}}) + \mathrm{activation}(\mathrm{output})\\
&amp;= \boxed{Lsb(32d+4hs) +( 4bsd+2bsV)}
\end{aligned}
$$&lt;h4 id="gradients--optimizer-states">&lt;a href="#gradients--optimizer-states" class="header-anchor">&lt;/a>Gradients &amp;amp; Optimizer States
&lt;/h4>&lt;p>现代优化器一般会使用高阶近似以及混合精度训练来提高训练的效率，这部分高阶近似也需要考虑内存占用。&lt;/p>
&lt;p>&lt;strong>Gradients&lt;/strong>
当 gradient 和 weight 精度一致时，对应的内存消耗一致，为 $\boxed{2P}$.&lt;/p>
&lt;p>&lt;strong>Optimizer states&lt;/strong>
&lt;a class="link" href="https://maosong.website/p/notes-on-adamw/" target="_blank" rel="noopener"
>AdamW&lt;/a> 优化器会保存一阶和二阶动量，以及一份 master weights, 精度一般为 FP32:&lt;/p>
&lt;ol>
&lt;li>FP32 master weights: $4P$&lt;/li>
&lt;li>FP32 first-order momentum: $4P$&lt;/li>
&lt;li>FP32 second-order momentum: $4P$&lt;/li>
&lt;/ol>
&lt;p>因此优化器状态需要 $\boxed{12P}$ 内存。&lt;/p>
&lt;p>对于其他优化器，我们也可以算出对应的内存需求，下表总结了 AdamW, bitsandbytes 和 SGD 三种 optimizer&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>optimizer&lt;/th>
&lt;th>master weights (FP32)&lt;/th>
&lt;th>momentum&lt;/th>
&lt;th>variance&lt;/th>
&lt;th>TOTAL&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>AdamW&lt;/td>
&lt;td>$4P$&lt;/td>
&lt;td>$4P$&lt;/td>
&lt;td>$4P$&lt;/td>
&lt;td>$12P$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>bitsandbytes&lt;/td>
&lt;td>$4P$&lt;/td>
&lt;td>$P$&lt;/td>
&lt;td>$P$&lt;/td>
&lt;td>$6P$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SGD&lt;/td>
&lt;td>$4P$&lt;/td>
&lt;td>$4P$&lt;/td>
&lt;td>0&lt;/td>
&lt;td>$8P$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>最终，训练阶段所需要的内存为&lt;/p>
$$
\text{Memory}_{\text{train}} = 16P+bs(32dL+4hsL+4d+2V)
$$&lt;p>下面我们展示 LLaMA 系列训练时不同部分的内存占比 (batch size=64, AdamW, GB)&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>weights&lt;/th>
&lt;th>gradients&lt;/th>
&lt;th>optimizer_states&lt;/th>
&lt;th>activations&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>LLaMA-7B&lt;/td>
&lt;td>12.55&lt;/td>
&lt;td>12.55&lt;/td>
&lt;td>75.31&lt;/td>
&lt;td>1545.81&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LLaMA-13B&lt;/td>
&lt;td>24.24&lt;/td>
&lt;td>24.24&lt;/td>
&lt;td>145.46&lt;/td>
&lt;td>2410.31&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LLaMA-33B&lt;/td>
&lt;td>60.59&lt;/td>
&lt;td>60.59&lt;/td>
&lt;td>363.54&lt;/td>
&lt;td>4691.06&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LLaMA-65B&lt;/td>
&lt;td>121.60&lt;/td>
&lt;td>121.60&lt;/td>
&lt;td>729.62&lt;/td>
&lt;td>7691.81&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="inference">&lt;a href="#inference" class="header-anchor">&lt;/a>Inference
&lt;/h3>&lt;p>LLM 推理阶段对的开销包含三部分&lt;/p>
$$
\text{Memory}_{\text{Inference}} = \text{Memory}(\text{weight}) + \text{Memory}(\text{activation}) + \text{Memory}(\text{KV cache})
$$&lt;p>weight memory 的内存占用为 $\boxed{2P}$. activation 内存占用比较小，&lt;a class="link" href="https://blog.eleuther.ai/transformer-math/" target="_blank" rel="noopener"
>transformer-math&lt;/a> 给出了一个经验值，即&lt;/p>
$$
\text{Memory}(\text{activation})\approx 0.2*\text{Memory}(\text{weight})=0.4P
$$&lt;p>该经验值适用于 batch size = 1 的自回归推理场景。weight 和 activation 这两部分开销只与模型本身有关，第三部分 KV cache 则与我们的生成内容长度相关，下面我们详细进行介绍&lt;/p>
&lt;h4 id="key-value-cache">&lt;a href="#key-value-cache" class="header-anchor">&lt;/a>Key Value Cache
&lt;/h4>&lt;p>Key Value Cache (KV Cache) 是 LLM 在推理过程中为了避免重复计算历史 token 对应的 key 和 value 而使用的一个&lt;strong>空间换时间的缓存机制&lt;/strong>。&lt;/p>
&lt;p>在 LLM 推理阶段，我们是 token-by-token 进行生成的，每次 attention 的计算都有如下形式&lt;/p>
$$
\begin{aligned}
\mathbf{q_t} &amp;= W_Q\mathbf{x_t}\\
\mathbf{k}_{:,t}&amp;=W_K[\mathbf{x_1},\dots,\mathbf{x_t}]\\
\mathbf{v}_{:,t}&amp;=W_V[\mathbf{x_1},\dots,\mathbf{x_t}]\\
\mathbf{o}_t&amp;=\mathrm{Attn}(\mathbf{q_t},\mathbf{k}_{:,t}, \mathbf{v}_{:,t})=\sum_{i=1}^t \frac{\alpha_{t,i}}{\sum_{t,i}\alpha_{t,i}}\mathbf{v_i},\ \alpha_{t,i} = \exp\left(\frac{\mathbf{q_t}^T\mathbf{k}_{i}}{\sqrt{d_k}}\right)
\end{aligned}
$$&lt;p>这里 $\mathbf{q_t}$ 是当前 token $\mathbf{x}_t$ 对应的 query, $\mathbf{k}_{:,t}$ 和 $\mathbf{v}_{:,t}$ 是历史 token $[\mathbf{x_1},\dots,\mathbf{x_t}]$ 对应的 key 和 value. 当我们处理下一个 token $\mathbf{x}_{t+1}$ 时， 对应的计算变成了&lt;/p>
$$
\begin{aligned}
\mathbf{q_t} &amp;= W_Q\mathbf{x_t}\\
\mathbf{k}_{:,t+1}&amp;=W_K[\mathbf{x_1},\dots,\mathbf{x_t},\mathbf{x}_{t+1}]=[\boxed{\mathbf{k}_{:,t}},W_K\mathbf{x}_{t+1}]\\
\mathbf{v}_{:,t+1}&amp;=W_V[\mathbf{x_1},\dots,\mathbf{x_t},\mathbf{x}_{t+1}]=[\boxed{\mathbf{v}_{:,t}},W_V\mathbf{x}_{t+1}]\\
\end{aligned}
$$&lt;p>也就是说，我们每生成一个 token, 都要重新计算一次历史 token 对应的 key 和 value, 因此生成一个包含 $s$ 个 token 的 sequence 时，每个 token 都需要计算其前序 token 的 key 和 value, 其对应的计算量为&lt;/p>
$$
\sum_{t=1}^s \mathcal{O}(t) = \mathcal{O}(s^2)
$$&lt;p>因此，一个自然的想法就是缓存历史 token 对应的 key 和 value, 在生成新的 token 时，我们只需从内存中加载计算好的结果，然后计算当前 token 对应的值 $W_K\mathbf{x}_{t+1}$ 和 $W_V\mathbf{x}_{t+1}$ 即可，这就是 KV cache. 使用 KV cache 之后，我们每次生成新的 token 时，仅需要计算当前 token 对应的 key 和 value, 此时总的计算复杂度为 $\mathcal{O}(s)$, 对应的空间复杂度为 $\mathcal{O}(s)$. 也就是以空间换时间。&lt;/p>
&lt;p>容易推导出一个基于 Multi-head attention LLM 的 KV cache 如下&lt;/p>
$$
\text{Memory}(\text{KV cache}) = s \times 2 \times 2 \times L\times h \times d_h
$$&lt;p>可以看到，KV Cache 占用不仅与模型配置有关，还与生成的 sequence length 有关，生成的 token 越多，KV Cache 这部分占用越高。&lt;/p>
&lt;p>最终，推理阶段模型本身的内存占用为&lt;/p>
$$
\text{Memory}_{\text{Inference}} = 2.4P+4sLhd_h
$$&lt;p>我们还是以 LLaMA 系列为例，结果如下 (batch size=1, GB, 括号里为 sequence length)&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Weights&lt;/th>
&lt;th>Activations&lt;/th>
&lt;th>KV Cache (1024)&lt;/th>
&lt;th>KV Cache (4096)&lt;/th>
&lt;th>KV Cache (16384)&lt;/th>
&lt;th>KV Cache (32768)&lt;/th>
&lt;th>KV Cache (131072)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>LLaMA-7B&lt;/td>
&lt;td>12.55&lt;/td>
&lt;td>2.51&lt;/td>
&lt;td>0.25&lt;/td>
&lt;td>1.00&lt;/td>
&lt;td>4.00&lt;/td>
&lt;td>8.00&lt;/td>
&lt;td>32.00&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LLaMA-13B&lt;/td>
&lt;td>24.24&lt;/td>
&lt;td>4.85&lt;/td>
&lt;td>0.39&lt;/td>
&lt;td>1.56&lt;/td>
&lt;td>6.25&lt;/td>
&lt;td>12.50&lt;/td>
&lt;td>50.00&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LLaMA-33B&lt;/td>
&lt;td>60.59&lt;/td>
&lt;td>12.12&lt;/td>
&lt;td>0.76&lt;/td>
&lt;td>3.05&lt;/td>
&lt;td>12.19&lt;/td>
&lt;td>24.38&lt;/td>
&lt;td>97.50&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LLaMA-65B&lt;/td>
&lt;td>121.60&lt;/td>
&lt;td>24.32&lt;/td>
&lt;td>1.25&lt;/td>
&lt;td>5.00&lt;/td>
&lt;td>20.00&lt;/td>
&lt;td>40.00&lt;/td>
&lt;td>160.00&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，随着输出长度增加，KV cache 的开销占比也逐渐了超过模型权重的内存占用。而实际中 KV cache 往往因 page granularity、padding 和 fragmentation 略高于理论值。&lt;/p>
&lt;h3 id="summary">&lt;a href="#summary" class="header-anchor">&lt;/a>Summary
&lt;/h3>&lt;p>我们将上面的结果汇总起来就得到下表的结果。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>component&lt;/th>
&lt;th>训练&lt;/th>
&lt;th>推理&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>weights&lt;/td>
&lt;td>$2P$&lt;/td>
&lt;td>$2P$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>optimizer states&lt;/td>
&lt;td>$12P$&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>gradients&lt;/td>
&lt;td>$2P$&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>activations&lt;/td>
&lt;td>$Lsb(32d+4hs) +( 4bsd+2bsV)$&lt;/td>
&lt;td>$\sim 0.4P$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>KV cache&lt;/td>
&lt;td>0&lt;/td>
&lt;td>$4sLhd_h$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TOTAL&lt;/td>
&lt;td>$16P+bs(32dL+4hsL+4d+2V)$&lt;/td>
&lt;td>$2.4P+4sLhd_h$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="analysis--optimizations">&lt;a href="#analysis--optimizations" class="header-anchor">&lt;/a>Analysis &amp;amp; Optimizations
&lt;/h2>&lt;p>接下来，我们将简单介绍一下如何优化训练和推理过程中的内存占用，我们将优化方法总结如下表所示。后面我们将一一进行详细介绍&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Stage&lt;/th>
&lt;th>methods&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>training&lt;/td>
&lt;td>- activation checkpointing&lt;br>- flash attention&lt;br>- Parallelism&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>inference&lt;/td>
&lt;td>- KV Cache Optimization&lt;br>- PagedAttention&lt;br>- RadixAttention&lt;br>- Attention mechanism&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="training-1">&lt;a href="#training-1" class="header-anchor">&lt;/a>Training
&lt;/h3>&lt;h4 id="mixed-precision-training">&lt;a href="#mixed-precision-training" class="header-anchor">&lt;/a>Mixed Precision Training
&lt;/h4>&lt;p>混合精度训练的核心思想是计算量大的模块使用低精度，计算量小的模块使用高精度。细节见 Mixed precision training, 最近的 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v3/" target="_blank" rel="noopener"
>DeepSeek-V3&lt;/a> 还进一步使用了 FP8 精度进行训练，大幅度提高了训练效率。&lt;/p>
&lt;h4 id="data-parallelism">&lt;a href="#data-parallelism" class="header-anchor">&lt;/a>Data Parallelism
&lt;/h4>&lt;p>第一个并行策略是数据并行 (data parallelism), 其基本思想是把模型复制到多个 GPU 上，并行处理数据，然后对 loss 进行求和再进行反向传播。现在最常使用的是微软提出的 ZeRO, 其核心思想为把 optimizer states, gradients, weights 分布到不同的 GPU 上，然后需要的时候再汇总到一起。ZeRO 根据切分的部分不同可以分为三种策略，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/llm-memory-computation/ZeRO-architecture.png"
width="1057"
height="528"
loading="lazy"
alt="Architecture of ZeRO"
class="gallery-image"
data-flex-grow="200"
data-flex-basis="480px"
>&lt;/p>
&lt;p>如上图所示，在 baseline 场景下，我们每个 GPU 上都保存有一份模型的 optimizer states, gradients, weights, 这就限制了 batch size, 进而降低了整体的计算效率。&lt;/p>
&lt;p>ZeRO 的关键改进在于利用 GPU 可以互相通信的性质来将 tensor 存储在不同的 GPU 上，这时&lt;strong>每个 GPU 上不再保存完整的复制，而是独特的一部分数据&lt;/strong>，在参与计算时，GPU 通过 all gather 来把数据汇总在一起，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/llm-memory-computation/GPU-all-gather.gif"
width="850"
height="383"
loading="lazy"
alt="All-gather of GPU (sourced from How to scale your model)"
class="gallery-image"
data-flex-grow="221"
data-flex-basis="532px"
>&lt;/p>
&lt;p>ZeRO1 只对 optimizer states 进行 shard, 因此其内存占用为&lt;/p>
$$
\text{Memory}_{\text{train}} = \text{Memory}(\text{weight}) + \text{Memory}(\text{activation}) + \frac{\text{Memory}(\text{optimizer})}{\text{\# GPUs}}+\text{Memory}(\text{gradient})
$$&lt;p>ZeRO2 在 ZeRO1 的基础上进一步对 gradient 也进行 shard, 其内存占用为&lt;/p>
$$
\text{Memory}_{\text{train}} = \text{Memory}(\text{weight}) + \text{Memory}(\text{activation}) + \frac{\text{Memory}(\text{optimizer})+\text{Memory}(\text{gradient})}{\text{\# GPUs}}
$$&lt;p>ZeRO3 在 ZeRO2 的基础上对 weight 也进行 shard, 其内存占用为&lt;/p>
$$
\text{Memory}_{\text{train}} = \text{Memory}(\text{activation}) + \frac{\text{Memory}(\text{weight}) + \text{Memory}(\text{optimizer})+\text{Memory}(\text{gradient})}{\text{\# GPUs}}
$$&lt;p>一般来说，我们比较少使用 ZeRO3, 因为其通信开销变为了原来的 1.5 倍。&lt;/p>
&lt;h4 id="activation-checkpointing">&lt;a href="#activation-checkpointing" class="header-anchor">&lt;/a>Activation Checkpointing
&lt;/h4>&lt;p>上一节我们介绍了使用 DP 来减少固定部分 (weight, optimizer states, gradients) 部分的占用，但实际上训练时占用部分更多的是 activation, 这部分内存占用会严重影响 batch size 的设置进而影响整体计算效率。我们对固定部分（与模型参数量相关）和非固定部分（与 batch size 相关）进行一个对比，结果如下所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Metric&lt;/th>
&lt;th>$d$&lt;/th>
&lt;th>$b, s$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>weight&lt;/td>
&lt;td>quadratic ($d^2$)&lt;/td>
&lt;td>independent&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>activation&lt;/td>
&lt;td>linear ($d$)&lt;/td>
&lt;td>linear ($bs$)&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>我们可以看到，虽然训练时 batch size 越大越好，但是由于 activation 也会随之增大，batch size 可能只能使用一个非常小的值。下图是 LLaMA 系列在 $b=64$ 时不同部分的内存占用：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/llm-memory-computation/memory_usage_bs-64.png"
width="1200"
height="600"
loading="lazy"
alt="memory usage of different components (bs=64)"
class="gallery-image"
data-flex-grow="200"
data-flex-basis="480px"
>&lt;/p>
&lt;p>从图表可看出，LLaMA-65B 在 batch size=64 时，激活值占用内存超 80%，远高于权重 / 梯度 / 优化器状态，而且随着 batch size 增加，这个比例会进一步上升。&lt;/p>
&lt;p>为了解决这个问题，我们一般会使用 &lt;strong>activation checkpointing&lt;/strong> 方法，这个方法是一个通过重新计算中间激活值，来减少内存占用的方法。其核心思想在于用计算复杂度换空间复杂度。&lt;a class="link" href="https://arxiv.org/pdf/2205.05198" target="_blank" rel="noopener"
>Reducing Activation Recomputation in Large Transformer Models&lt;/a> 给出了不同的 checkpointing 策略，需要的算力也不同相同，我们下表进行总结&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>No checkpointing&lt;/th>
&lt;th>Selective checkpointing&lt;/th>
&lt;th>full checkpointing&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>description&lt;/td>
&lt;td>stores everything needed&lt;/td>
&lt;td>store states stagely (e.g., the input to each layer)&lt;/td>
&lt;td>only store the input to the model&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>memory&lt;/td>
&lt;td>very high ($\text{Memory}(\text{activation})$)&lt;/td>
&lt;td>medium&lt;/td>
&lt;td>very low $2bsd$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>extra compute&lt;/td>
&lt;td>None&lt;/td>
&lt;td>medium&lt;/td>
&lt;td>very high $2Pbs$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>一般来说我们会结合 model parallelism 和 selective checkpointing 来实现一个均衡&lt;/p>
&lt;h4 id="model-parallelism">&lt;a href="#model-parallelism" class="header-anchor">&lt;/a>Model Parallelism
&lt;/h4>&lt;p>与 DP 在数据维度上进行切分不同，model parallelism 通过对模型进行切分来提高内存使用效率。Model Parallelism 又可以分为 Pipeline Parallelism (PP) 和 Tensor Parallelisim (TP)&lt;/p>
&lt;p>通过 PP 和 TP 我们可以将模型切分部署在多个 GPU 上进而减少内存占用，对应的计算方式为&lt;/p>
$$
\text{Memory}(\text{weight};\text{parallelism}) = \frac{\text{Memory}(\text{weight})}{\text{PP degree}\times\text{TP degree}}
$$&lt;p>实际情况中，我们还可以结合 ZeRO 以及 Model Paralelism, 我们根据 PP degree 和 TP degree 来决定 DP degree&lt;/p>
$$
\text{DP degree} = \frac{\text{\# GPUs}}{\text{PP degree}\times\text{TP degree}}
$$&lt;p>最终，我们把以上优化技巧汇总起来就得到 (假设我们采用 ZeRO1 和 Model Parallelism)&lt;/p>
$$
\text{Memory}_{\text{train}} \approx \frac{\text{Memory}(\text{weight})}{\text{PP degree}\times\text{TP degree}} + \frac{\text{Memory}(\text{activation})}{\text{TP degree}} + \frac{\text{Memory}(\text{optimizer})}{\text{\# GPUs}}+\frac{\text{Memory}(\text{gradient})}{\text{PP degree}}
$$&lt;p>这里&amp;gt; activation 中 &lt;strong>被 tensor-parallel 的部分&lt;/strong> 按 TP degree 缩减。&lt;/p>
&lt;p>关于 Parallelism 的具体细节见 Parallelism tutorial&lt;/p>
&lt;h4 id="flash-attention">&lt;a href="#flash-attention" class="header-anchor">&lt;/a>Flash Attention
&lt;/h4>&lt;p>在前面的分析中，我们给出了 attention softmax 这一部分的 activation 为 $2bhs^2$ 而 flashattention 通过 tiling 和 online-softmax 降低了这一部分的内存占用，进而提高整体的效率。&lt;/p>
&lt;p>具体细节见 &lt;a class="link" href="https://maosong.website/p/notes-on-flashattention/" target="_blank" rel="noopener"
>flash attention&lt;/a>&lt;/p>
&lt;h3 id="inference-1">&lt;a href="#inference-1" class="header-anchor">&lt;/a>Inference
&lt;/h3>&lt;h4 id="quantization">&lt;a href="#quantization" class="header-anchor">&lt;/a>Quantization
&lt;/h4>&lt;p>quantization 是用低精度加载模型权重从而降低推理阶段模型参数内存占用的一个方法。比如说原始模型使用了 BF16 精度，那么我们可以通过使用 int8 量化来将模型权重对应的内存从 $2P$ 降低到 $P$. 现在一些模型还会在训练阶段就加入 quantization, 比如 quantization aware training 以及 post-training quantization 等。这部分细节可以参考 &lt;a class="link" href="https://arxiv.org/pdf/2312.03863" target="_blank" rel="noopener"
>Efficient Large Language Models: A Survey&lt;/a>&lt;/p>
&lt;h4 id="kv-cache-optimization">&lt;a href="#kv-cache-optimization" class="header-anchor">&lt;/a>KV Cache Optimization
&lt;/h4>&lt;p>我们在前面已经介绍了 KV cache 可以通过以空间换时间来提高计算效率，但是随着输出长度增加，对应的 KV cache 也会越来越大，因此目前有相当一部分工作旨在降低 KV cache 占用，比如 KV Cache compression, quantization 等。这部分细节可以参考 &lt;a class="link" href="https://arxiv.org/pdf/2412.19442" target="_blank" rel="noopener"
>A Survey on Large Language Model Acceleration based on KV Cache Management&lt;/a>&lt;/p>
&lt;h4 id="attention">&lt;a href="#attention" class="header-anchor">&lt;/a>Attention
&lt;/h4>&lt;p>实际上，相当一部分工作都是通过优化 attention 来降低&lt;/p>
&lt;h4 id="inference-framework">&lt;a href="#inference-framework" class="header-anchor">&lt;/a>Inference Framework
&lt;/h4>&lt;p>现在也有一些推理框架专注于提高 LLM 的推理效率，下面是两个比较流行的推理框架&lt;/p>
&lt;ul>
&lt;li>SGLang: 定制化强，适用于复杂任务如 RL 推理等&lt;/li>
&lt;li>vLLM: 简单高效&lt;/li>
&lt;/ul>
&lt;p>对应的轻量化推理框架为&lt;/p>
&lt;ul>
&lt;li>nano-vLLM&lt;/li>
&lt;li>mini-SGLang&lt;/li>
&lt;/ul>
&lt;p>这部分&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>在本文中，我们详细介绍了 LLM 在训练和推理阶段的内存占用开销以及简要介绍了对应的优化方法。关键结论为：&lt;/p>
&lt;ul>
&lt;li>训练阶段内存核心瓶颈是激活值（随 batch size / 序列长度线性增长），推理阶段核心瓶颈是 KV Cache（随序列长度增长）；&lt;/li>
&lt;li>训练优化优先通过 ZeRO（多卡）+ activation checkpointing（单卡）降低内存，推理优化优先通过 KV Cache 优化 + 量化降低内存；&lt;/li>
&lt;li>所有内存计算均为理论值，实际落地需考虑显存碎片、硬件特性、通信开销等工程因素。&lt;/li>
&lt;/ul>
&lt;p>需要注意的是，所有内存计算均为理论值，实际落地需考虑显存碎片、硬件特性、通信开销等工程因素。下一步，我们将分别针对不同的优化方法来进行展开并详细介绍。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://blog.eleuther.ai/transformer-math/" target="_blank" rel="noopener"
>transformer-math&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://kipp.ly/transformer-inference-arithmetic/" target="_blank" rel="noopener"
>transformer inference arithmetic&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://zhuanlan.zhihu.com/p/687226668" target="_blank" rel="noopener"
>https://zhuanlan.zhihu.com/p/687226668&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2205.05198" target="_blank" rel="noopener"
>Reducing Activation Recomputation in Large Transformer Models&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://blog.eleuther.ai/transformer-math/" target="_blank" rel="noopener"
>https://blog.eleuther.ai/transformer-math/&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2412.19442" target="_blank" rel="noopener"
>A Survey on Large Language Model Acceleration based on KV Cache Management&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2312.03863" target="_blank" rel="noopener"
>Efficient Large Language Models: A Survey&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="appendix">&lt;a href="#appendix" class="header-anchor">&lt;/a>Appendix
&lt;/h2>&lt;h3 id="activation-visualization">&lt;a href="#activation-visualization" class="header-anchor">&lt;/a>Activation Visualization
&lt;/h3>&lt;p>LLaMA 系列的配置如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>s&lt;/th>
&lt;th>V&lt;/th>
&lt;th>L&lt;/th>
&lt;th>d&lt;/th>
&lt;th>d_ff&lt;/th>
&lt;th>h&lt;/th>
&lt;th>h_d&lt;/th>
&lt;th>P&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>LLaMA-7B&lt;/td>
&lt;td>2048&lt;/td>
&lt;td>32000&lt;/td>
&lt;td>32&lt;/td>
&lt;td>4096&lt;/td>
&lt;td>11008&lt;/td>
&lt;td>32&lt;/td>
&lt;td>128&lt;/td>
&lt;td>6738411520&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LLaMA-13B&lt;/td>
&lt;td>2048&lt;/td>
&lt;td>32000&lt;/td>
&lt;td>40&lt;/td>
&lt;td>5120&lt;/td>
&lt;td>13824&lt;/td>
&lt;td>40&lt;/td>
&lt;td>128&lt;/td>
&lt;td>13015859200&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LLaMA-33B&lt;/td>
&lt;td>2048&lt;/td>
&lt;td>32000&lt;/td>
&lt;td>60&lt;/td>
&lt;td>6656&lt;/td>
&lt;td>17920&lt;/td>
&lt;td>52&lt;/td>
&lt;td>128&lt;/td>
&lt;td>32528936960&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LLaMA-65B&lt;/td>
&lt;td>2048&lt;/td>
&lt;td>32000&lt;/td>
&lt;td>80&lt;/td>
&lt;td>8192&lt;/td>
&lt;td>22016&lt;/td>
&lt;td>64&lt;/td>
&lt;td>128&lt;/td>
&lt;td>65285652480&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>对应的可视化代码如下&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;span class="lnt">49
&lt;/span>&lt;span class="lnt">50
&lt;/span>&lt;span class="lnt">51
&lt;/span>&lt;span class="lnt">52
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">numpy&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">np&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">matplotlib.pyplot&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">plt&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">compute_memory&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">L&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">h&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">h_d&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">V&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">P&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">weights&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">P&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">gradients&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">P&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">optimizer_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">12&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">P&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">activations&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">L&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">b&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">32&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">4&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">h&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">4&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">b&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">s&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">b&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">s&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">V&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;weights&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">weights&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;gradients&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">gradients&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;optimizer_states&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">optimizer_states&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;activations&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">activations&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">b&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">64&lt;/span> &lt;span class="c1"># batch size for memory calculation&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">memory_data&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">model&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">params&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">models&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">items&lt;/span>&lt;span class="p">():&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">memory&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">compute_memory&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">params&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;L&amp;#34;&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">params&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;d&amp;#34;&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">params&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;h&amp;#34;&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">params&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;h_d&amp;#34;&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">params&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;V&amp;#34;&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">params&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;s&amp;#34;&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">params&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;P&amp;#34;&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">memory_data&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">model&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">memory&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">fig&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ax&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">subplots&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">figsize&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">12&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">6&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">model_names&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">list&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">memory_data&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">keys&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">GB&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1024&lt;/span> &lt;span class="o">**&lt;/span> &lt;span class="mi">3&lt;/span> &lt;span class="c1"># 1 GB in bytes&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">weights&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">memory_data&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">m&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="s2">&amp;#34;weights&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">GB&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">m&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">model_names&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">gradients&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">memory_data&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">m&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="s2">&amp;#34;gradients&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">GB&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">m&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">model_names&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">optimizer_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">memory_data&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">m&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="s2">&amp;#34;optimizer_states&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">GB&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">m&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">model_names&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">activations&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">memory_data&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">m&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="s2">&amp;#34;activations&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">GB&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">m&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">model_names&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">model_names&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">width&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">0.6&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Stacked bar chart&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">p1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ax&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">bar&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">weights&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">width&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">label&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;Weights&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">p2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ax&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">bar&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">gradients&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">width&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bottom&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">weights&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">label&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;Gradients&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">p3&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ax&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">bar&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">optimizer_states&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">width&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bottom&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">array&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">weights&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">array&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">gradients&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">label&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;Optimizer States&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">p4&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ax&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">bar&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">activations&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">width&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bottom&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">array&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">weights&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">array&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">gradients&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">array&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">optimizer_states&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">label&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;Activations&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">ax&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">set_xlabel&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;Model&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">ax&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">set_ylabel&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;Memory (GB)&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">ax&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">set_title&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s1">&amp;#39;Memory Usage Breakdown for LLaMA Series (batch size=&lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">b&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s1">)&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">ax&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">set_xticks&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">ax&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">set_xticklabels&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">model_names&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">rotation&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">45&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ha&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;right&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">ax&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">legend&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">ax&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">grid&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">axis&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;y&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">alpha&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.3&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tight_layout&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">show&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>Nvidia-GPU specs</title><link>https://maosong.website/p/nvidia-gpu-specs/</link><pubDate>Wed, 14 Jan 2026 11:09:19 +0800</pubDate><guid>https://maosong.website/p/nvidia-gpu-specs/</guid><description>&lt;h2 id="v100">&lt;a href="#v100" class="header-anchor">&lt;/a>V100
&lt;/h2>&lt;h3 id="v100-关键改进">&lt;a href="#v100-%e5%85%b3%e9%94%ae%e6%94%b9%e8%bf%9b" class="header-anchor">&lt;/a>V100 关键改进
&lt;/h3>&lt;ul>
&lt;li>Volta architecture&lt;/li>
&lt;li>SM architecture: 支持深度学习&lt;/li>
&lt;li>2nd NVIDIA NVLink&lt;/li>
&lt;li>HBM2 memory&lt;/li>
&lt;li>Volta Multi-process Service&lt;/li>
&lt;/ul>
&lt;h3 id="v100-技术规格">&lt;a href="#v100-%e6%8a%80%e6%9c%af%e8%a7%84%e6%a0%bc" class="header-anchor">&lt;/a>V100 技术规格
&lt;/h3>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Tesla Product&lt;/th>
&lt;th>Tesla K40&lt;/th>
&lt;th>Tesla M40&lt;/th>
&lt;th>Tesla P100&lt;/th>
&lt;th>Tesla V100&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>GPU&lt;/td>
&lt;td>GK180 (Kepler)&lt;/td>
&lt;td>GM200 (Maxwell)&lt;/td>
&lt;td>GP100 (Pascal)&lt;/td>
&lt;td>GV100 (Volta)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SMs&lt;/td>
&lt;td>15&lt;/td>
&lt;td>24&lt;/td>
&lt;td>56&lt;/td>
&lt;td>80&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TPCs&lt;/td>
&lt;td>15&lt;/td>
&lt;td>24&lt;/td>
&lt;td>28&lt;/td>
&lt;td>40&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FP32 Cores / GPU&lt;/td>
&lt;td>2880&lt;/td>
&lt;td>3072&lt;/td>
&lt;td>3584&lt;/td>
&lt;td>5120&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FP64 Cores / GPU&lt;/td>
&lt;td>960&lt;/td>
&lt;td>96&lt;/td>
&lt;td>1792&lt;/td>
&lt;td>2560&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Tensor Cores / GPU&lt;/td>
&lt;td>NA&lt;/td>
&lt;td>NA&lt;/td>
&lt;td>NA&lt;/td>
&lt;td>640&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GPU Boost Clock&lt;/td>
&lt;td>810/875 MHz&lt;/td>
&lt;td>1114 MHz&lt;/td>
&lt;td>1480 MHz&lt;/td>
&lt;td>1530 MHz&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Peak FP32 TFLOPS²&lt;/td>
&lt;td>5&lt;/td>
&lt;td>6.8&lt;/td>
&lt;td>10.6&lt;/td>
&lt;td>15.7&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Peak FP64 TFLOPS²&lt;/td>
&lt;td>1.7&lt;/td>
&lt;td>.21&lt;/td>
&lt;td>5.3&lt;/td>
&lt;td>7.8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Peak Tensor TFLOPS²&lt;/td>
&lt;td>NA&lt;/td>
&lt;td>NA&lt;/td>
&lt;td>NA&lt;/td>
&lt;td>125&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Memory Size&lt;/td>
&lt;td>Up to 12 GB&lt;/td>
&lt;td>Up to 24 GB&lt;/td>
&lt;td>16 GB&lt;/td>
&lt;td>16 GB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Memory Interface&lt;/td>
&lt;td>384-bit GDDR5&lt;/td>
&lt;td>384-bit GDDR5&lt;/td>
&lt;td>4096-bit HBM2&lt;/td>
&lt;td>4096-bit HBM2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TDP&lt;/td>
&lt;td>235 Watts&lt;/td>
&lt;td>250 Watts&lt;/td>
&lt;td>300 Watts&lt;/td>
&lt;td>300 Watts&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Manufacturing Process&lt;/td>
&lt;td>28 nm&lt;/td>
&lt;td>28 nm&lt;/td>
&lt;td>16 nm FinFET+&lt;/td>
&lt;td>12 nm FFN&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>内存规格&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>GPU&lt;/th>
&lt;th>Kepler GK180&lt;/th>
&lt;th>Maxwell GM200&lt;/th>
&lt;th>Pascal GP100&lt;/th>
&lt;th>Volta GV100&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Compute Capability&lt;/td>
&lt;td>3.5&lt;/td>
&lt;td>5.2&lt;/td>
&lt;td>6.0&lt;/td>
&lt;td>7.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Threads / Warp&lt;/td>
&lt;td>32&lt;/td>
&lt;td>32&lt;/td>
&lt;td>32&lt;/td>
&lt;td>32&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Max Warps / SM&lt;/td>
&lt;td>64&lt;/td>
&lt;td>64&lt;/td>
&lt;td>64&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Max Threads / SM&lt;/td>
&lt;td>2048&lt;/td>
&lt;td>2048&lt;/td>
&lt;td>2048&lt;/td>
&lt;td>2048&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Max Thread Blocks / SM&lt;/td>
&lt;td>32&lt;/td>
&lt;td>32&lt;/td>
&lt;td>32&lt;/td>
&lt;td>32&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Max 32-bit Registers / SM&lt;/td>
&lt;td>65536&lt;/td>
&lt;td>65536&lt;/td>
&lt;td>65536&lt;/td>
&lt;td>65536&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Max Registers / Block&lt;/td>
&lt;td>65536&lt;/td>
&lt;td>65536&lt;/td>
&lt;td>65536&lt;/td>
&lt;td>65536&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Max Registers / Thread&lt;/td>
&lt;td>255&lt;/td>
&lt;td>255&lt;/td>
&lt;td>255&lt;/td>
&lt;td>255&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Max Thread Block Size&lt;/td>
&lt;td>1024&lt;/td>
&lt;td>1024&lt;/td>
&lt;td>1024&lt;/td>
&lt;td>1024&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FP32 Cores / SM&lt;/td>
&lt;td>192&lt;/td>
&lt;td>128&lt;/td>
&lt;td>64&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ratio of SM Registers to FP32 Cores&lt;/td>
&lt;td>341&lt;/td>
&lt;td>512&lt;/td>
&lt;td>1024&lt;/td>
&lt;td>1024&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Shared Memory Size / SM&lt;/td>
&lt;td>16 KB/32 KB/ 48 KB&lt;/td>
&lt;td>96 KB&lt;/td>
&lt;td>64 KB&lt;/td>
&lt;td>Configurable up to 96 KB&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>系统规格&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Specification&lt;/th>
&lt;th>DGX-1 (Tesla P100)&lt;/th>
&lt;th>DGX-1 (Tesla V100)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>GPU&lt;/td>
&lt;td>8x Tesla P100 GPUs&lt;/td>
&lt;td>8x Tesla V100 GPUs&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TFLOPS&lt;/td>
&lt;td>170 (GPU FP16) + 3 (CPU FP32)&lt;/td>
&lt;td>1 (GPU Tensor PFLOP)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GPU Memory&lt;/td>
&lt;td>16 GB per GPU / 128 GB per DGX-1 Node&lt;/td>
&lt;td>16 GB or 32 GB per GPU / 128-256 GB per DGX-1 Node&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CPU&lt;/td>
&lt;td>Dual 20-core Intel® Xeon® E5-2698 v4&lt;/td>
&lt;td>Dual 20-core Intel® Xeon® E5-2698 v4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FP32 CUDA Cores&lt;/td>
&lt;td>28,672 Cores&lt;/td>
&lt;td>40,960 Cores&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>System Memory&lt;/td>
&lt;td>Up to 512 GB 2133 MHz DDR4 LRDIMM&lt;/td>
&lt;td>Up to 512 GB 2133 MHz DDR4 LRDIMM&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Storage&lt;/td>
&lt;td>4x 1.92 TB SSD RAID 0&lt;/td>
&lt;td>4x 1.92 TB SSD RAID 0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Network Interconnect&lt;/td>
&lt;td>Dual 10 GbE, 4 IB EDR&lt;/td>
&lt;td>Dual 10 GbE, 4 IB EDR&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>System Dimensions&lt;/td>
&lt;td>866 D x 444 W x 131 H (mm)&lt;/td>
&lt;td>866 D x 444 W x 131 H (mm)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>System Weight&lt;/td>
&lt;td>80 lbs&lt;/td>
&lt;td>80 lbs&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Max Power TDP&lt;/td>
&lt;td>3200 W&lt;/td>
&lt;td>3200 W&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Operating Temp&lt;/td>
&lt;td>10 - 35°C&lt;/td>
&lt;td>10 - 35°C&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="a100">&lt;a href="#a100" class="header-anchor">&lt;/a>A100
&lt;/h2>&lt;h3 id="a100-关键改进">&lt;a href="#a100-%e5%85%b3%e9%94%ae%e6%94%b9%e8%bf%9b" class="header-anchor">&lt;/a>A100 关键改进
&lt;/h3>&lt;ul>
&lt;li>Ampere 架构：使用 MIG 来将 A100 切分为更小的实例或者链接更多 GPU&lt;/li>
&lt;li>Tensor Cores: 312 TFLOPs/s&lt;/li>
&lt;li>NVLink: 更高的 throughput&lt;/li>
&lt;li>MIG (multi-instance GPU): 一个 A100 可以切分为至多 7 个硬件层面隔离的实例&lt;/li>
&lt;li>HBM2e: 更大的 HBM, 更快的 bandwidth, 更高的 DRAM 使用效率&lt;/li>
&lt;li>structure sparsity: 稀疏运算可以带来 2 倍的算力提升&lt;/li>
&lt;/ul>
&lt;h3 id="a100-技术规格">&lt;a href="#a100-%e6%8a%80%e6%9c%af%e8%a7%84%e6%a0%bc" class="header-anchor">&lt;/a>A100 技术规格
&lt;/h3>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>A100 80GB PCIe&lt;/th>
&lt;th>A100 80GB SXM&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>FP64&lt;/td>
&lt;td>9.7 TFLOPS&lt;/td>
&lt;td>9.7 TFLOPS&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FP64 Tensor Core&lt;/td>
&lt;td>19.5 TFLOPS&lt;/td>
&lt;td>19.5 TFLOPS&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FP32&lt;/td>
&lt;td>19.5 TFLOPS&lt;/td>
&lt;td>19.5 TFLOPS&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Tensor Float 32 (TF32)&lt;/td>
&lt;td>156 TFLOPS | 312 TFLOPS&lt;/td>
&lt;td>156 TFLOPS | 312 TFLOPS*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>BFLOAT16 Tensor Core&lt;/td>
&lt;td>312 TFLOPS | 624 TFLOPS*&lt;/td>
&lt;td>312 TFLOPS | 624 TFLOPS*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FP16 Tensor Core&lt;/td>
&lt;td>312 TFLOPS | 624 TFLOPS*&lt;/td>
&lt;td>312 TFLOPS | 624 TFLOPS*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>INT8 Tensor Core&lt;/td>
&lt;td>624 TOPS | 1248 TOPS*&lt;/td>
&lt;td>624 TOPS | 1248 TOPS*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GPU Memory&lt;/td>
&lt;td>80GB HBM2e&lt;/td>
&lt;td>80GB HBM2e&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GPU Memory Bandwidth&lt;/td>
&lt;td>1,935 GB/s&lt;/td>
&lt;td>2,039 GB/s&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Max Thermal Design Power (TDP)&lt;/td>
&lt;td>300W&lt;/td>
&lt;td>400W ***&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Multi-Instance GPU&lt;/td>
&lt;td>Up to 7 MIGs @ 10GB&lt;/td>
&lt;td>Up to 7 MIGs @ 10GB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Form Factor&lt;/td>
&lt;td>PCIe &lt;br>Dual-slot air-cooled or single-slot liquid-cooled&lt;/td>
&lt;td>SXM&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Interconnect&lt;/td>
&lt;td>NVIDIA® NVLink® Bridge &lt;br>for 2 GPUs: 600 GB/s ** &lt;br>PCIe Gen4: 64 GB/s&lt;/td>
&lt;td>NVLink: 600 GB/s &lt;br>PCIe Gen4: 64 GB/s&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Server Options&lt;/td>
&lt;td>Partner and NVIDIA-Certified Systems™ with 1-8 GPUs&lt;/td>
&lt;td>NVIDIA HGX™ A100-Partner and NVIDIA-Certified Systems with 4,8, or 16 GPUs NVIDIA DGX™ A100 with 8 GPUs&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="h100">&lt;a href="#h100" class="header-anchor">&lt;/a>H100
&lt;/h2>&lt;h3 id="h100-关键改进">&lt;a href="#h100-%e5%85%b3%e9%94%ae%e6%94%b9%e8%bf%9b" class="header-anchor">&lt;/a>H100 关键改进
&lt;/h3>&lt;ul>
&lt;li>Hopper 架构&lt;/li>
&lt;li>Tensor Core: 更强的 tensor core&lt;/li>
&lt;li>transformer engine: 加速基于 transformer 架构模型的训练&lt;/li>
&lt;li>NVLink: 900GB/s 的 bandwidth&lt;/li>
&lt;li>2nd MIG: 支持 multi-tenant, multi-user 使用&lt;/li>
&lt;li>DPX: 基于 DPX 指令集加速动态规划算法&lt;/li>
&lt;/ul>
&lt;h3 id="h100-技术规格">&lt;a href="#h100-%e6%8a%80%e6%9c%af%e8%a7%84%e6%a0%bc" class="header-anchor">&lt;/a>H100 技术规格
&lt;/h3>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>H100 SXM&lt;/th>
&lt;th>H100 NVL&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>FP64&lt;/td>
&lt;td>34 teraFLOPS&lt;/td>
&lt;td>30 teraFLOPs&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FP64 Tensor Core&lt;/td>
&lt;td>67 teraFLOPS&lt;/td>
&lt;td>60 teraFLOPs&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FP32&lt;/td>
&lt;td>67 teraFLOPS&lt;/td>
&lt;td>60 teraFLOPs&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TF32 Tensor Core*&lt;/td>
&lt;td>989 teraFLOPS&lt;/td>
&lt;td>835 teraFLOPs&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>BFLOAT16 Tensor Core*&lt;/td>
&lt;td>1,979 teraFLOPS&lt;/td>
&lt;td>1,671 teraFLOPS&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FP16 Tensor Core*&lt;/td>
&lt;td>1,979 teraFLOPS&lt;/td>
&lt;td>1,671 teraFLOPS&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FP8 Tensor Core*&lt;/td>
&lt;td>3,958 teraFLOPS&lt;/td>
&lt;td>3,341 teraFLOPS&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>INT8 Tensor Core*&lt;/td>
&lt;td>3,958 teraFLOPS&lt;/td>
&lt;td>3,341 teraFLOPS&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GPU Memory&lt;/td>
&lt;td>80GB&lt;/td>
&lt;td>94GB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GPU Memory Bandwidth&lt;/td>
&lt;td>3.35TB/s&lt;/td>
&lt;td>3.9TB/s&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Decoders&lt;/td>
&lt;td>7 NVDEC &lt;br>7 JPEG&lt;/td>
&lt;td>7 NVDEC &lt;br>7 JPEG&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Max Thermal Design Power (TDP)&lt;/td>
&lt;td>Up to 700W (configurable)&lt;/td>
&lt;td>350-400W (configurable)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Multi-Instance GPUs&lt;/td>
&lt;td>Up to 7 MIGS @ 10GB each&lt;/td>
&lt;td>Up to 7 MIGS @ 12GB each&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Form Factor&lt;/td>
&lt;td>SXM&lt;/td>
&lt;td>PCIe &lt;br>dual-slot air-cooled&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Interconnect&lt;/td>
&lt;td>NVIDIA NVLink™: 900GB/s &lt;br>PCIe Gen5: 128GB/s&lt;/td>
&lt;td>NVIDIA NVLink: 600GB/s &lt;br>PCIe Gen5: 128GB/s&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Server Options&lt;/td>
&lt;td>NVIDIA HGX H100 Partner and NVIDIA- &lt;br>Certified Systems™ with 4 or 8 GPUs &lt;br>NVIDIA DGX H100 with 8 GPUs&lt;/td>
&lt;td>Partner and NVIDIA-Certified Systems with 1–8 GPUs&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>NVIDIA AI Enterprise&lt;/td>
&lt;td>Add-on&lt;/td>
&lt;td>Included&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="h200">&lt;a href="#h200" class="header-anchor">&lt;/a>H200
&lt;/h2>&lt;h3 id="h200-关键改进">&lt;a href="#h200-%e5%85%b3%e9%94%ae%e6%94%b9%e8%bf%9b" class="header-anchor">&lt;/a>H200 关键改进
&lt;/h3>&lt;ul>
&lt;li>更高的 HBM 内存和带宽&lt;/li>
&lt;li>更高的 LLM inference 速度&lt;/li>
&lt;/ul>
&lt;h3 id="h200-技术规格">&lt;a href="#h200-%e6%8a%80%e6%9c%af%e8%a7%84%e6%a0%bc" class="header-anchor">&lt;/a>H200 技术规格
&lt;/h3>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>H200 SXM&lt;/th>
&lt;th>H200 NVL&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>FP64&lt;/td>
&lt;td>34 teraFLOPS&lt;/td>
&lt;td>30 teraFLOPs&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FP64 Tensor Core&lt;/td>
&lt;td>67 teraFLOPS&lt;/td>
&lt;td>60 teraFLOPs&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FP32&lt;/td>
&lt;td>67 teraFLOPS&lt;/td>
&lt;td>60 teraFLOPs&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TF32 Tensor Core*&lt;/td>
&lt;td>989 teraFLOPS&lt;/td>
&lt;td>835 teraFLOPs&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>BFLOAT16 Tensor Core*&lt;/td>
&lt;td>1,979 teraFLOPS&lt;/td>
&lt;td>1,671 teraFLOPS&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FP16 Tensor Core*&lt;/td>
&lt;td>1,979 teraFLOPS&lt;/td>
&lt;td>1,671 teraFLOPS&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FP8 Tensor Core*&lt;/td>
&lt;td>3,958 teraFLOPS&lt;/td>
&lt;td>3,341 teraFLOPS&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>INT8 Tensor Core*&lt;/td>
&lt;td>3,958 teraFLOPS&lt;/td>
&lt;td>3,341 teraFLOPS&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GPU Memory&lt;/td>
&lt;td>&lt;strong>141GB&lt;/strong>&lt;/td>
&lt;td>&lt;strong>141GB&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GPU Memory Bandwidth&lt;/td>
&lt;td>&lt;strong>4.8TB/s&lt;/strong>&lt;/td>
&lt;td>&lt;strong>4.8TB/s&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Decoders&lt;/td>
&lt;td>7 NVDEC &lt;br>7 JPEG&lt;/td>
&lt;td>7 NVDEC &lt;br>7 JPEG&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Confidential Computing&lt;/td>
&lt;td>Supported&lt;/td>
&lt;td>Supported&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Max Thermal Design Power (TDP)&lt;/td>
&lt;td>Up to 700W (configurable)&lt;/td>
&lt;td>Up to &lt;strong>600W&lt;/strong> (configurable)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Multi-Instance GPUs&lt;/td>
&lt;td>Up to 7 MIGS @ &lt;strong>18GB&lt;/strong> each&lt;/td>
&lt;td>Up to 7 MIGS @ &lt;strong>18GB&lt;/strong> each&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Form Factor&lt;/td>
&lt;td>SXM&lt;/td>
&lt;td>PCIe &lt;br>dual-slot air-cooled&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Interconnect&lt;/td>
&lt;td>NVIDIA NVLink™: 900GB/s &lt;br>PCIe Gen5: 128GB/s&lt;/td>
&lt;td>2- or 4-way NVIDIA NVLink bridge: ** 900GB/s** per GPU&lt;br>PCIe Gen5: 128GB/s&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Server Options&lt;/td>
&lt;td>NVIDIA HGX H200 Partner and NVIDIA- &lt;br>Certified Systems™ with 4 or 8 GPUs&lt;/td>
&lt;td>NVIDIA MGX™ H200 NVL partner and &lt;br>NVIDIA-Certified Systems with up to 8 GPUs&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>NVIDIA AI Enterprise&lt;/td>
&lt;td>Add-on&lt;/td>
&lt;td>Included&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>相比于 H100, H200 升级了 HBM 和 bandwidth&lt;/p>
&lt;h2 id="b200">&lt;a href="#b200" class="header-anchor">&lt;/a>B200
&lt;/h2>&lt;h3 id="b200-关键改进">&lt;a href="#b200-%e5%85%b3%e9%94%ae%e6%94%b9%e8%bf%9b" class="header-anchor">&lt;/a>B200 关键改进
&lt;/h3>&lt;ul>
&lt;li>blackwell 架构： GPU 之间的通信效率大幅度提升&lt;/li>
&lt;li>Grace CPU: GPU 可以与 Grace CPu 之间达到 900GB/s 的 bidirectional bandwidth&lt;/li>
&lt;li>5th NVIDIA NVLink: 可以链接 576 块 GPU 来支持计算，NVlink 的带宽可以达到 130TB/s&lt;/li>
&lt;li>RAS engine: 自动识别故障来提高效率&lt;/li>
&lt;li>NVIDIA networking&lt;/li>
&lt;/ul>
&lt;h3 id="b2100-技术规格">&lt;a href="#b2100-%e6%8a%80%e6%9c%af%e8%a7%84%e6%a0%bc" class="header-anchor">&lt;/a>B2100 技术规格
&lt;/h3>&lt;p>system specification 如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Specification&lt;/th>
&lt;th>GB200 NVL72&lt;/th>
&lt;th>GB200 NVL4&lt;/th>
&lt;th>HGX B200&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>NVIDIA Blackwell GPUs | Grace CPUs&lt;/td>
&lt;td>72 | 36&lt;/td>
&lt;td>4 | 2&lt;/td>
&lt;td>8 | 0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CPU Cores&lt;/td>
&lt;td>2,592 Arm® Neoverse V2 Cores&lt;/td>
&lt;td>144 Arm Neoverse V2 Cores&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Total NVFP4 Tensor Core²&lt;/td>
&lt;td>1,440 | 720 PFLOPS&lt;/td>
&lt;td>80 | 40 PFLOPS&lt;/td>
&lt;td>144 | 72 PFLOPS&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Total FP8/FP6 Tensor Core²&lt;/td>
&lt;td>720 PFLOPS&lt;/td>
&lt;td>40 PFLOPS&lt;/td>
&lt;td>72 PFLOPS&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Total Fast Memory&lt;/td>
&lt;td>31 TB&lt;/td>
&lt;td>1.8 TB&lt;/td>
&lt;td>1.4 TB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Total Memory Bandwidth&lt;/td>
&lt;td>576 TB/s&lt;/td>
&lt;td>32 TB/s&lt;/td>
&lt;td>62 TB/s&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Total NVLink Bandwidth&lt;/td>
&lt;td>130 TB/s&lt;/td>
&lt;td>7.2 TB/s&lt;/td>
&lt;td>14.4 TB/s&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>individual specification 如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Specification&lt;/th>
&lt;th>GB200 NVL72&lt;/th>
&lt;th>GB200 NVL4&lt;/th>
&lt;th>HGX B200&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>FP4 Tensor Core&lt;/td>
&lt;td>20 PFLOPS&lt;/td>
&lt;td>20 PFLOPS&lt;/td>
&lt;td>18 PFLOPS&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FP8/FP6 Tensor Core²&lt;/td>
&lt;td>10 PFLOPS&lt;/td>
&lt;td>10 PFLOPS&lt;/td>
&lt;td>9 PFLOPS&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>INT8 Tensor Core²&lt;/td>
&lt;td>10 POPS&lt;/td>
&lt;td>10 POPS&lt;/td>
&lt;td>9 POPS&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FP16/BF16 Tensor Core²&lt;/td>
&lt;td>5 PFLOPS&lt;/td>
&lt;td>5 PFLOPS&lt;/td>
&lt;td>4.5 PFLOPS&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TF32 Tensor Core²&lt;/td>
&lt;td>2.5 PFLOPS&lt;/td>
&lt;td>2.5 PFLOPS&lt;/td>
&lt;td>2.2 PFLOPS&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FP32&lt;/td>
&lt;td>80 TFLOPS&lt;/td>
&lt;td>80 TFLOPS&lt;/td>
&lt;td>75 TFLOPS&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FP64 / FP64 Tensor Core&lt;/td>
&lt;td>40 TFLOPS&lt;/td>
&lt;td>40 TFLOPS&lt;/td>
&lt;td>37 TFLOPS&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GPU Memory &lt;br>Bandwidth&lt;/td>
&lt;td>186 GB HBM3E &lt;br>8 TB/s&lt;/td>
&lt;td>186 GB HBM3E &lt;br>8 TB/s&lt;/td>
&lt;td>180 GB HBM3E &lt;br>7.7 TB/s&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Multi-Instance GPU (MIG)&lt;/td>
&lt;td>-&lt;/td>
&lt;td>7&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Decompression Engine&lt;/td>
&lt;td>-&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Decoders&lt;/td>
&lt;td>-&lt;/td>
&lt;td>7 NVDEC³ &lt;br>7 nvJPEG&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Max Thermal Design Power (TDP)&lt;/td>
&lt;td>Configurable up to 1,200 W&lt;/td>
&lt;td>Configurable up to 1,200 W&lt;/td>
&lt;td>Configurable up to 1,000 W&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Interconnect&lt;/td>
&lt;td>-&lt;/td>
&lt;td>Fifth-generation NVLink: 1.8 TB/s &lt;br>PCIe Gen5: 128 GB/s&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Server Options&lt;/td>
&lt;td>NVIDIA GB200 NVL72 partner and NVIDIA-Certified Systems™ with 72 GPUs&lt;/td>
&lt;td>NVIDIA MGX partner and NVIDIA-Certified Systems&lt;/td>
&lt;td>NVIDIA HGX B200 partner and NVIDIA-Certified Systems with 8 GPUs&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="b300">&lt;a href="#b300" class="header-anchor">&lt;/a>B300
&lt;/h2>&lt;h3 id="b300-关键改进">&lt;a href="#b300-%e5%85%b3%e9%94%ae%e6%94%b9%e8%bf%9b" class="header-anchor">&lt;/a>B300 关键改进
&lt;/h3>&lt;ul>
&lt;li>Blackwell 架构&lt;/li>
&lt;li>AI reasoning inference: 支持 test-time scaling, 对 attention layer 和 FLOPs 都有加速&lt;/li>
&lt;li>HBM3e: 支持更大的 batch size 和 throughput&lt;/li>
&lt;li>ConnectX-8 SuperNIC, 一个 host2 个 ConnectX-8 设备，支持 800Gb/s 的 GPU 之间通信&lt;/li>
&lt;li>Grace-CPU: 更强的表现和带宽&lt;/li>
&lt;li>5th NVIDIA NVLink: 更高的通信效率&lt;/li>
&lt;/ul>
&lt;h3 id="b3100-技术规格">&lt;a href="#b3100-%e6%8a%80%e6%9c%af%e8%a7%84%e6%a0%bc" class="header-anchor">&lt;/a>B3100 技术规格
&lt;/h3>&lt;p>system specification 如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>GB300 NVL72&lt;/th>
&lt;th>HGX B300&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Blackwell Ultra GPUs| Grace CPUs&lt;/td>
&lt;td>72 | 36&lt;/td>
&lt;td>8 | 0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CPU Cores&lt;/td>
&lt;td>2,592 Arm Neoverse V2 Cores&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Total FP4 Tensor Core&lt;/td>
&lt;td>1 1,440 PFLOPS | 1,080 PFLOPS&lt;/td>
&lt;td>144 PFLOPS | 108 PFLOPS&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Total FP8/FP6 Tensor Core&lt;/td>
&lt;td>2 720 PFLOPS&lt;/td>
&lt;td>72 PFLOPS&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Total Fast Memory&lt;/td>
&lt;td>37 TB&lt;/td>
&lt;td>2.1 TB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Total Memory Bandwidth&lt;/td>
&lt;td>576 TB/s&lt;/td>
&lt;td>62 TB/s&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Total NVLink Switch Bandwidth&lt;/td>
&lt;td>130 TB/s&lt;/td>
&lt;td>14.4 TB/s&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>individual specification 如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>GB300 NVL72&lt;/th>
&lt;th>HGX B300&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>FP4 Tensor Core&lt;/td>
&lt;td>20 PFLOPS | 15 PFLOPS&lt;/td>
&lt;td>18 PFLOPS | 14 PFLOPS&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FP8/FP6 Tensor Core2&lt;/td>
&lt;td>10 PFLOPS&lt;/td>
&lt;td>9 PFLOPS&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>INT8 Tensor Core2&lt;/td>
&lt;td>330 TOPS&lt;/td>
&lt;td>307 TOPS&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FP16/BF16 Tensor Core&lt;/td>
&lt;td>5 PFLOPS&lt;/td>
&lt;td>4.5 PLFOPS&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TF32 Tensor Core2&lt;/td>
&lt;td>2.5 PFLOPS&lt;/td>
&lt;td>2.2 PFLOPS&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FP32&lt;/td>
&lt;td>80 TFLOPS&lt;/td>
&lt;td>75 TFLOPS&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FP64/FP64 Tensor Core&lt;/td>
&lt;td>1.3 TFLOPS&lt;/td>
&lt;td>1.2 TFLOPS&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GPU Memory | Bandwidth&lt;/td>
&lt;td>279 GB HBM3E | 8 TB/s&lt;/td>
&lt;td>270 GB HBM3E | 7.7 TB/s&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Multi-Instance GPU (MIG)&lt;/td>
&lt;td>7&lt;/td>
&lt;td>7&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Decompression Engine&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Decoders&lt;/td>
&lt;td>7 NVDEC3 &lt;br>7 nvJPEG&lt;/td>
&lt;td>7 NVDEC3 &lt;br>7 nvJPEG&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Max Thermal Design Power (TDP)&lt;/td>
&lt;td>Configurable up to 1,400 W&lt;/td>
&lt;td>Configurable up to 1,100 W&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Interconnect&lt;/td>
&lt;td>Fifth-Generation NVLink: 1.8 TB/s &lt;br>PCIe Gen6: 256 GB/s&lt;/td>
&lt;td>Fifth-Generation NVLink: 1.8 TB/s &lt;br>PCIe Gen6: 256 GB/s&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Server Options&lt;/td>
&lt;td>NVIDIA GB300 NVL72 partner and &lt;br>NVIDIA-Certified Systems™&lt;/td>
&lt;td>NVIDIA HGX B300 partner and &lt;br>NVIDIA-Certified Systems&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf" target="_blank" rel="noopener"
>V100 white paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.nvidia.com/en-us/data-center/a100/" target="_blank" rel="noopener"
>A100&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://resources.nvidia.com/en-us-hopper-architecture/nvidia-h100-tensor-c" target="_blank" rel="noopener"
>Hopper Architecture&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.nvidia.com/en-us/data-center/h100/" target="_blank" rel="noopener"
>H100&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.nvidia.com/en-us/data-center/h200/" target="_blank" rel="noopener"
>H200&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.nvidia.com/en-us/data-center/gb200-nvl72/" target="_blank" rel="noopener"
>B200&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://resources.nvidia.com/en-us-blackwell-architecture/blackwell-ultra-datasheet" target="_blank" rel="noopener"
>B300&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://resources.nvidia.com/en-us-gpu-resources/blackwell-ultra-datasheet?lx=CPwSfP" target="_blank" rel="noopener"
>blackwell&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on GLaM</title><link>https://maosong.website/p/notes-on-glam/</link><pubDate>Tue, 06 Jan 2026 18:07:29 +0800</pubDate><guid>https://maosong.website/p/notes-on-glam/</guid><description>&lt;p>Google 在 2022 年 8 提出了 GLaM，一个基于 MoE 架构的大语言模型系列，模型超过了 GPT-3 的表现&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者在本文中证明了基于 MoE 架构的大语言模型可以达到与 dense 模型相同的性能，且计算更加高效。作者构建了 GLaM 大语言模型系列，包括 1.9B-A0.1B, 105B-A1.9B, 143B-A9.8B 以及 1.2T-A96.6B 等模型，尽管只激活了不到 $8\%$ 的参数，模型的 zero-shot, one-shot 和 few-shot 表现均超过了 GPT-3.&lt;/p>
&lt;p>作者认为，在相同的算力下，MoE 模型比 dense 模型的表现更好，MoE 模型是一个非常具有前景的方向。&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;p>训练数据集包括 1.6T token, 具体分布如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Dataset&lt;/th>
&lt;th>Tokens (B)&lt;/th>
&lt;th>Weight in mixture&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Filtered Webpages&lt;/td>
&lt;td>143&lt;/td>
&lt;td>0.42&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Wikipedia&lt;/td>
&lt;td>3&lt;/td>
&lt;td>0.06&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Conversations&lt;/td>
&lt;td>174&lt;/td>
&lt;td>0.28&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Forums&lt;/td>
&lt;td>247&lt;/td>
&lt;td>0.02&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Books&lt;/td>
&lt;td>390&lt;/td>
&lt;td>0.20&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>News&lt;/td>
&lt;td>650&lt;/td>
&lt;td>0.02&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>模型架构如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-glam/GLaM-architecture.png"
width="554"
height="681"
loading="lazy"
alt="GLaM model architecture"
class="gallery-image"
data-flex-grow="81"
data-flex-basis="195px"
>&lt;/p>
&lt;p>GLaM 的模型架构与 &lt;a class="link" href="https://maosong.website/p/gshard/" target="_blank" rel="noopener"
>GShard&lt;/a> 基本相同，作者将 transformer block 按照两个为一组，一组中一个 block 为 dense FFN, 另一个为 MoE layer, 交替进行。MoE layer 中，总专家个数为 64 个，激活专家个数为 2 个。&lt;/p>
&lt;p>模型配置如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>GLaM Model&lt;/th>
&lt;th>Type&lt;/th>
&lt;th>$n_{\text{params}}$&lt;/th>
&lt;th>$n_{\text{act-params}}$&lt;/th>
&lt;th>$L$&lt;/th>
&lt;th>$M$&lt;/th>
&lt;th>$H$&lt;/th>
&lt;th>$n_{\text{heads}}$&lt;/th>
&lt;th>$d_{\text{head}}$&lt;/th>
&lt;th>$E$&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>0.1B&lt;/td>
&lt;td>Dense&lt;/td>
&lt;td>130M&lt;/td>
&lt;td>130M&lt;/td>
&lt;td>12&lt;/td>
&lt;td>768&lt;/td>
&lt;td>3,072&lt;/td>
&lt;td>12&lt;/td>
&lt;td>64&lt;/td>
&lt;td>$-$&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>0.1B/64E&lt;/td>
&lt;td>MoE&lt;/td>
&lt;td>1.9B&lt;/td>
&lt;td>145M&lt;/td>
&lt;td>12&lt;/td>
&lt;td>768&lt;/td>
&lt;td>3,072&lt;/td>
&lt;td>12&lt;/td>
&lt;td>64&lt;/td>
&lt;td>64&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1.7B&lt;/td>
&lt;td>Dense&lt;/td>
&lt;td>1.7B&lt;/td>
&lt;td>1.700B&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>$-$&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1.7B/32E&lt;/td>
&lt;td>MoE&lt;/td>
&lt;td>20B&lt;/td>
&lt;td>1.878B&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>32&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1.7B/64E&lt;/td>
&lt;td>MoE&lt;/td>
&lt;td>27B&lt;/td>
&lt;td>1.879B&lt;/td>
&lt;td>24&lt;/td>
&lt;td>2,048&lt;/td>
&lt;td>8,192&lt;/td>
&lt;td>16&lt;/td>
&lt;td>128&lt;/td>
&lt;td>64&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1.7B/128E&lt;/td>
&lt;td>MoE&lt;/td>
&lt;td>53B&lt;/td>
&lt;td>1.881B&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>128&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1.7B/256E&lt;/td>
&lt;td>MoE&lt;/td>
&lt;td>105B&lt;/td>
&lt;td>1.886B&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>256&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>8B&lt;/td>
&lt;td>Dense&lt;/td>
&lt;td>8.7B&lt;/td>
&lt;td>8.7B&lt;/td>
&lt;td>32&lt;/td>
&lt;td>4,096&lt;/td>
&lt;td>16,384&lt;/td>
&lt;td>32&lt;/td>
&lt;td>128&lt;/td>
&lt;td>$-$&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>8B/64E&lt;/td>
&lt;td>MoE&lt;/td>
&lt;td>143B&lt;/td>
&lt;td>9.8B&lt;/td>
&lt;td>32&lt;/td>
&lt;td>4,096&lt;/td>
&lt;td>16,384&lt;/td>
&lt;td>32&lt;/td>
&lt;td>128&lt;/td>
&lt;td>64&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>137B&lt;/td>
&lt;td>Dense&lt;/td>
&lt;td>137B&lt;/td>
&lt;td>137B&lt;/td>
&lt;td>64&lt;/td>
&lt;td>8,192&lt;/td>
&lt;td>65,536&lt;/td>
&lt;td>128&lt;/td>
&lt;td>128&lt;/td>
&lt;td>$-$&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>64B/64E&lt;/td>
&lt;td>MoE&lt;/td>
&lt;td>1.2T&lt;/td>
&lt;td>96.6B&lt;/td>
&lt;td>64&lt;/td>
&lt;td>8,192&lt;/td>
&lt;td>32,768&lt;/td>
&lt;td>128&lt;/td>
&lt;td>128&lt;/td>
&lt;td>64&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>作者用 GLaM(8B/64E) 来表示一个 8B 参数的 dense model 中每隔一层被转换为 MoE layer&lt;/p>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>训练时，作者设置模型的上下文为 1024 token, batch size 为 1M, 优化器为 Adafactor, 作者还使用了 &lt;a class="link" href="https://maosong.website/p/gshard/" target="_blank" rel="noopener"
>GShard&lt;/a> 中提出来的 load balancing loss, tokenizer 为 SentencePiece&lt;/p>
&lt;p>GLaM 与 GPT-3 的对比如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>GPT-3&lt;/th>
&lt;th>GLAM&lt;/th>
&lt;th>relative&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>cost&lt;/td>
&lt;td>FLOPs / token (G)&lt;/td>
&lt;td>350&lt;/td>
&lt;td>&lt;strong>180&lt;/strong>&lt;/td>
&lt;td>-48.6%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Train energy (MWh)&lt;/td>
&lt;td>1287&lt;/td>
&lt;td>&lt;strong>456&lt;/strong>&lt;/td>
&lt;td>-64.6%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>accuracy on average&lt;/td>
&lt;td>Zero-shot&lt;/td>
&lt;td>56.9&lt;/td>
&lt;td>&lt;strong>62.7&lt;/strong>&lt;/td>
&lt;td>+10.2%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>One-shot&lt;/td>
&lt;td>61.6&lt;/td>
&lt;td>&lt;strong>65.5&lt;/strong>&lt;/td>
&lt;td>+6.3%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Few-shot&lt;/td>
&lt;td>65.2&lt;/td>
&lt;td>&lt;strong>68.1&lt;/strong>&lt;/td>
&lt;td>+4.4%&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>实验结果显示，GLaM 的训练效率更高，且表现更好&lt;/p>
&lt;p>作者还探究了提升 expert 个数对最终表现的影响，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-glam/GLaM-performance-vs-num-experts.png"
width="686"
height="354"
loading="lazy"
alt="Performance vs number of experts"
class="gallery-image"
data-flex-grow="193"
data-flex-basis="465px"
>&lt;/p>
&lt;p>可以看到，随着专家个数提升，模型的表现逐渐增强，但是当专家个数超过 64 个之后，模型的表现反而有所下降。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 GLaM 大语言模型系列，验证了 MoE 模型的有效性和效率，结果发现，MoE 模型可以在相同的算力下达到更好的表现，并且学习效率更高。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2112.06905" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on MiniMax-01</title><link>https://maosong.website/p/notes-on-minimax-01/</link><pubDate>Tue, 06 Jan 2026 17:38:01 +0800</pubDate><guid>https://maosong.website/p/notes-on-minimax-01/</guid><description>&lt;p>MiniMax-01 是一个基于 hybrid attention 架构的大模型系列，包含 MiniMax-Text-01 和 MiniMax-VL-01 两个模型，其中 MiniMax-Text-01 推理时支持 4M 的上下文长度，MiniMax-VL-01 支持 512B 的上下文长度&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>现有大部分模型的上下文长度为 32K-256K, 但实际上我们对于长上下文的需求已经超过了这个范围。已有的模型主要基于 self-attention, 这是一个平方复杂度的算法。&lt;/p>
&lt;p>为了解决 self-attention 的问题，相关工作如 sparse attention, linear attention, state space models 等都提出了对应的解决办法。但是已有的这些解决方法的问题就是表现不是很强劲。&lt;/p>
&lt;p>因此，作者在本文中就提出了一个基于 hybrid attention 架构的大语言模型系列 MiniMax-01. 作者主要从架构，数据和 infra 三个方面进行了改进。&lt;/p>
&lt;p>在架构上，作者使用了基于 Lightning Attention 的混合架构。作者还基于实际部署来决定模型的参数。为了最大化模型的表现，作者使用了 MoE 架构。&lt;/p>
&lt;p>已有的 infra 主要是针对基于 softmax 的 attention 进行优化的，MiniMax-01 包含 softmax attention, linear attention 和 MoE, 架构比较复杂，因此作者实现了 expert parallel 和 expert tensor parallel 来提高整体的计算效率和 GPU 之间的通信效率。作者还实现了 varlen ring attention 来减少计算冗余。最终，作者发现模型在 NVIDIA-H20 上的 MFU 超过了 75%.&lt;/p>
&lt;p>基于前面提到的架构设计，作者训练得到了 MiniMax-Text-01, 模型总参数为 456B, 激活参数为 45.9B, 专家个数为 32 个，激活专家个数为 2 个。 作者首先构建了高质量的数据集，然后构建了一个三阶段的训练 pipeline。&lt;/p>
&lt;p>基于 MiniMax-Text-01, 作者扩展得到了 MiniMax-VL-01,训练使用了&lt;strong>512B&lt;/strong> token&lt;/p>
&lt;p>作者总结本文的贡献如下：&lt;/p>
&lt;ol>
&lt;li>作者构建了一个领先的大模型系列，支持超过 4M 上下文&lt;/li>
&lt;li>作者构建了第一个大规模的基于 linear attention 的大语言模型系列&lt;/li>
&lt;li>作者详细介绍了使用的数据，模型和训练策略&lt;/li>
&lt;li>作者开源了模型&lt;/li>
&lt;/ol>
&lt;h2 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h2>&lt;p>模型架构如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-minimax-01/minimax_text_01_architecture.png"
width="737"
height="908"
loading="lazy"
alt="minimax_text_01_architecture"
class="gallery-image"
data-flex-grow="81"
data-flex-basis="194px"
>&lt;/p>
&lt;p>相比于原始的 transformer, MiniMax-01 做出了以下改进：&lt;/p>
&lt;ol>
&lt;li>将 attention block 按照 8 个 block 为一组，每组里只有最后 1 个 block 使用 softmax attention, 其余 7 个 block 使用 lightning attention&lt;/li>
&lt;li>使用 MoE 替换 FFN, MoE 总专家个数为 32 个，激活专家个数为 2 个&lt;/li>
&lt;li>softmax attention 使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-gqa/" target="_blank" rel="noopener"
>GQA&lt;/a>, 来提高内存加载效率, group size 为 8&lt;/li>
&lt;li>使用了 RoPE 作为 position embedding&lt;/li>
&lt;li>使用了 RMSNorm 替换了 LayerNorm&lt;/li>
&lt;/ol>
&lt;p>FFN: MoE (32 个专家，激活 2 个专家)&lt;/p>
&lt;p>transformer: 8 个 block 为一组，一组里前 7 个使用 Lightning attention，第 8 个使用 softmax attention，80 layers&lt;/p>
&lt;p>Attention ： GQA，group size=8，64 heads，&lt;/p>
&lt;h3 id="moe">&lt;a href="#moe" class="header-anchor">&lt;/a>MoE
&lt;/h3>&lt;p>在训练基于 MoE 的 LLM 时，有两种策略，分别是 token-drop 和 dropless, 前者保证每个专家处理的 token 个数差不多，可以提高效率，缺点是某些 token 不会被任何专家处理，某些 token 会被多个专家处理；后者是保证每个 token 都会被处理。&lt;/p>
&lt;p>在本文中，作者采取了 token-drop 的方式，作者为每个专家设置一个 capacity limit，超过这个 limit 之后该专家就不再处理新的 token.&lt;/p>
&lt;p>为了评估 MoE 模型的有效性，作者对比了 MoE 和 dense 模型的表现，结果如下图所示：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-minimax-01/MiniMax-01-dense-vs-MoE.png"
width="1402"
height="480"
loading="lazy"
alt="Performance of MoE v.s. Dense"
class="gallery-image"
data-flex-grow="292"
data-flex-basis="701px"
>&lt;/p>
&lt;p>结果发现，相同的算力下，MoE 模型比 dense 模型好。&lt;/p>
&lt;p>但是，当 scaling up 到更大的模型是，作者发现训练产生了 routing collapse 的情况，这是因为 routing 的分布过于集中。为了解决这个问题，作者使用了 auxiliary loss 以及 global router 两个方法&lt;/p>
&lt;p>&lt;strong>Auxiliary loss&lt;/strong>
作者首先构建了 auxiliary loss 来提高负载均衡, 也就是&lt;/p>
$$
\mathcal{L}_{B} = K\sum_{i=1}^K f_i P_i
$$&lt;p>这里 $f_i$ 是分配给第 $i$ 个专家的 token 比例， $P_i$ 是第 $i$ 个专家的平均 routing 概率。&lt;/p>
&lt;p>&lt;strong>Global router&lt;/strong>
作者还基于 &lt;a class="link" href="https://maosong.website/p/gshard/" target="_blank" rel="noopener"
>GShard&lt;/a> 构建了一个 global routing 策略，由于 GPU memory 限制，对于每一个 micro batch size, token 分布不仅在一个 EP group 内分布不平衡，在不同 EP group 之间可能也不平衡。因此，作者实现了一个 global token dispatching 策略。具体来说，在 token 分发到不同的 EP group 之前，作者使用 &lt;code>allgather&lt;/code> 来计算每个专家需要处理的 token. 这样就可以基于全局信息智能分配 token, 避免某些专家过载。&lt;/p>
&lt;h3 id="linear-attention">&lt;a href="#linear-attention" class="header-anchor">&lt;/a>Linear Attention
&lt;/h3>&lt;p>本文中使用的 linear attention 由 Lightning Attention 提出，其表达式为&lt;/p>
$$
O = \mathrm{Norm}((QK^T)V)
$$&lt;p>这里 $Q,K,V\in\mathbb{R}^{n\times n}$ 分别是 query, key 和 value, $n$ 和 $d$ 分别是序列长度和 hidden size.上式可以改变运算顺序，得到&lt;/p>
$$
O = \mathrm{Norm}(Q(K^TV))
$$&lt;p>这样，attention 的计算复杂毒就从 $O(n^2d)$ 变成了 $O(nd^2)$,&lt;/p>
&lt;h4 id="lightning-attention">&lt;a href="#lightning-attention" class="header-anchor">&lt;/a>Lightning Attention
&lt;/h4>&lt;p>当我们不考虑 attention mask 的时候，我们很轻松可以降低 attention 计算的复杂度。但实际上，LLM 会使用 causal mask, 也就是每个 token 智能看到其前面 token 的信息，这样我们的 attention 计算实际上是&lt;/p>
$$
O = \mathrm{Norm}[QK^T\odot M]V)
$$&lt;p>这里 $M_{ij}=\mathbb{1}(i\geq j)$ 是 attention mask.&lt;/p>
&lt;p>见 Lightning Attention&lt;/p>
&lt;h4 id="effectiveness-of-lightning-attention">&lt;a href="#effectiveness-of-lightning-attention" class="header-anchor">&lt;/a>Effectiveness of Lightning Attention
&lt;/h4>&lt;p>作者接下来分析了一下 softmax attention, lightning attention 和 hybrid attention 之间的效率&lt;/p>
&lt;p>首先，作者计算了一下三种架构的参数量以及 FLOPS. 作者分别使用 $l, d, h, b, n$ 来代表 Layer 数，hidden dimension, number of attention heads, batch size 和 sequence length.&lt;/p>
&lt;p>最终计算结果如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Architecture&lt;/th>
&lt;th>Parameter count&lt;/th>
&lt;th>FLOPs count&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Softmax&lt;/td>
&lt;td>$12ld^2$&lt;/td>
&lt;td>$72bnld^2 (1 + \frac{n}{6d} + \frac{5}{18d} )$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Lightning&lt;/td>
&lt;td>$12ld^2+2ld^2/h$&lt;/td>
&lt;td>$72bnld^2(1+\frac{1}{2h}+\frac{5}{18d})$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Hybrid&lt;/td>
&lt;td>$12ld^2+7ld^2/4h$&lt;/td>
&lt;td>$72bnld^2(1+\frac{n}{48d}+\frac{7}{16h}+\frac{5}{18d})$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;blockquote>
&lt;p>[!todo] TODO
compute these results&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Scaling law&lt;/strong>
作者接下来分别针对三个架构设计了 70M, 160M, 410M, 1B, 3B, 7B 系列模型，模型使用 300B token 进行训练，上下文长度为 8192, 对于每种架构，作者将 batch size 设置为 4M tokens. 与 &lt;a class="link" href="https://maosong.website/p/chinchilla-scaling-law/" target="_blank" rel="noopener"
>Chinchilla scaling law&lt;/a> 类似， 作者探究了以下模型的 scaling law, 结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-minimax-01/MiniMax-01-scaling-law.png"
width="1403"
height="584"
loading="lazy"
alt="Summary of Scaling law"
class="gallery-image"
data-flex-grow="240"
data-flex-basis="576px"
>&lt;/p>
&lt;p>从实验结果可以看到，在相同的算力下，lighting attention 倾向于使用更多的参数和 token, 但是其表现相比于 softmax attention 更好。&lt;/p>
&lt;p>&lt;strong>Performance&lt;/strong>
作者还对比了一下三种 attention 在 public benchmark 上的表现，结果如下图&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-minimax-01/MiniMax-01-performance-attention.png"
width="1394"
height="765"
loading="lazy"
alt="Performance of different attention architectures"
class="gallery-image"
data-flex-grow="182"
data-flex-basis="437px"
>&lt;/p>
&lt;p>是检验结果发现，lightning attention 和 softmax attention 除了在 retrieval 任务（Needle in a Haystack）上之外，表现都差不多。与之相对的是，hybrid attention 弥补了这一问题，大幅度提升了模型在 retrieval 任务上的表现&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Observation
Lightning attention 与 softmax attention 的效果差不多，但是其在长上下文任务上的表现比较差。Hybrid attention 可以解决这个问题。&lt;/p>
&lt;/blockquote>
&lt;p>作者还评估了一下三种 attention 的速度，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-minimax-01/Minimax-01-attention-speed-comparison.png"
width="692"
height="373"
loading="lazy"
alt="Training speed of various attention mechanisms"
class="gallery-image"
data-flex-grow="185"
data-flex-basis="445px"
>&lt;/p>
&lt;p>实验结果显示，softmax attention 随序列长度上升其速度急剧下降，Lightning attention 的速度基本没有太大变化。而 Hybrid attention 的速度介于两者之间。&lt;/p>
&lt;p>作者进一步对比了以下两种不同的变体：hybrid-cosformer2 以及 hybrid-hgrn2.这两个模型替换的逻辑与 minimax-01 的结果一致，都是 8 个 block 为一组，每组中前面 7 个 block 将 softmax attention 更换为对应的模块，最后一层不做改动。三种 hybrid attention 机制的表现如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-minimax-01/Minimax-01-hybrid-attention-comparison.png"
width="1377"
height="190"
loading="lazy"
alt="Performance of various hybrid-linear models"
class="gallery-image"
data-flex-grow="724"
data-flex-basis="1739px"
>&lt;/p>
&lt;p>实验结果显示，hybrid-lightning 的表现最好。&lt;/p>
&lt;p>作者最后对比了以下 hybrid-lightning 和 hybrid-window, hybrid window 在每个 group 的前 7 个 block 使用了 window attention, window size 为别为 256, 512, 1024.实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-minimax-01/MiniMax-01-hybrid-window-comparison.png"
width="1392"
height="369"
loading="lazy"
alt="Comparison between hybrid lighting and hybrid window"
class="gallery-image"
data-flex-grow="377"
data-flex-basis="905px"
>&lt;/p>
&lt;p>&lt;strong>Discussion&lt;/strong>
作者最后总结认为，虽然 linear attention 的效率很高，但是它们在 retrieval 相关的任务表现很差，而 retrieval 对于 In-context learning 来说是至关重要的。因此，作者采取了 hybrid 架构，来兼顾模型的效率以及表现。&lt;/p>
&lt;p>作者给出了一个解释。&lt;/p>
&lt;h3 id="ablation-study">&lt;a href="#ablation-study" class="header-anchor">&lt;/a>Ablation Study
&lt;/h3>&lt;p>作者主要进行了两个消融实验：&lt;/p>
&lt;ol>
&lt;li>Hybrid-lightning 与 softmax attention 的对比：作者训练了一个总参数 28B, 激活参数 5B 的 MoE 模型，然后作者使用 MiniMax-01 的方式替换每个 group 的 softmax attention, 并使用了 1T 的 token 进行训练&lt;/li>
&lt;li>pre-layer normalization 与 Post-layer normalization 的对比： 现有的 LLaMA 和 Qwen 等系列模型采用的都是 PreNorm 的方式。&lt;strong>PreNorm 可以让 gradient 通过 residual connection 传播更加直接，但是这也减少了模型的有效深度&lt;/strong>。反之，PostNorm 则可以保留模型的有效深度，但是其问题是会导致梯度消失或爆炸。作者构建了一个总参数为 60B, 激活参数为 9.3B 的 MoE 模型，包含 48 个 block, 模型训练使用 500B token. 模型有两个变体，一个使用 PreNorm, 另一个使用 PostNorm, 对于 PostNorm, 作者使用的是 DeepNorm.&lt;/li>
&lt;/ol>
&lt;p>最终表现如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-minimax-01/MiniMax-01-module-ablation.png"
width="1392"
height="238"
loading="lazy"
alt="Module ablations"
class="gallery-image"
data-flex-grow="584"
data-flex-basis="1403px"
>&lt;/p>
&lt;p>实验结果显示，hybrid lightning 的表现与 softmax 的表现相当，甚至超过了 softmax 的表现。&lt;/p>
&lt;p>另一方面，PostNorm 的表现也超过了 PreNorm 的表现。&lt;/p>
&lt;h3 id="model-spec">&lt;a href="#model-spec" class="header-anchor">&lt;/a>Model Spec
&lt;/h3>&lt;p>基于已有的模型设计，作者探究了如何决定模型的参数。作者的目标&lt;strong>在 performance 和 inference efficiency 之间达到一个平衡&lt;/strong>。&lt;/p>
&lt;p>作者将模型的参数限制在 500B 以下，要求能够在 $8\times 80G$ 的服务器上和 8-bit 的量化下面，支持 1M 的上下文长度。建模的问题如下：&lt;/p>
$$
\min_{P_{all}, P_{act}}\mathcal{L}(P_{all}, P_{act}, T), \quad \mathrm{s.t.}\ C_{compute}(P_{all}, P_{act}, T)&lt;C \text{ and } P_{all} &lt; 500B
$$&lt;p>其中 $\mathcal{L}$, $P_{all}$, $P_{act}$, $T$, $C_{compute}$, $C$ 分别代表损失，总参数，激活参数，训练的 token 数，算力消耗 y 以及算力的 budget.&lt;/p>
&lt;p>作者首先确定了几个关键的因素：&lt;/p>
&lt;ol>
&lt;li>softmax 和 lightning 的混合比例&lt;/li>
&lt;li>depth-to-width 的比例&lt;/li>
&lt;li>linear attention memory size 和 hidden size 的比例&lt;/li>
&lt;li>FFN 的 hidden size 大小&lt;/li>
&lt;li>RoPE 的 base frequency&lt;/li>
&lt;/ol>
&lt;p>作者通过实验发现，hybrid 架构需要更多的 layer 才能带来更好地表现。对于 layer 比较少的模型，其需要更多的 softmax attention layers 来达到相似的表现。&lt;/p>
&lt;p>作者还发现，提升 linear attention memory size 可以有效提高模型的表现。并且 RoPE 的 dimension 最好设置为 softmax attention dimension 的一半。&lt;/p>
&lt;p>基于这些发现，作者构建了 scaling law 来决定最优模型配置。作者训练了不同大小的模型，然后拟合 scaling law 的曲线，最后，作者将模型的总参数确定为 456B, 激活参数确定为 45.9B.&lt;/p>
&lt;h2 id="infra">&lt;a href="#infra" class="header-anchor">&lt;/a>Infra
&lt;/h2>&lt;p>本节介绍了一下 MiniMax-01 的 infra, 作者主要解决了以下几个关键问题：&lt;/p>
&lt;ol>
&lt;li>减少训练时 MoE 中的 all-to-all 通信开销，如何在内存使用，计算效率和 all-to-all 通信开销之间达到一个平衡&lt;/li>
&lt;li>在处理长上下文时，不同 GPU 之间的信息需要共享，这会导致额外的通信开销。因此，如何减少长上下文情形下的通信开销也是一个挑战&lt;/li>
&lt;li>如何提高 lightning attention 在 inference 阶段的效率。当前只在训练阶段做了优化，但是推理阶段也需要提高模型的效率&lt;/li>
&lt;/ol>
&lt;h3 id="moe-optimization">&lt;a href="#moe-optimization" class="header-anchor">&lt;/a>MoE Optimization
&lt;/h3>&lt;p>优化 MoE 架构的主要目的是&lt;strong>最小化通信开销&lt;/strong>，特别是 all-to-all 的通信开销。为了解决这个问题，作者构建了一个 token-grouping-based overlap scheme, 来让通信和计算尽可能并行执行，也就是在计算的同时进行通信。为了保证通信的正确性，每个 ProcessGroup 之间的通信操作必须串行执行，这就导致了不同 group 之间的通信无法 overlap, 也就造成了 idle time 的产生，示意图如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-minimax-01/MiniMax-01-EP-overlap.png"
width="702"
height="451"
loading="lazy"
alt="Expert parallel overlap illustration"
class="gallery-image"
data-flex-grow="155"
data-flex-basis="373px"
>&lt;/p>
&lt;p>但是，在 MiniMax-01 上应用这种方法时，作者发现如果使用 TP 来切分 expert 的参数的话，会导致计算密度过低。&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Explanation
针对 MoE 模块使用 TP 导致计算密度过低的原因是 MoE 模块是稀疏的，计算式大量 GPU 都处于空闲状态，因此计算密度低。&lt;/p>
&lt;/blockquote>
&lt;p>如果不使用 TP 的话，就必须使用更大的 PP 配置，也就是使用更多的 GPU, 但是使用 PP 并不会减少 activation 的内存占用，这在长上下文训练时，会增加内存消耗，而且不会提升整体的训练速度。因此，我们需要一种新的策略，使其能够：&lt;/p>
&lt;ol>
&lt;li>平衡内存使用与计算密度&lt;/li>
&lt;li>优化特定模型和任务的训练过程&lt;/li>
&lt;/ol>
&lt;p>基于这个目标，在本文中作者提出了 ETP, 也就是 Expert Tensor Parallel, 用于管理 expert 的参数划分。同时，作者还提出了 EDP, 也就是 Expert Data Parallel, 来将每个 expert 的数据并行封装在一起。&lt;code>world_size&lt;/code> 满足两个条件：&lt;/p>
$$
\texttt{world\_size} = \texttt{size}_{PP}\times \texttt{size}_{DP}\times \texttt{size}_{CP}\times \texttt{size}_{TP}
$$&lt;p>以及&lt;/p>
$$
\texttt{world\_size} = \texttt{size}_{PP}\times \texttt{size}_{EDP}\times \texttt{size}_{ETP}\times \texttt{size}_{EP}
$$&lt;p>首先，作者构建了 EP-ETP 策略，用于平衡内存使用以及计算密度，这个过程包括了四个步骤：&lt;/p>
&lt;ol>
&lt;li>all-to-all dispatch: （EP）将每个 token 分发到 expert 所在 GPU 上&lt;/li>
&lt;li>allgather: （TP）将输入的 token 进行切分，分发给对应的 TP GPU 上&lt;/li>
&lt;li>Expert:（TP）执行 expert 的计算过程&lt;/li>
&lt;li>ReduceScatter: （TP）将 TP 输出的结果进行汇总然后再分发给不同的 TP&lt;/li>
&lt;li>all-to-all combine: 将不同 expert 的输出结果进行汇总，传递给下一层&lt;/li>
&lt;/ol>
&lt;p>但是，由于同一个 ProcessGroup 内部的通信必须串行执行，如果计算效率比较高的话，就会产生 bubble, 作者的改进方法是提高计算量，让计算消耗的时间与通信消耗的时间大致相当。&lt;/p>
&lt;p>作者还尝试降低 ProcessGroup 的 size, 这样可以进一步提高计算和通信之间的 overlap, 但是问题是降低 group size 之后，group 的数量会增加，而这会导致 scheduling 以及让通信变为 CPU-bound. 作者认为这需要根据具体场景来进行设置。最终，通过优化，作者将 MoE 模块的通信开销降低了 50%.&lt;/p>
&lt;p>改进的示意图如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-minimax-01/MiniMax-01-EP-ETP-overlap.png"
width="1136"
height="660"
loading="lazy"
alt="EP-ETP overlap"
class="gallery-image"
data-flex-grow="172"
data-flex-basis="413px"
>&lt;/p>
&lt;h3 id="long-context-optimization">&lt;a href="#long-context-optimization" class="header-anchor">&lt;/a>Long Context Optimization
&lt;/h3>&lt;p>为了处理不同长度的 sequence, 作者使用了 sequence packing 的技巧。&lt;/p>
&lt;p>&lt;strong>Varlen Ring Attention&lt;/strong>
当前主要是使用 ring attention 来划分数据，但是 ring-attention 与 sequence packing 是冲突的。已有工作如 &lt;a class="link" href="https://maosong.website/p/chinchilla-scaling-law/" target="_blank" rel="noopener"
>flash attention&lt;/a> 支持 varlen data packing, 但是不支持 ring attention; 而 TransformerEngine 实现了 ring attention, 但不支持真正的 varlen. 因此作者的目的就是解决这个问题。&lt;/p>
&lt;p>TransformerEngine 的问题在于每个 sequence 必须是 $2\times \texttt{size}_{CP}$ 的整数倍，当 CP 很大且样本长度分布未知的时候，padding 会很严重，导致算力和内存消耗严重。&lt;/p>
&lt;p>作者的解决方法为使用 varlen ring attention. 具体做法就是在每一步通信中，都基于 attention mask 的 offset 来处理不同的 sequence. 示意图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-minimax-01/MiniMax-01-varlen-ring-attention.png"
width="969"
height="374"
loading="lazy"
alt="Varlen Ring Attention"
class="gallery-image"
data-flex-grow="259"
data-flex-basis="621px"
>&lt;/p>
&lt;p>&lt;strong>LASP+&lt;/strong>
对于 lightning attention, 之前的做法是使用 LASP 算法来实现 linear attention. 但是 LASP 的问题在于其是一个串行依赖，这样导致训练效率变低。&lt;/p>
&lt;p>为了解决这个问题，作者提出了 LASP+, 包括以下几个步骤&lt;/p>
&lt;ol>
&lt;li>Local Prefix Sum Calculation: 每个 GPU 先计算自己局部 prefix sum, 作者将其记为 KVL&lt;/li>
&lt;li>Global Synchronization via AllGather: 然后，作者通过 AllGather 将自己的 KVL 发送给其他 GPU. 这样就可以一次完成所有通信&lt;/li>
&lt;li>Prefix Sum Computation: 最后，每个 GPU 利用所有 KVL, 分别计算自己需要的 prefix sum.&lt;/li>
&lt;/ol>
&lt;p>通过这样一个优化，我们可以将 LASP 的延迟降低 $N_{pcn}$ 倍，这里 $N_{pcn}$ 是并行计算节点的个数。&lt;/p>
&lt;p>作者进一步结合了前面的 Varlen ring attention, 主要包括 padding to block size 和 sequential concatenation 两个步骤。&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Remark
LISP+ 说明了现在大模型分布式训练的一个重要趋势，也就是用通信换并行。特别是 GPU 通信带宽特别高的情况。因此，算法设计不仅要考虑理论复杂度还要考虑实际中硬件并行的利用率。&lt;/p>
&lt;/blockquote>
&lt;h3 id="lightning-attention-inference-optimization">&lt;a href="#lightning-attention-inference-optimization" class="header-anchor">&lt;/a>Lightning Attention Inference Optimization
&lt;/h3>&lt;p>已有的 lightning attention 并没有考虑实际的部署需求。作者提出了四个策略来优化 lightning attention 的 inference efficiency:&lt;/p>
&lt;p>&lt;strong>Batched Kernel Fusion&lt;/strong>
作者首先对 memory-bound kernels 进行融合。在 prefill phase, 作者将多步操作，比如 Q, K, V projection, padding, partitioning 等放在一个 kernel 里，来减少内存 I/O. 在 decoding phase, 作者将 KV 的计算也放在一个 kernel 里，计算完之后就直接写入 KV cache 而不经过缓冲区&lt;/p>
&lt;p>&lt;strong>PD separation&lt;/strong>
inference 的 decoding 阶段模型每次只生成 1 个 token, 此时任务是 memory-bound, 也就是瓶颈在显存读写而不是算力，我们只需要很少的 GPU SMs 就可以跑起来。因此，作者设计了两个不同的 CUDA streams 来分别处理 length 为 1 和 length&amp;gt;1 的情况，这样就可以有效提高整体的计算效率和平衡 GPU 的利用率。&lt;/p>
&lt;p>&lt;strong>Multi-level padding&lt;/strong>
已有的 padding 技巧主要是将输入的多个 sequence 都 resize 到固定长度方便处理。但是在推理的时候，使用预设的固定长度 (block size) 会导致无效计算量增加。因此，作者提出了 multi-level padding, 也就是提供不同的 block size, 比如 32, 64, 128. 这样可以有效避免模型因为 padding 而产生的额外计算开销。&lt;/p>
&lt;p>&lt;strong>StridedBatchedMatmul Extension&lt;/strong>
这一点主要是针对 Hopper GPU 进行的优化。Hopper 架构相比于 Ampere 架构有一些新的特性，作者基于这些特性来优化整体的性能。&lt;/p>
&lt;p>作者使用 WGMMA 来优化计算，用 TMA 来优化内存访问效率，实现计算与内存访问重叠，减少等待时间。最终，作者的目的是希望基于不同的 GPU 架构来动态调整 Pipeline stages, 以达到更好的性能。&lt;/p>
&lt;p>通过以上这些优化，作者在 H20 上达到了 75% 的 MFU. 作者发现，由于使用了 lightning attention, attention 不分计算从传统的 95% 以上降低到了 12% 左右，不再是算力的瓶颈部分。&lt;/p>
&lt;h2 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h2>&lt;p>作者首先介绍了预训练的数据构成，然后作者介绍了如何选取高质量的训练数据，最后，作者介绍了实验的参数。&lt;/p>
&lt;h3 id="data">&lt;a href="#data" class="header-anchor">&lt;/a>Data
&lt;/h3>&lt;p>数据来源包括学术文献，数据，互联网语料以及代码。作者构建了如下的数据清洗策略：&lt;/p>
&lt;ol>
&lt;li>data quality enhancement: 作者使用了 rule-based cleaning 以及去重。然后，作者构建了一个 reward labeler，来从 knowledge depth, practical helpfulness 以及 categorical distribution 三个维度进行打分。&lt;/li>
&lt;li>data formatting optimization. 作者发现，类似 markdown 的格式会降低数据整体的多样性和质量。因此，作者构建了一个嵌套的文档格式，来将对话和 QA 数据组织起来，进而平衡自然数据和结构化数据之间的平衡性。&lt;/li>
&lt;li>data mixture investigation. 作者发现完全去除掉低质量的数据也会影响模型在下游任务上的表现。因此作者平衡了高质量数据和低质量数据&lt;/li>
&lt;/ol>
&lt;p>作者基于 BPE 算法来构建 tokenizer,最终 tokenizer 的大小为 &lt;strong>200K&lt;/strong>&lt;/p>
&lt;p>接下来，作者通过实验来确定最优的数据配置，给定数据集 $\mathcal{D}$, 作者作如下备择假设&lt;/p>
$$
H_1: \mu_{T_{\mathcal{D}}}> \mu_{T_{baseline}}
$$&lt;p>这里 $\mu$ 代表模型表现的加权平均， $T$ 代表模型的表现分布。也就是说，加入新的数据集之后，其平均表现大于 baseline 的表现，则我们认为这个数据集是不被拒绝的。&lt;/p>
&lt;p>作者通过多项选择题来进行评估，评估时作者计算每个选项 completion 的概率，最终的分布计算方式为&lt;/p>
$$
\log \mathrm{acc}_{\mathrm{norm}^2}(x) = \log\mathrm{softmax}_{p'(c\in C_x)}\left\{(p'(c^*))\right\}
$$&lt;p>其中 $p_i'(c)=\frac{p_i(c)}{\mathrm{bytes(c)}}$ 是 normalized 之后的选项概率。作者在训练的时候，通过实验来保证这个 metric 的稳定性，除此之外，作者还加入了一个 discriminator, 也就是 $\Delta_{obvious}/\sigma_{seed}$, 其中 $\Delta_{obvious}$ 表示不同模型表现的差距， $\sigma_{seed}$ 表示不同随机种子的方差。&lt;/p>
&lt;h3 id="training-strategy">&lt;a href="#training-strategy" class="header-anchor">&lt;/a>Training Strategy
&lt;/h3>&lt;p>作者使用 Xavier 对模型参数进行初始化，然后使用了 DeepNorm 作为 Normalization 模块。作者使用 AdamW 作为优化器，其中 $\beta_1=0.9$, $\beta_2=0.95$, weight decay 为 $0.1$.&lt;/p>
&lt;p>训练的 sequence length 为 8192，batch size 分别为 16M, 32M (69B tokens), 64M (790B tokens), 128M (4.7T tokens).&lt;/p>
&lt;p>作者基于 training loss 和 critical batch size 来设计 schedule. 作者基于小模型的实验结果来拟合一个 power-law 关系，结果如下图所示：&lt;/p>
&lt;p>基于实验结果，作者&lt;/p>
&lt;h2 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h2>&lt;h3 id="prompt-collection">&lt;a href="#prompt-collection" class="header-anchor">&lt;/a>Prompt Collection
&lt;/h3>&lt;p>作者从多个 domain 收集到了数百万 diverse 和高质量的 query,作者基于任务类型，domain 和难度对 prompt 进行分类。接下来作者对 prompt 进行去重以及平衡难度问题。prompt 覆盖 long context, math, reasoning 等 domain&lt;/p>
&lt;h3 id="reward-model">&lt;a href="#reward-model" class="header-anchor">&lt;/a>Reward Model
&lt;/h3>&lt;p>作者基于以下几个原则来设计 reward:&lt;/p>
&lt;ul>
&lt;li>Correctness: 对于 math 任务，作者使用 MiniMax-Text-01 来生成 binary reward; 对于 code 任务，作者使用沙盒环境运行代码，根据通过率来给出奖励&lt;/li>
&lt;li>Truthfulness: 作者使用先进的语言模型来评估 response 的 factual accuracy&lt;/li>
&lt;li>Helpfulness: 作者实现了基于规则的验证系统，来评估回答的 coherence, depth, contextual relevance 和 stylistic appropriateness.&lt;/li>
&lt;li>Harmlessness: 作者基于 Constitutional AI principles, 构建了一系列协议来保证模型输出的安全性。&lt;/li>
&lt;/ul>
&lt;h3 id="sft">&lt;a href="#sft" class="header-anchor">&lt;/a>SFT
&lt;/h3>&lt;p>作者通过使用多个 expert model 来生成多样化的回答。然后，作者通过 n-gram 以及 semantic similarity 来提高数据的多样性和质量。&lt;/p>
&lt;h3 id="rl">&lt;a href="#rl" class="header-anchor">&lt;/a>RL
&lt;/h3>&lt;p>RL 训练包括 offline stage 以及 online stage.&lt;/p>
&lt;p>&lt;strong>Offline RL&lt;/strong>
在 offline stage, 作者使用 DPO 来进行训练。数据构造过程也比较简单，使用模型来进行采样，评估，选取最好和最差的作为正样本和负样本。&lt;/p>
&lt;p>&lt;strong>Online RL&lt;/strong>
这个阶段使用 GRPO 进行训练，作者发现使用 SFT 阶段的 prompt 会导致训练饱和，因此，作者并没有采用 SFT 阶段的 prompts&lt;/p>
&lt;p>作者主要针对 GRPO 进行了一下改进：&lt;/p>
&lt;ol>
&lt;li>Important Sampling Weight Clipping. 作者发现，PPO 以及 GRPO 只使用了单侧的 clipping, 这会导致训练的不稳定性。因此作者构建了额外的 clipping 策略。&lt;/li>
&lt;li>KL divergence optimization. 作者发现 KL divergence 也会导致训练不稳定，因此作者对 KL divergence 进行了 reformulate 来稳定梯度&lt;/li>
&lt;li>Balanced Advantage Estimation. 作者确保正样本和负样本的对 reward 的贡献差不多，来提高训练的稳定性。&lt;/li>
&lt;/ol>
&lt;h3 id="safety-alignment">&lt;a href="#safety-alignment" class="header-anchor">&lt;/a>Safety Alignment
&lt;/h3>&lt;p>作者还加入了一个 safety alignment 阶段，用于提升模型的安全性。&lt;/p>
&lt;p>数据包括三类：&lt;/p>
&lt;ol>
&lt;li>safety-category specific prompts.&lt;/li>
&lt;li>Real-world user data collection.&lt;/li>
&lt;li>prompt augmentation.&lt;/li>
&lt;/ol>
&lt;p>接下来，作者使用一个无害的 reward model 来对模型的输出进行打分。&lt;/p>
&lt;h3 id="training-recipe">&lt;a href="#training-recipe" class="header-anchor">&lt;/a>Training Recipe
&lt;/h3>&lt;p>最终，post-training 一共包括 5 个 stage，训练时，作者将 RoPE 的 base frequency 保持在 10M.&lt;/p>
&lt;ol>
&lt;li>Initial short-context training: 模型的上下文长为 8192，然后进行 SFT&lt;/li>
&lt;li>Extended context training: 模型的上下文长度为 1,032,192. 训练数据包括 50% 长文本和 50% 短文本&lt;/li>
&lt;li>Short-context Preference Optimization: 模型的上下文长度为 8192, 然后进行 DPO 的训练&lt;/li>
&lt;li>Long-context Preference Optimization: 模型的上下文长度为 1,032,192, 然后进行 DPO 的训练&lt;/li>
&lt;li>Online Reinforcement Learning: RL 的训练，与上文一致。&lt;/li>
&lt;/ol>
&lt;p>训练的配置如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Stage I&lt;/th>
&lt;th>Stage II&lt;/th>
&lt;th>Stage III&lt;/th>
&lt;th>Stage IV&lt;/th>
&lt;th>Stage V&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Sequence Length&lt;/td>
&lt;td>8192&lt;/td>
&lt;td>1032192&lt;/td>
&lt;td>8192&lt;/td>
&lt;td>1032192&lt;/td>
&lt;td>8192&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Epoch&lt;/td>
&lt;td>2&lt;/td>
&lt;td>2&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Batch Size&lt;/td>
&lt;td>128&lt;/td>
&lt;td>80&lt;/td>
&lt;td>64&lt;/td>
&lt;td>64&lt;/td>
&lt;td>512&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Max LR&lt;/td>
&lt;td>1e-5&lt;/td>
&lt;td>3e-6&lt;/td>
&lt;td>5e-7&lt;/td>
&lt;td>5e-7&lt;/td>
&lt;td>1e-6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Min LR&lt;/td>
&lt;td>1e-6&lt;/td>
&lt;td>3e-6&lt;/td>
&lt;td>5e-8&lt;/td>
&lt;td>5e-7&lt;/td>
&lt;td>1e-7&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LR Decay&lt;/td>
&lt;td>Cosine&lt;/td>
&lt;td>Constant&lt;/td>
&lt;td>Cosine&lt;/td>
&lt;td>Constant&lt;/td>
&lt;td>Cosine&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="llm-evaluation">&lt;a href="#llm-evaluation" class="header-anchor">&lt;/a>LLM Evaluation
&lt;/h2>&lt;p>作者对比了 GPT-4o, Claude-3.5-Sonnet, Gemini-1.5-Pro, Qwen2.5-72B, DeepSeek-V3 以及 LLaMA3.1-405B, 实验结果如下图&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-minimax-01/MiniMax-01-text-performance.png"
width="1393"
height="953"
loading="lazy"
alt="Performance of MiniMax-Text-01"
class="gallery-image"
data-flex-grow="146"
data-flex-basis="350px"
>&lt;/p>
&lt;p>作者还评估了以下 MiniMax-Text-01 在长上下文情境下表现，我们这里列出 Ruler 上的结果，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-minimax-01/MiniMax-01-text-Ruler-performance.png"
width="1334"
height="250"
loading="lazy"
alt="MiniMax-Text-01 performance on Ruler benchmark"
class="gallery-image"
data-flex-grow="533"
data-flex-basis="1280px"
>&lt;/p>
&lt;p>可以看到在 1M 上下文时，模型的表现超过了 Gemini-1.5-Pro 的表现。&lt;/p>
&lt;hr>
&lt;h2 id="vlm-architecture">&lt;a href="#vlm-architecture" class="header-anchor">&lt;/a>VLM Architecture
&lt;/h2>&lt;p>作者基于 MiniMax-Text-01 开发了 MiniMax-VL-01, 来扩展模型的多模态理解能力。MiniMax-VL-01 是一个标准的 ViT-MLP-LLM 架构，其中：&lt;/p>
&lt;ul>
&lt;li>ViT: 从零开始训练的 ViT, 参数为 303M&lt;/li>
&lt;li>MLP: 2 层的 MLP, 激活函数为 GeLU&lt;/li>
&lt;li>LLM: MiniMax-Text-01&lt;/li>
&lt;/ul>
&lt;p>作者实现了动态分辨率策略，用于处理不同大小的图片。每张图片都被切分为 $336\times 336$ 的大小， 然后作者还加入了一个 thumbnail image 用于提取全局信息。&lt;/p>
&lt;h2 id="data-1">&lt;a href="#data-1" class="header-anchor">&lt;/a>Data
&lt;/h2>&lt;p>为了训练 vision encoder, 作者使用了 &lt;strong>694M&lt;/strong> 的 Image-caption pair 数据，为了提高数据质量，作者对其中 180M 的数据进行的 caption 的改写，训练的时候原始数据和重写数据的采样概率都是 $50\%$.&lt;/p>
&lt;p>作者还从公开数据收集到了 100M 的图片，每个图片都有详细的描述，这些描述都由一个 caption model 生成，然后再由人类进行修正。每条描述大概包含 300 个 text token&lt;/p>
&lt;p>作者还构建了一个高质量的 instruction dataset, 这个数据集是通过预定义一系列任务，然后对于每个任务分别生成对应的 QA pair 得到的。在训练的时候，作者平衡了不同任务的采样概率。&lt;/p>
&lt;p>最后，作者对收集收集数据的类别分布进行了可视化，结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-minimax-01/MiniMax-01-VL-data-distribution.png"
width="692"
height="707"
loading="lazy"
alt="Visualization of top rags of sampled instruction data"
class="gallery-image"
data-flex-grow="97"
data-flex-basis="234px"
>&lt;/p>
&lt;h2 id="training">&lt;a href="#training" class="header-anchor">&lt;/a>Training
&lt;/h2>&lt;p>训练一共包含 5 个 stage, 第一个 stage 用于单独训练 ViT, 后面三个 stage 分别用于对齐和提升模型的多模态能力。&lt;/p>
&lt;p>&lt;strong>Stage 0: ViT training&lt;/strong>
作者基于 ViT-L/14 来训练 vision encoder, 作者使用了基于对比学习的方法，参考了 CoCa 里的方法。训练过程中，作者首先在 224 的图片精度下训练了 &lt;strong>37B&lt;/strong> 的 image-caption pairs. 接下来，作者在 336 的精度下使用了 &lt;strong>1.2B&lt;/strong> 的 pairs 进行微调。最终，模型在 ImageNet-1K 的表现达到了 80.55%.&lt;/p>
&lt;p>&lt;strong>Stage 1: Modality alignment&lt;/strong>
这个阶段的目的是对齐视觉模态和文本模态，作者冻结 LLM, 只训练 ViT 和 MLP. 这个阶段一共使用了&lt;strong>80B&lt;/strong> 的 image description data. 作者发现，在这个阶段，提升图片精度对模型表现提升影响不大。&lt;/p>
&lt;p>&lt;strong>Stage 2: Enhancement of Vision Understanding&lt;/strong>
这个阶段其实就是 instruction tuning, 作者解冻所有参数，然后使用了 &lt;strong>420B&lt;/strong> 多模态 token 以及 &lt;strong>21B&lt;/strong> MiniMax-Text-01 的 post-training token 来提高模型的指令跟随能力。&lt;/p>
&lt;p>&lt;strong>Stage 3: Enhancement of User Experience&lt;/strong>
这个阶段的目的是进一步提高模型在真实场景下模型的避险。作者构建了高质量的多模态数据，包括 &lt;strong>44.8B&lt;/strong> 的多模态 token, 这个阶段模型训练了一个 epoch.&lt;/p>
&lt;p>&lt;strong>Stage 4: Enhancement of Preference&lt;/strong>
这个阶段的目的是对齐，作者使用 &lt;a class="link" href="https://maosong.website/p/notes-on-dpo/" target="_blank" rel="noopener"
>DPO&lt;/a> 来训练模型，数据集包括 &lt;strong>40,000&lt;/strong> 条数据。数据构建过程如下：&lt;/p>
&lt;ol>
&lt;li>prompt selection: 作者基于真实用户交互数据构建了高质量的 prompt&lt;/li>
&lt;li>Response generation: 作者通过多次采样，然后使用 MiniMax-Text-01 来生成多样化的数据&lt;/li>
&lt;li>Reward Assignment: 接下来作者使用 MiniMax-Text- 来评估最终的结果&lt;/li>
&lt;li>Pair Construction: 最后，基于评估结果来构建 preference pairs.&lt;/li>
&lt;/ol>
&lt;p>作者还加入了一些纯文本的 preference data.&lt;/p>
&lt;blockquote>
&lt;p>Observation
作者发现，如果 foundation model 比较强，使用 DPO 的话可能会导致过拟合，因此作者的做法是在一个 epoch 之前就停止训练。&lt;/p>
&lt;/blockquote>
&lt;h2 id="evaluation">&lt;a href="#evaluation" class="header-anchor">&lt;/a>Evaluation
&lt;/h2>&lt;p>作者对比了 GPT-4o, Claude-3.5-Sonnet, Gemini-1.5-Pro, Qwen2-VL-72B, InternVL2.5-78B 和 LLaMA-3.2-90B, 实验结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-minimax-01/MiniMax-01-VL-performance.png"
width="1342"
height="937"
loading="lazy"
alt="Performance of MiniMax-VL-01"
class="gallery-image"
data-flex-grow="143"
data-flex-basis="343px"
>&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>在本文中，作者提出了 MiniMax-01 大模型系列，包括 MiniMax-Text-01 和 MiniMax-VL-01 两个模型，前者是一个总参数为 456B, 激活参数为 45.9B 的基于 MoE 和 hybrid attention 的大语言模型，后者是基于 MiniMax-Text-01 的多模态大模型。作者详细介绍了模型的架构，数据和训练。&lt;/p>
&lt;p>作者认为，未来有以下探索方向：&lt;/p>
&lt;ol>
&lt;li>Long-context Evaluation: 如何更好评估模型的长上下文能力。现有的 long-context benchmark 的任务都比较简单，很难反应模型的真实能力&lt;/li>
&lt;li>Model Architecture: 当前的模型还有 1/8 的模块包含 softmax attention, 如何构建更高效的架构来去掉 softmax attention 是一个值得探究的方向。&lt;/li>
&lt;li>Complex Programming Tasks: 如何提高模型的 coding 能力也是一个值得探索的方向。&lt;/li>
&lt;/ol></description></item><item><title>Notes on DeepSeek-V3.2</title><link>https://maosong.website/p/notes-on-deepseek-v3.2/</link><pubDate>Tue, 06 Jan 2026 17:30:40 +0800</pubDate><guid>https://maosong.website/p/notes-on-deepseek-v3.2/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者首先回顾了开源模型如 &lt;a class="link" href="https://maosong.website/p/notes-on-minimax-01/" target="_blank" rel="noopener"
>MiniMax-01&lt;/a>, &lt;a class="link" href="https://maosong.website/p/notes-on-kimi-k2/" target="_blank" rel="noopener"
>Kimi-k2&lt;/a>, &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a>, &lt;a class="link" href="https://maosong.website/p/notes-on-glm-4.5/" target="_blank" rel="noopener"
>GLM-4.5&lt;/a> 和闭源模型的进展，作者指出，现在的开源模型和闭源模型在表现上仍然存在较大差距。作者认为这种差距主要是由于三个原因：&lt;/p>
&lt;ol>
&lt;li>Transformer 提出的 softmax attention 在处理长文本时效率非常低&lt;/li>
&lt;li>已有的开源模型在 post-training 阶段使用的算力不够&lt;/li>
&lt;li>开原模型的泛化和指令跟随能力不如闭源模型&lt;/li>
&lt;/ol>
&lt;p>基于这三个问题，DeepSeek-V3.2 分别进行了改进：&lt;/p>
&lt;ol>
&lt;li>在架构上，作者提出了 DSA，一个高效的稀疏注意力机制，用于降低计算复杂度&lt;/li>
&lt;li>在 post-training 阶段，作者使用了比 pre-training 阶段高 $10\%$ 的算力，用于提高模型的能力&lt;/li>
&lt;li>作者提出了一个 pipeline 用于提高模型在工具调用场景下的 reasoning 能力&lt;/li>
&lt;/ol>
&lt;p>通过实验作者发现，模型达到了和 &lt;a class="link" href="https://maosong.website/p/notes-on-kimi-k2/" target="_blank" rel="noopener"
>Kimi-k2&lt;/a> 以及 GPT-5 差不多的 reasoning 表现。&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;p>DeepSeek-V3.2 与 DeepSeek-V3.1 不同之处在于使用了 DeepSeek Sparse Attention (DSA). 架构如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3.2/DeepSeek-V3-2-architecture.png"
width="1285"
height="672"
loading="lazy"
alt="Attention architecture of DeepSeek-V3.2-Exp"
class="gallery-image"
data-flex-grow="191"
data-flex-basis="458px"
>&lt;/p>
&lt;p>DSA 包含两个模块：&lt;/p>
&lt;ol>
&lt;li>lightning indexer&lt;/li>
&lt;li>fine-grained token selection mechanism&lt;/li>
&lt;/ol>
&lt;p>其中，lightning indexer 负责计算 query token $h_t\in\mathbb{R}^d$ 和一个 preceding token $h_s\in\mathbb{R}^d$ 之间的 index score $I_{t,s}$ 来决定 query token 选择的 token:&lt;/p>
$$
I_{t,s} = \sum_{j=1}^{H_I}w_{t,j}^I \mathrm{ReLU}(q_{t,j}^I\cdot k_s^I)
$$&lt;p>其中， $H^I$ 代表 indexer heads 的个数，$q_{t,j}^I\in\mathbb{R}^{d^I}$ 和 $w_{t,j}^I$ 由 query token $h_t$ 得到，$k_s^I\in\mathbb{R}^{d^I}$ 由 preceding token $h_s$ 得到&lt;/p>
&lt;p>给定 query token $h_t$ 对应的 index score $\{I_{t,s}\}$, fine-grained token selection mechanism 负责选取 top-K index score 对应的 key-value entries $\{c_s\}$, 然后 attention 的输出由 query token 个选取的 key value entries 得到：&lt;/p>
$$
u_t = \mathrm{Attn}(h_t,\{c_s\mid I_{t,s}\in \mathrm{TopK}(I_{t,:})\})
$$&lt;blockquote>
&lt;p>[!Recall]
MoBA 也提出了类似的方法，但是 MoBA 是一个无需训练的策略&lt;/p>
&lt;/blockquote>
&lt;p>受 &lt;a class="link" href="https://maosong.website/p/notes-on-nsa/" target="_blank" rel="noopener"
>NSA&lt;/a> 启发，作者实现了基于 &lt;a class="link" href="https://maosong.website/p/notes-on-mqa/" target="_blank" rel="noopener"
>MQA&lt;/a> 模式的 &lt;a class="link" href="https://maosong.website/p/notes-on-mla/" target="_blank" rel="noopener"
>MLA&lt;/a>, 其中 latent vector 对于 query token 所有的 query heads 都是共享的。示意图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3.2/DeepSeek-V3-2-modes-of-MLA.png"
width="1344"
height="533"
loading="lazy"
alt="Different modes of MLA"
class="gallery-image"
data-flex-grow="252"
data-flex-basis="605px"
>&lt;/p>
&lt;h3 id="continue-pre-training">&lt;a href="#continue-pre-training" class="header-anchor">&lt;/a>Continue Pre-training
&lt;/h3>&lt;p>作者在 DeepSeek-V3.1 的基础上进行了 continue pre-training. Continue pre-training 包含两个阶段：&lt;/p>
&lt;p>&lt;strong>Dense Warm-up stage&lt;/strong>
这个阶段用于训练 lightning indexer, 作者冻结除 lightning indexer 之外的参数，为了对齐 indexer output 和 main attention distribution, 对于第 $t$ 个 query token, 作者首先计算所有 attention heads 的 main attention score 之和，然后在 sequence 层面进行 L1-normalization 得到 $p_{t,:}\in\mathbb{R}^t$, 最后计算 lightning indexer 输出与 $p_{t,:}$ 之间的 KL divergence:&lt;/p>
$$
\mathcal{L}^I = \sum_{t}\mathcal{D}_{KL}(p_{t,:}\ \Vert\ \mathrm{Softmax}(I_{t,:}))
$$&lt;p>这个阶段训练一共使用了 &lt;strong>2.1B&lt;/strong> 的 token, lr 为 1e-3, 训练的步数为 1000 steps, batch size 为 16.&lt;/p>
&lt;p>&lt;strong>Sparse training stage&lt;/strong>
这个阶段模型所有的参数都参与训练，该阶段的目的是让模型学习到 DSA 的 sparse pattern. 训练时，作者让 lightning indexer 的输出与 $p_{t,S_t}$ 之间的输出进行对齐，其中 $S_t=\{s\mid I_{t,s}\in\mathrm{TopK}(I_{t,:})\}$:&lt;/p>
$$
\mathcal{L}^I = \sum_{t}\mathcal{D}_{KL}(p_{t,S_t}\ \Vert\ \mathrm{Softmax}(I_{t,:}))
$$&lt;p>实际训练时，lightning indexer 仅接受 $\mathcal{L}^I$ 的反向传播，而 LLM 则仅接受 next-token prediction loss. 这个阶段模型一共使用了&lt;strong>943.7B&lt;/strong> token, 其中 $K$ 设置为 $2048$. 学习率为 $7.3\times 1e-6$, 训练步数为 15,000 steps, batch size 为 480.&lt;/p>
&lt;h3 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h3>&lt;p>post-training 与 DeepSeek-V3.1 一致：&lt;/p>
&lt;p>&lt;strong>Specialist Distillation&lt;/strong>
作者基于 DeepSeek-V3.2 base 构建了不同领域的 specialized model, 这些领域包括：&lt;/p>
&lt;ol>
&lt;li>math&lt;/li>
&lt;li>competitive programming&lt;/li>
&lt;li>general logical reasoning&lt;/li>
&lt;li>agentic coding&lt;/li>
&lt;li>agentic search&lt;/li>
&lt;/ol>
&lt;p>每个 specialized model 都使用 RL 进行训练，训练数据包括 long CoT reasoning 数据以及 direct response generation 数据，specialized model 训练完毕之后，就被用于生产 domain-specific data, 作者通过实验发现，基于这种蒸馏方法，模型的表现仅比 specialized model 低一点，并且这个 gap 可以被后续的 RL 训练所抵消。&lt;a class="link" href="https://maosong.website/p/notes-on-glm-4.5/" target="_blank" rel="noopener"
>GLM-4.5&lt;/a> 也采取了类似的做法&lt;/p>
&lt;p>&lt;strong>Mixed RL Training&lt;/strong>
作者使用了 GRPO 算法进行训练，与 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v3/" target="_blank" rel="noopener"
>DeepSeek-V3&lt;/a> 不同，作者将 reasoning, agent 以及 human alignment 的 RL 训练合并为了一个阶段，作者认为这种方法可以平衡模型在多个 domain 上的表现，并且可以防止 multi-stage training 带来的灾难性遗忘问题。对于 reasoning 和 agent 任务，作者使用了 rule-basd outcome reward, length penalty 以及 language consistency reward. 对于通用任务，作者使用了 generative reward model, 每个 prompt 都有对应的 rubris 用于 evaluation. 作者构建 reward 时主要考虑了:&lt;/p>
&lt;ol>
&lt;li>length versus accuracy&lt;/li>
&lt;li>language consistency versus accuracy&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>DeepSeek-V3.2-Speciale&lt;/strong>
除了 DeepSeek-V3.2 之外，作者还训练了 DeepSeek-V3.2-Speciale 模型，该模型仅使用 reasoning 数据进行训练，reasoning 数据包含了 DeepSeek-Math-V2 的训练数据以及 reward 方法。训练时，作者降低了 length penalty 的惩罚系数，最终 DeepSeek-V3.2-Speciale 模型拥有更强的 reasoning 能力&lt;/p>
&lt;h4 id="scaling-grpo">&lt;a href="#scaling-grpo" class="header-anchor">&lt;/a>Scaling GRPO
&lt;/h4>&lt;p>作者在 GRPO 的基础上对 KL estimate 进行了改进（见 &lt;a class="link" href="https://maosong.website/p/notes-on-kl-divergence/" target="_blank" rel="noopener"
>KL divergence&lt;/a>），使用了 importance sampling 对 K3 estimator 进行修正:&lt;/p>
$$
\mathcal{D}_{\mathrm{KL}}(\pi_\theta(o_{i,t})\Vert\pi_{\mathrm{ref}}(o_{i,t})) = \frac{\pi_\theta(o_{i,t}\mid q, o_{i,&lt;t})}{\pi_{\mathrm{old}}(o_{i,t}\mid q, o_{i,&lt;t})}\left(\frac{\pi_{\mathrm{ref}}(o_{i,t}\mid q, o_{i,&lt;t})}{\pi_\theta(o_{i,t}\mid q, o_{i,&lt;t})}-\frac{\pi_{\mathrm{ref}}(o_{i,t}\mid q, o_{i,&lt;t})}{\pi_\theta(o_{i,t}\mid q, o_{i,&lt;t})}-1\right)
$$&lt;p>使用 K3 estimator 之后，现在 KL estimator 的梯度估计就变成无偏估计了，从而提高了整体训练的稳定性。作者还发现不同任务对 KL regularization 的需求不一致，对数学等 domain, 我们应该采取较小的 KL penalty 或者不使用 KL penalty 反而能提升性能&lt;/p>
&lt;p>&lt;strong>Off-Policy Sequence Masking&lt;/strong>
作者还使用了 sequence masking 来提高 off-policy data 的数据使用效率，由于不同 rollout 的完成时间不一致，训练过程中会出现 off-policy 现象，即某些 mini-batch 不是由当前 policy 产生，这个现象在 &lt;a class="link" href="https://maosong.website/p/notes-on-magistral/" target="_blank" rel="noopener"
>Magistral&lt;/a> 中也有提到，这种训练 - 推理不一致性会进一步加剧 off-policy 程度，为了提高训练稳定性，作者将 policy divergence 程度比较高的 sequence 给 mask 掉，更新后的损失函数如下所示&lt;/p>
$$
\mathcal{J}_{\mathrm{GRPO}}(\theta) = \mathbb{E}_{(q,a)\sim\mathcal{D},\{o_i\}_{i=1}^G\sim \pi_{\theta_{old}}(\cdot\mid q)}\left[ \frac{1}{G}\sum_{i=1}^G\frac{1}{|o_i|}\sum_{t=1}^{|o_i|}\min\left(r_{i,t}(\theta)\hat{A}_{i,t},\mathrm{clip}\left(r_{i,t}(\theta), 1-\epsilon, 1+\epsilon\right)\hat{A}_{i,t}\right)M_{i,t}-\beta \mathcal{D}_{\mathrm{KL}}(\pi_\theta(o_{i,t})\Vert\pi_{\mathrm{ref}}(o_{i,t}))\right]
$$&lt;p>其中，&lt;/p>
$$
M_{i,t} = \begin{cases}
0, &amp;\hat{A}_{i,t}&lt;0, \frac{1}{|o_i|}\sum_{t=1}^{|o_i|}\log r_{i,t}(\theta)&lt;\delta\\
1, &amp;\text{otherwise}
\end{cases}
$$&lt;p>用于决定是否对当前 sequence 进行 mask, $\delta$ 是一个超参数控制 policy divergence 程度。作者认为，模型主要从自身的错误进行学习，而 off-policy 的负样本模型学习提升有限甚至有害。作者发现加入这个 masking 策略之后，模型的训练稳定性有所提升。&lt;/p>
&lt;p>&lt;strong>Keep Routing&lt;/strong>
MoE 模型在进行 On-policy RL 训练是，由于 policy 的更新，新 policy 和旧 policy 专家的 routing 可能会不一致，这种不一致性会降低训练的稳定性以及 off-policy 现象。为了解决这个问题，作者在采样室，保存了训练阶段所使用的 expert routing path, 来保证训练推理的一致性。这种策略可以有效提高针对 MoE 模型的 RL 训练稳定性&lt;/p>
&lt;p>&lt;strong>Keep Sampling Mask&lt;/strong>
作者发现，使用 top-p 和 top-K 可以提高 LLM 输出的质量，但是这种采样擦略也会导致 $\pi_{\mathrm{old}}$ 和 $\pi_{\theta}$ action space 的不匹配，因此，作者记录了 $\pi_{\mathrm{old}}$ 采样过程中的 truncation mask, 然后再训练的时候将其应用到 $\pi_{\theta}$ 上，作者发现通过这种方式可以有效提高 RL 训练过程中的 language consistency&lt;/p>
&lt;h4 id="thinking-in-tool-use">&lt;a href="#thinking-in-tool-use" class="header-anchor">&lt;/a>Thinking in Tool-Use
&lt;/h4>&lt;p>本节的目的是希望能够将 reasoning 能力应用到工具调用的场景下。&lt;/p>
&lt;p>作者发现，如果使用 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-r1/" target="_blank" rel="noopener"
>DeepSeek-R1&lt;/a> 的策略，在下一轮对话到来时，丢弃到当前的 reasoning content 会让模型重新生成针对问题的 CoT, 从而产生 token inefficiency. 为了解决这个问题，作者构建了一个上下文管理策略，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3.2/DeepSeek-V3-2-context-management-tool-calling.png"
width="1281"
height="677"
loading="lazy"
alt="Context Management of DeepSeek V3.2 in tool-calling senarios"
class="gallery-image"
data-flex-grow="189"
data-flex-basis="454px"
>&lt;/p>
&lt;p>具体做法为：&lt;/p>
&lt;ul>
&lt;li>reasoning content 只有当有新的 user message 进入时才会丢弃；如果只有工具调用相关的 message, 则 reasoning content 会保留&lt;/li>
&lt;li>移除 reasoning content 时，会保留对应的工具调用及其结果&lt;/li>
&lt;/ul>
&lt;p>在训练时，作者认为模型已经掌握了比较好的指令跟随能力，我们仅需要将 reasoning data (non-agentic) 和 non-reasoning data(agentic) 以不同的 prompt 输入给模型就能够得到比较好的结果。&lt;/p>
&lt;p>对于训练的数据，作者认为 RL 任务的多样性可以有效提高模型的 robustness, 因此作者构建了不同的环境及其对应的 prompt, 生成的任务如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>number of tasks&lt;/th>
&lt;th>environment&lt;/th>
&lt;th>prompt&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>code agent&lt;/td>
&lt;td>24667&lt;/td>
&lt;td>real&lt;/td>
&lt;td>extracted&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>search agent&lt;/td>
&lt;td>50275&lt;/td>
&lt;td>real&lt;/td>
&lt;td>synthesized&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>general agent&lt;/td>
&lt;td>4417&lt;/td>
&lt;td>synthesized&lt;/td>
&lt;td>synthesized&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>code interpreter&lt;/td>
&lt;td>5908&lt;/td>
&lt;td>real&lt;/td>
&lt;td>extracted&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;ul>
&lt;li>search agent: 作者使用了 multi-agent 的策略，包括 question-construction agent 用于构建 QA pair, multiple answer-generation agent 用于构建不同的 response, 一个 verification agent 用于评估生成的 response. 最后作者使用 generative reward model 来评分&lt;/li>
&lt;li>code agent: 作者爬取了 Github 上的 pull request, 然后进行过滤，接下来作者通过一个 environment-setup agent 来构建对应的环境&lt;/li>
&lt;li>code interpreter agent: 作者使用 jupyter notebook 作为代码解释器来解决复杂的 reasoning tasks, 包括 math, logic, data science 等&lt;/li>
&lt;li>general agent: 作者构建了验证简单解决困难的任务。首先作者基于 agent 和 task category 来生成或检索相关数据；接下来作者合成一个任务相关的工具集合；最后，作者让一个 agent 来提出任务以及对应的解法，并不断提高任务的难度。最后得到 &lt;code>&amp;lt;environment, tools, task, verifier&amp;gt;&lt;/code> 的 tuple 格式&lt;/li>
&lt;/ul>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>作者对比了 DeepSeek-V3.2-Exp 和 Claude-4.5 Sonnet, GPT-5, Gemini 3.0 Pro, Kimi-K2 thinking, MiniMax M2 的表现，评测结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3.2/DeepSeek-V3-2-performance.png"
width="1345"
height="817"
loading="lazy"
alt="Performance of DeepSeek V3.2"
class="gallery-image"
data-flex-grow="164"
data-flex-basis="395px"
>&lt;/p>
&lt;p>结果显示，DeepSeek V 3.2 和 GPT-high 在 reasoning 任务上的表现差不多。作者认为，进一步提高 RL 阶段的算力可以有效提高模型的表现&lt;/p>
&lt;p>作者还对比了 DeepSeek-V 3.2-Speciale 的表现，结果显示，提高 token budget 之后，模型的表现显著提高，与 Gemini 3.0 Pro 可以相比，但是其 token efficiency 仍然弱于 Gemini 3.0 Pro, 结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3.2/DeepSeek-V3-2-speciale-performance.png"
width="1299"
height="421"
loading="lazy"
alt="Performance of DeepSeek V 3.2 speciale"
class="gallery-image"
data-flex-grow="308"
data-flex-basis="740px"
>&lt;/p>
&lt;p>接下来作者验证了 synthesis agentic tasks 对模型表现的影响。首先，作者随机采样一批样本使用闭源 LLM 进行测试，发现闭源模型表现最好为 $62\%$, 这说明了合成数据对于 DeepSeek V3.2 和闭源模型都是有挑战的。&lt;/p>
&lt;p>其次，作者探究了合成数据是否能够提高 RL 的泛化性，作者构建了两个额外模型：&lt;/p>
&lt;ul>
&lt;li>SFT: 在 SFT checkpoint 上进行 RL&lt;/li>
&lt;li>Exp: 仅在 search 以及 code environment 上进行 RL&lt;/li>
&lt;/ul>
&lt;p>对比结果发现，合成数据缺失可以有效提高模型的表现&lt;/p>
&lt;p>作者还对比了 DSA 的效率，DSA 可以将 attention 的计算复杂度由 $\mathcal{O}(L^2)$ 降低到 $\mathcal{O}(LK)$, 其中 $K$ 是选取的 top-K tokens. 尽管 lightning indexer 的复杂度仍然是 $\mathcal{O}(L^2)$, 但是其计算量远小于 MLA, 作者对比了两者的效率，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3.2/DeepSeek-V3-2-efficiency.png"
width="1323"
height="547"
loading="lazy"
alt="Efficiency of DeepSeek-V3.2"
class="gallery-image"
data-flex-grow="241"
data-flex-basis="580px"
>&lt;/p>
&lt;p>可以看到，DeepSeek-V3.2 的 prefilling 和 decoding 效率都远高于 DeepSeek-V3.1&lt;/p>
&lt;p>最后，作者对比了不同上下文管理策略对模型表现的影响，对比策略有：&lt;/p>
&lt;ol>
&lt;li>Summary: 对轨迹进行总结然后重新初始化 rollout&lt;/li>
&lt;li>Discard 75%: 丢弃到初始 75% 的工具调用历史&lt;/li>
&lt;li>Discard-all: 丢弃掉所有的工具调用历史&lt;/li>
&lt;li>Parallel-fewest step: 多次采样然后保留步数最少得轨迹&lt;/li>
&lt;/ol>
&lt;p>实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3.2/DeepSeek-V3-2-context-management-ablation.png"
width="1286"
height="643"
loading="lazy"
alt="Ablation study on context management"
class="gallery-image"
data-flex-grow="200"
data-flex-basis="480px"
>&lt;/p>
&lt;p>结果显示，使用上下文管理策略之后，模型的表现有了显著提升。并且，discard-all 策略虽然很简单，但是其表现非常好。作者认为如何根据不同场景来选取合适的策略是一个待解决的问题。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 DeepSeek-V 3.2, DeepSeek-V 3.2 使用了一个稀疏注意力机制来提高模型在长上下文场景下的计算效率。作者还通过提升 RL 阶段的算力来提高模型在下游任务上的表现。最后，作者合成了大规模的 agentic task 来提升模型的 agent 能力。&lt;/p>
&lt;p>作者认为，相比于 Gemini 3.0 Pro, 模型的知识广度仍然有限。并且，目前模型的 token efficiency 仍然是一个问题，模型需要更长的轨迹输出才能达到 Gemini 3.0 Pro 的表现。最后，模型解决复杂问题的能力仍然弱于闭源模型。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2512.02556" target="_blank" rel="noopener"
>DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Gemini3.0</title><link>https://maosong.website/p/notes-on-gemini3.0/</link><pubDate>Tue, 06 Jan 2026 10:26:39 +0800</pubDate><guid>https://maosong.website/p/notes-on-gemini3.0/</guid><description>&lt;p>Gemini 3.0 是是 Google 新一代最强模型，model card 介绍了 Gemini 3.0 系列的评估结果以及基本能力&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>Gemini 3.0 系列包含&lt;/p>
&lt;ul>
&lt;li>Gemini 3.0 Pro&lt;/li>
&lt;li>Gemini 3.0 Flash&lt;/li>
&lt;li>Gemini 3.0 Pro Image
三个模型&lt;/li>
&lt;/ul>
&lt;p>Gemini 3.0 Pro 拥有原生多模态以及 reasoning 能力，可以处理 text, audio, images, video 以及 code repositories 等模态。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>modalities&lt;/th>
&lt;th>context&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>input&lt;/td>
&lt;td>text, images, audio, video&lt;/td>
&lt;td>1M&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>output&lt;/td>
&lt;td>text&lt;/td>
&lt;td>64K&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Gemini 3.0 Flash 与 Gemini 3.0 Pro 基本一致，与 &lt;a class="link" href="https://maosong.website/p/notes-on-gemini2.5/" target="_blank" rel="noopener"
>Gemini2.5&lt;/a> 相同，应该是采取了蒸馏的方式来实现更高的吞吐速度以及效率&lt;/p>
&lt;p>Gemini 3.0 Pro Image 基于 Gemini 3.0 Pro 开发，是一个支持 text, image prompt 的图片生成模型&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;p>模型从零开始训练，使用了 MoE 架构和 Transformer 架构&lt;/p>
&lt;p>模型使用 TPU 进行训练，训练架构为 JAX 和 ML Pathways.&lt;/p>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>Gemini 3.0 Pro 对比了 &lt;a class="link" href="https://maosong.website/p/notes-on-gemini2.5/" target="_blank" rel="noopener"
>Gemini2.5&lt;/a> , Claude Sonnet 4.5 和 GPT-5.1&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gemini3.0/Gemini-3-0-pro-performance.png"
width="1081"
height="963"
loading="lazy"
alt="Performance of Gemini 3.0 Pro"
class="gallery-image"
data-flex-grow="112"
data-flex-basis="269px"
>&lt;/p>
&lt;p>Gemini 3.0 Flash 对比了 Gemini 3.0 Pro, Gemini 2.5 Flash, Gemini 2.5 Pro, Claude Sonnet 4.5, GPT-5.2 和 Grok 4.1 Fast.&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gemini3.0/Gemini-3-0-Flash-performance.png"
width="1082"
height="1073"
loading="lazy"
alt="Performance of Gemini 3.0 Flash"
class="gallery-image"
data-flex-grow="100"
data-flex-basis="242px"
>&lt;/p>
&lt;p>Gemini 3.0 Pro Image 对比了 Gemini 2.5 Flash Image, GPT-Image 1, Seedream v4, Flux Pro Kontext Max&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gemini3.0/Gemini-3-0-Pro-Image-performance.png"
width="1372"
height="795"
loading="lazy"
alt="Performance of Gemini 3.0 Pro Image on existing capabilities"
class="gallery-image"
data-flex-grow="172"
data-flex-basis="414px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gemini3.0/Gemini-3-0-Pro-Image-performance-new.png"
width="1332"
height="721"
loading="lazy"
alt="Performance of Gemini 3.0 Pro Image on new capabilities"
class="gallery-image"
data-flex-grow="184"
data-flex-basis="443px"
>&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf" target="_blank" rel="noopener"
>Gemini 3.0 Pro Model Card&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Flash-Model-Card.pdf" target="_blank" rel="noopener"
>Gemini 3.0 Flash Model Card&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Image-Model-Card.pdf" target="_blank" rel="noopener"
>Gemini 3.0 Pro Image Model Card&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Softmax</title><link>https://maosong.website/p/notes-on-softmax/</link><pubDate>Sat, 27 Dec 2025 16:39:53 +0800</pubDate><guid>https://maosong.website/p/notes-on-softmax/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>softmax 函数用于将 $K$ 个实数转换为一个 $K$ 维概率分布。其具体做法是先对所有元素指数化，即求 $e^x$, 然后每个元素除以所有指数的和。即&lt;/p>
$$
\begin{aligned}
\mathrm{softmax}:\mathbb{R}^K&amp;\to (0,1)^K\\
\mathrm{softmax}(\mathbf{z}) &amp;=\left[\frac{e^{z_1}}{\sum_{j=1}^Ke^{z_j}},\dots,\frac{e^{z_K}}{\sum_{j=1}^Ke^{z_j}}\right]
\end{aligned}
$$&lt;h2 id="analysis">&lt;a href="#analysis" class="header-anchor">&lt;/a>Analysis
&lt;/h2>&lt;h3 id="properties">&lt;a href="#properties" class="header-anchor">&lt;/a>Properties
&lt;/h3>&lt;p>softmax 的第一个性质是 shift invariance, 即&lt;/p>
$$
\mathrm{softmax}(\mathbf{z}+c) = \mathrm{softmax}(\mathbf{z})
$$&lt;p>证明比较容易：&lt;/p>
$$
\mathrm{softmax}(\mathbf{z}+c)_i = \frac{e^{z_i+c}}{\sum_{j=1}^Ke^{z_j+c}} = \frac{e^ce^{z_i}}{e^c\sum_{j=1}^Ke^{z_j}} = \frac{e^{z_i}}{\sum_{j=1}^Ke^{z_j}}=\mathrm{softmax}(\mathbf{z})_i,\ i=1,\dots,K
$$&lt;h3 id="gradient">&lt;a href="#gradient" class="header-anchor">&lt;/a>Gradient
&lt;/h3>&lt;p>向量输入下 Softmax 函数的 Jacobian 矩阵推导&lt;/p>
&lt;p>设输入为向量 $\mathbf{z} = [z_1, z_2, \dots, z_d]^\top \in \mathbb{R}^d$，Softmax 函数的输出为向量 $\mathbf{a} = [a_1, a_2, \dots, a_d]^\top \in \mathbb{R}^d$，其中每个元素定义为：&lt;/p>
$$
a_j = \text{softmax}(\mathbf{z})_j = \frac{e^{z_j}}{\sum_{k=1}^d e^{z_k}}
$$&lt;p>记分母（归一化因子）为 $S = \sum_{k=1}^d e^{z_k}$，则 $a_j = e^{z_j}/S$.&lt;/p>
&lt;p>我们分两种情况计算 $\frac{\partial a_j}{\partial z_k}$：&lt;/p>
&lt;p>当 $j = k$ 时， 此时求 $a_j$ 对自身输入 $z_j$ 的偏导数：&lt;/p>
$$
\frac{\partial a_j}{\partial z_j} = \frac{\partial}{\partial z_j} \left( \frac{e^{z_j}}{S} \right) = \frac{e^{z_j}S-e^{z_j}e^{z_j}}{S^2}=\frac{e^{z_j}}{S}\left(1-\frac{e^{z_j}}{S}\right)=a_j(1-a_j)
$$&lt;p>当 $j \neq k$ 时， 此时求 $a_j$ 对输入 $z_k$ 的偏导数有：&lt;/p>
$$
\frac{\partial a_j}{\partial z_k} = \frac{\partial}{\partial z_k} \left( \frac{e^{z_j}}{S} \right) = \frac{0\cdot S-e^{z_j}e^{z_k}}{S^2}=-\frac{e^{z_j}e^{z_k}}{S}=-a_ja_j
$$&lt;p>综合以上两种情况，Jacobian 矩阵 $\mathbf{J}$ 可表示为：&lt;/p>
$$
\mathbf{J} = \text{diag}(\mathbf{a}) - \mathbf{a} \mathbf{a}^\top
$$&lt;h2 id="interpretation">&lt;a href="#interpretation" class="header-anchor">&lt;/a>Interpretation
&lt;/h2>&lt;h3 id="soft-argmax">&lt;a href="#soft-argmax" class="header-anchor">&lt;/a>Soft Argmax
&lt;/h3>&lt;p>softmax 是 argmax 的 smooth approximation, 所以实际上 softmax 指的是 “soft argmax&amp;quot;. 为了证明这一点，我们首先定义如下函数&lt;/p>
$$
\mathrm{softmax}(\mathbf{z};\tau) =\mathrm{softmax}(\mathbf{z}/\tau)=\left[\frac{e^{z_1/\tau}}{\sum_{j=1}^Ke^{z_j/\tau}},\dots,\frac{e^{z_K/\tau}}{\sum_{j=1}^Ke^{z_j/\tau}}\right]
$$&lt;p>易知， $\mathrm{softmax}(\mathbf{z})=\mathrm{softmax}(\mathbf{z};1)$. 并且，$\mathrm{softmax}$ 还是一个光滑函数&lt;/p>
&lt;p>我们定义 smooth approximation 为&lt;/p>
&lt;blockquote>
&lt;p>Definition
如果 $\lim_{\tau\to0^+}\mathrm{softmax}(\mathbf{z};\tau)=\mathbb{1}_{\arg\max(\mathbf{z})}$, 则我们说 $\mathrm{softmax}(\cdot;\tau)$ 是 $\arg\max$ 的光滑近似，特别地，$\mathrm{softmax}(\cdot)$ 是 $\arg\max$ 的光滑近似。
这里 $\arg\max(\mathbf{z})=\arg\max_k z_k$ 是最大值的索引， $\mathbb{1}\in\{0,1\}^K$ 是示性函数 (indicator function), 即 $\mathbb{1}_{\arg\max(\mathbf{z})}[i]=1$ 当且仅当 $z_i=\max_jz_j$.&lt;/p>
&lt;/blockquote>
&lt;p>我们下面来进行证明。我们不妨假设最大值唯一，其 index 为 $m$, 即 $z_m = \max_i z_i$. 由前面的性质，我们有：&lt;/p>
$$
\mathrm{softmax}(\mathbf{z};\tau) = \mathrm{softmax}(\mathbf{z}-z_m;\tau) =\left[\frac{e^{(z_1-z_m)/\tau}}{\sum_{j=1}^Ke^{(z_j-z_m)/\tau}},\dots,\frac{e^{(z_K-z_m)/\tau}}{\sum_{j=1}^Ke^{(z_j-z_m)/\tau}}\right]
$$&lt;p>此时，我们有&lt;/p>
$$
\lim_{\tau\to0^+}\mathrm{softmax}(\mathbf{z};\tau)_i = \begin{cases}
1, &amp;\text{if }i = m\\
0, &amp;\text{otherwise}
\end{cases}
$$&lt;p>当最大值不唯一的时候，我们记 $\mathcal{I} = \{i\in[K]\mid z_i=\max_j z_j\}$, 与上面方法类似，最终 $\mathrm{softmax}(\cdot;\tau)$ 的结果为&lt;/p>
$$
\lim_{\tau\to0^+}\mathrm{softmax}(\mathbf{z};\tau)_i = \begin{cases}
1/|\mathcal{I}|, &amp;\text{if }i \in \mathcal{I}\\
0, &amp;\text{otherwise}
\end{cases}
$$&lt;p>因此，我们就证明了 softmax 是 argmax 函数的 smooth approximation.&lt;/p>
&lt;h3 id="statistical-mechanics">&lt;a href="#statistical-mechanics" class="header-anchor">&lt;/a>Statistical Mechanics
&lt;/h3>&lt;h3 id="temperature">&lt;a href="#temperature" class="header-anchor">&lt;/a>Temperature
&lt;/h3>&lt;p>我们前面介绍了 $\mathrm{softmax}(\mathbf{z};\tau)$ 函数，这里的 $\tau$ 实际上被称为温度 (temperature), 它控制了输入的 variance, $T$ 越大，输入的 variance 越低，输出就倾向于均匀分布，而 $T$ 越小，则说明输入的 variance 越高，输出就倾向于 one-hot 分布。&lt;/p>
&lt;p>我们前面已经证明了后者，现在我们来证明一下前者，证明思路也很简单，$T\to+\infty$ 时，$e^{x/T}\to 1$, 因而&lt;/p>
$$
\lim_{\tau\to+\infty}\mathrm{softmax}(\mathbf{z};\tau)_i =\frac1K,\ i=1,\dots,K
$$&lt;p>下面是可视化的代码以及结果&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">numpy&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">np&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">matplotlib.pyplot&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">plt&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">scipy.interpolate&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">make_interp_spline&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">softmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">e_x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">exp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">max&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">e_x&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">e_x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sum&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">num_elements&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">15&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">indices&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num_elements&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">logits&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">linspace&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mf">3.5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">3.5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_elements&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">scales&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="mf">0.01&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">1.0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">5.0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">10.0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">100.0&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">figure&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">figsize&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">10&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">6&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">s&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">scales&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">probs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">softmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">logits&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x_smooth&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">linspace&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">indices&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">min&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">indices&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">max&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="mi">300&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">spl&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">make_interp_spline&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">indices&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">probs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">y_smooth&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">clip&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">spl&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x_smooth&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># Clip to ensure no negative artifacts&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">plot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x_smooth&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y_smooth&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">label&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s1">&amp;#39;Scale = &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s1">&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">linewidth&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">uniform_prob&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">1.0&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">indices&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">axhline&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">uniform_prob&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">color&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;black&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">linestyle&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;:&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">alpha&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.6&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">label&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s1">&amp;#39;Uniform distribution&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">xticks&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">indices&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">xlabel&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;Logit Index&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">fontsize&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">12&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ylabel&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;Softmax Probability&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">fontsize&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">12&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">title&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;Impact of Variance Scaling on Softmax Distribution&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">fontsize&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">14&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">legend&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">title&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;Variance Scale&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">grid&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">linestyle&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;--&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">alpha&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.5&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tight_layout&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">show&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;img src="https://maosong.website/p/notes-on-softmax/softmax_impact_variance.png"
width="989"
height="590"
loading="lazy"
alt="impact of variance on softmax"
class="gallery-image"
data-flex-grow="167"
data-flex-basis="402px"
>&lt;/p>
&lt;p>可以看到，当 variance 比较小的时候，输出的分布接近于均匀分布，而 variance 越大，输出的分布越接近 One-hot 分布。&lt;/p>
&lt;p>在 attention 的计算过程中，我们也有 softmax 函数，为了在 softmax 过程中避免 variance 的影响，现在会在计算 softmax 之前加入 normalization layer 来提前进行归一化。见 &lt;a class="link" href="https://maosong.website/p/notes-on-qk-norm/" target="_blank" rel="noopener"
>QK-norm&lt;/a>.&lt;/p>
&lt;h2 id="algorithms">&lt;a href="#algorithms" class="header-anchor">&lt;/a>Algorithms
&lt;/h2>&lt;h3 id="implementation">&lt;a href="#implementation" class="header-anchor">&lt;/a>Implementation
&lt;/h3>&lt;p>由于 $e^x$ 在实际计算时，非常容易溢出，因此在实现的时候，我们往往会考虑其数值稳定性。实际上，现在的 softmax 函数基本由 logsumexp 实现，logsumexp 函数定义如下&lt;/p>
$$
\mathrm{logsumexp}(\mathbf{z}) = \log \left(\sum_{i=1}^K e^{z_i}\right)
$$&lt;p>softmax 函数与 logsumexp 函数的关系如下&lt;/p>
$$
\begin{aligned}
\mathrm{softmax}(\mathbf{z}) &amp;=\exp\log\left(\frac{e^{\mathbf{z}}}{\sum_{j=1}^Ke^{z_j}}\right)\\
&amp;= \exp\left(\mathbf{z} - \log\left(\sum_{i=1}^K e^{z_i}\right)\right)\\
&amp;= \exp(\mathbf{z} - \mathrm{logsumexp}(\mathbf{z}))
\end{aligned}
$$&lt;p>考虑前面提到的 $e^x$ 数值溢出的问题，我们的输入会先经过 shift, 减掉最大值。此时我们有&lt;/p>
$$
\mathrm{softmax}(\mathbf{z}) = \mathrm{softmax}(\mathbf{z}-c) = \exp((\mathbf{z}-c) - \mathrm{logsumexp}(\mathbf{z}-c))
$$&lt;p>这里我们使用了前面推导出来的 shift invariance 性质。对应的代码实现如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">softmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">max&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">keepdim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">values&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">log_sum_exp&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">log&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">exp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">keepdim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">exp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">log_sum_exp&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="gumbel-softmax-reparametrization-trick">&lt;a href="#gumbel-softmax-reparametrization-trick" class="header-anchor">&lt;/a>Gumbel-softmax Reparametrization Trick
&lt;/h3>&lt;p>TODO&lt;/p>
&lt;h3 id="online-softmax">&lt;a href="#online-softmax" class="header-anchor">&lt;/a>Online Softmax
&lt;/h3>&lt;p>注意到我们在计算 softmax 时，需要加载 $\mathbf{z}$ 的全部信息，如果 $\mathbf{z}$ 非常大的话，会产生频繁的内存读写进而影响整体效率。因此 &lt;a class="link" href="https://maosong.website/p/notes-on-flashattention/" target="_blank" rel="noopener"
>flash attention&lt;/a> 中提出了 online softmax 算法来减少内存访问开销。&lt;/p>
&lt;p>其具体做法是假设我们的输入被分为若干个 block, 即 $\mathbf{z}=[\mathbf{z}^1;\dots,\mathbf{z}^n]\in\mathbb{R}^K$, 这里 $\mathbf{z}^i\in\mathbb{R}^{K/n}$ ($K\mod n=0$).&lt;/p>
&lt;p>对于 $\mathbf{z}\in\mathbb{R}^K$, flash attention 定义如下结果&lt;/p>
$$
m(\mathbf{z}) = \max_i z_i,\ f(\mathbf{z}) = [e^{z_1-m(\mathbf{z})},\dots,e^{z_K-m(\mathbf{z})}], \ \ell(\mathbf{z})=\sum_if(z)_i, \ \mathrm{softmax}(\mathbf{z}) = \frac{f(\mathbf{z})}{\ell(\mathbf{z})}
$$&lt;p>对于 $\mathbf{z}=[\mathbf{z}^1;\dots,\mathbf{z}^n]\in\mathbb{R}^K$, 我们现在的计算方式为&lt;/p>
$$
\begin{aligned}
m_i(\mathbf{z}) &amp;= \max([\mathbf{z}^1;\dots;\mathbf{z}^i]) = \max(m_{i-1}(\mathbf{z}),m(\mathbf{z}^i))\\
\ell_i(\mathbf{z}) &amp;= \sum_{j=1}^if(\mathbf{z}^j) = \exp(m_{i-1}(\mathbf{z}) - m_i(\mathbf{z}))\ell(\mathbf{z}^{i-1}) + \exp(\mathbf{z}^i-m_i(\mathbf{z}))
\end{aligned}
$$&lt;p>因此，如果我们额外记录 $m(x)$ 以及 $\ell(x)$ 这两个量，那么我们可以每次仅计算 softmax 的一个 block. 计算完毕之后，$m_i(\mathbf{z})$ 和 $\ell_i(\mathbf{z})$ 就分别代表了 global max 和 global denominator.&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>我们回顾了机器学习中 softmax function 的基本定义与性质&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://en.wikipedia.org/wiki/Softmax_function" target="_blank" rel="noopener"
>Softmax function&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://blog.ando.ai/posts/softmax-to-the-max/" target="_blank" rel="noopener"
>Softmax to the Max&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/1805.02867" target="_blank" rel="noopener"
>Online normalizer calculation for softmax&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on NoPE</title><link>https://maosong.website/p/notes-on-nope/</link><pubDate>Wed, 24 Dec 2025 15:19:42 +0800</pubDate><guid>https://maosong.website/p/notes-on-nope/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>【参考文献 1】中系统性对比了 &lt;a class="link" href="https://maosong.website/p/notes-on-alibi/" target="_blank" rel="noopener"
>AliBi&lt;/a>, &lt;a class="link" href="https://maosong.website/p/notes-on-position-encoding/" target="_blank" rel="noopener"
>RoPE&lt;/a> , &lt;a class="link" href="https://maosong.website/p/notes-on-t5/" target="_blank" rel="noopener"
>T5&lt;/a> 提出的 T5 bias 以及 Transformer 提出的绝对位置编码 (APE).&lt;/p>
&lt;p>作者发现，常用的方法在 length generalization 上表现并不是最好的，而 NoPE 不需要额外的计算开销反而效果最好。&lt;/p>
&lt;p>【参考文献 2】 进一步探究了 NoPE 长度外推的泛化性。作者有三点发现：&lt;/p>
&lt;ol>
&lt;li>NoPE 相比于 RoPE, 其长度外推泛化能力更强&lt;/li>
&lt;li>对于 NoPE 来说，模型会在还没有到达预训练上下文长度之前，表现就出现下降的情况&lt;/li>
&lt;li>通过调整 softmax 的温度超参数，我们可以提高 NoPE 的长度外推泛化性能力。&lt;/li>
&lt;/ol>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;p>【参考文献 1】对比了不同 position encoding 的相似度，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-nope/NoPE-similarity.png"
width="1173"
height="488"
loading="lazy"
alt="Distance of NoPE with other position encodings"
class="gallery-image"
data-flex-grow="240"
data-flex-basis="576px"
>&lt;/p>
&lt;p>实验结果表明，NoPE 与 &lt;a class="link" href="https://maosong.website/p/notes-on-t5/" target="_blank" rel="noopener"
>T5&lt;/a> 提出的 T5 bias 最相似。&lt;/p>
&lt;p>作者在理论上推导出了 NoPE 的两个性质：&lt;/p>
&lt;p>&lt;strong>Theorem 1 (Absolute Encoding)&lt;/strong>
Let $x$ be an input sequence of length $T + 1$ to the model. Then, the first layer of $f_θ$ can recover absolute positions $[1, . . . , T + 1]$ in the hidden state $H^{(1)}$. That is, there exist $W_Q, W_K , W_V , W_O, W_1$, and $W_2$ such that the self-attention and feedforward operations in the first layer compute absolute positions and write it to the next hidden state.&lt;/p>
&lt;p>&lt;strong>Theorem 2 (Relative Encoding)&lt;/strong>
Suppose that the hidden state $H^{(1)}$ contains absolute positional information, as stated in Theorem 1, and assume that it is not overwritten by any subsequent layers. Then, the self-attention in all subsequent layers can implement a relative positional encoding: there exists a parameterization of fθ such that, for $\ell ≥ 2$, the attention dot product between query $q_n$ and key $k_m$ at positions n and m can be expressed as:&lt;/p>
$$
\langle q_n, k_m\rangle = f_{cnt}(q, k) + f_{rel}(n − m)
$$&lt;p>where $f_{cnt}$ is a function of their content, and $f_{rel}$ is a function of their relative distance.&lt;/p>
&lt;p>【参考文献 2】探究了 softmax 中 normalization factor 对模型表现的影响，作者定义 attention 为&lt;/p>
$$
\mathrm{Attn}(q,k,v) = \mathrm{softmax}\left(\lambda q^Tk\right)v
$$&lt;p>实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-nope/NoPE-softmax-hyperparameter.png"
width="1335"
height="356"
loading="lazy"
alt="impact of softmax hyper-parameter on NoPE"
class="gallery-image"
data-flex-grow="375"
data-flex-basis="900px"
>&lt;/p>
&lt;p>结果说明，通过调整 $\lambda$ 我们可以有效提高 NoPE 的上下文扩展泛化能力&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>NoPE 说明在 transformer 中我们可以不需要加入位置编码模块，这两篇论文均验证了 NoPE 的有效性。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://openreview.net/forum?id=Drrl2gcjzl" target="_blank" rel="noopener"
>The Impact of Positional Encoding on Length Generalization in Transformers&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://aclanthology.org/2024.findings-acl.834" target="_blank" rel="noopener"
>Length Generalization of Causal Transformers without Position Encoding&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on ALiBi</title><link>https://maosong.website/p/notes-on-alibi/</link><pubDate>Wed, 24 Dec 2025 15:10:55 +0800</pubDate><guid>https://maosong.website/p/notes-on-alibi/</guid><description>&lt;p>meta 等提出了 ALiBi, 一个通过 linear biases 来实现位置编码的方法来提高 LLM 在推理阶段的外推能力。&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>当下，有若干种位置编码的方式：&lt;/p>
&lt;ol>
&lt;li>Sinusoidal position embeddings: Transformer 提出的正弦位置编码&lt;/li>
&lt;li>RoPE: &lt;a class="link" href="https://maosong.website/p/notes-on-position-encoding/" target="_blank" rel="noopener"
>RoPE&lt;/a> 提出的旋转位置编码&lt;/li>
&lt;li>T5 bias: &lt;a class="link" href="https://maosong.website/p/notes-on-t5/" target="_blank" rel="noopener"
>T5&lt;/a> 提出的相对位置编码&lt;/li>
&lt;/ol>
&lt;p>作者通过实验对比了不同的位置编码方法，发现这些方法在推理阶段的外推能力都比较差。&lt;/p>
&lt;p>为了解决这个问题，作者提出了 ALiBi (attention with linear biases), 一个几乎不增加计算和内存开销的位置编码方法，来提高 LLM 在推理阶段的外推能力。&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;p>作者将外推能力定义为&lt;/p>
&lt;blockquote>
&lt;p>a model’s ability to continue performing well as the number of input tokens during validation increases beyond the number of tokens on which the the model was trained.&lt;/p>
&lt;/blockquote>
&lt;p>计 $L$ 为训练阶段的上下文长度， $L_{valid}$ 为推理阶段的上下文长度。&lt;/p>
&lt;p>作者首先对比了不同的位置编码方法的外推能力，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-alibi/ALiBi-comparison-different-position-embedding.png"
width="1155"
height="448"
loading="lazy"
alt="Comparison of different position embeddings"
class="gallery-image"
data-flex-grow="257"
data-flex-basis="618px"
>&lt;/p>
&lt;p>结果显示，不同位置编码在推理阶段扩展模型的上下文能力均有限。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Context Length&lt;/th>
&lt;th>$L$&lt;/th>
&lt;th>$L_{valid}$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Sinusoidal&lt;/td>
&lt;td>512&lt;/td>
&lt;td>50&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>1024&lt;/td>
&lt;td>50&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RoPE&lt;/td>
&lt;td>512&lt;/td>
&lt;td>200&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>1024&lt;/td>
&lt;td>100&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>T5 bias&lt;/td>
&lt;td>512&lt;/td>
&lt;td>600&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>1024&lt;/td>
&lt;td>800&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ALiBi&lt;/td>
&lt;td>512&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>1024&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>为了解决这个问题，作者提出了 AliBi, 其表达式为&lt;/p>
$$
\mathrm{softmax}(q_iK^T+m\cdot [-(i-1),\dots,-2,-1,0])
$$&lt;p>其中 $m$ 是一个和 heads 相关的超参数。如果我们有 8 个 heads, 则对应的 scaling 值分别为 $[1/2^1,1/2^2,\dots,1/2^8]$, 如果我们有 16 个 heads, 则我们对 8 个 heads 的结果进行插值，得到 $[1/2^{0.5},1/2^1,\dots,1/2^8]$. ALiBi 的示意图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-alibi/ALiBi-illustration.png"
width="559"
height="245"
loading="lazy"
alt="illustration of ALiBi"
class="gallery-image"
data-flex-grow="228"
data-flex-basis="547px"
>&lt;/p>
&lt;p>ALiBi 通过 bias 惩罚了较远的 query-key pairs, 并且不同的 heads 的惩罚项也不同，从而每个 head 对距离的信息敏感度也不尽相同。&lt;/p>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>ALiBi 在 WikiText-103 上的实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-alibi/ALiBi-performance.png"
width="747"
height="347"
loading="lazy"
alt="Performance of ALiBi"
class="gallery-image"
data-flex-grow="215"
data-flex-basis="516px"
>&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者分析了已有的 position embedding 方法，发现已有的方法在推理阶段均不能有效扩展模型的上下文长度。因此，作者提出了 AliBi, 一个通过 linear bias 来增加位置信息的方法，作者通过实验验证了 ALiBi 的有效性。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://openreview.net/forum?id=R8sQPpGCv0" target="_blank" rel="noopener"
>Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on T5</title><link>https://maosong.website/p/notes-on-t5/</link><pubDate>Wed, 24 Dec 2025 15:07:08 +0800</pubDate><guid>https://maosong.website/p/notes-on-t5/</guid><description>&lt;p>google 在 2020 年发表了 T5 (Text-to-Text Transfer Transformer), 一个使用统一框架来将所有 NLP 任务转换为 text-to-text 格式的迁移学习框架。&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者首先回顾了迁移学习和 pre-training, 迁移学习是提高模型在下游任务上表现的一类方法，但是目前还没有一个能够对比各种方法的框架。pre-training 通过在大量数据上进行预训练然后再进行微调，可以有效提高模型在下游任务上的表现。&lt;/p>
&lt;p>为了解决这两个问题，作者首先将所有的文本处理任务统一为 &amp;ldquo;text-to-text&amp;rdquo; 的形式，这样我们就可以对比不同架构，训练方式以及数据对模型表现的影响&lt;/p>
&lt;p>作者提到，本文并不是提供一个新的方法，而是详细对比不同方法，为后续研究提供基础。&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="model">&lt;a href="#model" class="header-anchor">&lt;/a>Model
&lt;/h3>&lt;p>在架构上，作者使用了 Transformer 的 encoder-decoder 架构，但是作者做了几点修改&lt;/p>
&lt;ol>
&lt;li>作者提出了 T5 bias, 一个用于替换原始 transformer 绝对位置编码的相对位置编码形式&lt;/li>
&lt;li>作者使用了 RMSNorm 替换了 Transformer 中的 LayerNorm.&lt;/li>
&lt;/ol>
&lt;h3 id="data">&lt;a href="#data" class="header-anchor">&lt;/a>Data
&lt;/h3>&lt;p>作者基于 Common Crawl 构建训练数据集，作者对数据进行了清洗，最终数据集大小为 750GB. 作者将这个数据集记为 C4 (Clean Crawled Corpus).&lt;/p>
&lt;h3 id="downstream-tasks">&lt;a href="#downstream-tasks" class="header-anchor">&lt;/a>Downstream Tasks
&lt;/h3>&lt;p>下游任务包括：&lt;/p>
&lt;ol>
&lt;li>text classification: GLUE, SuperGLUE&lt;/li>
&lt;li>abstractive summarization: CNN/Daily Mail&lt;/li>
&lt;li>question answering: SQuAD&lt;/li>
&lt;li>translation: WMT English to German, French and Romanian&lt;/li>
&lt;/ol>
&lt;h3 id="input-and-output-format">&lt;a href="#input-and-output-format" class="header-anchor">&lt;/a>Input and Output Format
&lt;/h3>&lt;p>所有任务的输入输出都被转换为 text-to-text 格式。&lt;/p>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>作者使用的 baseline 模型是一个基于 encoder-decoder 架构的 transformer 模型，其大小以及 configuration 与 BERT base 差不多，最终模型参数量为 220M。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>field&lt;/th>
&lt;th>num layers&lt;/th>
&lt;th>hidden size&lt;/th>
&lt;th>MLP hidden size&lt;/th>
&lt;th>num heads&lt;/th>
&lt;th>head dim&lt;/th>
&lt;th>dropout&lt;/th>
&lt;th>seq len&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>value&lt;/td>
&lt;td>12&lt;/td>
&lt;td>768&lt;/td>
&lt;td>3072&lt;/td>
&lt;td>12&lt;/td>
&lt;td>64&lt;/td>
&lt;td>0.1&lt;/td>
&lt;td>512&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>训练时，作者使用了 AdaFactor 优化器，batch size 为 512， 训练使用了 34B token. 学习率作者使用了 inverse square root learning schedule: $1/\sqrt{\max(n,k)}$, $n$ 和 $k$ 分别代表当前 step 和 warming up steps.&lt;/p>
&lt;p>作者基于 sentencepiece (见 &lt;a class="link" href="https://maosong.website/p/hands-on-llm1-tokenizer/" target="_blank" rel="noopener"
>LLM tokenizer&lt;/a>) 构建了 Tokenizer, 覆盖 English, German, French 和 Romanian 四种语言。&lt;/p>
&lt;p>模型训练的目标函数为 BERT 使用的 &amp;ldquo;masked language modeling&amp;rdquo;, 格式如下所示&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl"># original text
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Thank you for inviting me to your party last week.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"># inputs
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Thank you &amp;lt;X&amp;gt; me to your party &amp;lt;Y&amp;gt; week.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"># targets
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&amp;lt;X&amp;gt; for inviting &amp;lt;Y&amp;gt; last &amp;lt;Z&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>首先，作者对比了不同的架构。作者对比了如下三种 transformer 的变体：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-t5/T5-architecture-variants.png"
width="887"
height="410"
loading="lazy"
alt="variants of transformer architecture"
class="gallery-image"
data-flex-grow="216"
data-flex-basis="519px"
>&lt;/p>
&lt;p>实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-t5/T5-ablation-architecture-variants.png"
width="1234"
height="363"
loading="lazy"
alt="Performance of different architecture variants"
class="gallery-image"
data-flex-grow="339"
data-flex-basis="815px"
>&lt;/p>
&lt;p>结果显示，encoder-decoder 架构，denoising 训练目标的效果最好。并且，当 layers 减少一半之后，模型的表现大幅度下降。共享参数的 encoder-decoder 架构表现比 prefix LM 效果更好&lt;/p>
&lt;p>接下来作者针对 denoising 的配置进行了测试，实验结果发现 BERT-style 的训练目标效果最好，并且 corruption 比例对模型的表现影响有限，作者使用了 BERT 的配置，即 $15\%$ 的 token 被 masked 掉。对于 span length, 作者通过实验发现不同的 span length 对结果影响不大。因此，作者将 span length 设置为 $3$.&lt;/p>
&lt;p>在数据上，作者发现：&lt;/p>
&lt;ol>
&lt;li>对数据进行过滤可以提高模型的表现&lt;/li>
&lt;li>使用 in-domain 的数据可以提高模型在该 domain 上的表现，但是问题在于 In-domain 的数据往往比较少&lt;/li>
&lt;li>数据量过少时，模型会出现 memorization，也就是过拟合的情况&lt;/li>
&lt;/ol>
&lt;h2 id="overall">&lt;a href="#overall" class="header-anchor">&lt;/a>Overall
&lt;/h2>&lt;p>作者总结前面的发现，构建了 5 个 size 的模型：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>layers&lt;/th>
&lt;th>hidden size&lt;/th>
&lt;th>FFN hidden size&lt;/th>
&lt;th>num heads&lt;/th>
&lt;th>head dimension&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Small&lt;/td>
&lt;td>6&lt;/td>
&lt;td>512&lt;/td>
&lt;td>2048&lt;/td>
&lt;td>8&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Base&lt;/td>
&lt;td>12&lt;/td>
&lt;td>768&lt;/td>
&lt;td>3072&lt;/td>
&lt;td>12&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Large&lt;/td>
&lt;td>24&lt;/td>
&lt;td>1024&lt;/td>
&lt;td>4096&lt;/td>
&lt;td>16&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>3B&lt;/td>
&lt;td>24&lt;/td>
&lt;td>1024&lt;/td>
&lt;td>16384&lt;/td>
&lt;td>32&lt;/td>
&lt;td>128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>11B&lt;/td>
&lt;td>24&lt;/td>
&lt;td>1024&lt;/td>
&lt;td>65536&lt;/td>
&lt;td>128&lt;/td>
&lt;td>128&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 T5, 一个统一所有文本处理任务的迁移学习框架，作者系统性探究了架构，数据以及训练对模型最终表现的影响。最终作者基于 encoder-decoder transformer 架构以及 denoising training objective 训练得到了 T5 系列大语言模型。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://jmlr.org/papers/v21/20-074.html" target="_blank" rel="noopener"
>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>GPipe</title><link>https://maosong.website/p/gpipe/</link><pubDate>Tue, 23 Dec 2025 16:49:25 +0800</pubDate><guid>https://maosong.website/p/gpipe/</guid><description>&lt;p>google 在 2018 年提出了 GPipe, 一个使用 pipeline parallelism 来训练大规模神经网络的并行策略&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>大规模神经网络已经在计算机视觉和自然语言处理等任务上取得了突破性进展。但是目前训练大规模神经网络存在的问题时，我们无法在单一 GPU 上训练我们的模型。基于多 GPU 训练模型需要考虑模型的切分以及通信优化。&lt;/p>
&lt;p>为了解决这个问题，作者提出了 GPipe, 一个用于将大规模性模型分割部署到不同设备上的并行计算策略。&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="notation">&lt;a href="#notation" class="header-anchor">&lt;/a>Notation
&lt;/h3>&lt;p>作者首先定义 notation 如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>notation&lt;/th>
&lt;th>description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$L$&lt;/td>
&lt;td>number of layers&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$w_i$&lt;/td>
&lt;td>weights of a layer&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$f_i$&lt;/td>
&lt;td>forward function of a layer&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$F_k=f_j\circ\cdots\circ f_i$&lt;/td>
&lt;td>forward of a partition&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$B_k$&lt;/td>
&lt;td>backward of a partition&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$K$&lt;/td>
&lt;td>number of partitions&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$N$&lt;/td>
&lt;td>batch size&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$M$&lt;/td>
&lt;td>micro batch size&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="gpipe">&lt;a href="#gpipe" class="header-anchor">&lt;/a>GPipe
&lt;/h3>&lt;p>首先是 naive pipeline parallelism (naive PP), 我们的输入为一个 batch, 然后我们依次计算 $F_1$, 通信传输，计算 $F_2$, 计算完成之后，我们再进行反向传播，更新参数。最后继续下一个 batch 的计算。&lt;/p>
&lt;p>总体的过程如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/gpipe/GPipe-PP-illustration.png"
width="528"
height="700"
loading="lazy"
alt="illustration of pipeline parallelism"
class="gallery-image"
data-flex-grow="75"
data-flex-basis="181px"
>&lt;/p>
&lt;p>下面是一个按照时间轴给出的例子&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/gpipe/GPipe-naive-PP-example.png"
width="1173"
height="318"
loading="lazy"
alt="an example of naive PP with 4 devices"
class="gallery-image"
data-flex-grow="368"
data-flex-basis="885px"
>&lt;/p>
&lt;p>naive PP 的问题在于，每个时刻只有一个 GPU 在工作，GPU 的利用效率很低。因此，GPipe 的做法在于将一个 batch 切分为 $M$ 个更小的 micro-batch, 下面是一个 $M=4$ 的例子&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/gpipe/GPipe-micro-batch-example.png"
width="713"
height="288"
loading="lazy"
alt="An example of pipeline parallelism with 4 devices and 4 micro batches"
class="gallery-image"
data-flex-grow="247"
data-flex-basis="594px"
>&lt;/p>
&lt;p>通过切分更小的 batch，我们可以提高 GPU 的利用率&lt;/p>
&lt;h3 id="analysis">&lt;a href="#analysis" class="header-anchor">&lt;/a>Analysis
&lt;/h3>&lt;h4 id="bubble">&lt;a href="#bubble" class="header-anchor">&lt;/a>Bubble
&lt;/h4>&lt;p>接下来作者分析了 GPipe 的 bubble 情况，bubble 指的是 PP 过程中的 GPU idle time.&lt;/p>
&lt;p>对于 naive PP 来说，一个 GPU 工作时，其余 GPU 都处于空闲状态，因此其 bubble 为&lt;/p>
$$
T_{bubble} = (K-1)(F+B)
$$&lt;p>总的计算时间为&lt;/p>
$$
T_{total} = K(F+B)
$$&lt;p>从而 bubble rate 为&lt;/p>
$$
Bubble_{naive} = \frac{T_{bubble}}{T_{total}} = \frac{K-1}{K}
$$&lt;p>当 $K=8$ 时，我们有 $Bubble_{naive}=87.5\%$, 也就是说，当前训练的 GPU 空闲率为 $87.5\%$.&lt;/p>
&lt;p>对于 GPipe 来说，由于我们将一个 batch 拆分为了更小的 batch, 我们可以提高 GPU 的利用率。&lt;/p>
&lt;p>此时，我们的 bubble time 仍然是 $T_{bubble} = (K-1)(F+B)$. 但是，现在同一时刻工作的 GPU 变多了，从上面的示意图可以看到，前向过程所需要的时间为第一个 micro batch 运行的时间加上 $M-1$ 个 batch 运行所需要的时间，反向同理，因此，GPipe 的总计算时间为&lt;/p>
$$
T_{total} = (M+K-1)(F+B)
$$&lt;p>从而 GPipe 的 bubble rate 为&lt;/p>
$$
Bubble_{naive} = \frac{T_{bubble}}{T_{total}} = \frac{K-1}{M+K-1}
$$&lt;p>当我们令 $M=8, K=8$ 时，我们有 $Bubble_{naive}=46.7\%$, 可以看到，通过提高 micro batch 数量，我们可以显著降低 bubble rate.&lt;/p>
&lt;h4 id="activation-memory">&lt;a href="#activation-memory" class="header-anchor">&lt;/a>Activation Memory
&lt;/h4>&lt;p>对于 naive PP 来说，我们需要缓存每一层的输入，因此 activation memory 为 $\mathcal{O}(N\times L/K)$, 而使用 activation checkpointing 之后，我们现在的 activation memory 为&lt;/p>
$$
\mathcal{O}(N + \frac{L}{K}\times\frac{N}{M})
$$&lt;p>其中第一项代表了 boundary activation, 第二项代表了 Internal activation.&lt;/p>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>作者在 image classification, machine translation 任务上进行了实验。&lt;/p>
&lt;p>作者还进一步分析了影响 GPipe 性能的因素，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/gpipe/GPipe-performance-analysis.png"
width="541"
height="348"
loading="lazy"
alt="Time step breakdown"
class="gallery-image"
data-flex-grow="155"
data-flex-basis="373px"
>&lt;/p>
&lt;p>可以看到，activation checkpointing 是 GPipe 的主要开销来源。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 GPipe, 一个针对大规模神经网络训练的并行策略。通过将模型切分部署在不同的设备上以及使用 micro batch, 我们可以显著提高硬件的利用效率以及训练稳定性。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/1811.06965" target="_blank" rel="noopener"
>GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://siboehm.com/articles/22/pipeline-parallel-training" target="_blank" rel="noopener"
>Pipeline-Parallelism: Distributed Training via Model Partitioning&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Base of RoPE Bounds Context Length</title><link>https://maosong.website/p/base-of-rope-bounds-context-length/</link><pubDate>Mon, 22 Dec 2025 11:34:42 +0800</pubDate><guid>https://maosong.website/p/base-of-rope-bounds-context-length/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>&lt;a class="link" href="https://maosong.website/p/notes-on-position-encoding/" target="_blank" rel="noopener"
>RoPE&lt;/a> 已经成为了大多数 LLM 使用的 position encoding 范式，但是，RoPE 与 LLM long context 之间的关系还没有被探索清楚。在本文中，作者就探究了 base frequency 与 LLM context capability 之间的关系，并给出了一个达到指定上下文长度所需要的 base frequency 的 lower bound.&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;p>首先作者回顾了 attention 与 &lt;a class="link" href="https://maosong.website/p/notes-on-position-encoding/" target="_blank" rel="noopener"
>RoPE&lt;/a> 的定义， 关键就是 RoPE 这部分，如下所示&lt;/p>
$$
A_{ij} = (R_{i,\theta}q_i)^T(R_{j,\theta}k_j) = q_i^TR_{i-j,\theta}k_j
$$&lt;p>这里 $\theta$ 就是 base frequency, 作者总结不同模型的 base frequency 配置如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Llama-7B&lt;/th>
&lt;th>Llama2-7B&lt;/th>
&lt;th>Llama3-8B&lt;/th>
&lt;th>Mistral-7B&lt;/th>
&lt;th>Baichuan2-7B&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Base frequency&lt;/td>
&lt;td>10,000&lt;/td>
&lt;td>10,000&lt;/td>
&lt;td>500,000&lt;/td>
&lt;td>1,000,000&lt;/td>
&lt;td>10,000&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Context length&lt;/td>
&lt;td>2048&lt;/td>
&lt;td>4096&lt;/td>
&lt;td>8192&lt;/td>
&lt;td>32,768&lt;/td>
&lt;td>4,096&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>接下来，作者回顾了 &lt;a class="link" href="https://maosong.website/p/notes-on-yarn/" target="_blank" rel="noopener"
>YARN&lt;/a>. 其核心思想在于，预训练阶段所有可能的 $\cos(t-s)\theta_i$ 都见过，才能保证模型的 OOD 表现&lt;/p>
&lt;p>作者认为 base frequency 的设置应该满足两个条件：&lt;/p>
&lt;ol>
&lt;li>&lt;strong>The closer token gets more attention&lt;/strong>: 当前的 token 应该给邻近的 token 更高的注意力&lt;/li>
&lt;li>&lt;strong>The similar token gets more attention&lt;/strong>: 当前的 token 应该给相似的 token 更高的注意力&lt;/li>
&lt;/ol>
&lt;p>在 &lt;a class="link" href="https://maosong.website/p/notes-on-position-encoding/" target="_blank" rel="noopener"
>RoPE&lt;/a> 中，作者已经给出了 $A_{ij}$ 与相对距离 $|i-j|$ 之间的关系。因此第一个性质已经满足了。&lt;/p>
&lt;p>接下来，作者分析了相似 token 的性质，作者定义 token 的相似性如下：&lt;/p>
$$
\mathbb{E}_{q,k^*}[q^TR_{m,\theta}k^*] - \mathbb{E}_{q,k}[q^TR_{m,\theta}k]
$$&lt;p>这里 $k^*=q+\epsilon$ 代表了相似的 token, 而 $k$ 是一个随机 token. 作者给出的结论如下&lt;/p>
&lt;p>&lt;strong>Theorem&lt;/strong>
假设 $q,k\in\mathbb{R}^d$ 独立同分布，它们的标准差为 $\sigma\in\mathbb{R}$, 则对于 $k^*=q+\epsilon$, $\epsilon$ 是一个随机变量满足 $\mathbb{E}[\epsilon]=0$, 则我们有&lt;/p>
$$
\mathbb{E}_{q,k^*}[q^TR_{m,\theta}k^*] - \mathbb{E}_{q,k}[q^TR_{m,\theta}k] = 2\sigma^2\sum_{i=0}^{d/2-1}\cos(m\theta_i)
$$&lt;p>作者定义 $B_{m,\theta}=\sum_{i=0}^{d/2-1}\cos(m\theta_i)$, 作者认为给定 $\theta$, 模型的上下文长度 $L_\theta$ 满足&lt;/p>
$$
L_\theta = \sup\{L\mid B_{m,\theta}\geq 0, \forall m\in[L]\}
$$&lt;p>也就是说，base frequency 决定了 LLM 的上下文长度。作者给出了不同的上下文长度对应的 base frequency 如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Context Len.&lt;/th>
&lt;th>1k&lt;/th>
&lt;th>2k&lt;/th>
&lt;th>4k&lt;/th>
&lt;th>8k&lt;/th>
&lt;th>16k&lt;/th>
&lt;th>32k&lt;/th>
&lt;th>64k&lt;/th>
&lt;th>128k&lt;/th>
&lt;th>256k&lt;/th>
&lt;th>512k&lt;/th>
&lt;th>1M&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Lower Bound&lt;/td>
&lt;td>4.3e3&lt;/td>
&lt;td>1.6e4&lt;/td>
&lt;td>2.7e4&lt;/td>
&lt;td>8.4e4&lt;/td>
&lt;td>3.1e5&lt;/td>
&lt;td>6.4e5&lt;/td>
&lt;td>2.1e6&lt;/td>
&lt;td>7.8e6&lt;/td>
&lt;td>3.6e7&lt;/td>
&lt;td>6.4e7&lt;/td>
&lt;td>5.1e8&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>总的来说，远距离衰减性保证了模型会更关注邻近的 token, 而相似 token 保证了模型能够区分出真正有意义的 token.&lt;/p>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>作者首先分析了 base frequency 在 fine-tuning 阶段对模型上下文能力的影响，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/base-of-rope-bounds-context-length/base-frequency-fine-tuning-impact.png"
width="1072"
height="465"
loading="lazy"
alt="impact of base frequency on fine-tuning stage"
class="gallery-image"
data-flex-grow="230"
data-flex-basis="553px"
>&lt;/p>
&lt;p>从实验结果可以看到，当 base frequency 低于阈值时，模型的表现急剧下降。&lt;/p>
&lt;p>作者进一步探讨了 base frequency 对于模型 pre-training 阶段的影响，结果也是一样的，即非常小的 base frequency 会限制模型的 context 能力，结果下图所示 （三行分别代表了 base frequency 为 1e2, 1e4 和 1e6 的情况）&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/base-of-rope-bounds-context-length/base-frequency-pre-training-stage.png"
width="1044"
height="725"
loading="lazy"
alt="impact of base frequency on pre-training stage"
class="gallery-image"
data-flex-grow="144"
data-flex-basis="345px"
>&lt;/p>
&lt;p>可以看到，尽管 perplexity 都差不多，但是使用更大的 base frequency 其长上下文能力明显更好。&lt;/p>
&lt;p>作者进一步分析了为什么较小的 base frequency 会影响模型的长上下文能力。作者认为较小的 base frequency 会导致 $B_{m,\theta}$ 接近于 0， 从而模型难以区分随机 token 和相似 token, 这样模型只能依赖于邻近 token 进行学习，这样就限制了模型的长上下文能力&lt;/p>
&lt;p>作者还进一步对比了提高 base frequency 与 Interpolation 两种做法，实验结果如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/base-of-rope-bounds-context-length/base-frequency-comparison-interpolation.png"
width="865"
height="180"
loading="lazy"
alt="comparison with interpolation"
class="gallery-image"
data-flex-grow="480"
data-flex-basis="1153px"
>&lt;/p>
&lt;p>实验结果说明，Interpolation 在上下文超过 30K 之后，其 $B_{m,\theta}\leq0$ 的 次数显著增加，表明了其和上下文能力之间的关系。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中，探究了 RoPE 中 base frequency 与 LLM 上下文能力之间的关系，发现了提高模型的上下文能力需要关注 RoPE 的 base frequency 超参数，并给出了对应的 lower bound. 作者通过实验验证了这个观点。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2405.14591" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on NSA</title><link>https://maosong.website/p/notes-on-nsa/</link><pubDate>Mon, 15 Dec 2025 17:39:16 +0800</pubDate><guid>https://maosong.website/p/notes-on-nsa/</guid><description>&lt;p>DeepSeek 在 25 年 1 月提出了 Natively trainable Sparse Attention (NSA), 一个软硬件结合的稀疏注意力机制，NSA 可以在提高模型推理效率的同时提高计算效率。&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>现有的大模型主要是基于 Transformer 提出的 softmax attention, 其主要问题在于随上下文长度增加，其 latency 也上升更快。理论估计，对于 64k 上下文长度的输出，softmax attention 部分的计算占 $70\%\sim80\%$ 的 latency.&lt;/p>
&lt;p>为了解决 softmax 的 high latency 问题，，一个做法就是使用稀疏注意力机制，如 MInference 等，但是这些系数注意力机制大多没有实际部署，且它们一般只在 inference 阶段使用&lt;/p>
&lt;p>作者认为解决这个问题有两个挑战：&lt;/p>
&lt;ol>
&lt;li>Hardware-aligned inference speedup: 降低 inference latency 需要算法与硬件结合，不能只关注算法层面的改进&lt;/li>
&lt;li>Training-aware algorithm design: 需要在训练阶段也支持算法，从而可以降低训练的算力消耗并且保持模型的表现&lt;/li>
&lt;/ol>
&lt;p>为了解决这两个问题，作者就提出了 natively trainable sparse attention (NSA) 架构。NSA 通过将 key 和 value 分割为不同的 block, 然后基于三种 path: compressed coarse-grained tokens, selectively retrained fine-grained tokens 以及 sliding windows for local contextual information 来进行处理和过滤。NSA 提出了两点观点改进：&lt;/p>
&lt;ol>
&lt;li>Hardware-aligned system: 优化了 blockwise sparse attention 来平衡 arithmetic intensity.&lt;/li>
&lt;li>Training-aware design: 支持端到端的训练和部署&lt;/li>
&lt;/ol>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="overview">&lt;a href="#overview" class="header-anchor">&lt;/a>Overview
&lt;/h3>&lt;p>作者首先回顾了 attention 的定义如下：&lt;/p>
$$
\mathbf{o}_t=\mathrm{Attn}(\mathbf{q_t},\mathbf{k}_{:,t}, \mathbf{v}_{:,t})=\sum_{i=1}^t \frac{\alpha_{t,i}}{\sum_{t,i}\alpha_{t,i}}\mathbf{v_i},\ \alpha_{t,i} = \exp\left(\frac{\mathbf{q_t}^T\mathbf{k}_{i}}{\sqrt{d_k}}\right)
$$&lt;p>其中 $\mathbf{q_t}\in\mathbb{R}^{d_k}$.&lt;/p>
&lt;p>接下来是 Arithmetic Intensity. Arithmetic intensity 指的是 FLOPs 与内存访问次数之比。由于现在的 GPU 都是计算密集型设备，理想情况下应该是 Arithmetic intensity 越高越好。&lt;/p>
&lt;p>对于 causal self-attention 来说，在训练以及 prefilling 阶段，由于 batch 较大，因此整体的 Arithmetic intensity 较高，因而这两个阶段是 computer-bound. 但是在 decoding 阶段，由于其 token-by-token generation 的性质，每次生成新的 token 时都需要重新加载 KV cache, 因而是 memory-bound.&lt;/p>
&lt;p>从而我们的优化目标也变得不一致：在训练阶段，我们希望降低计算消耗，而在推理 (decodng) 阶段，我们希望降低内存访问次数。&lt;/p>
&lt;p>基于这两个目标，作者提出了使用 $\mathbf{k}_{:,t}, \mathbf{v}_{:,t}$ 的子集 $\tilde{K}_t, \tilde{V}_t$ 来参与计算，其对应的 attention 如下所示&lt;/p>
$$
\tilde{K}_t=f_K(\mathbf{q_t},\mathbf{k}_{:,t}, \mathbf{v}_{:,t}), \tilde{V}_t=f_V(\mathbf{q_t},\mathbf{k}_{:,t}, \mathbf{v}_{:,t}), \mathbf{o}_t=\mathrm{Attn}(\mathbf{q_t},\tilde{K}_t, \tilde{V}_t)
$$&lt;p>我们还可以结合不同的方法来进行组合：&lt;/p>
$$
\mathbf{o}_t^*=\sum_{c\in\mathcal{C}}g_t^c\mathrm{Attn}(\mathbf{q_t},\tilde{K}_t^c, \tilde{V}_t^c)
$$&lt;p>作者在本文中使用了三种方法 $\mathcal{C}=\{\mathrm{cmp},\mathrm{slc},\mathrm{win}\}$, 分别代表了 compression, selection 以及 sliding window, $g_t^c\in[0,1]$ 代表了不同方法对应的 gating score, 类似于 MoE 的 gating layer, $g_t^c$ 由一个 MLP 和一个 sigmoid activation 生成。最终 NSA 的架构如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-nsa/NSA-architecture.png"
width="1364"
height="402"
loading="lazy"
alt="Overview of NSA architecture"
class="gallery-image"
data-flex-grow="339"
data-flex-basis="814px"
>&lt;/p>
&lt;p>作者定义 $N_t$ 代表参与计算的 KV 的总个数：&lt;/p>
$$
N_t = \sum_{c\in\mathcal{C}} \mathrm{size}[\tilde{K}_t^c].
$$&lt;p>作者使用了一个较高的 sparsity ratio 来保证 $N_t&lt;&lt;t$.&lt;/p>
&lt;h3 id="design">&lt;a href="#design" class="header-anchor">&lt;/a>Design
&lt;/h3>&lt;p>接下来作者分别介绍了每一部分的设计&lt;/p>
&lt;h4 id="token-compression">&lt;a href="#token-compression" class="header-anchor">&lt;/a>Token Compression
&lt;/h4>&lt;p>对于 token compression, 其定义如下：&lt;/p>
$$
\tilde{K}_t^{\mathrm{cmp}} = f_K^{\mathrm{cmp}}(\mathbf{k}_{:,t})=\left\{\phi(\mathbf{k}_{id+1:id+l})\mid 0\leq i\leq \left\lfloor\frac{t-l}{d}\right\rfloor\right\}\in\mathbb{R}^{d_k\times \left\lfloor\frac{t-l}{d}\right\rfloor}
$$&lt;p>其中 $l$ 是 block size, $d$ 是 sliding stride, $\phi:\mathbb{R}^{l\times d_k}\to \mathbb{R}^d_k$ 是一个 MLP 用于将 block key 映射为一个单一的 key. 对于 $\tilde{V}_t^{\mathrm{cmp}}$ 作者也使用了类似的做法。&lt;/p>
&lt;h4 id="token-selection">&lt;a href="#token-selection" class="header-anchor">&lt;/a>Token Selection
&lt;/h4>&lt;p>仅使用 compressed token 的话，可能会丢失一些细粒度的信息。因此，作者额外提出了 token selection 机制来解决这个问题。&lt;/p>
&lt;p>作者使用的做法是 blockwise selection. 这样做的原因有两点：&lt;/p>
&lt;ol>
&lt;li>hardware efficiency. 这样做的原因是 GPU 访问内存是在 block 层面进行的，因而更加高效&lt;/li>
&lt;li>inherent distribution patterns of attention scores. MInference 证明了 attention score 在空间上存在连续性。即相邻的 key 对应的重要性非常相似&lt;/li>
&lt;/ol>
&lt;p>为了实现 block-wise selection, 作者首先将 key value sequences 分割为 blocks, 然后针对每个 blocks 分配 Importance score.&lt;/p>
&lt;p>作者首先介绍了如何计算不同 block 的 importance score.&lt;/p>
&lt;p>如果 selection block size 与 compression block size ，即 $l'=l$ 相同的话，则我们可以直接用 compression block 提供的信息：&lt;/p>
$$
\mathbf{p}_{t}^{\mathrm{cmp}} = \mathrm{sotmax}\left(\mathbf{q}_t^T\tilde{K}_t^{\mathrm{cmp}}\right)
$$&lt;p>其中 $\mathbf{p}_{t}^{\mathrm{cmp}}\in\mathbb{R}^{\left\lfloor\frac{t-l}{d}\right\rfloor+1}$ 代表了 $\mathbf{q}_t$ 和 compressed key $\tilde{K}_t^{\mathrm{cmp}}$ 之间的 attention score.&lt;/p>
&lt;p>如果 $l'\neq l$ 的话，作者通过空间关系来进行计算，假设 $l\leq l'$, $d\mid l$, $d\mod l'$, 则我们有&lt;/p>
$$
\mathbf{p}_{t}^{\mathrm{slc}}[j] = \sum_{m=0}^{l'/d-1}\sum_{n=0}^{l/d-1}\mathbf{p}_{t}^{\mathrm{cmp}}\left[\frac{l'}{d}j-m-n\right]
$$&lt;p>对于 &lt;a class="link" href="https://maosong.website/p/notes-on-gqa/" target="_blank" rel="noopener"
>GQA&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/notes-on-mqa/" target="_blank" rel="noopener"
>MQA&lt;/a>, 由于其 KV-cache 在 heads 之间共享，因此我们必须保证不同 heads 之间的 consistency, 因此作者提出了 shared importance score 如下：&lt;/p>
$$
\mathbf{p}_{t}^{\mathrm{slc}'} = \sum_{h=1}^H\mathbf{p}_{t}^{\mathrm{slc},(h)}
$$&lt;p>接下来，对于每个 block 及其对应的 Importance score, 作者保存 top-$n$ sparse blcoks, 如下所示&lt;/p>
$$
\begin{aligned}
\mathcal{I}_t &amp;= \{i\mid \mathrm{rank}(p_t^{\mathrm{slc}'}[i])\leq n\}\\
\tilde{K}_t^{\mathrm{slc}} &amp;= \mathrm{Cat}\left[\{\mathbf{k}_{il'+1:(i+1)l'}\mid i\in \mathcal{I}_t\}\right]
\end{aligned}
$$&lt;p>其中 $\mathrm{rank}(\cdot)$ 代表了降序排列的 importance scores. $\mathcal{I}_t$ 是选择出来的 block indices, $\mathrm{Cat}(\cdot)$ 表示了 concatenation operation. $\tilde{K}_t^{\mathrm{slc}}\in\mathbb{R}^{d_k\times il'}$ 代表了选择出来的 key.&lt;/p>
&lt;h4 id="sliding-window">&lt;a href="#sliding-window" class="header-anchor">&lt;/a>Sliding Window
&lt;/h4>&lt;p>为了避免 local pattern 对 compression token 以及 selection token 的学习产生影响，作者额外使用了一个 branch 来学习这个 local pattern. 其具体做法就是维持一个 sliding window 用于最近的若干个 token, 即&lt;/p>
$$
\tilde{K}_t^{\mathrm{win}} = \mathbf{k}_{t-w:t}, \tilde{V}_t^{\mathrm{win}} = \mathbf{v}_{t-w:t}
$$&lt;p>这里 $w$ 是 window size.&lt;/p>
&lt;p>为了进一步避免 shortcut learning, 对于三个 branch 作者提供了不同的 key 和 values&lt;/p>
&lt;h4 id="kernel-design">&lt;a href="#kernel-design" class="header-anchor">&lt;/a>Kernel Design
&lt;/h4>&lt;p>接下来是针对硬件设计进行的优化。由于 flash attention 2 对 compression attention 以及 sliding window attention 已经支持的比较好，作者这里介绍了如何针对 selection attention 进行优化。&lt;/p>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>作者构建了一个 27B-A3B 的 MoE 模型，attention 基于 GQA, MoE 基于 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseekmoe/" target="_blank" rel="noopener"
>DeepSeekMoE&lt;/a>. 模型配置如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>field&lt;/th>
&lt;th>value&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>layers&lt;/td>
&lt;td>30&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>hidden dimension&lt;/td>
&lt;td>2560&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>head groups&lt;/td>
&lt;td>4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>attention heads&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>query head dimension&lt;/td>
&lt;td>192&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>value head dimension&lt;/td>
&lt;td>128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>routed experts&lt;/td>
&lt;td>72&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>shared experts&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>activated experts&lt;/td>
&lt;td>6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>dense layers&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>NSA 配置如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>field&lt;/th>
&lt;th>value&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$l$&lt;/td>
&lt;td>32&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$d$&lt;/td>
&lt;td>16&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$l'$&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$n$&lt;/td>
&lt;td>16&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$w$&lt;/td>
&lt;td>512&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>其中 selection blocks 包含初始的一个 block 以及最近的 2 个 block.&lt;/p>
&lt;p>模型先在 8K 的上下文长度下使用 270B token 进行预训练，接下来在使用 &lt;a class="link" href="https://maosong.website/p/notes-on-yarn/" target="_blank" rel="noopener"
>YARN&lt;/a> 将模型上下文通过 continual pre-training 以及 SFT 扩展到 32K. 训练过程的损失如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-nsa/NSA-training-loss.png"
width="1073"
height="780"
loading="lazy"
alt="Training loss of NSA"
class="gallery-image"
data-flex-grow="137"
data-flex-basis="330px"
>&lt;/p>
&lt;p>作者从 general performance, long-context performance 以及 CoT reasoning performance 三个层面来评估 NSA 的表现。&lt;/p>
&lt;p>首先是 NSA 与其他 sparse attention 以及 baseline 在通用任务上表现的对比，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-nsa/NSA-general-performance.png"
width="1355"
height="181"
loading="lazy"
alt="Performance of NSA on general benchmarks"
class="gallery-image"
data-flex-grow="748"
data-flex-basis="1796px"
>&lt;/p>
&lt;p>接下来是 NSA 在 LongBench 上的表现：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-nsa/NSA-long-context-performance.png"
width="1361"
height="358"
loading="lazy"
alt="Performance of NSA on LongBench"
class="gallery-image"
data-flex-grow="380"
data-flex-basis="912px"
>&lt;/p>
&lt;p>作者还使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-r1/" target="_blank" rel="noopener"
>DeepSeek-R1&lt;/a> 中的知识蒸馏方法，结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Generation token limit&lt;/th>
&lt;th>8192&lt;/th>
&lt;th>16384&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Full Attention-R&lt;/td>
&lt;td>0.046&lt;/td>
&lt;td>0.092&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>NSA-R&lt;/td>
&lt;td>0.121&lt;/td>
&lt;td>0.146&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>上面的结果均验证了 NSA 的有效性&lt;/p>
&lt;h2 id="analysis">&lt;a href="#analysis" class="header-anchor">&lt;/a>Analysis
&lt;/h2>&lt;p>接下来，作者分析了 NSA 的性质。作者首先对比了 NSA 和 flash attention 2 的训练速度，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-nsa/NSA-training-speed-performance.png"
width="883"
height="514"
loading="lazy"
alt="Performance comparison between NSA and flash attention 2"
class="gallery-image"
data-flex-grow="171"
data-flex-basis="412px"
>&lt;/p>
&lt;p>可以看到，相比于 flash attention 2, NSA 在 forward 过程和 backward 过程的的效率分别提升了 9 倍和 6 倍。作者认为这是由于两个优点：&lt;/p>
&lt;ol>
&lt;li>NSA 使用了 block-wise memory access, 提高了 tensor core 的利用率&lt;/li>
&lt;li>loop scheduling 减少了 KV transfer 时的 kernel 冗余&lt;/li>
&lt;/ol>
&lt;p>作者还对比了不同 attention 的解码速度，在 NSA 中，每次只需要 $\left\lfloor\frac{s-l}{d}\right\rfloor+nl'+w$ 个 token 就可以完成计算，作者对比不同 attention 所需余姚的 token 如下表所示如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Context Length&lt;/th>
&lt;th>8192&lt;/th>
&lt;th>16384&lt;/th>
&lt;th>32768&lt;/th>
&lt;th>65536&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Full attention&lt;/td>
&lt;td>8192&lt;/td>
&lt;td>16384&lt;/td>
&lt;td>32768&lt;/td>
&lt;td>65536&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>NSA&lt;/td>
&lt;td>2048&lt;/td>
&lt;td>2560&lt;/td>
&lt;td>3584&lt;/td>
&lt;td>5632&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>speedup&lt;/td>
&lt;td>4x&lt;/td>
&lt;td>6.4x&lt;/td>
&lt;td>9.1x&lt;/td>
&lt;td>11.6x&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="discussion">&lt;a href="#discussion" class="header-anchor">&lt;/a>Discussion
&lt;/h2>&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 NSA, 一个通过软硬件协同结合 compression, selection 以及 sliding window 的稀疏注意力机制，作者通过实验验证了其有效性。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2502.11089" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>MoE tutorial</title><link>https://maosong.website/p/moe-tutorial/</link><pubDate>Sat, 13 Dec 2025 16:04:04 +0800</pubDate><guid>https://maosong.website/p/moe-tutorial/</guid><description>&lt;p>本 blog 详细介绍了 MoE 模型的一些关键设计与相关实验结果，为 MoE 模型的学习提供基础。&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;h3 id="motivation">&lt;a href="#motivation" class="header-anchor">&lt;/a>Motivation
&lt;/h3>&lt;p>现有大部分大语言模型均是基于 Transformer 架构，&lt;a class="link" href="https://maosong.website/p/kaplan-scaling-law/" target="_blank" rel="noopener"
>Kaplan scaling law&lt;/a> 通过实验说明，大语言模型的表现与算力，数据，模型参数量息息相关。但是，对于 dense 模型来说，我们提高模型参数量时，必须同时提高所使用的算力。这就限制了大模型的 scaling law.&lt;/p>
&lt;p>而 MoE 模型的解决方法为在计算时只激活部分参数，这样，我们就可以在同等激活参数量/算力下训练更大参数量的模型，从而达到更好地表现。&lt;/p>
&lt;p>因此，MoE 模型的核心思想在于&lt;/p>
&lt;blockquote>
&lt;p>使用相同的激活参数量/算力，提高模型总参数量，从而达到更好的表现。&lt;/p>
&lt;/blockquote>
&lt;h3 id="definition">&lt;a href="#definition" class="header-anchor">&lt;/a>Definition
&lt;/h3>&lt;p>MoE 模型和 dense 模型的示意图如下，图源 &lt;a class="link" href="https://maosong.website/p/notes-on-olmoe/" target="_blank" rel="noopener"
>olmoe&lt;/a>&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/moe-tutorial/olmoe-MoE_architecture.png"
width="1356"
height="912"
loading="lazy"
alt="olmoe-MoE_architecture"
class="gallery-image"
data-flex-grow="148"
data-flex-basis="356px"
>&lt;/p>
&lt;p>一个 MoE layer 包括两个模块：&lt;/p>
&lt;ol>
&lt;li>Router：Router 负责为 token 指定合适的专家&lt;/li>
&lt;li>Expert：Expert 负责处理 token&lt;/li>
&lt;/ol>
&lt;p>对于输入 $x\in\mathbb{R}^d$, 我们假设有 $N$ 个 Expert，router 一般是一个 linear layer 再加上一个 gating function (softmax 或者 sigmoid， 我们本文中使用 softmax), 其构建了 $\mathbb{R}^d\to\mathbb{R}^N$ 的映射，定义为：&lt;/p>
$$
G(x) =[G_1(x),\dots,G_N(x)] = \mathrm{softmax}(W_gx + b)\in\mathbb{R}^N
$$&lt;p>其中 $W_g\in\mathbb{R}^{N\times d}$, $b\in\mathbb{R}^N$ 是可学习的参数。$G_{i}(x)$ 代表了当前 token $x$ 选择第 $i$ 个 Expert 的概率。&lt;/p>
&lt;p>一般来说，Expert 会使用和 dense 模型一样的 MLP, 我们记为&lt;/p>
$$
E_i(x) = \mathrm{FFN}(x), \quad i = 1,\dots,N
$$&lt;p>接下来，基于 $G(x)$ 和 $E(x)$, 我们会使用合适的方法来挑选 $K&lt;N$ 个 Expert 出来，其中 $K$ 是给定的超参数，我们记挑选出来的 $K$ 个 Expert 的 index 为 $e_1,\dots,e_K$, 即&lt;/p>
$$
e_i \in \{i\mid G_i(x)\in \mathrm{TopK}(G_i(x), K)\},\ i=1,\dots,K
$$&lt;p>最终 MoE layer 的输出为&lt;/p>
$$
y = \sum_{i=1}^K\mathrm{Normalize}(G_{e_i}(x)) E_{e_i}(x)
$$&lt;p>这里 $\mathrm{Normalize}(\cdot)$ 代表我们对于输出进行归一化，即&lt;/p>
$$
\mathrm{Normalize}(G_{e_i}) = \frac{\exp(G_{e_i})}{\sum_{i=1}^K \exp(G_{e_i})},\quad i=1,\dots,K
$$&lt;h3 id="why-moe">&lt;a href="#why-moe" class="header-anchor">&lt;/a>Why MoE
&lt;/h3>&lt;p>选择 MoE 的原因有三点：效率, scaling law 以及表现。&lt;/p>
&lt;h4 id="efficiency">&lt;a href="#efficiency" class="header-anchor">&lt;/a>Efficiency
&lt;/h4>&lt;p>MoE 训练更加高效，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/moe-tutorial/Switch-Transformer-speed-comparison.png"
width="839"
height="672"
loading="lazy"
alt="Comparison between moe and dense models (Switch Transformer)"
class="gallery-image"
data-flex-grow="124"
data-flex-basis="299px"
>&lt;/p>
&lt;p>&lt;a class="link" href="https://maosong.website/p/switch-transformer/" target="_blank" rel="noopener"
>Switch Transformer&lt;/a> 的 实验结果说明，MoE model 的训练效率比 dense model 快 7 倍左右。其他模型也有类似结论。总的来说，MoE 模型相比于 dense 模型，训练效率更高。&lt;/p>
&lt;h4 id="scaling-law">&lt;a href="#scaling-law" class="header-anchor">&lt;/a>Scaling Law
&lt;/h4>&lt;p>MoE 模型可以突破传统 scaling law 的限制，在算力固定的情况下，我们可以通过提高 MoE 模型的稀疏度来进一步提高模型的表现&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/moe-tutorial/EL-activation-ratio-impact.png"
width="829"
height="485"
loading="lazy"
alt="Impact of the Activation Ratio A on Loss and Efficiency"
class="gallery-image"
data-flex-grow="170"
data-flex-basis="410px"
>&lt;/p>
&lt;p>如上图所示，在 FLOPs 给定的情况下，随着模型稀疏度的提高，模型的表现和效率都有提升&lt;/p>
&lt;h4 id="performance">&lt;a href="#performance" class="header-anchor">&lt;/a>Performance
&lt;/h4>&lt;p>MoE 模型的表现更强，如下图所示，MoE 模型的训练，验证损失以及在下游任务上的表现均超过了 dense 模型&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/moe-tutorial/olmoe-MoE-vs-Dense-training-efficiency.png"
width="1155"
height="626"
loading="lazy"
alt="MoE vs. Dense (olmoe)"
class="gallery-image"
data-flex-grow="184"
data-flex-basis="442px"
>&lt;/p>
&lt;h3 id="timeline">&lt;a href="#timeline" class="header-anchor">&lt;/a>Timeline
&lt;/h3>&lt;p>激活参数比例变化图如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/moe-tutorial/moe_timeline_parameters.png"
width="3320"
height="1764"
loading="lazy"
alt="activation ratio of MoE models"
class="gallery-image"
data-flex-grow="188"
data-flex-basis="451px"
>&lt;/p>
&lt;p>可以看到，当前大部分模型的激活比例都在 $5\%$ 左右。&lt;/p>
&lt;p>另一方面，从专家的激活比例变化图如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/moe-tutorial/moe_timeline_experts.png"
width="3294"
height="1764"
loading="lazy"
alt="activation ratio (experts) of moe models"
class="gallery-image"
data-flex-grow="186"
data-flex-basis="448px"
>&lt;/p>
&lt;p>可以看到，现在大部分模型总专家数都在 200-400 左右，&lt;a class="link" href="https://maosong.website/p/notes-on-kimi-k2/" target="_blank" rel="noopener"
>Kimi-k2&lt;/a> 认为提高专家个数可以提高模型表现，而 LongCat 则是使用了 phantom expert 机制&lt;/p>
&lt;h2 id="moe-design">&lt;a href="#moe-design" class="header-anchor">&lt;/a>MoE Design
&lt;/h2>&lt;h3 id="experts-design">&lt;a href="#experts-design" class="header-anchor">&lt;/a>Experts Design
&lt;/h3>&lt;h4 id="number-of-experts">&lt;a href="#number-of-experts" class="header-anchor">&lt;/a>Number of Experts
&lt;/h4>&lt;p>一般来说，专家个数越多，模型越稀疏，模型表现越好。扩展专家个数有两个方式：&lt;/p>
&lt;ol>
&lt;li>直接增加专家个数，这会导致模型参数量上升，如 &lt;a class="link" href="https://maosong.website/p/switch-transformer/" target="_blank" rel="noopener"
>Switch Transformer&lt;/a>&lt;/li>
&lt;li>对已有的专家进行切分，将大专家切分为小专家，如 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseekmoe/" target="_blank" rel="noopener"
>DeepSeekMoE&lt;/a>&lt;/li>
&lt;/ol>
&lt;p>&lt;a class="link" href="https://maosong.website/p/switch-transformer/" target="_blank" rel="noopener"
>Switch Transformer&lt;/a> 也通过实验发现，增加专家个数可以显著提高模型的训练效率和表现，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/moe-tutorial/Switch-Transformer-scaling-law.png"
width="1244"
height="497"
loading="lazy"
class="gallery-image"
data-flex-grow="250"
data-flex-basis="600px"
>&lt;/p>
&lt;p>可以看到，当我们增加专家个数的时候，模型的表现是持续提升的。并且当我们增加专家个数之后，模型的训练效率也有所提升。&lt;/p>
&lt;p>&lt;a class="link" href="https://maosong.website/p/notes-on-deepseekmoe/" target="_blank" rel="noopener"
>DeepSeekMoE&lt;/a> 提出了 fine-granularity expert 的概念，其做法是通过减少 expert 的大小在相同参数量的场景下使用更多的专家。实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/moe-tutorial/DeepSeekMoE-ablation-experts.png"
width="1197"
height="552"
loading="lazy"
alt="DeepSeek-Moe shared experts ablation study"
class="gallery-image"
data-flex-grow="216"
data-flex-basis="520px"
>&lt;/p>
&lt;p>可以看到，在稀疏度 (激活专家个数占总专家个数比例) 不变的情况下，提高专家的粒度，可以提高模型的表现。&lt;/p>
&lt;p>&lt;a class="link" href="https://maosong.website/p/notes-on-olmoe/" target="_blank" rel="noopener"
>olmoe&lt;/a> 对 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseekmoe/" target="_blank" rel="noopener"
>DeepSeekMoE&lt;/a> 的这个观点进行了验证，结果如下图所示，&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/moe-tutorial/olmoe-expert-granularity.png"
width="1446"
height="592"
loading="lazy"
alt="Expert granularity"
class="gallery-image"
data-flex-grow="244"
data-flex-basis="586px"
>&lt;/p>
&lt;p>结果显示，当专家粒度从 8E-1A 扩展到 32E-4A 时，模型在 HellaSwag 上的表现提升了 $10\%$, 但是进一步扩展到 64E-8A 时，模型的表现提升不到 $2\%$, 这说明了无限制提升专家粒度对模型的提升越来越有限。&lt;/p>
&lt;p>&lt;a class="link" href="https://maosong.website/p/notes-on-kimi-k2/" target="_blank" rel="noopener"
>Kimi-k2&lt;/a> 探究了针对 MoE 模型 sparsity 的 scaling law, 结果也说明，提升 sparsity 可以提高模型的表现。因此，其相对于 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v3/" target="_blank" rel="noopener"
>DeepSeek-V3&lt;/a> 使用了 $50\%$ 额外的的专家数。&lt;a class="link" href="https://maosong.website/p/notes-on-ling-mini-beta/" target="_blank" rel="noopener"
>Ling-mini-beta&lt;/a> 进一步验证了这个观点。&lt;/p>
&lt;h4 id="shared-experts">&lt;a href="#shared-experts" class="header-anchor">&lt;/a>Shared Experts
&lt;/h4>&lt;p>Shared Expert 由 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseekmoe/" target="_blank" rel="noopener"
>DeepSeekMoE&lt;/a> 提出，其基本思想为，固定某几个专家，响应所有的 token，这样可以让某些专家学习到共有的知识，而让其他的专家学习到特定的知识。这个方法随后被 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen1.5/" target="_blank" rel="noopener"
>Qwen1.5&lt;/a>, &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2/" target="_blank" rel="noopener"
>Qwen2&lt;/a> , &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5/" target="_blank" rel="noopener"
>Qwen2.5&lt;/a> 以及 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v3/" target="_blank" rel="noopener"
>DeepSeek-V3&lt;/a> 所采用。&lt;/p>
&lt;p>&lt;a class="link" href="https://maosong.website/p/notes-on-deepseekmoe/" target="_blank" rel="noopener"
>DeepSeekMoE&lt;/a> 给出的实验结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/moe-tutorial/DeepSeekMoE-ablation-experts.png"
width="1197"
height="552"
loading="lazy"
alt="DeepSeek-Moe shared experts ablation study"
class="gallery-image"
data-flex-grow="216"
data-flex-basis="520px"
>&lt;/p>
&lt;p>作者发现，当使用 shared experts 之后，模型在大部分 benchmark 上的表现都有所提升。&lt;/p>
&lt;p>&lt;a class="link" href="https://maosong.website/p/notes-on-olmoe/" target="_blank" rel="noopener"
>olmoe&lt;/a> 在 32 个专家下进行了实验，比较了 4 个激活专家和 3 个激活专家 +1 个共享专家两种设置的表现，结果如下：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/moe-tutorial/olmoe-shared-experts.png"
width="1156"
height="477"
loading="lazy"
alt="Olmoe shared experts performance"
class="gallery-image"
data-flex-grow="242"
data-flex-basis="581px"
>&lt;/p>
&lt;p>作者认为，加入 shared experts 之后，组合的可能性有所减少，这会降低模型的泛化性。因此，在 olmoe 中，作者没有使用 shared experts.&lt;/p>
&lt;blockquote>
&lt;p>虽然 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen1.5/" target="_blank" rel="noopener"
>Qwen1.5&lt;/a>, &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2/" target="_blank" rel="noopener"
>Qwen2&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5/" target="_blank" rel="noopener"
>Qwen2.5&lt;/a> 都使用了 shared experts, 但是后续的 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a> 中却并没有使用。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a class="link" href="https://maosong.website/p/notes-on-ling-mini-beta/" target="_blank" rel="noopener"
>Ling-mini-beta&lt;/a> 通过实验得出的结论为，shared expert 应该是一个非零的尽可能小的值，作者认为将 shared expert 设置为 1 是一个比较合理的选择。&lt;/p>
&lt;h3 id="activation-function">&lt;a href="#activation-function" class="header-anchor">&lt;/a>Activation Function
&lt;/h3>&lt;p>一般来说，在选取 top-K 专家时，我们会对 gating layer 的输出进行归一化，通常我们会使用 softmax function:&lt;/p>
$$
G(x) = \mathrm{softmax}(W_gx + b)\in\mathbb{R}^N
$$&lt;p>但是，在 &lt;a class="link" href="https://maosong.website/p/notes-on-loss-free-balancing/" target="_blank" rel="noopener"
>Loss-Free Balancing&lt;/a> 中，作者通过实验发现，使用 sigmoid 作为激活函数效果更好，即&lt;/p>
$$
G(x) = \mathrm{sigmoid}(W_gx + b)\in\mathbb{R}^N
$$&lt;p>其对应的实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/moe-tutorial/loss-free-ablation-gating-function.png"
width="917"
height="573"
loading="lazy"
alt="Ablation study on gating function"
class="gallery-image"
data-flex-grow="160"
data-flex-basis="384px"
>&lt;/p>
&lt;p>实验结果显示，sigmoid function 对于超参数更加 robust, 且表现也更好一些。 下面是一些使用不同激活函数的模型例子&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Activation function&lt;/th>
&lt;th>Models&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>softmax&lt;/td>
&lt;td>Step 3, Kimi-K2, gpt-oss-120B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>sigmoid&lt;/td>
&lt;td>GLM-4.5, dots.llm1, DeepSeek-V3&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="routing-z-loss">&lt;a href="#routing-z-loss" class="header-anchor">&lt;/a>Routing Z-loss
&lt;/h3>&lt;p>Routing Z-loss 由 &lt;a class="link" href="https://maosong.website/p/st-moe/" target="_blank" rel="noopener"
>ST-MoE&lt;/a> 提出， &lt;a class="link" href="https://maosong.website/p/switch-transformer/" target="_blank" rel="noopener"
>Switch Transformer&lt;/a> 发现在 gating layer 中使用 &lt;code>float32&lt;/code> 精度可以提高训练稳定性，但是这还不够，因此 &lt;a class="link" href="https://maosong.website/p/st-moe/" target="_blank" rel="noopener"
>ST-MoE&lt;/a> 使用了如下的 router Z-loss:&lt;/p>
$$
\mathcal{L}_z(x) = \frac1B\sum_{i=1}^B\left(\log\sum_{j=1}^N e^{x_j^{(i)}}\right)^2
$$&lt;p>其中 $B$ 是 batch size ，$x_j^{(i)}=[W_gx_i+b]_j$ 代表了第 $j$ 个专家对 $i$ 个 token 的激活 logits. &lt;a class="link" href="https://maosong.website/p/notes-on-olmoe/" target="_blank" rel="noopener"
>olmoe&lt;/a> 实验验证结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/moe-tutorial/olmoe-routing-z-loss.png"
width="1162"
height="419"
loading="lazy"
alt="Ablation study on Routing Z-loss"
class="gallery-image"
data-flex-grow="277"
data-flex-basis="665px"
>&lt;/p>
&lt;p>可以看到，加入 router Z-loss 之后，模型训练的稳定性有所提升，因此 Olmoe 采取了这个改进，但是后续的 MoE 模型使用 Z-loss 较少，个人猜测原因是 &lt;a class="link" href="https://maosong.website/p/notes-on-loss-free-balancing/" target="_blank" rel="noopener"
>Loss-Free Balancing&lt;/a> 中提出的加入额外的 loss 会影响 nex-token prediction loss&lt;/p>
&lt;h3 id="routing-strategy">&lt;a href="#routing-strategy" class="header-anchor">&lt;/a>Routing Strategy
&lt;/h3>&lt;p>routing 策略直接决定了 MoE 模型的有效性。在为专家分配 token 的时候，我们有如下方式：&lt;/p>
&lt;ol>
&lt;li>为每个 token 选取若干个专家&lt;/li>
&lt;li>为每个专家选取若个个 token&lt;/li>
&lt;li>动态分配 token 与专家之间的关系&lt;/li>
&lt;/ol>
&lt;p>三种选择方式如下图所示，图源 &lt;a class="link" href="https://arxiv.org/pdf/2209.01667" target="_blank" rel="noopener"
>MoE survey&lt;/a>&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/moe-tutorial/MoE_routing.png"
width="1070"
height="458"
loading="lazy"
class="gallery-image"
data-flex-grow="233"
data-flex-basis="560px"
>&lt;/p>
&lt;h4 id="expert-choice">&lt;a href="#expert-choice" class="header-anchor">&lt;/a>Expert Choice
&lt;/h4>&lt;p>每个专家选取 top-k 的 token，此时每个专家处理的 token 个数是相同的，这个方法的好处是自带 load balance。缺点是自回归生成的方式没有完整序列长度的信息，从而导致 token dropping，也就是某些 token 不会被任何专家处理，某些 token 会被多个专家处理。&lt;/p>
&lt;p>目前采用这个策略的有 OpenMoE-2， 核心思想是 dLLM 的输出长度固定，expert choice 策略更有效&lt;/p>
&lt;h4 id="token-choice">&lt;a href="#token-choice" class="header-anchor">&lt;/a>Token Choice
&lt;/h4>&lt;p>每个 token 选取 top-k 的专家，好处是每个 token 都会被处理，缺点是容易导致负载不均衡。因此，一般需要加上负载均衡或者 token dropping 策略来提高负载均衡&lt;/p>
&lt;p>&lt;strong>Capacity Factor&lt;/strong>
由 &lt;a class="link" href="https://maosong.website/p/switch-transformer/" target="_blank" rel="noopener"
>Switch Transformer&lt;/a> 提出，其定义为&lt;/p>
$$
\text{expert capacity} = \left(\frac{\text{tokens per batch}}{\text{number of experts}}\right) * \text{capacity factor}
$$&lt;p>设置 capacity factor 之后，当某个专家处理的 token 个数超过 capacity 之后，概专家的计算就会直接跳过，退化为 residual connection. 后续 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v2/" target="_blank" rel="noopener"
>DeepSeek-V2&lt;/a> 也采用了这种策略，但是 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v3/" target="_blank" rel="noopener"
>DeepSeek-V3&lt;/a> 弃用&lt;/p>
&lt;p>&lt;strong>Load balancing Loss&lt;/strong>
在训练目标中加入负载均衡损失，要求每个专家处理的 token 个数的分布尽可能均匀。&lt;/p>
&lt;p>这部分具体见 &lt;a class="link" href="https://maosong.website/p/load-balancing-tutorial/" target="_blank" rel="noopener"
>Load Balancing loss&lt;/a>&lt;/p>
&lt;h4 id="global-choice">&lt;a href="#global-choice" class="header-anchor">&lt;/a>Global Choice
&lt;/h4>&lt;p>全局分配决定 token 和专家的匹配关系，后续 Qwen 提出了 &lt;a class="link" href="https://maosong.website/p/notes-on-global-batch-load-balancing/" target="_blank" rel="noopener"
>Global-batch load balancing&lt;/a> 使用了这种方式来提高专家的特化程度&lt;/p>
&lt;h4 id="dynamic-routing">&lt;a href="#dynamic-routing" class="header-anchor">&lt;/a>Dynamic Routing
&lt;/h4>&lt;p>根据输入 token 的难度动态决定激活专家的个数。LongCat 使用了一个 Phantom expert 的方法来实现根据 token 的难度动态分配专家。具体来说，除了 $N$ 个专家之外，MoE 还包括 $Z$ 个 zero-computation expert (现在一共有 $N+Z$ 个专家参与计算), 其计算方式如下&lt;/p>
$$
\begin{aligned}
y &amp;= \sum_{i=1}^{K}\mathrm{Normalize}(G_{e_i}) E_{e_i}(x), e_i \in \{i\mid G_i(x)\in \mathrm{TopK}(G_i(x), K)\}\\
E_{e_i}(x) &amp;= \begin{cases}
FFN_{e_i}(x), &amp; \text{ if }1\leq i\leq N\\
x, &amp; \text{ otherwise }\\
\end{cases}
\end{aligned}
$$&lt;blockquote>
&lt;p>注：LongCat 还是用了 &lt;a class="link" href="https://maosong.website/p/notes-on-loss-free-balancing/" target="_blank" rel="noopener"
>Loss-Free Balancing&lt;/a>, 我们这里省略掉了。&lt;/p>
&lt;/blockquote>
&lt;h4 id="overview">&lt;a href="#overview" class="header-anchor">&lt;/a>Overview
&lt;/h4>&lt;p>现在几乎所有的模型都选择方式 1，即每个 token 选取 top-k 的专家。 &lt;a class="link" href="https://maosong.website/p/notes-on-olmoe/" target="_blank" rel="noopener"
>olmoe&lt;/a> 对比了以下方式 1 和方式 2 的表现，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/moe-tutorial/olmoe-routing-strategy.png"
width="1157"
height="450"
loading="lazy"
alt="MoE routing strategy EC v.s. TC"
class="gallery-image"
data-flex-grow="257"
data-flex-basis="617px"
>&lt;/p>
&lt;p>可以看到，加入 load balancing loss 之后，相比于 Expert Choice, Token Choice 的表现更好。但是，expert choice 更加高效，作者认为 expert choice 更适用于多模态，因为丢掉 noise image tokens 对 text token 影响会比较小。因此，在 olmoe 中，作者使用 token choice 作为 routing 策略&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/moe-tutorial/celebras-routing-landscape.png"
width="1102"
height="762"
loading="lazy"
alt="Routing strategy overview (celebras)"
class="gallery-image"
data-flex-grow="144"
data-flex-basis="347px"
>&lt;/p>
&lt;h3 id="upcycling">&lt;a href="#upcycling" class="header-anchor">&lt;/a>Upcycling
&lt;/h3>&lt;p>upsampling 是一个将 dense model 转化为 MoEmodel 的方法，具体做法就是我们复制 dense model 中的 FFN layer 得到对应 MoE layer 中的 Expert，然后我们再结合 router 训练，这样可以提高整体的训练效率。相关模型有 MiniCPM, &lt;a class="link" href="https://maosong.website/p/notes-on-qwen1.5/" target="_blank" rel="noopener"
>Qwen1.5&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/mixstral-8x7b/" target="_blank" rel="noopener"
>Mixtral MoE&lt;/a> (疑似)&lt;/p>
&lt;p>实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/moe-tutorial/olmoe-sparse-upcycling.png"
width="1440"
height="680"
loading="lazy"
class="gallery-image"
data-flex-grow="211"
data-flex-basis="508px"
>&lt;/p>
&lt;p>从已有的结果来看，MoE 模型会被 dense 模型的一些超参数所限制，且训练不是很稳定。因此，现在一般不采用这种方法&lt;/p>
&lt;h2 id="analysis-on-moe">&lt;a href="#analysis-on-moe" class="header-anchor">&lt;/a>Analysis on MoE
&lt;/h2>&lt;h3 id="specialization-of-experts">&lt;a href="#specialization-of-experts" class="header-anchor">&lt;/a>Specialization of Experts
&lt;/h3>&lt;p>OpenMoE 分析了 MoE 模型的特化程度，其结论如下&lt;/p>
&lt;ol>
&lt;li>作者发现，大部分专家对于不同的 domain 没有出现 specialization 情况，对于 math domain, specialization 现象比较明显，作者认为这是因为 math domain 包含更多的 special tokens&lt;/li>
&lt;li>对于不同的语言，有部分专家出现 specialization 现象&lt;/li>
&lt;li>部分专家对于 position ID 有 specialization 现象，并且连续的 token 更偏好相同的专家&lt;/li>
&lt;li>作者还发现，部分专家对于 Token ID 有 specialization 现象，作者将这种现象称为 &lt;strong>Context-independent Specialization&lt;/strong>.&lt;/li>
&lt;li>专家还会对语义相似的 token 进行聚类，并且这种聚类在训练早期就已经发生，作者认为其原因在于重新分配 token 会增加最终的 loss&lt;/li>
&lt;li>对于 token dropping, 作者发现越靠后的 token, 其被 drop 的概率比例也越高。并且对于指令跟随数据，更多的 token 都会被丢掉，因此作者认为指令跟随数据是 MoE 模型的一种 OOD 数据&lt;/li>
&lt;/ol>
&lt;h3 id="saturation-of-experts">&lt;a href="#saturation-of-experts" class="header-anchor">&lt;/a>Saturation of Experts
&lt;/h3>&lt;p>&lt;a class="link" href="https://maosong.website/p/notes-on-olmoe/" target="_blank" rel="noopener"
>olmoe&lt;/a> 探究了训练过程中激活的专家和训练结束后激活的专家的匹配程度，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/moe-tutorial/olmoe-router-saturation.png"
width="1149"
height="393"
loading="lazy"
class="gallery-image"
data-flex-grow="292"
data-flex-basis="701px"
>&lt;/p>
&lt;p>实验结果说明，训练 $1\%$ 的数据之后，就有 $40\%$ 的 routing 和训练完毕的 routing 一致，当训练 $40\%$ 的数据之后，这个比例提升到了 $80\%$. 作者认为，这是专家特化的结果，初始的 routing 如果改变的话会带来表现下降，因此模型倾向于使用固定的专家处理特定的 token&lt;/p>
&lt;p>作者还发现，later layers 比 early layers 饱和更快，early layer, 特别是 layer 0, 饱和的非常慢。作者认为，这是 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseekmoe/" target="_blank" rel="noopener"
>DeepSeekMoE&lt;/a> 放弃在第一层使用 MoE layer 的原因，因为 load balancing loss 收敛更慢。后续 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v2/" target="_blank" rel="noopener"
>DeepSeek-V2&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v3/" target="_blank" rel="noopener"
>DeepSeek-V3&lt;/a> 均在 early layer 上使用 dense layer 替换掉了 MoE layer&lt;/p>
&lt;h2 id="optimization">&lt;a href="#optimization" class="header-anchor">&lt;/a>Optimization
&lt;/h2>&lt;p>MoE 模型的优势在于表现好，但是模型参数往往非常大，为了方便使用，我们需要对训练好的 MoE 模型进行优化，目前主要有蒸馏，专家剪枝/合并以及量化等优化方法&lt;/p>
&lt;p>蒸馏是一个将大模型能力传递给小模型的做法，目前已有的包括：&lt;/p>
&lt;ol>
&lt;li>&lt;a class="link" href="https://maosong.website/p/switch-transformer/" target="_blank" rel="noopener"
>Switch Transformer&lt;/a> 通过蒸馏，在仅使用 $1/20$ 参数的情况下，保留了稀疏教师模型 $30\%$ 的表现&lt;/li>
&lt;li>&lt;a class="link" href="https://maosong.website/p/notes-on-gemini2.5/" target="_blank" rel="noopener"
>Gemini2.5&lt;/a> 通过蒸馏 Gemini2.5 Pro 得到 Gemini2.5 Flash&lt;/li>
&lt;li>&lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-r1/" target="_blank" rel="noopener"
>DeepSeek-R1&lt;/a> 通过蒸馏来提升小语言模型的 reasoning 能力&lt;/li>
&lt;li>&lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a> 对于小语言模型的训练使用了 off-policy distillation 和 on-policy distillation 来训练小语言模型&lt;/li>
&lt;/ol>
&lt;h2 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h2>&lt;p>我们这里展示基于 &lt;a class="link" href="https://maosong.website/p/notes-on-olmoe/" target="_blank" rel="noopener"
>olmoe&lt;/a> 的代码，代码如下&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;span class="lnt">49
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">OlmoeSparseMoeBlock&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="fm">__init__&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_experts&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_experts&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">top_k&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_experts_per_tok&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">norm_topk_prob&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">norm_topk_prob&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">gate&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_experts&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">experts&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ModuleList&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">OlmoeMLP&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_experts&lt;/span>&lt;span class="p">)])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">batch_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sequence_length&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_dim&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># hidden_states: (batch * sequence_length, hidden_dim)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># router_logits: (batch * sequence_length, n_experts)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">router_logits&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">gate&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">routing_weights&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">F&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">softmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">router_logits&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># routing_weights: (batch * sequence_length, top_k)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># selected_experts: indices of top_k experts&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">routing_weights&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">selected_experts&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">topk&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">routing_weights&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">top_k&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">norm_topk_prob&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">routing_weights&lt;/span> &lt;span class="o">/=&lt;/span> &lt;span class="n">routing_weights&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">keepdim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># we cast back to the input dtype&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">routing_weights&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">routing_weights&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dtype&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">final_hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zeros&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">(&lt;/span>&lt;span class="n">batch_size&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">sequence_length&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_dim&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dtype&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">device&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">device&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># One hot encode the selected experts to create an expert mask&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># this will be used to easily index which expert is going to be selected&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">expert_mask&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">functional&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">one_hot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">selected_experts&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_classes&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_experts&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">permute&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Loop over all available experts in the model and perform the computation on each expert&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">expert_idx&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_experts&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">expert_layer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">experts&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">expert_idx&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">idx&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">top_x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">where&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">expert_mask&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">expert_idx&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Index the correct hidden states and compute the expert hidden state for&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># the current expert. We need to make sure to multiply the output hidden&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># states by `routing_weights` on the corresponding tokens (top-1 and top-2)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">current_state&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">top_x&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reshape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">current_hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">expert_layer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">current_state&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">routing_weights&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">top_x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">idx&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># However `index_add_` only support torch tensors for indexing so we&amp;#39;ll use&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># the `top_x` tensor here.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">final_hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">index_add_&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">top_x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">current_hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dtype&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">final_hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">final_hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reshape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">batch_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sequence_length&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">final_hidden_states&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">router_logits&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="infra">&lt;a href="#infra" class="header-anchor">&lt;/a>Infra
&lt;/h2>&lt;p>针对 MoE 模型的 infra 主要涉及 expert parallelism (EP), EP 将 MoE layer 的计算分为了三个阶段：&lt;/p>
&lt;ol>
&lt;li>all-to-all dispatch: 基于 gating layer 的结果，将 token 通信传输到对应专家所在的 GPU 上&lt;/li>
&lt;li>computation: 执行计算，即 $E_i(x)$.&lt;/li>
&lt;li>all-to-all combine: 收集专家计算的结果，即 $y = \sum_{i=1}^K\mathrm{Normalize}(G_{e_i}(x)) E_{e_i}(x)$.&lt;/li>
&lt;/ol>
&lt;p>其框架图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/moe-tutorial/MoE-EP-pipeline.png"
width="1131"
height="832"
loading="lazy"
alt="pipeline of Expert Parallelism (EP)"
class="gallery-image"
data-flex-grow="135"
data-flex-basis="326px"
>&lt;/p>
&lt;h2 id="challenges">&lt;a href="#challenges" class="header-anchor">&lt;/a>Challenges
&lt;/h2>&lt;ul>
&lt;li>构建针对 MoE 模型的 infra 比较困难，相关工作有 DeepSeek 提出的 DeepEP.&lt;/li>
&lt;li>训练不稳定，特别是负载均衡。负载均衡做不好容易导致模型崩塌。&lt;/li>
&lt;/ul>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>在本文中，我们系统性回顾了 MoE 的相关概念，MoE 模型已经是现在大语言模型的主流架构，比如商业模型 &lt;a class="link" href="https://maosong.website/p/notes-on-gemini2.5/" target="_blank" rel="noopener"
>Gemini2.5&lt;/a>, 开源领先的模型 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v3/" target="_blank" rel="noopener"
>DeepSeek-V3&lt;/a> , &lt;a class="link" href="https://maosong.website/p/notes-on-llama4-blog/" target="_blank" rel="noopener"
>LLaMA4&lt;/a> 以及 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a> 等都采用了 MoE 的架构，如何进一步优化 MoE 的训练方式是当前研究的一个重点方向。&lt;/p>
&lt;h2 id="appendix">&lt;a href="#appendix" class="header-anchor">&lt;/a>Appendix
&lt;/h2>&lt;p>MoE model information&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Year&lt;/th>
&lt;th>total parameters&lt;/th>
&lt;th>activated parameters&lt;/th>
&lt;th>shared expert&lt;/th>
&lt;th>Routed experts&lt;/th>
&lt;th>activated experts&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>ST-MoE&lt;/td>
&lt;td>2022/4&lt;/td>
&lt;td>269B&lt;/td>
&lt;td>32B&lt;/td>
&lt;td>0&lt;/td>
&lt;td>64&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Mistral&lt;/td>
&lt;td>2024/1&lt;/td>
&lt;td>47B&lt;/td>
&lt;td>13B&lt;/td>
&lt;td>0&lt;/td>
&lt;td>8&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DeepSeek-MoE&lt;/td>
&lt;td>2024/1&lt;/td>
&lt;td>145B&lt;/td>
&lt;td>22B&lt;/td>
&lt;td>4&lt;/td>
&lt;td>128&lt;/td>
&lt;td>12&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DeepSeek-V2&lt;/td>
&lt;td>2024/5&lt;/td>
&lt;td>236B&lt;/td>
&lt;td>21B&lt;/td>
&lt;td>2&lt;/td>
&lt;td>160&lt;/td>
&lt;td>6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LLaMA4&lt;/td>
&lt;td>2025/4&lt;/td>
&lt;td>400B&lt;/td>
&lt;td>17B&lt;/td>
&lt;td>1&lt;/td>
&lt;td>128&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DeepSeek-V3&lt;/td>
&lt;td>2024/12&lt;/td>
&lt;td>671B&lt;/td>
&lt;td>37B&lt;/td>
&lt;td>1&lt;/td>
&lt;td>256&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen3&lt;/td>
&lt;td>2025/5&lt;/td>
&lt;td>235B&lt;/td>
&lt;td>22B&lt;/td>
&lt;td>0&lt;/td>
&lt;td>128&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>dots.llm1&lt;/td>
&lt;td>2025/6&lt;/td>
&lt;td>142B&lt;/td>
&lt;td>14B&lt;/td>
&lt;td>2&lt;/td>
&lt;td>128&lt;/td>
&lt;td>6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Step 3&lt;/td>
&lt;td>2025/7&lt;/td>
&lt;td>316B&lt;/td>
&lt;td>38B&lt;/td>
&lt;td>1&lt;/td>
&lt;td>48&lt;/td>
&lt;td>3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Kimi-K2&lt;/td>
&lt;td>2025/7&lt;/td>
&lt;td>1043B&lt;/td>
&lt;td>32B&lt;/td>
&lt;td>8&lt;/td>
&lt;td>384&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GLM-4.5&lt;/td>
&lt;td>2025/8&lt;/td>
&lt;td>355B&lt;/td>
&lt;td>32B&lt;/td>
&lt;td>1&lt;/td>
&lt;td>160&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>gpt-oss&lt;/td>
&lt;td>2025/8&lt;/td>
&lt;td>116B&lt;/td>
&lt;td>5B&lt;/td>
&lt;td>0&lt;/td>
&lt;td>128&lt;/td>
&lt;td>4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LongCat&lt;/td>
&lt;td>2025/9&lt;/td>
&lt;td>560B&lt;/td>
&lt;td>27B*&lt;/td>
&lt;td>0&lt;/td>
&lt;td>768*&lt;/td>
&lt;td>12&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ling-1T&lt;/td>
&lt;td>2025/10&lt;/td>
&lt;td>1000B&lt;/td>
&lt;td>51B&lt;/td>
&lt;td>1&lt;/td>
&lt;td>256&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;blockquote>
&lt;p>Remark
LongCat 采用了动态激活的方式，因此其结果有一个浮动范围。&lt;/p>
&lt;/blockquote>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2101.03961" target="_blank" rel="noopener"
>Switch Transformer&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2209.01667" target="_blank" rel="noopener"
>MoE Survey&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://openreview.net/forum?id=xXTkbTBmqq" target="_blank" rel="noopener"
>olmoe&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2006.16668" target="_blank" rel="noopener"
>GShard&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.cnblogs.com/rossiXYZ/p/18800825" target="_blank" rel="noopener"
>blog&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2501.16352v1" target="_blank" rel="noopener"
>MoE a big data perspective&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://huggingface.co/blog/moe" target="_blank" rel="noopener"
>Mixture of Experts Explained&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.cerebras.ai/blog/moe-guide-why-moe" target="_blank" rel="noopener"
>MoE Fundamentals: Sparse Models Are the Future&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.cerebras.ai/blog/moe-guide-router" target="_blank" rel="noopener"
>MoE router&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2507.17702" target="_blank" rel="noopener"
>Ling-Mini-beta&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Ling-mini-beta</title><link>https://maosong.website/p/notes-on-ling-mini-beta/</link><pubDate>Sat, 13 Dec 2025 15:58:51 +0800</pubDate><guid>https://maosong.website/p/notes-on-ling-mini-beta/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>目前已经有了针对 dense LLM 的 scaling law, 如 &lt;a class="link" href="https://maosong.website/p/kaplan-scaling-law/" target="_blank" rel="noopener"
>Kaplan scaling law&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/chinchilla-scaling-law/" target="_blank" rel="noopener"
>Chinchilla scaling law&lt;/a>.&lt;/p>
&lt;p>但是，对于 MoE 模型，目前还缺乏一个比较系统的 scaling law.&lt;/p>
&lt;p>为了解决这个问题，作者提出了 efficiency leverage (EL), 用于衡量 MoE 模型的效率，其定义为&lt;/p>
$$
EL(\mathcal{X}_{\mathrm{MoE}}\mid \mathrm{Dense})=\frac{C_{\mathrm{Dense}}}{C_{{\mathrm{MoE}}}}
$$&lt;p>其中 $C_{\mathrm{Dense}}, C_{{\mathrm{MoE}}}$ 分别代表了训练模型所需要的算力。EL 衡量了 moe 模型达到对应 dense 模型表现所需要的算力，EL 值越大，说明 MoE 模型效率越高。EL 的可视化如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-ling-mini-beta/EL-definition.png"
width="555"
height="459"
loading="lazy"
alt="Definition of EL"
class="gallery-image"
data-flex-grow="120"
data-flex-basis="290px"
>&lt;/p>
&lt;p>作者通过训练多个模型，探究了 MoE 架构与 EL 之间的关系。作者发现，MoE 模型的表现主要与专家激活比例以及算力相关。基于 scaling law, 作者训练了 Ling-mini-beta, 一个 17.5B-A0.85B 的 MoE 模型，其表现超过了 6.1B dense 模型的表现。&lt;/p>
&lt;h2 id="preliminary">&lt;a href="#preliminary" class="header-anchor">&lt;/a>Preliminary
&lt;/h2>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Notation&lt;/th>
&lt;th>description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$N$&lt;/td>
&lt;td>total parameters&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$N_a$&lt;/td>
&lt;td>active parameters&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$E$&lt;/td>
&lt;td>routed experts&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$E_a$&lt;/td>
&lt;td>activated experts&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$E_s$&lt;/td>
&lt;td>shared experts&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>作者定义 activation ratio 如下:&lt;/p>
$$
A = \frac{E_a+E_s}{E+E_s}
$$&lt;p>定义 sharing ratio 如下&lt;/p>
$$
S=\frac{E_s}{E_a+E_s}
$$&lt;p>定义 expert granularity 如下&lt;/p>
$$
G = \frac{d_{\mathrm{model}}}{d_{\mathrm{Expert}}}
$$&lt;p>与 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-llm/" target="_blank" rel="noopener"
>DeepSeek-LLM&lt;/a> 一样，作者使用 $C=MD$ 来表示算力，non-embedding FLOPs $M$ 和训练 token 数 $D$ 之间的关系。&lt;/p>
&lt;h3 id="hyper-parameters">&lt;a href="#hyper-parameters" class="header-anchor">&lt;/a>Hyper Parameters
&lt;/h3>&lt;p>作者首先探究了针对 MoE 模型的超参数配置，最终你和出来的结果如下图&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-ling-mini-beta/EL-hyperparameter-scaling-law.png"
width="1377"
height="586"
loading="lazy"
alt="Scaling laws for optimal hyperparameters"
class="gallery-image"
data-flex-grow="234"
data-flex-basis="563px"
>&lt;/p>
&lt;p>实验结果说明，相比于 dense model, MoE model 需要更大的 batch size.&lt;/p>
&lt;p>作者基于这个 scaling law 进行了验证，结果说明这个 scaling law 比较准确。&lt;/p>
&lt;h3 id="parameters-and-dataset-size">&lt;a href="#parameters-and-dataset-size" class="header-anchor">&lt;/a>Parameters and Dataset Size
&lt;/h3>&lt;p>接下来作者探究了对于模型参数量以及训练 token 个数之间的 scaling law, 求解的问题如下&lt;/p>
$$
(M^{opt}, D^{opt}) = \arg\min_{M,D}\mathcal{L}(M,D;C,A,G,S)\quad s.t.\ C=MD
$$&lt;p>实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-ling-mini-beta/EL-model-data-scaling.png"
width="1374"
height="537"
loading="lazy"
alt="Scaling laws for optimal model scale and data size"
class="gallery-image"
data-flex-grow="255"
data-flex-basis="614px"
>&lt;/p>
&lt;p>结果说明，不同架构对应的系数接近 $0.5$, 说明我们应该将算力均衡分配到数据和 model size 上。领一方面，MoE 模型可以通过使用更多的数据来达到更优的表现。&lt;/p>
&lt;h2 id="efficiency-leverage">&lt;a href="#efficiency-leverage" class="header-anchor">&lt;/a>Efficiency Leverage
&lt;/h2>&lt;p>作者将 Efficiency Leverage (EL) 定义为给定算力 $C_{target}$ 和一个 MoE 模型 $\mathcal{X}_{MoE}$, 对应 dense 模型达到 $\mathcal{X}_{MoE}$ 相同的表现所需要的算力 $C_{dense}$, 即&lt;/p>
$$
\begin{aligned}
&amp;EL(\mathcal{X}_{\mathrm{MoE}}\mid \mathrm{Dense};C_{target})=\frac{C_{\mathrm{Dense}}}{C_{{\mathrm{MoE}}}}\\
s.t.\ &amp; |\mathcal{L}(C_{MoE}, \mathcal{X}_{\mathrm{MoE}})-\mathcal{L}(C_{dense}, \mathcal{X}_{\mathrm{dense}})|\leq \epsilon (\epsilon\to 0)
\end{aligned}
$$&lt;p>EL 值越高说明 MoE 模型越有效。为了公平起见，dense 模型和 MoE 模型的架构参数基本相同，作者只改变 $d_{model}, d_{ffn}, d_{expert}$ 以及 $n_{layer}$.&lt;/p>
&lt;p>接下来，作者就探究了给定算力的情况下，最优的 MoE 配置，即&lt;/p>
$$
(A^{opt}, G^{opt}, S^{opt}) = \arg\min_{(A,G,S)\in\mathcal{X}_{\mathrm{MoE}}}EL(\mathcal{X}_{\mathrm{MoE}}\mid \mathrm{Dense};C)
$$&lt;h2 id="scaling-law">&lt;a href="#scaling-law" class="header-anchor">&lt;/a>Scaling Law
&lt;/h2>&lt;p>首先，坐着探究了最优的 activation ratio $A$, 即&lt;/p>
$$
A^{opt} = \arg\min_{A}\mathcal{L}(A;C,M,G,S)
$$&lt;p>拟合的结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-ling-mini-beta/EL-impact-of-A.png"
width="1380"
height="487"
loading="lazy"
alt="Impact of the activation ratio"
class="gallery-image"
data-flex-grow="283"
data-flex-basis="680px"
>&lt;/p>
&lt;p>实验结果表明：&lt;/p>
&lt;ol>
&lt;li>模型表现随激活比例降低 er 提高&lt;/li>
&lt;li>更系数的模型对于算力的提升其效率也提升更快&lt;/li>
&lt;/ol>
&lt;p>然后，作者探究了最优的 granularity ratio, 即&lt;/p>
$$
G^{opt} = \arg\min_{G}\mathcal{L}(G;C,M,A,S)
$$&lt;p>拟合的结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-ling-mini-beta/EL-impact-of-G.png"
width="1376"
height="497"
loading="lazy"
alt="Impact of granularity"
class="gallery-image"
data-flex-grow="276"
data-flex-basis="664px"
>&lt;/p>
&lt;p>可以看到，无限制提升 granularity 并不会提高模型的表现。并且，不同的算力对应的最优 granularity 处于一个固定的范围&lt;/p>
&lt;p>接下来，作者探究了最优的 shared expert ratio, 即&lt;/p>
$$
S^{opt} = \arg\min_{S}\mathcal{L}(S;C,M,A,G)
$$&lt;p>拟合的结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-ling-mini-beta/EL-impact-of-S.png"
width="1376"
height="487"
loading="lazy"
class="gallery-image"
data-flex-grow="282"
data-flex-basis="678px"
>&lt;/p>
&lt;p>结果说明 shared expert 的比例也不是越多越好，其存在最优值。并且给定算力的情况下，非零最小值的 shared expert 表现最好。因此作者认为，一个 shared expert 的效果最好。&lt;/p>
&lt;p>作者还探究了其他可能的因素，结论如下：&lt;/p>
&lt;ol>
&lt;li>与 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v3/" target="_blank" rel="noopener"
>DeepSeek-V3&lt;/a> 一样，将 early layer 替换为 dense layer 可以避免 routing imbalance, 并且不会损害模型的表现&lt;/li>
&lt;li>attention 应该占 $30\%\sim40\%$ 左右的算力才能保证模型的表现和效率。进一步提升 attention 的算力占比虽然会提升表现但是会降低推理效率。&lt;/li>
&lt;/ol>
&lt;p>通过前面的发现，作者将 shared expert 设置为 1 个，然后探究 EL 与 activation ratio $A$, granularity $G$, FLOPs $C$ 之间的关系。&lt;/p>
&lt;p>首先，作者分别假设 $EL$ 与 $A$, $G$, $C$ 之间存在如下关系：&lt;/p>
$$
\begin{aligned}
\log EL_{C,G}(\hat{A}) &amp;= a_A\log\hat{A}, \text{ where }\frac{1}{\hat{A}}=\frac{1}{A+(1/A_{start}-1/A_{\max})^{-1}}+\frac{1}{A_{\max}}\\
\log EL_{C,A}(\hat{G}) &amp;= a_G+b_G(\log G(\log G+c_G))\\
\log EL_{A,G}(C) &amp;= a_C\log C+c_C
\end{aligned}
$$&lt;p>拟合的结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-ling-mini-beta/EL-scaling-A-G-C.png"
width="1380"
height="427"
loading="lazy"
alt="Scaling behavior of EL"
class="gallery-image"
data-flex-grow="323"
data-flex-basis="775px"
>&lt;/p>
&lt;p>结果显示：&lt;/p>
&lt;ol>
&lt;li>提升算力以及降低 activation ratio 都可以提高 EL&lt;/li>
&lt;li>granularity 对 EL 的影响在不同算力的情况下都是一致的&lt;/li>
&lt;li>对于 MoE 模型，提升算力可以提高 EL&lt;/li>
&lt;/ol>
&lt;p>作者的结论为，activation ratio 是影响 MoE EL 的核心因素。并且随着算力的提升，MoE EL 会越来越明显。&lt;/p>
&lt;p>作者因此构建了一个统一的公式来统一三个因素&lt;/p>
$$
EL(A,G,C) = \hat{A}^{\alpha+\gamma(\log G)^2+\beta \log G}
$$&lt;p>其中 $\alpha=a+d\log C$ 代表了 EL 和 activation ratio 之间的关系。拟合出来的参数如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>$\alpha$&lt;/th>
&lt;th>$d$&lt;/th>
&lt;th>$\gamma$&lt;/th>
&lt;th>$\beta$&lt;/th>
&lt;th>$A_{start}$&lt;/th>
&lt;th>$A_{\max}$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>1.23&lt;/td>
&lt;td>-7.61e-2&lt;/td>
&lt;td>1.67e-2&lt;/td>
&lt;td>-1.17e-1&lt;/td>
&lt;td>1.63e-2&lt;/td>
&lt;td>5.28e+16&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>基于这个结果，作者发现在 1e22 FLOPs 的算力下，一个 activation ratio 为 $3.1\%$, granularity 为 $12$ 的 MoE 模型，其 EL 为 $7$.&lt;/p>
&lt;h2 id="ling-mini-beta">&lt;a href="#ling-mini-beta" class="header-anchor">&lt;/a>Ling-mini-beta
&lt;/h2>&lt;p>基于上一节的发现，作者构建了 Ling-mini-beta, 一个 17.5B 总参数，激活参数为 0.85B 的 MoE 模型。训练使用了 1T token, 模型参数如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>$n_{\text{layers}}$&lt;/th>
&lt;th>$d_{\text{model}}$&lt;/th>
&lt;th>$d_{\text{ffn}}$&lt;/th>
&lt;th>$d_{\text{expert}}$&lt;/th>
&lt;th>$n_{\text{heads}}$&lt;/th>
&lt;th>$n_{\text{kv\_head}}$&lt;/th>
&lt;th>$E$&lt;/th>
&lt;th>$E_a$&lt;/th>
&lt;th>$E_s$&lt;/th>
&lt;th>$N$&lt;/th>
&lt;th>$N_a$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Dense 6.1B&lt;/td>
&lt;td>28&lt;/td>
&lt;td>4096&lt;/td>
&lt;td>14336&lt;/td>
&lt;td>-&lt;/td>
&lt;td>32&lt;/td>
&lt;td>8&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>6.11B&lt;/td>
&lt;td>6.11B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ling-mini-beta (A0.8B)&lt;/td>
&lt;td>20&lt;/td>
&lt;td>2048&lt;/td>
&lt;td>5120&lt;/td>
&lt;td>384&lt;/td>
&lt;td>16&lt;/td>
&lt;td>4&lt;/td>
&lt;td>384&lt;/td>
&lt;td>12&lt;/td>
&lt;td>1&lt;/td>
&lt;td>17.5B&lt;/td>
&lt;td>0.85B&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>训练的损失变化情况如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-ling-mini-beta/EL-training-dynamics.png"
width="867"
height="596"
loading="lazy"
alt="Dynamic of training loss"
class="gallery-image"
data-flex-grow="145"
data-flex-basis="349px"
>&lt;/p>
&lt;p>从图中我们可以看出，dense model 一开始的损失下降比较快，但是其最终表现不如 moe 模型。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>在本文中，作者提出了 Efficiency Leverage, 一个衡量 MoE 模型相对于 dense 模型计算效率的 metric, 作者构建了针对 MoE 模型的 scaling law. scaling 揭示了两个主要影响 MoE 模型效率的因素：算力与激活参数比例。基于 scaling law, 作者构建了 Ling-mini-beta, 一个 17B-A0.8B 的 MoE 模型，其效率超过了对应 dense 模型的 7 倍。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2507.17702" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Load Balancing tutorial</title><link>https://maosong.website/p/load-balancing-tutorial/</link><pubDate>Thu, 11 Dec 2025 16:10:08 +0800</pubDate><guid>https://maosong.website/p/load-balancing-tutorial/</guid><description>&lt;p>我们在本文中探讨关于 load balancing loss 的定义，性质和推广&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>我们在 &lt;a class="link" href="https://maosong.website/p/moe-tutorial/" target="_blank" rel="noopener"
>MoE tutorial&lt;/a> 中已经介绍了 MoE 模块，MoE 尽管可以在相同的算力下扩大模型的 size, 但是其问题在于训练时容易出现负载不均衡，也就是说只有少数几个专家被激活，其他专家处于闲置状态，从而导致模型性能下降&lt;/p>
&lt;p>为了解决这个问题，一个通用的做法是使用 load balancing loss. load balancing loss 可以有效实现负载均衡，让各个专家被激活的概率差不多。&lt;/p>
&lt;p>Load balancing loss 一共经历了如下几个阶段，发展路线如下图所示&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-gdscript3" data-lang="gdscript3">&lt;span class="line">&lt;span class="cl">&lt;span class="n">flowchart&lt;/span> &lt;span class="n">TD&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">C&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">Bengio&lt;/span> &lt;span class="n">et&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">at&lt;/span> &lt;span class="mi">2015&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">--&amp;gt;&lt;/span> &lt;span class="n">A&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">Shazeer&lt;/span> &lt;span class="n">et&lt;/span> &lt;span class="n">al&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2017&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">A&lt;/span> &lt;span class="o">--&amp;gt;&lt;/span> &lt;span class="n">B&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">GShard&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">B&lt;/span> &lt;span class="o">--&amp;gt;&lt;/span> &lt;span class="n">F&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">Switch&lt;/span> &lt;span class="n">Transformer&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">F&lt;/span> &lt;span class="o">--&amp;gt;&lt;/span> &lt;span class="n">D&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">Loss&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">Free&lt;/span> &lt;span class="n">Balancing&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">F&lt;/span> &lt;span class="o">--&amp;gt;&lt;/span> &lt;span class="n">E&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">Global&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">batch&lt;/span> &lt;span class="nb">load&lt;/span> &lt;span class="n">balancing&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="notation">&lt;a href="#notation" class="header-anchor">&lt;/a>Notation
&lt;/h3>&lt;p>我们假设输入的 batch size 为 $B$, 总专家个数为 $N$, 激活专家个数为 $K$, $G_i(x_j)$ 代表 gating layer 预测的第 $i$ 个专家对于 token $x_j$ 的重要性程度。&lt;/p>
&lt;h3 id="coefficient-of-variation">&lt;a href="#coefficient-of-variation" class="header-anchor">&lt;/a>Coefficient of Variation
&lt;/h3>&lt;p>第一个阶段的 load balancing loss 针对广义的 MoE 模型。&lt;/p>
&lt;p>&lt;strong>Bengio et.at 2015&lt;/strong>
作者提出了一个让 batch 里每个专家平均激活概率分布更均匀的损失函数，其定义如下所示&lt;/p>
$$
L_b = \sum_{i=1}^N\left\Vert \frac1B\sum_{j=1}^BG_i(x_j) - \frac1N\right\Vert_2
$$&lt;p>其主要目标是让每个专家的平均激活概率接近 $1/N$, 即均匀分布.&lt;/p>
&lt;p>&lt;strong>Noam et.al 2017&lt;/strong>
作者对 (Bengio et.at 2015) 进行了改进，提出了变异系数 $\mathrm{CV}(\cdot)$ 来保证负载均衡，其定义如下：&lt;/p>
$$
L_{importance} = \mathrm{CV}(\mathrm{Importance}(X))^2
$$&lt;p>其中&lt;/p>
$$
\mathrm{Importance}_i(X) = \sum_{j=1}^B G_i(x_j),\ \mathrm{CV}(X)= \frac{\mathrm{var}[X]}{\mathbb{E}[X]},\ i=1,\dots,N
$$&lt;p>注意到 $\mathrm{var}[X]\geq0$, 当且仅当 $X$ 为均匀分布时 $\mathrm{var}[X]=0$, 因此 important loss 可以让每个专家在一个 batch 中的激活的平均概率尽可能一致。&lt;/p>
&lt;p>但是平均概率一致不代表各个专家处理的 token 个数一致，比如一个专家可能处理比较少的 token, 但是每个 token 的权重都很大。因此，作者额外加入了 load balancing loss&lt;/p>
$$
\mathcal{L}_{\mathrm{load}} = \mathrm{CV}(\mathrm{Load}(X))^2
$$&lt;p>其中&lt;/p>
$$
\begin{aligned}
\mathrm{Load}_i(X) &amp;= \mathrm{soft\_estimation}\left(\sum_{j=1}^B\mathbb{1}(\text{token }j \text{ selects expert }i)\right)\\
&amp;=\mathrm{soft\_estimation}\left(\sum_{j=1}^B\mathbb{1}\{i\in\mathrm{TopK}(\{G_i(x_j)\}_{i=1}^{E}, K)\}\right),\ i=1,\dots,N
\end{aligned}
$$&lt;p>这里 $\mathrm{soft\_estimation}$ 是作者针对离散变量进行的一个光滑化处理，以方便反向传播。&lt;/p>
&lt;h3 id="gshard">&lt;a href="#gshard" class="header-anchor">&lt;/a>GShard
&lt;/h3>&lt;p>&lt;a class="link" href="https://maosong.website/p/gshard/" target="_blank" rel="noopener"
>GShard&lt;/a> 进一步对 (Noam et.al 2017) 提出的 loss 进行了简化，首先注意到 $\sum_{i=1}^N\mathrm{Load}_i(X)=BK$, 因此我们有&lt;/p>
$$
\begin{aligned}
\mathcal{L}_{\mathrm{load}} &amp;= \mathrm{CV}(\mathrm{Load}(X))^2 \\
&amp;= \left(\frac{\mathrm{var}[\mathrm{Load}(X)]}{\mathbb{E}[\mathrm{Load}(X)]}\right)^2\\
&amp;= \left(C_1[\mathrm{Load}(X)]\right)^2\\
&amp;= C_1\mathbb{E}[\mathrm{Load}(X)^2]-C_2
\end{aligned}
$$&lt;p>这里 $C_1,C_2$ 均为常数。GShard 并没有使用 soft estimation, 因此这里的 $\mathrm{Load}_i(X)$ 定义为&lt;/p>
$$
\mathrm{Load}_i(X)=\sum_{j=1}^B\mathbb{1}(\text{token }j \text{ selects expert }i),\ i=1,\dots,N
$$&lt;p>其代表了一个 batch 里专家 $i$ 被激活的次数，理想情况下，所有专家被激活的次数应该是一致的，从而我们就实现了负载均衡。&lt;/p>
&lt;p>但是现在的问题是，$\mathrm{Load}_i(X)$ 是一个离散变量，为了解决这个问题，作者使用了重要性来进行近似，即&lt;/p>
$$
\begin{aligned}
\mathcal{L}_{\mathrm{load}} &amp;= \mathbb{E}[\mathrm{Load}(X)^2]\\
&amp;\approx \mathbb{E}[\mathrm{Load}(X)\cdot\mathrm{Importance}(X)]
\end{aligned}
$$&lt;h3 id="switch-transformer">&lt;a href="#switch-transformer" class="header-anchor">&lt;/a>Switch Transformer
&lt;/h3>&lt;p>&lt;a class="link" href="https://maosong.website/p/switch-transformer/" target="_blank" rel="noopener"
>Switch Transformer&lt;/a> 进一步对 &lt;a class="link" href="https://maosong.website/p/gshard/" target="_blank" rel="noopener"
>GShard&lt;/a> 提出 load balancing loss 进行了规范化，也是现在大部分 load balancing loss 使用的形式，其定义如下：&lt;/p>
$$
\mathcal{L}_{\mathrm{load}} = \sum_{i=1}^N f_i P_i
$$&lt;p>其中，&lt;/p>
$$
f_i = \frac{1}{B}\mathrm{Load}_i(X)=\frac{1}{B}\sum_{j=1}^B\mathbb{1}(\text{token }j \text{ selects expert }i),\ i=1,\dots,N
$$&lt;p>代表了分配给专家 $i$ 的 token 比例，&lt;/p>
$$
P_i = \frac{1}{B}\mathrm{Importance}_i(X) = \frac{1}{B}\sum_{j=1}^B G_i(x_j)
$$&lt;p>代表了这个 batch 里专家 $i$ 的平均激活概率。&lt;/p>
&lt;p>从而，这个形式和 GShard 的形式实际上是等价的：&lt;/p>
$$
\mathcal{L}_{\mathrm{load}} = \sum_{i=1}^N f_i P_i = \mathbb{E}[\mathbf{f}\cdot \mathbf{P}] = N \mathbb{E}[\mathrm{Load}(X)\cdot\mathrm{Importance}(X)]
$$&lt;h2 id="analysis">&lt;a href="#analysis" class="header-anchor">&lt;/a>Analysis
&lt;/h2>&lt;p>目前关于 load balancing loss 可以实现负载均衡的数学分析是比较少的。&lt;a class="link" href="https://maosong.website/p/switch-transformer/" target="_blank" rel="noopener"
>Switch Transformer&lt;/a> 中进行了如下分析：&lt;/p>
&lt;blockquote>
&lt;p>[!quote]
The auxiliary loss of Equation 4 encourages uniform routing since it is minimized under a uniform distribution.&lt;/p>
&lt;/blockquote>
&lt;p>但实际上，这个分析是错误的，Switch transformer 中定义的 load balancing loss 并不是在均匀分布时取最小值，而是取最大值。考虑如下情形，&lt;/p>
$$
\mathbf{f}=[1,\dots,0],\ \mathbf{P}=[0,\dots,1]
$$&lt;p>此时，我们有&lt;/p>
$$
\mathbf{f}\cdot \mathbf{P} = 0\leq [1/N,\dots,1/N]\cdot [1/N,\dots,1/N] = 1/N.
$$&lt;p>我找了很多资料，最后发现苏剑林老师在 &lt;a class="link" href="https://spaces.ac.cn/archives/10735" target="_blank" rel="noopener"
>MoE环游记：2、不患寡而患不均&lt;/a> 这篇 blog 中进行了分析。我这里基于苏剑林老师的 blog 进行了一下总结，推荐大家看苏剑林老师的原文。&lt;/p>
&lt;p>分析的核心思想就是，尽管 load balancing loss 目标函数没有意义，但是我们通过 straight-through estimator (STE)，就可以得到一个有意义的目标函数，其梯度与 load balancing loss 目标函数的梯度是一致的，这样 load balancing loss 目标函数就完成了其负载均衡的目标。&lt;/p>
&lt;p>注意到 load balancing loss 的最终目标是让每个专家处理的 token 个数尽可能一致，也就是&lt;/p>
$$
\mathcal{L}_{\mathrm{load}} = \sum_{j=1}^B \left(f_i-\frac1B\right)^2
$$&lt;p>但是 $f_i=\mathrm{Load}_i(X)$ 是一个离散变量不可导，因此，基于 STE, 我们可以将其改写为&lt;/p>
$$
\mathcal{L}_{\mathrm{load}} = \sum_{j=1}^B \left(P_i+\mathrm{sg}[f_i-P_i]-\frac1B\right)^2
$$&lt;p>其中 $\mathrm{sg}[\cdot]$ 满足：&lt;/p>
$$
\mathrm{sg}[\cdot]=\begin{cases}
x, &amp;\text{forward pass}\\
0, &amp;\text{backward pass}
\end{cases}
$$&lt;p>此时我们目标函数的梯度就变成了&lt;/p>
$$
\begin{aligned}
\nabla_\theta\mathcal{L}_{\mathrm{load}} &amp;= \sum_{j=1}^B 2\nabla_\theta\left(P_i+\mathrm{sg}[f_i-P_i]-\frac1B\right)\left(P_i+\mathrm{sg}[f_i-P_i]-\frac1B\right)\\
&amp;= 2\sum_{j=1}^B\nabla_\theta P_i\left(f_i-\frac1B\right)\\
&amp;= 2\sum_{j=1}^B\nabla_\theta P_i\left(f_i-\frac1B\right)\\
&amp;= 2\nabla_\theta \left(\sum_{j=1}^Bf_iP_i\right)
\end{aligned}
$$&lt;p>这样就对应上了我们之前的目标函数。总之，目标函数尽管本身没有意义，但是其对应的梯度却可以让模型实现负载均衡的目标。&lt;/p>
&lt;h2 id="extension">&lt;a href="#extension" class="header-anchor">&lt;/a>Extension
&lt;/h2>&lt;h3 id="loss-free-load-balancing-strategy">&lt;a href="#loss-free-load-balancing-strategy" class="header-anchor">&lt;/a>Loss Free Load Balancing Strategy
&lt;/h3>&lt;p>Loss-Free load balancing 是 DeepSeek 提出来的一个无需 load balancing loss 实现负载均衡的方法，其核心思想是 load balancing loss 会影响语言建模性能，因此在 $\mathrm{TopK}$ 操作时，作者加入一些扰动，从而让负载高的专家被选择的概率降低，让负载低的专家被选择的概率提高。&lt;/p>
&lt;p>具体细节见 &lt;a class="link" href="https://maosong.website/p/notes-on-loss-free-balancing/" target="_blank" rel="noopener"
>Loss-Free Balancing&lt;/a>.&lt;/p>
&lt;h3 id="global-batch-load-balancing-loss-strategy">&lt;a href="#global-batch-load-balancing-loss-strategy" class="header-anchor">&lt;/a>Global-batch Load Balancing Loss Strategy
&lt;/h3>&lt;p>Global-batch Load Balancing 是 Qwen 提出来的一个提高负载均衡的方法。其核心思想是，现有的 token choice 只是在 batch 层面达到负载均衡，这种局部均衡的性质可能会对模型的全局均衡性产生影响。因此，作者就预先在 global batch 层面对专家进行分配，从而提高专家在不同 domain 上的特化程度。&lt;/p>
&lt;p>具体细节见 &lt;a class="link" href="https://maosong.website/p/notes-on-global-batch-load-balancing/" target="_blank" rel="noopener"
>Global-batch load balancing&lt;/a>&lt;/p>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>&lt;a class="link" href="https://maosong.website/p/notes-on-olmoe/" target="_blank" rel="noopener"
>olmoe&lt;/a> 对比了加入 load balancing loss 之后模型的表现变化情况，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/load-balancing-tutorial/olmoe-load-balancing-loss.png"
width="1450"
height="532"
loading="lazy"
alt="Ablation study on load balancing loss"
class="gallery-image"
data-flex-grow="272"
data-flex-basis="654px"
>&lt;/p>
&lt;p>可以看到，加入 load balancing loss 之后，模型的表现均超过了不加时的表现。&lt;/p>
&lt;p>作者进一步分析了不同专家在加/不加 load balancing loss 时的激活情况，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/load-balancing-tutorial/ollmoe-load-balancing-expert-assignment.png"
width="1443"
height="566"
loading="lazy"
alt="visualization of expert assignment on load balancing"
class="gallery-image"
data-flex-grow="254"
data-flex-basis="611px"
>&lt;/p>
&lt;p>结果显示，load balancing loss 确实可以让不同专家的激活概率大致相当。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>在本文中，我们介绍了 load balancing loss 的演化，定义，分析以及相关的实验结果。&lt;/p>
&lt;h2 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2006.16668" target="_blank" rel="noopener"
>GShard&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://openreview.net/pdf/BNYMo3QRxh7PwR1riEDL.pdf" target="_blank" rel="noopener"
>Bengio et.at 2015&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.cs.toronto.edu/~hinton/absps/Outrageously.pdf" target="_blank" rel="noopener"
>Noam et.al 2017&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2408.15664" target="_blank" rel="noopener"
>Loss free balancing&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2501.11873" target="_blank" rel="noopener"
>Global-batch load balancing loss&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://spaces.ac.cn/archives/10735" target="_blank" rel="noopener"
>MoE环游记：2、不患寡而患不均&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Global-batch load balancing</title><link>https://maosong.website/p/notes-on-global-batch-load-balancing/</link><pubDate>Thu, 11 Dec 2025 16:09:34 +0800</pubDate><guid>https://maosong.website/p/notes-on-global-batch-load-balancing/</guid><description>&lt;p>Qwen 在 25 年 2 月提出了 global batching load balancing loss strategy, 其在 global level 上考虑每个专家的负载均衡，从而提高模型的表现&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>实现 MoE 模型负载均衡的原因有两点：&lt;/p>
&lt;ol>
&lt;li>Effectiveness: 通过负载均衡才能更高效地利用各个专家&lt;/li>
&lt;li>Efficiency: 一般需要使用 expert parallel 来部署 MoE 模型，不均衡的负载会大幅度降低前向过程&lt;/li>
&lt;/ol>
&lt;p>已有的框架如 DeepSpeed, Megablocks 和 Megatron-Core 都是在 micro-batch level 上计算负载均衡损失的，但是，一个 micro-batch 通常只包含少数序列，因此 load balancing loss 就要求各个专家在每个序列上均匀分布。&lt;/p>
&lt;p>针对这个问题，作者在本文中提出的解决方法是在 global-batch 层面考虑负载均衡，&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;p>MoE 模型定义如下&lt;/p>
$$
y = \sum_{i\in N_E,g_i\in\mathrm{TopK}(g)}g_i(x)E_i(x)
$$&lt;p>其中 $E_i$ 是对应的专家, $g_i$ 是对应的权重&lt;/p>
&lt;p>Load balancing loss 定义如下&lt;/p>
$$
\mathrm{LBL} = N_E\sum_{i=1}^{N_E}f_iP_i
$$&lt;p>一般来说，MoE 模型训练时会使用 expert parallel 策略，此时 load balancing loss 修改为&lt;/p>
$$
\mathrm{LBL}_{\mathrm{micro}}= \frac{1}{N_P}\sum_{j=1}^{N_P}\left(N_E\sum_{i=1}^{N_E}f_i^hP_i^j\right)
$$&lt;p>其中 $N_P$ 是 parallel groups 的个数。在这种情况下，模型需要再每个 parallel group 中实现负载均衡。但是某一个 mircro-batch 里可能只包含某一个 domain 的序列，因此各个专家被强制要求在每一个 domain 中也均衡分布。作者认为，这种方式会限制专家的能力，进而影响模型的表现。&lt;/p>
&lt;p>作者的解决方法在于，获取每个 parallel group 的 $f_i$ 然后求 global-batch 的 $\bar{f}_i$.&lt;/p>
$$
\mathrm{LBL}_{\mathrm{global}}=N_E\sum_{i=1}^{N_E}\bar{f}_i\bar{P}_i=N_E\sum_{i=1}^{N_E}\bar{f}_i\left(\frac{1}{N_P}\sum_{j=1}^{N_P}P_j\right) = \frac{1}{N_P}\sum_{j=1}^{N_P}\left(N_E\sum_{i=1}^{N_E}\bar{f}_iP_i^j\right)
$$&lt;p>实际中，由于计算节点数量有限，micro-batch size 之和可能会小于 global-batch size, 因此我们会使用 gradient accumulation. 在这种情况下， 作者使用了一个 buffer 来保存多个 micro-batch 的专家选择次数，最终算法实现过程如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-global-batch-load-balancing/Global-batch-LBL-approximation.png"
width="891"
height="339"
loading="lazy"
alt="Approximate Global-Batch LBL"
class="gallery-image"
data-flex-grow="262"
data-flex-basis="630px"
>&lt;/p>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>作者使用了不同大小的模型来进行实验，作者用 &lt;strong>Balance BSZ&lt;/strong> 来表现计算 expert selection frequency 时的 token 数，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-global-batch-load-balancing/Global-batch-LBL-performance.png"
width="954"
height="598"
loading="lazy"
alt="Performance of different balance methods and Balance BSZ"
class="gallery-image"
data-flex-grow="159"
data-flex-basis="382px"
>&lt;/p>
&lt;p>可以看到，global LBL 可以提高模型的表现，并且，当 Balance BSZ 增加时，模型的表现也会提升。作者发现对于 &lt;a class="link" href="https://maosong.website/p/notes-on-loss-free-balancing/" target="_blank" rel="noopener"
>Loss-Free Balancing&lt;/a> 来说，使用 global batch 的效果也更好。&lt;/p>
&lt;p>作者还发现，global LBL 会提高专家的特化程度，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-global-batch-load-balancing/Global-batch-LBL-specialization.png"
width="1221"
height="571"
loading="lazy"
alt="The impact of the Balance BSZ"
class="gallery-image"
data-flex-grow="213"
data-flex-basis="513px"
>&lt;/p>
&lt;p>可以看到，使用默认的 LBL loss, 各个专家的特化程度很低，而使用本文的 global LBL 之后，专家的特化程度有了大幅度的提高。&lt;/p>
&lt;p>作者还进一步发现，随着 Balance BSZ 的提高，模型表现也持续提升，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-global-batch-load-balancing/Global-batch-LBL-balance-BSZ.png"
width="613"
height="455"
loading="lazy"
alt="Performance against Balance BSZ"
class="gallery-image"
data-flex-grow="134"
data-flex-basis="323px"
>&lt;/p>
&lt;p>作者认为，synchronization 和 buffer 机制相比于 micro-batch 来说可以带来大幅度提升。&lt;/p>
&lt;h2 id="analysis">&lt;a href="#analysis" class="header-anchor">&lt;/a>Analysis
&lt;/h2>&lt;p>首先，为了分析 global batch LBL 优于 Micro batch 的关键原因，作者先同步所有 group 的矩阵 $G$, 然后再从全局 token 中随机选一批，用这批 token 计算专家选择频率。通过这个方法，我们可以保证 token 数量与 micro-batch 一致，但是分布于 Global-batch 一致。实验结果发现，这种方法的表现优于 micro-batch LBL, 说明 global batch LBL 的优势在于 Token 分布更全局，而不是 token 数量更多。&lt;/p>
&lt;p>作者还分析了 global batch LBL 和 micro batch LBL 两种模式，作者认为前者是后者的一个宽松版约束，作者发现从后者切换为前者之后，模型的表现可以得到进一步提升，但是其表现仍然不如一开始就使用 global batch LBL 更好。作者分析原因认为，这是因为 expert 收敛速度比较快。&lt;/p>
&lt;p>作者进一步通过降低 micro batch LBL 权重来探究是否可以达到同样的表现，结果发现适度江都权重确实可以提高模型的表现，但是降低太多会损害模型的表现，即此时出现了负载不均衡的现象&lt;/p>
&lt;p>作者对比了 global batch LBL 和 micro batch LBL 效率发现，前者比后者慢 $2\%$ 左右，这个差距几乎可以忽略不计&lt;/p>
&lt;p>作者进一步分析了不同 balancing 方式的专家特化程度，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-global-batch-load-balancing/Global-batch-LBL-balance-topK-score.png"
width="1269"
height="656"
loading="lazy"
alt="The topK score sums across layers"
class="gallery-image"
data-flex-grow="193"
data-flex-basis="464px"
>&lt;/p>
&lt;p>实验结果发现：&lt;/p>
&lt;ol>
&lt;li>使用 global batch LBL 之后，topK sum 明显变大，这说明 routing 和 language modeling 任务对齐地更好&lt;/li>
&lt;li>global batch LBL 的专家在不同任务上的特化程度更明显&lt;/li>
&lt;li>micro batch LBL 的 topK sum 比较小&lt;/li>
&lt;li>Loss-free balancing 的 topK sum 介于 micro batch LBL 和 global batch LBL 之间&lt;/li>
&lt;/ol>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 global LBL 来在全局为负载均衡提供指导，结果发现通过在更大的范围进行负载均衡的计算，我们可以有效提高专家的特化程度以及提高模型在下游任务上的表现&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2501.11873" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on DPO</title><link>https://maosong.website/p/notes-on-dpo/</link><pubDate>Tue, 09 Dec 2025 10:43:11 +0800</pubDate><guid>https://maosong.website/p/notes-on-dpo/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>传统的偏好优化主要基于 RLHF, 其过程为：SFT, reward modeling, RLHF. 其中 reward model 的训练至关重要，对最终模型的表现有非常大的影响。但是 RLHF 的问题在于其训练复杂且经常不稳定。&lt;/p>
&lt;p>为了解决这个问题，作者提出了 Direct Preference Optimization (DPO), DPO 通过构建 reward function 和最优策略之间的关系，进而通过训练 policy model 来同时完成 reward model 的训练。这样，我们就避免了 reward model 的训练。结果发现，DPO 的表现超过了之前的偏好优化方法。&lt;/p>
&lt;h2 id="background">&lt;a href="#background" class="header-anchor">&lt;/a>Background
&lt;/h2>&lt;p>作者首先回顾了 RLHF, RLHF 的 pipeline 如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-dpo/RLHF-pipeline.png"
width="2560"
height="1440"
loading="lazy"
alt="RLHF-pipeline"
class="gallery-image"
data-flex-grow="177"
data-flex-basis="426px"
>&lt;/p>
&lt;p>其包含了三个步骤：&lt;/p>
&lt;p>&lt;strong>SFT&lt;/strong>
RLHF 首先基于 base model 通过 SFT 得到一个初始模型 $\pi^{\mathrm{SFT}}$.&lt;/p>
&lt;p>&lt;strong>Reward modeling&lt;/strong>
接下来，我们给定输入 $x$, 对 $\pi^{\mathrm{SFT}}$ 采样得到 $(y_1,y_2)\sim \pi^{\mathrm{SFT}}(y\mid x)$. 输出 $y_1, y_2$ 然后由人类进行打分得到偏好关系 $y_w&lt;y_l\mid x$, 表示 $y_w$ 比 $y_l$ 更符合人类的 pian hao 偏好，一般来说我们假设真实的偏好是由一个 reward function $r^*(x,y)$ 产生的，即 $y_w\succ y_l \Leftrightarrow r^*(x, y_w)>r^*(x, y_l)$. reward modeling 通常基于 Bradley-Terry (BT) 模型得到，BT model 定义人类真实偏好分布 $p^*$ 如下：&lt;/p>
$$
p^*(y_1>y_2\mid x)= \frac{\exp(r^*(x, y_1))}{\exp(r^*(x, y_1))+\exp(r^*(x, y_2))}
$$&lt;p>假设我们从分布 $p^*$ 中采集到一个数据集 $\mathcal{D}=\{(x^{(i)},y_w^{(i)},y_l^{(i)})\}_{i=1}^N$, 我们可以通过 MLE 来估计得到一个 reward model $r_{\phi}(x,y)$, 通过将这个问题转换为一个二分类问题，我们得到对应的 negative log-likelihood loss 如下：&lt;/p>
$$
\mathcal{L}_R(r_\phi, D) = -\mathbb{E}_{(x,y_w,y_l)\sim \mathcal{D}}\left[\log \sigma\left(r_\phi(x,y_w)-r_\phi(x,y_l)\right)\right]
$$&lt;p>其中 $\sigma$ 是 logistic function. 在 LLM 中，$r_\phi(x,y)$ 通常由 $\pi^{\mathrm{SFT}}$ 初始化得到，然后我们在 $\pi^{\mathrm{SFT}}$ 最后一层加入一个 linear layer 来得到对应的 reward 的预测值。一般地，为了降低 reward function 的 variance, 之前的工作会进行 normalization, 即 $\mathbb{E}_{(x,y)}\sim\mathcal{D}[r_{\phi}(x,y)]=0$ for all $x$.&lt;/p>
&lt;p>&lt;strong>RL fine-tuning&lt;/strong>
这个阶段，我们基于学习到的 reward function $r_\phi(x,y)$ 来为 LLM 的训练提供奖励，作者使用了和 RLHF 一样的目标函数：&lt;/p>
$$
\max_{\pi_\theta}\quad \mathbb{E}_{x\sim \mathcal{D}, y\sim \pi_\theta(y\mid x)}\left[r_\phi(x,y)\right] - \beta\mathbb{D}_{\mathrm{KL}}\left[\pi_\theta(y\mid x)\Vert \pi_{\mathrm{ref}}(y\mid x)\right]
$$&lt;p>这里 $\beta>0$ 是超参数，用于控制 $\pi_\theta$ 相对于 $\pi_{\mathrm{ref}}$ 的偏离程度， $\pi_{\mathrm{ref}}$ 一般就是 $\pi^{\mathrm{SFT}}$. 实际上， $\pi_\theta$ 也由 $\pi^{\mathrm{SFT}}$ 初始化.&lt;/p>
&lt;h2 id="dpo">&lt;a href="#dpo" class="header-anchor">&lt;/a>DPO
&lt;/h2>&lt;p>DPO 的主要目标是构建一个更简单的 policy optimization 方法。与 RLHF 不同，DPO 跳过了 reward modeling 这一阶段，而是直接使用偏好数据来优化大语言模型&lt;/p>
&lt;p>为了实现这个目标，作者第一步就是构建 reward model 和 policy model 之间的关系。注意到&lt;/p>
$$
\begin{aligned}
&amp;\max_{\pi_\theta}\ \mathbb{E}_{x\sim \mathcal{D}, y\sim \pi_\theta(y\mid x)}\left[r_\phi(x,y)\right] - \beta\mathbb{D}_{\mathrm{KL}}\left[\pi_\theta(y\mid x)\Vert \pi_{\mathrm{ref}}(y\mid x)\right]\\
&amp;= \max_{\pi_\theta}\ \mathbb{E}_{x\sim \mathcal{D}}\mathbb{E}_{y\sim \pi_\theta(y\mid x)}\left[r_\phi(x,y) - \beta\log\frac{\pi_\theta(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)}\right]\\
&amp;= \min_{\pi_\theta}\ \mathbb{E}_{x\sim \mathcal{D}}\mathbb{E}_{y\sim \pi_\theta(y\mid x)}\left[\log\frac{\pi_\theta(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)}-\frac{1}{\beta}r_\phi(x,y)\right]\\
&amp;= \min_{\pi_\theta}\ \mathbb{E}_{x\sim \mathcal{D}}\mathbb{E}_{y\sim \pi_\theta(y\mid x)}\left[\log\frac{\pi_\theta(y\mid x)}{\frac{1}{Z(x)}\pi_{\mathrm{ref}}(y\mid x)\exp\left(\frac1\beta r_\phi(x,y)\right)}-\log Z(x)\right]\\
\end{aligned}
$$&lt;p>其中 $Z(x)$ 是 partition function, 定义如下&lt;/p>
$$
Z(x) = \sum_{y} \pi_{\mathrm{ref}}(y\mid x)\exp\left(\frac1\beta r_\phi(x,y)\right)
$$&lt;p>注意到 partition function 只是 $x$ 和 $\pi_{\mathrm{ref}}$ 的函数，而不依赖于 $\pi_\theta$, 因此我们定义&lt;/p>
$$
\pi^*(y\mid x) = \frac{1}{Z(x)}\pi_{\mathrm{ref}}(y\mid x)\exp\left(\frac1\beta r_\phi(x,y)\right)
$$&lt;p>易知 $\pi^*$ 满足 $\pi^*(y\mid x)\geq0, \forall y$, 以及 $\sum_y \pi^*(y\mid x)=1$. 因为 $Z(x)$ 不依赖于 $y$, 因此我们可以进一步简化上面的目标函数如下&lt;/p>
$$
\begin{aligned}
&amp;\min_{\pi_\theta}\ \mathbb{E}_{x\sim \mathcal{D}}\left[\mathbb{E}_{y\sim \pi_\theta(y\mid x)}\left[\log\frac{\pi_\theta(y\mid x)}{\pi^*(y\mid x)}\right]-\log Z(x)\right] \\
&amp;= \min_{\pi_\theta}\ \mathbb{E}_{x\sim \mathcal{D}}\left[\mathbb{D}_{\mathrm{KL}}\left[\pi_\theta(y\mid x)\Vert \pi^*(y\mid x)\right]-\log Z(x)\right]\\
&amp;= \boxed{\min_{\pi_\theta}\ \mathbb{E}_{x\sim \mathcal{D}}\left[\mathbb{D}_{\mathrm{KL}}\left[\pi_\theta(y\mid x)\Vert \pi^*(y\mid x)\right]\right]}
\end{aligned}
$$&lt;p>而上述目标函数的最小值为 0，当且仅当 $\pi_\theta=\pi^*$, 此时我们的最优 policy 为&lt;/p>
$$
\pi_\theta^*(y\mid x) = \pi^*(y\mid x) = \frac{1}{Z(x)}\pi_{\mathrm{ref}}(y\mid x)\exp\left(\frac1\beta r_\phi(x,y)\right),\ \forall x\in\mathcal{D}
$$&lt;p>直接求解 $\pi_\theta^*$ 非常困难，因为这涉及到 $Z(x)$ 的计算，这个时候作者就提出了一个关键改变，即我们从上述的 $\pi_\theta^*$ 反向推到出 $r(x,y)$:&lt;/p>
$$
r(x,y) = \beta\log\frac{\pi_\theta(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)}+\beta\log Z(x)
$$&lt;p>我们可以基于这个公式来推到出最优的 reward function $r^*$ 以及对应的最优策略 $\pi^*$.&lt;/p>
&lt;p>此时，我们的表达式里仍然含有 $Z(x)$, 但是当我们使用 Bradley-Terry 模型之后，我们就可以得到真实的人类偏好分布，计算过程如下所示&lt;/p>
$$
\begin{aligned}
p^*(y_w>y_l\mid x)&amp;= \sigma\left(r_\phi(x,y_w)-r_\phi(x,y_l)\right)\\
&amp;= \boxed{\sigma\left(\beta\log\frac{\pi_\theta(y_w\mid x)}{\pi_{\mathrm{ref}}(y_w\mid x)}-\beta\log\frac{\pi_\theta(y_l\mid x)}{\pi_{\mathrm{ref}}(y_l\mid x)}\right)}
\end{aligned}
$$&lt;p>这样，基于 MLE 的目标函数就是&lt;/p>
$$
\mathcal{L}_{\mathrm{DPO}}(\pi_{\theta};\pi_{\mathrm{ref}})= -\mathbb{E}_{(x,y_w,y_l)\in\mathcal{D}}\left[\sigma\left(\beta\log\frac{\pi_\theta(y_w\mid x)}{\pi_{\mathrm{ref}}(y_w\mid x)}-\beta\log\frac{\pi_\theta(y_l\mid x)}{\pi_{\mathrm{ref}}(y_l\mid x)}\right)\right]
$$&lt;p>通过这种方式，我们就避免了 reward model 的训练。&lt;/p>
&lt;p>接下来，作者分析了一下 DPO 目标函数的梯度，&lt;/p>
$$
\begin{aligned}
\nabla_\theta \mathcal{L}_{\mathrm{DPO}}(\pi_{\theta};\pi_{\mathrm{ref}})&amp;=-\nabla_\theta \mathbb{E}_{(x,y_w,y_l)\in\mathcal{D}}\left[\log\sigma(u)\right]\\
&amp;= -\mathbb{E}_{(x,y_w,y_l)\in\mathcal{D}}\left[\frac{\nabla_\theta \sigma(u)}{\sigma(u)}\nabla_\theta u\right]\\
&amp;= -\mathbb{E}_{(x,y_w,y_l)\in\mathcal{D}}\left[\frac{\sigma(u)(1-\sigma(u))}{\sigma(u)}\nabla_\theta u\right]\\
&amp;= -\mathbb{E}_{(x,y_w,y_l)\in\mathcal{D}}\left[(1-\sigma(u))\nabla_\theta u\right]\\
&amp;= -\mathbb{E}_{(x,y_w,y_l)\in\mathcal{D}}\left[\sigma(-u)\nabla_\theta u\right]\\
&amp;= -\mathbb{E}_{(x,y_w,y_l)\in\mathcal{D}}\left[\beta\sigma\left[\beta\log\frac{\pi_\theta(y_l\mid x)}{\pi_{\mathrm{ref}}(y_l\mid x)}-\beta\log\frac{\pi_\theta(y_w\mid x)}{\pi_{\mathrm{ref}}(y_w\mid x)}\right]\left[\nabla_\theta\log\pi(y_w\mid x) - \nabla_\theta \log\pi(y_l\mid x)\right]\right]\\
\end{aligned}
$$&lt;p>从梯度来看，当我们的 reward 估计错误时，即 $\sigma(-u)>0$ 时， DPO 会提高 $y_w$ 的生成可能性以及降低 $y_l$ 的生成可能性，从而提高模型对于偏好输出的可能性。&lt;/p>
&lt;p>最终，DPO 的 pipeline 如下：&lt;/p>
&lt;ol>
&lt;li>收集偏好数据 $y_1,y_2\sim\pi_{\mathrm{ref}}(\cdot\mid x)$, 然后通过人类标注得到偏好数据集 $\mathcal{D}=\{(x^{(i)},y_w^{(i)},y_l^{(i)})\}_{i=1}^N$&lt;/li>
&lt;li>基于 $\mathcal{L}_{\mathrm{DPO}}(\pi_{\theta};\pi_{\mathrm{ref}})$ 优化大语言模型参数&lt;/li>
&lt;/ol>
&lt;p>一般来说，我们将 $\pi_{\mathrm{ref}}$ 初始化为 $\pi^{\mathrm{SFT}}$, 但是如果我们没有 $\pi^{\mathrm{SFT}}$ 时，我们可以通过最大似然估计来得到 $\pi_{\mathrm{ref}}$, 即&lt;/p>
$$
\pi_{\mathrm{ref}} = \arg\max_{\pi}\mathbb{E}_{(x,y_w,y_l)\sim\mathcal{D}}\log \pi(y_w\mid x)
$$&lt;h2 id="theoretical-analysis-of-dpo">&lt;a href="#theoretical-analysis-of-dpo" class="header-anchor">&lt;/a>Theoretical Analysis of DPO
&lt;/h2>&lt;blockquote>
&lt;p>Definition
两个 reward function $r(x,y)$ 和 $r'(x,y)$ 等价当且仅当存在函数 $f$ 满足 $r(x,y)-r'(x,y)=f(x)$。&lt;/p>
&lt;/blockquote>
&lt;p>上述定义给出了一个等价关系，将 reward function 分割成了不同的等价类。接下来，作者给出了两个引理：&lt;/p>
&lt;p>第一个引理说明同一个等价类里的 reward function 对应的偏好分布一致&lt;/p>
&lt;blockquote>
&lt;p>Lemma 1
在 Plack-Luce 框架下，如 Bradley-Terry model, 同一个等价类里的 reward function 得到的偏好分布是一致的&lt;/p>
&lt;/blockquote>
&lt;p>第二个引理说明了最优策略对应的 reward function 都在一个等价类里&lt;/p>
&lt;blockquote>
&lt;p>Lemma 2
在有限制的情况下，同一个等价类里的 reward function 得到的最优策略是一样的&lt;/p>
&lt;/blockquote>
&lt;p>Lemma 2 说明我们只要从最优的等价类里任意找到一个 reward function, 则最终的效果是一样的。&lt;/p>
&lt;blockquote>
&lt;p>Theorem 1
假设我们有一个 reference model $\pi_{\mathrm{ref}}(\cdot\mid x)>0$ for all prompt-answer pairs $(x,y)$, 则对于某个模型 $\pi(y\mid x)$, 则与 $\pi_{\mathrm{ref}}$ 对应的等价类里的 reward function 都可以表示为如下形式 $r(x,y) = \beta\frac{\pi(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)},\ \beta>0$.&lt;/p>
&lt;/blockquote>
&lt;p>我们也可以通过 Theorem 1 来显示推导出 DPO 选择的 reward function:&lt;/p>
$$
\sum_{y}\underbrace{\pi_{\mathrm{ref}}(y\mid x)\exp\left(\frac1\beta r_\phi(x,y)\right)}_{\pi(y\mid x)}=1
$$&lt;p>回顾前面 $\pi^*(y\mid x)$ 的定义，我们知道 $\pi(y\mid x)$ 实际上是针对 $r(x,y)$ 推导出来的最优策略的 partition function.&lt;/p>
&lt;p>注意到我们的初始目标函数可以改写为如下形式&lt;/p>
$$
\min_{\pi_\theta}\ \mathbb{E}_{x\sim \mathcal{D}}\left[\underbrace{r_{\phi}(x,y)-\beta\log Z(x)}_{f(r_\phi, \pi_{\mathrm{ref}},\beta)}-\beta\underbrace{ \frac{\pi_\theta(y\mid x)}{\pi_{\mathrm{ref}}(y\mid x)}}_{\text{KL}}\right]
$$&lt;p>此时，我们可以将 $f(r_\phi, \pi_{\mathrm{ref}},\beta)$ 里的 normalization term 视作为 $\pi_{\mathrm{ref}}$ 的 soft value function, 这个 soft value function 不影响最终的结果，但是没有的话会导致训练时的 variance 很高。 DPO 通过 re-parameterization, 得到的奖励函数不需要 baseline, 因而解决了训练不稳定的问题。&lt;/p>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>作者首先对比了不同偏好优化算法的表现，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-dpo/DPO-robustness-performance.png"
width="1180"
height="452"
loading="lazy"
alt="Performance of DPO"
class="gallery-image"
data-flex-grow="261"
data-flex-basis="626px"
>&lt;/p>
&lt;p>从实验结果可以看到，DPO 对于不同的 KL Values 和不同的采样温度其表现都非常好，并且更加 robust&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>在本文中，作者提出了 DPO，一个针对 LLM 偏好优化的训练范式，DPO 构建了最优的 policy 与 reward function 之间的关系，从而避免了训练 reward model, 让模型可以直接从偏好数据集中进行学习和训练。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://openreview.net/forum?id=HPuSIXJaa9" target="_blank" rel="noopener"
>openreview&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on DeepSeek-V3</title><link>https://maosong.website/p/notes-on-deepseek-v3/</link><pubDate>Mon, 08 Dec 2025 11:14:45 +0800</pubDate><guid>https://maosong.website/p/notes-on-deepseek-v3/</guid><description>&lt;p>DeepSeek 在 24 年 11 月发布了 DeepSeek-V3, 一个仅花费 2.8M H800 hours 的大语言模型，且在各个 benchmark 上达到了 SOTA 表现&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者在本文中提出了 DeepSeek-V3, 一个 671B-A37B 的 MoE 大语言模型。&lt;/p>
&lt;p>在训练目标和架构上，作者做了如下改进：&lt;/p>
&lt;ol>
&lt;li>efficiency inference: 采用了 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v2/" target="_blank" rel="noopener"
>DeepSeek-V2&lt;/a> 提出的 &lt;a class="link" href="https://maosong.website/p/notes-on-mla/" target="_blank" rel="noopener"
>MLA&lt;/a>&lt;/li>
&lt;li>cost-effective training: 采用了 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseekmoe/" target="_blank" rel="noopener"
>DeepSeekMoE&lt;/a> 提出了 MoE 架构&lt;/li>
&lt;li>auxiliary-loss-free strategy: 采用了 &lt;a class="link" href="https://maosong.website/p/notes-on-loss-free-balancing/" target="_blank" rel="noopener"
>Loss-Free Balancing&lt;/a> 提出的 loss balancing 策略&lt;/li>
&lt;li>multi-token prediction: 采用了 MTP 的训练目标来提升模型的表现&lt;/li>
&lt;/ol>
&lt;p>在训练上，作者做了如下改进：&lt;/p>
&lt;ol>
&lt;li>使用了 FP8 混合精度进行训练并验证了其在大规模模型上的有效性&lt;/li>
&lt;li>作者构建了 DualPipe 算法用于高效的 pipeline parallelism&lt;/li>
&lt;li>构建了 cross-node all-to-all communication kernel 来高效使用 InfiniBand 以及 NVLink bandwidth&lt;/li>
&lt;li>优化了 memory footprint, 来避免使用 tensor parallelism&lt;/li>
&lt;/ol>
&lt;p>预训练阶段，DeepSeek-V3 使用了&lt;strong>14.8T&lt;/strong> token. 在 mid-training 阶段，作者将模型的上下文长度由 8K 扩展到 32K, 再扩展到 128K.&lt;/p>
&lt;p>后训练阶段，作者使用了 SFT 和 RL 两个阶段来提高模型的表现，作者还对 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-r1/" target="_blank" rel="noopener"
>DeepSeek-R1&lt;/a> 进行蒸馏来提高模型的 reasoning 能力&lt;/p>
&lt;h2 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h2>&lt;h3 id="basic-architecture">&lt;a href="#basic-architecture" class="header-anchor">&lt;/a>Basic Architecture
&lt;/h3>&lt;p>DeepSeek-V3 的架构与 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v2/" target="_blank" rel="noopener"
>DeepSeek-V2&lt;/a> 的架构一致，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3/DeepSeek-V2-architecture.png"
width="1324"
height="1064"
loading="lazy"
alt="Architecture of DeepSeek-V2"
class="gallery-image"
data-flex-grow="124"
data-flex-basis="298px"
>&lt;/p>
&lt;p>MLA 的介绍见 &lt;a class="link" href="https://maosong.website/p/notes-on-mla/" target="_blank" rel="noopener"
>MLA&lt;/a>, MoE 架构的介绍见 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseekmoe/" target="_blank" rel="noopener"
>DeepSeekMoE&lt;/a>. DeepSeek-V3 在 DeepSeekMoE 的基础上做了两点改变：&lt;/p>
&lt;ol>
&lt;li>受 &lt;a class="link" href="https://maosong.website/p/notes-on-loss-free-balancing/" target="_blank" rel="noopener"
>Loss-Free Balancing&lt;/a> 启发，作者使用了 sigmoid fu nction 来计算 affinity score&lt;/li>
&lt;li>对于 selected affinity score 应用了 normalization&lt;/li>
&lt;/ol>
&lt;p>在 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseekmoe/" target="_blank" rel="noopener"
>DeepSeekMoE&lt;/a> 的基础上，作者使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-loss-free-balancing/" target="_blank" rel="noopener"
>Loss-Free Balancing&lt;/a>. 其表达式如下&lt;/p>
$$
g_{i,t} = \begin{cases}
s_{i,t}, &amp; s_{i,t}+b_i\in\mathrm{Topk}(\{s_{i,j}+b_j\mid 1\leq j\leq N\}, K)\\
0, &amp;\text{otherwise}
\end{cases}
$$&lt;p>其中 $b_i$ 仅影响 routing, 训练时，如果对应的 expert 负载不均衡，则对 $b_i$ 进行更新，更新方式为增加/减少 $\gamma$, 这里 $\gamma$ 是一个超参数&lt;/p>
&lt;p>为了提高 routing 在 sequence 层面的负载均衡，作者还是用了一个 complementary sequence-wise balance loss:&lt;/p>
$$
\begin{aligned}
\mathcal{L}_{\mathrm{Bal}} &amp;= \alpha\sum_{i=1}^{N_r} f_iP_i\\
f_i &amp;= \frac{}{}\sum_{t=1}^T\mathbb{1}(s_{i,t}\in\mathrm{TopK}(\{s_{j,t}\mid 1\leq j \leq N_r\}, K_r))\\
s_{i,y}' &amp;= \frac{s_{i,t}}{\sum_{j=1}^{N_r}s_{j,t}}\\
P_i &amp;= \frac1T\sum_{t=1}^T s_{i,t}'
\end{aligned}
$$&lt;p>其中 $\alpha$ 是 balance factor, 为超参数，$T$ 是 sequence length, 加入这个损失后，每个 sequence 上的负载会变得更加均衡&lt;/p>
&lt;p>与 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseekmoe/" target="_blank" rel="noopener"
>DeepSeekMoE&lt;/a> 一样，作者也在 node 层面实现负载均衡，具体做法就是，根据 node 中所包含 expert 的 affinity score 之和来选取最高的 $M$ 个 nodes, 这样就可以进一步提高 computation 和 communication 之间的 overlap.&lt;/p>
&lt;p>由于 DeepSeek-V3 负载均衡比较好，因此作者使用了 no token-dropping strategy.&lt;/p>
&lt;h3 id="mtp">&lt;a href="#mtp" class="header-anchor">&lt;/a>MTP
&lt;/h3>&lt;p>受 MTP 启发，DeepSeek-V3 也构建了 MTP 模块，作者认为 MTP 模块有两个优势：&lt;/p>
&lt;ol>
&lt;li>MTP objective 提供了更多的学习信号，进而提高了数据使用效率&lt;/li>
&lt;li>MTP 可以让模型更好预测未来的 token&lt;/li>
&lt;/ol>
&lt;p>与 MTP 不同，DeepSeek-V3 【todo】, DeepSeek-V3 所使用的 MTP 模块架构图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3/DeepSeek-V3-MTP.png"
width="1203"
height="536"
loading="lazy"
alt="Illustration of MTP"
class="gallery-image"
data-flex-grow="224"
data-flex-basis="538px"
>&lt;/p>
&lt;p>MTP 模块使用了 $D$ sequential modules 来预测未来的 $D$ 个 token. 其中，第 $k$ 个 MTP 模块包含一个共享的 embedding layer $\mathrm{Emb}(\cdot)$ , 一个共享的 output head $\mathrm{OutHead}(\cdot)$, 一个 transformer block $\mathrm{TRM}_k(\cdot)$ 和一个 projection matrix $M_k\in\mathbb{R}^{d\times 2d}$.&lt;/p>
&lt;p>对于第 $i$ 个 token 以及第 $k$ 个 MTP 模块，作者首先将第 $k-1$ 个 MTP 模块的第 $i$ 个 token 的 hidden states $h_i^{k-1}\in\mathbb{R}^d$ 和第 $i+k$ 个 token 的 embedding $\mathrm{Emb}(t_{i+1})\in\mathbb{R}^d$ 联合在一起&lt;/p>
$$
h_i'^{k}=M_k[\mathrm{RMSNorm(h_{i}^{k-1});\mathrm{RMSNorm(\mathrm{Emb}(t_{i+k}))}}]
$$&lt;p>其中 $[\cdot;\cdot]$ 代表 concatenation 操作。当 $k=1$ 时，$h_{i}^{k-1}$ 就代表了 main model 的输出。每个 MTP 模块的 embedding layer 和 main model 的 embedding layer 是共享的，接下来， $h_i'^k$ 作为第 $k$ 个 MTP 模块 transformer block 的输入，得到&lt;/p>
$$
h_{i}^k = \mathrm{TRM}_k(h_{i}'^l)
$$&lt;p>最后，共享的 output head 输出对应的概率分布：&lt;/p>
$$
P_{i+k+1}^k = \mathrm{OutHead}(h_i^k)
$$&lt;p>这里的 $\mathrm{OutHead}(\cdot)$ 的权重与 main model 也是共享的。作者这里提到，所使用的思想与 EAGLE 是类似的，但是不同的地方在于，EAGLE 主要是用于 speculative decoding, 而 MTP 主要用于提升训练。&lt;/p>
&lt;p>MTP 的训练目标为未来 $T-k-1$ 个 token 的 cross-entropy loss:&lt;/p>
$$
\mathcal{L}_{\mathrm{MTP}}^k = \mathrm{CrossEntropy}(P_{2+k:T+1}^k,t_{2+k:T+1}) = -\frac1T\sum_{t=k+2}^{T+1}\log P_i^k[t_i],
$$&lt;p>其中 $T$ 是 sequence length, $t_i$ 是第 $i$ 个位置对应的 ground truth token, $P_i^k[t_i]$ 代表低 $k$ 个 MTP 模块给出的 $t_i$ 的预测概率。最后，作者对所有的 MTP loss 进行求和，得到&lt;/p>
$$
\mathcal{L}_{\mathrm{MTP}} = \frac{\lambda}{D}\sum_{k=1}^D \mathcal{L}_{\mathrm{MTP}}^k.
$$&lt;h2 id="infra">&lt;a href="#infra" class="header-anchor">&lt;/a>Infra
&lt;/h2>&lt;p>DeepSeek-V3 训练使用了 2048 张 H800, 每个 node 包含 8 张 GPU, node 内部使用 NVLink 和 NVSwitch 进行连接，node 之间使用 InfiniBand 进行连接&lt;/p>
&lt;p>与之前的 DeepSeek 系列相同，DeepSeek-V3 也是用了 HAI-LLM 框架来支持训练。训练时，DeepSeek-V3 使用了 16-way PP, 64-way EP (spanning 8 nodes, &lt;a class="link" href="https://maosong.website/p/gshard/" target="_blank" rel="noopener"
>GShard&lt;/a>) 以及 ZeRO-1 DP.&lt;/p>
&lt;p>作者主要进行了三点优化：&lt;/p>
&lt;ol>
&lt;li>构建了 DualPipe 用于高效 pipeline parallelism&lt;/li>
&lt;li>构建了 cross-node all-to-all communication kernels 来高效利用 IB 以及 NVLink bandwidth&lt;/li>
&lt;li>优化了训练时的 memory footprint, 使得训练时不再依赖 TP&lt;/li>
&lt;/ol>
&lt;h3 id="training-framework">&lt;a href="#training-framework" class="header-anchor">&lt;/a>Training Framework
&lt;/h3>&lt;h4 id="dualpipe">&lt;a href="#dualpipe" class="header-anchor">&lt;/a>DualPipe
&lt;/h4>&lt;p>DeepSeek-V3 中，由于 cross-node EP, computation-to-communication ratio 近似为 1:1, 为了解决这个问题，作者提出了 DualPipe. DualPipe 的核心思想是将 forward 和 backward 过程中的 computation 以及 communication 进行重叠。与 ZeroBubble 类似，作者将每个 chunk 分为四个部分：attention, all-to-all dispatch, MLP 以及 all-to-all combine. 对于 attention 和 MLP, 作者还进一步将 backward 拆分为 针对权重和输入的 backward. 其示意图如下所示，这里橙色部分代表 forward, 绿色代表了针对输入的 backward, 蓝色代表了针对权重的 backward&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3/DeepSeek-V3-overlapping-strategy.png"
width="1101"
height="125"
loading="lazy"
alt="Overlapping stategy of DeepSeek-V3"
class="gallery-image"
data-flex-grow="880"
data-flex-basis="2113px"
>&lt;/p>
&lt;p>示意图里包含两个 block, 我们记为 block1, block2, 前向计算过程为&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">dispatch(F, block1) -&amp;gt; MLP(F, block1) -&amp;gt; combine(F, block1) -&amp;gt; attention(F, block2)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>其中，&lt;code>dispatch(F, block1)&lt;/code> 与 MLP 的反向传播 &lt;code>MLP(B, block1)&lt;/code> 计算重叠, &lt;code>MLP(F, block1)&lt;/code> 与 MLP 反向传播的 dispatch &lt;code>dispatch(B, block1)&lt;/code> 通信重叠，&lt;code>combine(F, block1)&lt;/code> 与 attention 反向传播的 &lt;code>attention(B, block2)&lt;/code> 重叠，&lt;code>attention(F, block2)&lt;/code> 与反向传播的 combine &lt;code>combine(block2)&lt;/code> 重叠。下面是一个具体的例子&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3/DeepSeek-V3-DualPipe-scheduling.png"
width="1212"
height="210"
loading="lazy"
alt="DualPipe scheduling"
class="gallery-image"
data-flex-grow="577"
data-flex-basis="1385px"
>&lt;/p>
&lt;p>作者进一步对比了 DualPipe, 1F1B 和 ZeroBubble, 结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Method&lt;/th>
&lt;th>Bubble&lt;/th>
&lt;th>Parameter&lt;/th>
&lt;th>Activation&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>1F1B&lt;/td>
&lt;td>$(PP-1)(F+B)$&lt;/td>
&lt;td>$1\times$&lt;/td>
&lt;td>$PP$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ZB1P&lt;/td>
&lt;td>$(PP-1)(F+B-2W)$&lt;/td>
&lt;td>$1\times$&lt;/td>
&lt;td>$PP$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DualPipe (Ours)&lt;/td>
&lt;td>$(\frac{PP}{2}-1)(F\&amp;B+B-3W)$&lt;/td>
&lt;td>$2\times$&lt;/td>
&lt;td>$PP+1$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>这里 $F$ 是 forward chunk 的执行时间，$B$ 是 backward chunk 执行的时间，$W$ 是一个 chunk &amp;ldquo;backward for weights&amp;rdquo; 的执行时间，$F\&amp;B$ 是一个 chunk 前向反向传播重叠的时间。可以看到，DualPipe 只使用了额外的 $1/PP$ 倍的 peak activation memory, 就大幅度降低了 bubble 时间&lt;/p>
&lt;h4 id="cross-node-all-to-all-communication">&lt;a href="#cross-node-all-to-all-communication" class="header-anchor">&lt;/a>Cross-node All-to-all Communication
&lt;/h4>&lt;p>作者针对 DualPipe 构建了 cross-node all-to-all communication kernels 来提高通信效率。&lt;/p>
&lt;p>作者提到，跨节点通信使用的是 IB, 节点内部通信使用的是 NVLink， 通信方式如下图所示。&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3/GPU-communication-pattern.png"
width="1360"
height="749"
loading="lazy"
alt="Communication of GPU"
class="gallery-image"
data-flex-grow="181"
data-flex-basis="435px"
>&lt;/p>
&lt;p>对于 H800 来说,NVLink 的带宽为 160GB/s, IB 的带宽为 50GB/s, 因此 NVLInk 的通信效率是 IB 的 3.2 倍，即节点内部通信效率高于节点之间的通信效率。为了提高通信效率，作者限制每个 token 只能被分发到至多 4 个节点上（&lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v2/" target="_blank" rel="noopener"
>DeepSeek-V2&lt;/a> 里提到的 device limited routing）。其具体通信方式为想传输到目标 node index 相同的 rank 上，然后再通过节点内部通信传输到目标的 rank 上。通过这种方式，我们可以让节点间通信与节点内部通信进行重叠，从而每个 token 可以从一个节点上选取 3.2 个专家，这样 DeepSeek-V3 可以在不损失效率的情况下最高选取 $13$ 个专家。&lt;/p>
&lt;p>作者进一步采用了 warp specialization 技巧来将 20 个 SM 划分为 10 个通信 channel. 作者分别针对 dispatching 和 combing 阶段使用了不同数量的 warps.&lt;/p>
&lt;h4 id="memory-saving">&lt;a href="#memory-saving" class="header-anchor">&lt;/a>Memory saving
&lt;/h4>&lt;p>作者使用了如下技巧来减少内存访问：&lt;/p>
&lt;ol>
&lt;li>Recomputation of RMSNorm and MLA Up-Projection. 在 backward 过程中重新计算 RMSNorm operation 以及 MLA up projection 来避免存储器对应的输出&lt;/li>
&lt;li>Exponential Moving Average in CPU. 作者将 EMA 参数保存在 CPU 中，然后进行异步更新来进一步减少内存访问&lt;/li>
&lt;li>Shared Embedding and Output Head for Multi-Token Prediction. 作者将 embedding layer 和 output head 放在一个 PP rank 上，这样就可以提高 MTP 的内存访问效率&lt;/li>
&lt;/ol>
&lt;h3 id="fp8-training">&lt;a href="#fp8-training" class="header-anchor">&lt;/a>FP8 Training
&lt;/h3>&lt;p>作者提出了一个基于 FP8 的混合精度训练框架。作者提出了两个改进方案：&lt;/p>
&lt;ol>
&lt;li>分组量化，将 tensor 按照 tile 分组或者按照 block 分组来分组量化，这样就避免了全局量化的精度损失&lt;/li>
&lt;li>高精度累加，作者在乘法计算时，使用了 FP8 格式，然后在累加阶段，使用了更高精度的格式&lt;/li>
&lt;/ol>
&lt;p>为了进一步减少内存和通信开销，对于 activation 的 cache 以及 dispatch, 作者使用了 FP8 数据格式，然后对于优化器状态，作者使用了 BF16 数据格式。下面是对上面改进的具体说明。&lt;/p>
&lt;h4 id="mixed-precision-training">&lt;a href="#mixed-precision-training" class="header-anchor">&lt;/a>Mixed Precision Training
&lt;/h4>&lt;p>作者在本文中提出了使用 FP8 混合精度进行预训练，作者参考了 low precision training 构建 FP8 训练框架，即计算量高的使用 FP8 精度，计算量低的使用原本的数据精度, 框架如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3/DeepSeek-V3-mixed-precision.png"
width="1210"
height="365"
loading="lazy"
alt="Mix-precision training of DeepSeek-V3"
class="gallery-image"
data-flex-grow="331"
data-flex-basis="795px"
>&lt;/p>
&lt;p>其中各个模块使用的精度如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Precision&lt;/th>
&lt;th>Modules&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>FP8&lt;/td>
&lt;td>Linear (Fprop, Dgrad, Wgrad)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>higher precision&lt;/td>
&lt;td>- embedding&lt;br>- output head&lt;br>- Moe gating&lt;br>- normalization&lt;br>- attention operator&lt;br>- master weights&lt;br>- weight gradients&lt;br>- optimizer states&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="enhancing-low-precision-training-accuracy">&lt;a href="#enhancing-low-precision-training-accuracy" class="header-anchor">&lt;/a>Enhancing Low-precision Training Accuracy
&lt;/h4>&lt;p>作者介绍了几个策略用于提高 FP8 混合精度训练的表现：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3/DeepSeek-V3-quantization.png"
width="1162"
height="588"
loading="lazy"
alt="Quantization of DeepSeek-V3"
class="gallery-image"
data-flex-grow="197"
data-flex-basis="474px"
>&lt;/p>
&lt;ol>
&lt;li>fine-grained quantization: 对于激活值，作者将 tensor 分割为 $1\times 128$ 大小的 groups, 然后每个 group 内部进行 quantization; 对于权重，作者将其分割为 $128\times 128$ 大小的 groups, 然后进行 quantization, 这样可以降低 quantization error, 如上图左图所示&lt;/li>
&lt;li>increasing accumulation precision: 低精度训练会带来 underflow 的问题，而 FP8 GEMM 累加仅能保留约 14bit 精度，远低于 FP32 的 32bit 精度。为了解决这个问题，作者采用了 Tensor Core 不分累加 +CUDA core 高精度聚合的协同策略。即在 Tensor Core 上执行 MMA 指令时，先按照 14bit 精度对 $N_C$ 个元素进行累加，当达到 $N_C$ 时，作者将结果复制到 CUDA core 的 FP32 寄存器中，在 FP32 精度下完成累加。过程如上图右图所示&lt;/li>
&lt;li>Mantissa over exponents. 与之前的工作不同，作者使用了 E4M3 的数据格式来达到更高的精度&lt;/li>
&lt;li>Online Quantization. 为了降低 quantization 的误差，作者实时计算了 activation block 以及 weight block 的最大绝对值来导出 scaling factor 并量化为 FP8 精度&lt;/li>
&lt;/ol>
&lt;h4 id="low-precision-storage-and-communication">&lt;a href="#low-precision-storage-and-communication" class="header-anchor">&lt;/a>Low Precision Storage and Communication
&lt;/h4>&lt;p>作者通过压缩 cached activation 和 optimizer states 来进一步减少内存访问以及通信访问次数&lt;/p>
&lt;ul>
&lt;li>Low-precision optimizer states: 对于 optimizer states, 作者使用了 BF16 数据格式来保存 &lt;a class="link" href="https://maosong.website/p/notes-on-adamw/" target="_blank" rel="noopener"
>AdamW&lt;/a> 的优化器的一阶和二阶动量，但是对于 master weight 和 gradients 作者仍然使用了 FP32 来保证训练的数值稳定性&lt;/li>
&lt;li>Low-Precision Activation: 对于 linear operator, 作者将其 cache activation 使用 FP8 格式进行存储，对于 attention 的输出，作者使用了 E5M6 数据格式来存储 activations, 这些 activations 的 tile size 为 $1\times 128$, 作者还是用了 2 的幂次作为 scale factor 来减少 quantizationerror; 对于 SwiGLU, 作者将其输入也保存为 FP8 的数据格式来降低内存消耗&lt;/li>
&lt;li>Low-Precision Communication: 在 MoE up-projection 之前，作者将 attention 的输出量化为 FP8 数据格式再执行 dispatch 操作，这样可以降低通信开销。在反向传播的时候同理，先进行 FP8 量化，然后再进行反向 dispatch. 对于 combine 阶段，为了避免精度损失，作者还是使用了 BF16 数据格式&lt;/li>
&lt;/ul>
&lt;p>作者对比了 FP8 和 BF16 精度训练，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3/DeepSeek-V3-FP8-performance.png"
width="1056"
height="305"
loading="lazy"
alt="FP8 v.s. BF16"
class="gallery-image"
data-flex-grow="346"
data-flex-basis="830px"
>&lt;/p>
&lt;p>实验结果显示，FP8 混合精度训练的损失降低不足 $0.25\%$.&lt;/p>
&lt;h3 id="inference-and-deployment">&lt;a href="#inference-and-deployment" class="header-anchor">&lt;/a>Inference and Deployment
&lt;/h3>&lt;p>作者在 H800 集群上部署 DeepSeek-V3, 作者分别针对 prefilling 和 decoding 两个阶段进行了优化&lt;/p>
&lt;h4 id="prefilling">&lt;a href="#prefilling" class="header-anchor">&lt;/a>Prefilling
&lt;/h4>&lt;p>prefilling 阶段在 4 节点 32 GPU 上进行，并行策略如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>模块&lt;/th>
&lt;th>并行策略&lt;/th>
&lt;th>说明&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Dense MLP&lt;/td>
&lt;td>1-wat TP&lt;/td>
&lt;td>减少 TP 通信&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Attention&lt;/td>
&lt;td>4-way TP, SP, 8-way DP&lt;/td>
&lt;td>SP 用于长文本处理&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MoE&lt;/td>
&lt;td>32-way EP&lt;/td>
&lt;td>每个 GPU 包含 8 个专家&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>为了实现负载均衡，作者提出了&lt;strong>redundant experts&lt;/strong>的策略，具体就是将负载比较高的专家进行复制。模型会周期性进行统计，然后计算出负载比较高的专家，接下来每个 GPU 除了原来的 8 个专家之外，还会 host 一个额外的 redundant expert.&lt;/p>
&lt;p>为了提高 throughput, 作者同时处理两个 micro-batch, 来重叠 attention, MoE 和 dispatch, combine 通信，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3/DeepSeek-V3-prefilling-overlap.png"
width="661"
height="316"
loading="lazy"
alt="Prefiling overlap of DeepSeek-V3"
class="gallery-image"
data-flex-grow="209"
data-flex-basis="502px"
>&lt;/p>
&lt;p>作者还探索了 dynamic redundancy 策略，即每个 GPU host 更多的专家（比如 16 个）然后 inference 的每一步仅激活其中的 9 个，在进行 all-to-all operation 时，作者首先进行 global optimal routing 来计算最合适的专家。作者认为 prefilling 计算量很大，因此 routing 的计算量可以忽略不计&lt;/p>
&lt;h4 id="decoding">&lt;a href="#decoding" class="header-anchor">&lt;/a>Decoding
&lt;/h4>&lt;p>在 decoding 阶段，作者在 40 个节点 320 GPU 上进行部署，并行策略如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>模块&lt;/th>
&lt;th>并行策略&lt;/th>
&lt;th>说明&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Attention&lt;/td>
&lt;td>4-way TP, SP, 80-way DP&lt;/td>
&lt;td>SP 用于长文本处理&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MoE&lt;/td>
&lt;td>320-way EP&lt;/td>
&lt;td>每个 GPU 包含 8 个专家&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>这个阶段，每个 GPU 只 host 一个专家，64 个 GPU 负责 host redundant expert 以及 shared expert. 作者通过在 IB 上直接进行 P2P 的传输来降低通信的开销，作者还是用了 IBGDA 来进一步最小化 latency 以及提高通信效率&lt;/p>
&lt;p>在这个阶段，attention 的计算占据了大部分时间，因此，作者采取了如下的 overlap 策略&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3/DeepSeek-V3-decoding-overlap.png"
width="896"
height="308"
loading="lazy"
alt="Decoding overlap of DeepSeek-V3"
class="gallery-image"
data-flex-grow="290"
data-flex-basis="698px"
>&lt;/p>
&lt;p>由于 decoding 阶段一般 batch size 比较小，因此整个 decoding 的瓶颈在于 memory access, 因此为了避免影响 attention 的计算效率，作者仅安排一小部分 SM 用于 dispatch-MoE-combine.&lt;/p>
&lt;h3 id="suggestions-on-hardware-design">&lt;a href="#suggestions-on-hardware-design" class="header-anchor">&lt;/a>Suggestions on Hardware Design
&lt;/h3>&lt;h4 id="communication-hardware">&lt;a href="#communication-hardware" class="header-anchor">&lt;/a>Communication Hardware
&lt;/h4>&lt;p>尽管作者将计算与通信重叠来提高训练效率，但是已有的通信仍然依赖于 SM, 这样限制了计算的效率。作者希望工能够构建专门针对通信的设备，另一方面，作者希望统一 IB 和 NVLink 来降低实现难度&lt;/p>
&lt;h4 id="computation-hardware">&lt;a href="#computation-hardware" class="header-anchor">&lt;/a>Computation Hardware
&lt;/h4>&lt;ol>
&lt;li>提升 FP8 GEMM 的累加精度，当前精度只有 14bit, 作者希望能够在 GPU 设计上进行改进，将这个精度提升到 34-bit&lt;/li>
&lt;li>支持 tile 以及 block 层面的 quantization, 尽管前文作者已经设计出了改进的算法，但是 Tensor Core 以及 CUDA 之前平凡的数据移动降低了计算效率，作者希望未来能够支持细粒度的 quantization&lt;/li>
&lt;li>online quantization, 作者希望将 FP8 cast 以及 TMA 结合在一起，从而 quantization 可以在传输的时候完成计算，减少了内存读写。作者还建议使用 warp-level cast instruction&lt;/li>
&lt;li>Transposed GEMM operations. 本文中，矩阵被切分为不同的 tiles, 在计算的时候需要先加载这些 tiles 然后进行 dequantization, transpose 等操作，作者希望未来能够直接支持 transposed reads of matrices&lt;/li>
&lt;/ol>
&lt;h2 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h2>&lt;h3 id="pre-training-data">&lt;a href="#pre-training-data" class="header-anchor">&lt;/a>Pre-training Data
&lt;/h3>&lt;p>相比于 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v2/" target="_blank" rel="noopener"
>DeepSeek-V2&lt;/a>, DeepSeek-V3 提升了数学和代码数据的比例，以及增加了多语种数据。最终训练数据一共包括 &lt;strong>14.8T&lt;/strong>&lt;/p>
&lt;p>作者还使用了 DeepSeekCoder-V2 里应用的 Fill in the middle 策略来让模型基于上下文越策中间的文本，对应的数据格式如下&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;lt;|fim_begin|&amp;gt;f_pre&amp;lt;|fim_hole|&amp;gt;f_suf&amp;lt;|fim_hole|&amp;gt;f_middle&amp;lt;|fim_end|&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>这个结构与 sequence packing 结合在一起。&lt;/p>
&lt;p>Tokenizer 基于 BBPE, 大小为 128K tokens. 在训练时，作者将随机一部分 combine token 进行切分来减少 token boundary bias 问题&lt;/p>
&lt;h3 id="hyper-parameters">&lt;a href="#hyper-parameters" class="header-anchor">&lt;/a>Hyper-parameters
&lt;/h3>&lt;p>模型参数如下表所示，最终 DeepSeek-V3 拥有 671B 总参数，激活参数为 37B&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>variable&lt;/th>
&lt;th>notation&lt;/th>
&lt;th>value&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>layers&lt;/td>
&lt;td>$\ell$&lt;/td>
&lt;td>61&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>dense layers&lt;/td>
&lt;td>-&lt;/td>
&lt;td>3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>hidden dimension&lt;/td>
&lt;td>$d$&lt;/td>
&lt;td>7168&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>num of attention heads&lt;/td>
&lt;td>$n_h$&lt;/td>
&lt;td>128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>head dimension&lt;/td>
&lt;td>$d_h$&lt;/td>
&lt;td>128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>KV compression dimension&lt;/td>
&lt;td>$d_c$&lt;/td>
&lt;td>512&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>query compression dimension&lt;/td>
&lt;td>$d_c'$&lt;/td>
&lt;td>1536&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>decouple query and key dimension&lt;/td>
&lt;td>$d_h^R$&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>routed expert&lt;/td>
&lt;td>$N_r$&lt;/td>
&lt;td>256&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>shared expert&lt;/td>
&lt;td>$N_s$&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MoE hidden dimension&lt;/td>
&lt;td>$d_{MoE}$&lt;/td>
&lt;td>2048&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>activated experts&lt;/td>
&lt;td>$K$&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>limited node routing&lt;/td>
&lt;td>$M$&lt;/td>
&lt;td>4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MTP depth&lt;/td>
&lt;td>$D$&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>训练时，作者使用了 learning rate scheduling, batch size scheduling 等方法。对于 PP, routed expert 会均匀分布为 8node 对应的 64 个 GPU 上，预训练时模型的上下文长度为 4k&lt;/p>
&lt;h3 id="long-context-extension">&lt;a href="#long-context-extension" class="header-anchor">&lt;/a>Long Context Extension
&lt;/h3>&lt;p>作者使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-yarn/" target="_blank" rel="noopener"
>YARN&lt;/a> 来扩展模型的上下文长度，模型上下文长度经过两个额外的训练阶段从 4K 扩展到 32K 再扩展到 128K, 均训练了 1000 步。&lt;/p>
&lt;p>YARN 配置与 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v2/" target="_blank" rel="noopener"
>DeepSeek-V2&lt;/a> 基本一致，在额外的 decoupled query and key 上作者没有应用这一点。参数配置如下表&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>parameter&lt;/th>
&lt;th>$s$&lt;/th>
&lt;th>$\alpha$&lt;/th>
&lt;th>$\beta$&lt;/th>
&lt;th>$\sqrt{t}$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>value&lt;/td>
&lt;td>40&lt;/td>
&lt;td>1&lt;/td>
&lt;td>32&lt;/td>
&lt;td>$0.1\ln s+1$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>第一阶段的上下文长度为 32K, batch size 为 1920, 第二个阶段的上下文长度为 128K, batch size 为 480.&lt;/p>
&lt;h3 id="performance">&lt;a href="#performance" class="header-anchor">&lt;/a>Performance
&lt;/h3>&lt;p>DeepSeek-V3 base 的表现下图所示，作者对比了 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v2/" target="_blank" rel="noopener"
>DeepSeek-V2&lt;/a>, &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5/" target="_blank" rel="noopener"
>Qwen2.5&lt;/a>, &lt;a class="link" href="LLaMA%203.1" >LLaMA 3.1&lt;/a>&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3/DeepSeek-V3-base-performance.png"
width="1065"
height="1083"
loading="lazy"
alt="Performance of DeepSeek-V3 base"
class="gallery-image"
data-flex-grow="98"
data-flex-basis="236px"
>&lt;/p>
&lt;h3 id="discussion">&lt;a href="#discussion" class="header-anchor">&lt;/a>Discussion
&lt;/h3>&lt;p>作者首先验证了 MTP 的有效性，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3/DeepSeek-V3-ablation-MTP.png"
width="979"
height="451"
loading="lazy"
alt="Ablation study on MTP"
class="gallery-image"
data-flex-grow="217"
data-flex-basis="520px"
>&lt;/p>
&lt;p>可以看到，在模型架构相同，训练数据相同的情况下，1-MTP 的效果超过了 baseline 的表现，说明了 MTP 策略的有效性&lt;/p>
&lt;p>接下来，作者还验证了 &lt;a class="link" href="https://maosong.website/p/notes-on-loss-free-balancing/" target="_blank" rel="noopener"
>Loss-Free Balancing&lt;/a> 的有效性，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3/DeepSeek-V3-ablation-loss-free-balancing.png"
width="987"
height="458"
loading="lazy"
alt="Ablation on loss-free balancing"
class="gallery-image"
data-flex-grow="215"
data-flex-basis="517px"
>&lt;/p>
&lt;p>可以看到，loss-free Balancing 的效果比 loss balancing 的效果更好&lt;/p>
&lt;p>接下来，作者对比了 loss-free balancing 和 sequence-wise auxiliary loss, 即 batch-wise v.s. sequence-wise. 作者认为，前者的约束更灵活，因为其不要求 in-domain balance. 作者在测试集上进行可视化，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3/DeepSeek-V3-ablation-batch-wise.png"
width="1106"
height="522"
loading="lazy"
alt="Ablation study on batch-wise load balancing"
class="gallery-image"
data-flex-grow="211"
data-flex-basis="508px"
>&lt;/p>
&lt;p>实验结果发现，loss-free 策略对应的 expert specialization 更强。作者进一步设计了一个 batch-wise auxiliary loss 来实现 batch-wise load balance, 结果发现，这种策略也能达到和 loss-free balancing 一样的效果，这说明了 batch-wise load balancing 效果更好&lt;/p>
&lt;p>最后，作者提了两点 loss-free 策略的问题：&lt;/p>
&lt;ol>
&lt;li>在特定的 sequence 或者小 batch 里出现负载不均衡。作者通通过增大 batch size 来解决这个问题&lt;/li>
&lt;li>在推理阶段因为 domain-shift 导致的负载不均衡。作者实现了一个基于 redundant expert 的推理框架来解决这个问题&lt;/li>
&lt;/ol>
&lt;h2 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h2>&lt;h3 id="sft">&lt;a href="#sft" class="header-anchor">&lt;/a>SFT
&lt;/h3>&lt;p>post-training 包含 1.5M 样本，数据包括 reasoning 数据以及 non-reasoning 数据，前者由 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-r1/" target="_blank" rel="noopener"
>DeepSeek-R1&lt;/a> 合成，后者由 DeepSeek-V2.5 合成&lt;/p>
&lt;p>SFT 时，作者训练了两个 epoch, 使用了 sequence packing 技巧&lt;/p>
&lt;h3 id="rl">&lt;a href="#rl" class="header-anchor">&lt;/a>RL
&lt;/h3>&lt;p>Reward model 包含 rule-based reward model 和 model-based reward model.&lt;/p>
&lt;p>RL 训练使用的算法为 GRPO&lt;/p>
&lt;h3 id="post-training-performance">&lt;a href="#post-training-performance" class="header-anchor">&lt;/a>Post-training Performance
&lt;/h3>&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3/DeepSeek-V3-instruct-performance.png"
width="1112"
height="813"
loading="lazy"
alt="Performance of DeepSeek-V3"
class="gallery-image"
data-flex-grow="136"
data-flex-basis="328px"
>&lt;/p>
&lt;h3 id="post-training-discussion">&lt;a href="#post-training-discussion" class="header-anchor">&lt;/a>Post-training Discussion
&lt;/h3>&lt;p>作者首先探究了 Distillation 对模型表现的影响，作者使用 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-r1/" target="_blank" rel="noopener"
>DeepSeek-R1&lt;/a> 来蒸馏 DeepSeek-V2.5, 结果如下图所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>LiveCodeBench-CoT&lt;/th>
&lt;th>&lt;/th>
&lt;th>MATH-500&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Pass@1&lt;/td>
&lt;td>Length&lt;/td>
&lt;td>Pass@1&lt;/td>
&lt;td>Length&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DeepSeek-V2.5 Baseline&lt;/td>
&lt;td>31.1&lt;/td>
&lt;td>718&lt;/td>
&lt;td>74.6&lt;/td>
&lt;td>769&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DeepSeek-V2.5 +R1 Distill&lt;/td>
&lt;td>37.4&lt;/td>
&lt;td>783&lt;/td>
&lt;td>83.2&lt;/td>
&lt;td>1510&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，distillation 可以显著提高模型的表现。但是其问题在于也会让模型输出的长度增加。作者认为知识蒸馏是 post-training optimization 的一个重要方向。&lt;/p>
&lt;p>接下来，作者讨论了 self-rewarding, 具体做法就是使用 DeepSeek-V3 来对评估结果进行投票，结果发现最终的效果很好，作者认为 LLM 可以很好地将非结构化信息转换为 rewards.&lt;/p>
&lt;p>最后，作者讨论了 MTP. MTP 可以于 speculative decoding 结合，进一步提高 decoding 的速度。通过实验作者发现，second token prediction 的接受率在 $85\%\sim 90\%$ 之间，说明了其有效性。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>在本文中，作者提出了 DeepSeek-V3, 一个 671B-A37B 的 MoE 大语言模型，训练 token 数为 &lt;strong>14.8T&lt;/strong>. 作者使用了 loss-free-balancing strategy 来实现负载均衡。训练时作者使用了 FP8 混合精度。评估发现 DeepSeek-V3 的达到了 SOTA 表现。&lt;/p>
&lt;p>作者认为，DeepSeek-V3 model size 太大，不适合部署。第二，部署策略需要进一步改进。&lt;/p>
&lt;p>最后，作者认为未来工作有以下几点：&lt;/p>
&lt;ol>
&lt;li>改进模型架构，进一步提高训练以及推理效率，扩展模型的上下文&lt;/li>
&lt;li>提升训练数据的数量和质量&lt;/li>
&lt;li>提高模型的 reasoning 能力&lt;/li>
&lt;li>更详尽的评估&lt;/li>
&lt;/ol>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2412.19437" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Gemini2.5</title><link>https://maosong.website/p/notes-on-gemini2.5/</link><pubDate>Sat, 06 Dec 2025 18:14:15 +0800</pubDate><guid>https://maosong.website/p/notes-on-gemini2.5/</guid><description>&lt;h2 id="介绍">&lt;a href="#%e4%bb%8b%e7%bb%8d" class="header-anchor">&lt;/a>介绍
&lt;/h2>&lt;p>DeepMind 在 6 月 17 号发布了 Gemini2.x 系列的技术报告，包括&lt;/p>
&lt;ul>
&lt;li>Gemini 2.5 Pro&lt;/li>
&lt;li>Gemini 2.5 Flash&lt;/li>
&lt;li>Gemini 2.0 Flash (earlier)&lt;/li>
&lt;li>Gemini 2.0 Flash-Lite (earlier)&lt;/li>
&lt;/ul>
&lt;p>技术报告简单说了一些技术细节，主要还是模型的评估&lt;/p>
&lt;blockquote>
&lt;p>注：Gemini 2.0 Flash 和 Gemini 2.0 Flash-Lite 将要被 Gemini 2.5 Flash 和 Gemini 2.5 Flash-Lite 取缔，见最新的 blog&lt;/p>
&lt;/blockquote>
&lt;p>Gemini2.x 系列亮点：&lt;/p>
&lt;ol>
&lt;li>领先的 coding 和 reasoning 能力&lt;/li>
&lt;li>超过 1M 的上下文，可以处理超过 3 个小时的 video&lt;/li>
&lt;li>集成 long context, multimodal 和 reasoning 三种能力的 agentic workflow 能力&lt;/li>
&lt;/ol>
&lt;p>模型能力对比&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Gemini 1.5 Flash&lt;/th>
&lt;th>Gemini 1.5 Pro&lt;/th>
&lt;th>Gemini 2.0 Flash-Lite&lt;/th>
&lt;th>Gemini 2.0 Flash&lt;/th>
&lt;th>Gemini 2.5 Flash&lt;/th>
&lt;th>Gemini 2.5 Pro&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Input Modalities&lt;/td>
&lt;td>Text, Image, Video, Audio&lt;/td>
&lt;td>Text, Image, Video, Audio&lt;/td>
&lt;td>Text, Image, Video, Audio&lt;/td>
&lt;td>Text, Image, Video, Audio&lt;/td>
&lt;td>Text, Image, Video, Audio&lt;/td>
&lt;td>Text, Image, Video, Audio&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Input length&lt;/td>
&lt;td>1M&lt;/td>
&lt;td>2M&lt;/td>
&lt;td>1M&lt;/td>
&lt;td>1M&lt;/td>
&lt;td>1M&lt;/td>
&lt;td>1M&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Output modalities&lt;/td>
&lt;td>Text&lt;/td>
&lt;td>Text&lt;/td>
&lt;td>Text&lt;/td>
&lt;td>Text, Image*&lt;/td>
&lt;td>Text, Audio*&lt;/td>
&lt;td>Text, Audio&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Output length&lt;/td>
&lt;td>8K&lt;/td>
&lt;td>8K&lt;/td>
&lt;td>8K&lt;/td>
&lt;td>8K&lt;/td>
&lt;td>64K&lt;/td>
&lt;td>64K&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Thinking&lt;/td>
&lt;td>No&lt;/td>
&lt;td>No&lt;/td>
&lt;td>No&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>Dynamic&lt;/td>
&lt;td>Dynamic&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Supports tool use?&lt;/td>
&lt;td>No&lt;/td>
&lt;td>No&lt;/td>
&lt;td>No&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Knowledge cutoff&lt;/td>
&lt;td>November 2023&lt;/td>
&lt;td>November 2023&lt;/td>
&lt;td>June 2024&lt;/td>
&lt;td>June 2024&lt;/td>
&lt;td>January 2025&lt;/td>
&lt;td>January 2025&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>模型场景使用对比&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Gemini 2.5 Flash-Lite&lt;/th>
&lt;th>Gemini 2.5 Flash&lt;/th>
&lt;th>Gemini 2.5&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Thinking&lt;/td>
&lt;td>No&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>使用场景&lt;/td>
&lt;td>大规模调用&lt;/td>
&lt;td>日常使用&lt;/td>
&lt;td>coding 或者 reasoning 人物&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>速度&lt;/td>
&lt;td>非常快&lt;/td>
&lt;td>快&lt;/td>
&lt;td>一半&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>表现&lt;/td>
&lt;td>一半&lt;/td>
&lt;td>强&lt;/td>
&lt;td>非常强&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>输入价格&lt;/td>
&lt;td>0.1&lt;/td>
&lt;td>0.3&lt;/td>
&lt;td>1.25&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>输出价格&lt;/td>
&lt;td>0.4&lt;/td>
&lt;td>2.5&lt;/td>
&lt;td>10&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>模型表现&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gemini2.5/Gemini_2_5_performance.png"
width="3840"
height="2160"
loading="lazy"
alt="Gemini_2_5_performance"
class="gallery-image"
data-flex-grow="177"
data-flex-basis="426px"
>&lt;/p>
&lt;p>模型吞吐量对比&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gemini2.5/Gemini_2_5_throughput.png"
width="2156"
height="960"
loading="lazy"
alt="Gemini_2_5_throughput"
class="gallery-image"
data-flex-grow="224"
data-flex-basis="539px"
>&lt;/p>
&lt;h2 id="架构数据与训练">&lt;a href="#%e6%9e%b6%e6%9e%84%e6%95%b0%e6%8d%ae%e4%b8%8e%e8%ae%ad%e7%bb%83" class="header-anchor">&lt;/a>架构，数据与训练
&lt;/h2>&lt;h3 id="架构">&lt;a href="#%e6%9e%b6%e6%9e%84" class="header-anchor">&lt;/a>架构
&lt;/h3>&lt;p>Gemini2.5 是一个&lt;strong>基于 MoE 的 transformer 架构&lt;/strong>，支持 text, vision, audio 模态&lt;/p>
&lt;p>Flash 系列使用的是知识蒸馏的方法训练得到的，训练时使用了 $k$-sparse 的策略，也就是只保留教师模型输出概率最高的 $k$ 的词以及对应的概率。作者认为知识蒸馏可以有效提高小模型的能力。&lt;/p>
&lt;h3 id="infra">&lt;a href="#infra" class="header-anchor">&lt;/a>Infra
&lt;/h3>&lt;p>Gemini 系列在 TPUv5p 的架构上进行训练。作者主要提了两点：&lt;/p>
&lt;ol>
&lt;li>Slice-Granularity Elasticity：可以在部分 TPU 出现故障时快速切换并继续训练&lt;/li>
&lt;li>Split-Phase SDC detection：通过轻量级重放和校验机制，在几分钟内就能识别出有问题的硬件设备&lt;/li>
&lt;/ol>
&lt;h3 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h3>&lt;p>post-training 包含 SFT，reward model 以及 RL 的训练。&lt;/p>
&lt;p>在 RL 阶段，奖励来自 verifiable rewards 和 model-based generative rewards&lt;/p>
&lt;h3 id="能力提升">&lt;a href="#%e8%83%bd%e5%8a%9b%e6%8f%90%e5%8d%87" class="header-anchor">&lt;/a>能力提升
&lt;/h3>&lt;p>技术报告提到了几个方面能力的提升&lt;/p>
&lt;p>&lt;strong>code&lt;/strong>
pre-training 阶段，加入了大量的代码数据，作者还评估了代码数据的质量
post-training 阶段，作者基于 reasoning 能力构建了一系列的工程任务，来提高模型解决问题的能力&lt;/p>
&lt;p>&lt;strong>Factuality&lt;/strong>
通过 search 和 tool use，reason about output 以及 issue follow-up queries 来验证 factual accuracy&lt;/p>
&lt;p>&lt;strong>Multilinguality&lt;/strong>
预训练时使用了 400 多种语言的语料进行训练&lt;/p>
&lt;p>&lt;strong>Audio&lt;/strong>
训练模型完成 audio generation 任务，生成的时候使用了&lt;strong>causal audio representation&lt;/strong>，训练数据覆盖了 200 多种语言&lt;/p>
&lt;p>&lt;strong>Video&lt;/strong>
通过降低每帧视频对应的 visual token 个数（258-&amp;gt; 66），来让模型可以处理 3 个小时的视频&lt;/p>
&lt;h2 id="evaluation">&lt;a href="#evaluation" class="header-anchor">&lt;/a>Evaluation
&lt;/h2>&lt;p>对比了 Claude_4, o3, &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-r1/" target="_blank" rel="noopener"
>DeepSeek-R1&lt;/a> 和 Grok-1&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gemini2.5/Gemini_2_5_evaluation.png"
width="1754"
height="1174"
loading="lazy"
alt="Gemini_2_5_evaluation"
class="gallery-image"
data-flex-grow="149"
data-flex-basis="358px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gemini2.5/Gemini_2_5_video_understanding_performance.png"
width="1724"
height="988"
loading="lazy"
alt="Gemini_2_5_video_understanding_performance"
class="gallery-image"
data-flex-grow="174"
data-flex-basis="418px"
>&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>结论里作者主要提到了两点&lt;/p>
&lt;ol>
&lt;li>模型能力的提升已经超过了 benchmark 的构建速度和成本&lt;/li>
&lt;li>未来如何设计经济的，覆盖广的，能动态调整难度的 benchmark 是一个关键问题&lt;/li>
&lt;/ol>
&lt;p>技术报告中作者还提到了 Gemini Plays Pokemon 的 case study，作者提到了两点问题：&lt;/p>
&lt;ol>
&lt;li>作者发模型对视觉信息的依赖程度并不是很高&lt;/li>
&lt;li>尽管模型上下文长度超过了 1M，但是对于这种复杂的 long horizon 问题，当输入超过了 100K token 之后，模型倾向于重复过去的行为，而不是生成新的计划
因此，未来如何解决 multi-turn, long-horizon 的 agentic task 也是一个值得探究的方向。&lt;/li>
&lt;/ol>
&lt;h2 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://blog.google/products/gemini/gemini-2-5-model-family-expands/" target="_blank" rel="noopener"
>Gemini 2.5 Flash/Flash Lite&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://blog.google/products/gemini/gemini-2-5-model-family-expands/" target="_blank" rel="noopener"
>Gemini 2.5 Technical Report&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on olmoe</title><link>https://maosong.website/p/notes-on-olmoe/</link><pubDate>Sat, 06 Dec 2025 18:08:11 +0800</pubDate><guid>https://maosong.website/p/notes-on-olmoe/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者在本文中主要想完成三个目标：&lt;/p>
&lt;ol>
&lt;li>基于已有框架尝试训练一个 MoE 大语言模型&lt;/li>
&lt;li>详细分析 MoE 模型的 routing 机制&lt;/li>
&lt;li>为后续 MoE 模型开发提供经验&lt;/li>
&lt;/ol>
&lt;p>基于这三个目标，作者在本文中发布了 4 个 MoE 模型。预训练时，作者使用了 $52.25\%$ 的代码数据来提高模型的表现。作者还是用了 UL2 作为训练目标。&lt;/p>
&lt;p>通过实验作者发现 MoE 机制存在以下三个性质：&lt;/p>
&lt;ol>
&lt;li>Context-independent Specialization: 即 MoE 模型倾向于基于 token semantics 进行聚类，而不是 context&lt;/li>
&lt;li>early routing learning: routing 的分布情况在训练早期就已经确定了&lt;/li>
&lt;li>drop towards the end: 使用 token dropping 策略之后，越往后的的 token 被丢弃掉的概率也就越大&lt;/li>
&lt;/ol>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="model">&lt;a href="#model" class="header-anchor">&lt;/a>Model
&lt;/h3>&lt;p>作者介绍了模型使用的数据集如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-olmoe/OpenMoE-data-mixture.png"
width="938"
height="492"
loading="lazy"
alt="data mixture of OpenMoE"
class="gallery-image"
data-flex-grow="190"
data-flex-basis="457px"
>&lt;/p>
&lt;p>模型的 tokenizer 基于 umT5, 大小为 256K, umT5 支持多语种，并且还有 fallback 机制来处理 OOV 的 token&lt;/p>
&lt;p>模型配置如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>OpenMoE-Base/16E&lt;/th>
&lt;th>OpenMoE-8B/32E&lt;/th>
&lt;th>OpenMoE-34B/32E&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>total params&lt;/td>
&lt;td>650M&lt;/td>
&lt;td>8.7B&lt;/td>
&lt;td>34B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>activated params&lt;/td>
&lt;td>142M&lt;/td>
&lt;td>2.1B&lt;/td>
&lt;td>6B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>total experts&lt;/td>
&lt;td>16&lt;/td>
&lt;td>32&lt;/td>
&lt;td>32&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>activated experts&lt;/td>
&lt;td>2&lt;/td>
&lt;td>2&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>shared experts&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Layout&lt;/td>
&lt;td>every 4&lt;/td>
&lt;td>every 6&lt;/td>
&lt;td>every 4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>hidden_dim&lt;/code>&lt;/td>
&lt;td>768&lt;/td>
&lt;td>2048&lt;/td>
&lt;td>3072&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>ffn_hidden_dim&lt;/code>&lt;/td>
&lt;td>3072&lt;/td>
&lt;td>8192&lt;/td>
&lt;td>12288&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>num_heads&lt;/code>&lt;/td>
&lt;td>12&lt;/td>
&lt;td>24&lt;/td>
&lt;td>24&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>head_dim&lt;/code>&lt;/td>
&lt;td>64&lt;/td>
&lt;td>128&lt;/td>
&lt;td>128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>num_layers&lt;/code>&lt;/td>
&lt;td>12&lt;/td>
&lt;td>24&lt;/td>
&lt;td>32&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="training">&lt;a href="#training" class="header-anchor">&lt;/a>Training
&lt;/h3>&lt;p>作者使用了 &lt;a class="link" href="https://maosong.website/p/load-balancing-tutorial/" target="_blank" rel="noopener"
>Load Balancing loss&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/st-moe/" target="_blank" rel="noopener"
>ST-MoE&lt;/a> 提出的 Router Z-loss, 最终训练的目标函数与 &lt;a class="link" href="https://maosong.website/p/st-moe/" target="_blank" rel="noopener"
>ST-MoE&lt;/a> 一致&lt;/p>
&lt;p>作者在训练时，还是用了 UL2, UL2 结合了 mask language modeling 和 casual language modeling 两种训练方式。&lt;/p>
&lt;p>作者首先对各个实验配置进行了验证，其中关键接论文模型很容易在 code data 上达到比较高的准确率以及比较低的 loss, 作者认为这是因为 code data 中存在大量的特殊符号。&lt;/p>
&lt;p>训练过程中，作者发现模型在训练一定步数之后，容易出现过饱和现象，因此作者将训练目标函数由 UL2 降为 CasualLM, 并未作者还将代码数据的比例降低到 $15\%$.&lt;/p>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;h3 id="analysis-openmoe">&lt;a href="#analysis-openmoe" class="header-anchor">&lt;/a>Analysis OpenMoE
&lt;/h3>&lt;ol>
&lt;li>作者发现，大部分专家对于不同的 domain 没有出现 specialization 情况，对于 math domain, specialization 现象比较明显，作者认为这是因为 math domain 包含更多的 special tokens&lt;/li>
&lt;li>对于不同的语言，有部分专家出现 specialization 现象&lt;/li>
&lt;li>部分专家对于 position ID 有 specialization 现象，并且连续的 token 更偏好相同的专家&lt;/li>
&lt;li>作者还发现，部分专家对于 Token ID 有 specialization 现象，作者将这种现象称为 &lt;strong>Context-independent Specialization&lt;/strong>.&lt;/li>
&lt;li>专家还会对语义相似的 token 进行聚类，并且这种聚类在训练早期就已经发生，作者认为其原因在于重新分配 token 会增加最终的 loss&lt;/li>
&lt;li>对于 token dropping, 作者发现月考后的 token, 其被 drop 的概率比例也越高。并且对于指令跟随数据，更多的 token 都会被丢掉，因此作者认为指令跟随数据是 MoE 模型的一种 OOD 数据&lt;/li>
&lt;/ol>
&lt;h3 id="analysis-on-other-moe-models">&lt;a href="#analysis-on-other-moe-models" class="header-anchor">&lt;/a>Analysis on other MoE Models
&lt;/h3>&lt;p>作者还分析了 &lt;a class="link" href="https://maosong.website/p/mixstral-8x7b/" target="_blank" rel="noopener"
>Mixtral MoE&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseekmoe/" target="_blank" rel="noopener"
>DeepSeekMoE&lt;/a> 两个模型的专家路由情况。这两个模型采用了 dropless token routing 的机制。&lt;/p>
&lt;p>首先，作者分析了这两个模型对于 TokenID 的敏感性，结果发现，DeepSeek-MoE 的 specialization 现象比较明显，而 Mixtral MoE 由于使用了 upcycling, 其 specialization 现象不太明显，作者认为这是因为 upcycling 导致每个专家的权重都差不多。最终，作者认为对于 training from stratch 的 MoE model 这个 specialization 现象更明显。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中提出了一个全开源的 MoE 大模型系列，包括 0.6B, 8.7B 和 34B 三个 size, 作者还对 MoE 中的 routing 进行了详细的分析，结果发现【【】】，这些发现有助于后续的 MoE 模型架构的研究。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2402.01739" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Qwen3 VL</title><link>https://maosong.website/p/notes-on-qwen3-vl/</link><pubDate>Fri, 05 Dec 2025 10:12:01 +0800</pubDate><guid>https://maosong.website/p/notes-on-qwen3-vl/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者在本文中提出了 Qwen3-VL 系列多模态大模型，包括 4 个 dense 模型和两个 MoE 模型，模型的上下文长度为 256K, 通过数据和训练上的优化，作者保持了模型的纯文本能力。最终 Qwen3-VL 包括 non-thinking 和 thinking variants.&lt;/p>
&lt;p>在架构上，Qwen3-VL 进行了三点改进：&lt;/p>
&lt;ol>
&lt;li>Interleaved MRoPE: 作者解决了 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5-vl/" target="_blank" rel="noopener"
>Qwen2.5-VL&lt;/a> 提出的 MRoPE 在长视频理解场景下的频谱不平衡问题&lt;/li>
&lt;li>DeepStack: 作者使用了 DeepStack 来提取 &lt;a class="link" href="https://maosong.website/p/notes-on-vit/" target="_blank" rel="noopener"
>ViT&lt;/a> 不同 layer 的视觉特征&lt;/li>
&lt;li>Explicit Video timestamps: 作者使用了绝对时间来标记 frame 来提供更直接的时间信息&lt;/li>
&lt;/ol>
&lt;p>在数据上，作者使用了 image caption, OCR, grounding, spatial reasoning, code, long documents 以及 temporally grounded video 等数据，作者 还是用了 GUI-agent interaction 数据来提高模型的 action 能力&lt;/p>
&lt;p>在训练上，Qwen3-VL 包含两个大的阶段：pre-training 和 post-traing, pre-training 包含 4 个小阶段，post-training 包含 3 个阶段。&lt;/p>
&lt;h2 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h2>&lt;p>Qwen3-VL 的架构如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen3-vl/Qwen3-VL-Architecture.png"
width="5908"
height="3413"
loading="lazy"
alt="Architecture of Qwen3-VL"
class="gallery-image"
data-flex-grow="173"
data-flex-basis="415px"
>&lt;/p>
&lt;p>其中，&lt;/p>
&lt;ul>
&lt;li>LLM: LLM 使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a> 系列大语言模型，包括 2B, 4B, 8B, 32B 四个 dense model 以及 30B-A3B, 235B-A22B 两个 moe 模型&lt;/li>
&lt;li>Vision Encoder: encoder 基于 [[SigLip-2]] 初始化，然后使用了 dynamic input resolutions 进行 continue training, 作者使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-comp/" target="_blank" rel="noopener"
>CoMP&lt;/a> 提出的 2D-RoPE 以及 interpolate absolute position embedding, 最终包括 SigLip2-SO-400M 和 SigLip-Large (300M) 两个 size, 后者用于 2B 和 4B 两个 size&lt;/li>
&lt;li>Patch Merger: 一个 2 层的 MLP, 将四个 visual token 压缩为 1 个&lt;/li>
&lt;/ul>
&lt;h3 id="interleaved-mrope">&lt;a href="#interleaved-mrope" class="header-anchor">&lt;/a>Interleaved MRoPE
&lt;/h3>&lt;p>这部分介绍见 [[MRoPE-Interleave]]&lt;/p>
&lt;h3 id="deepstack">&lt;a href="#deepstack" class="header-anchor">&lt;/a>DeepStack
&lt;/h3>&lt;p>受 Deepstack 启发，作者从 vision encoder 的中间层（具体来说是第 8， 16， 24 层）提取对应的视觉特征，然后经过 MLP 与 LLM 对应 layer 的视觉 token 直接进行相加。&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen3-vl/Qwen3-VL-DeepStack.png"
width="1723"
height="1024"
loading="lazy"
alt="architecture of DeepStack"
class="gallery-image"
data-flex-grow="168"
data-flex-basis="403px"
>&lt;/p>
&lt;h3 id="video-timestamp">&lt;a href="#video-timestamp" class="header-anchor">&lt;/a>Video Timestamp
&lt;/h3>&lt;p>作者发现，&lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5-vl/" target="_blank" rel="noopener"
>Qwen2.5-VL&lt;/a> 中使用的 MRoPE 存在如下问题：&lt;/p>
&lt;ol>
&lt;li>将 temporal position 与绝对时间绑定之后，对于长视频会产生非常大且稀疏的 temporal position ids&lt;/li>
&lt;li>需要使用不同的 FPS 进行采样来提高模型的泛化性&lt;/li>
&lt;/ol>
&lt;p>为了解决这个问题，作者使用了一个 textual token-based time encoding strategy, 其中每个 video temporal patch 对应的 timestamp 表示为 &lt;code>&amp;lt;3.0 seconds&amp;gt;&lt;/code>, 这样视频会被处理为以下格式&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;lt;0.0 seconds&amp;gt; &amp;lt;video token&amp;gt; &amp;lt;video token&amp;gt; ... &amp;lt;4.0 seconds&amp;gt; &amp;lt;video token&amp;gt; &amp;lt;video token&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>在训练时，作者还使用了 seconds 以及 HMS 两种格式来提高模型对于不同格式的泛化能力。作者认为，虽然这种表示会提高上下文长度，但是也能够提高模型 video grounding 或者 dense captioning 等时序信息敏感任务的表现&lt;/p>
&lt;h2 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h2>&lt;h3 id="training-recipe">&lt;a href="#training-recipe" class="header-anchor">&lt;/a>Training Recipe
&lt;/h3>&lt;p>预训练阶段包含 4 个阶段，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen3-vl/Qwen3-VL-pre-training-recipe.png"
width="1209"
height="216"
loading="lazy"
alt="Qwen3-VL pretraining recipe"
class="gallery-image"
data-flex-grow="559"
data-flex-basis="1343px"
>&lt;/p>
&lt;ul>
&lt;li>Stage 0: 这一阶段的目的是对齐视觉特征和文本特征，只训练 Patch merger, 训练使用了 67B token, 覆盖 image-caption, knowledge, OCR 数据，上下文长度为 8192&lt;/li>
&lt;li>Stage 1: 这一阶段所有参数都参加训练，训练使用了 1Ttoken, 作者在训练是加入了纯文本数据，最终数据包含 interleaved image-text, visual grounding, VQA, STEM, video 数据，上下文长度为 8192&lt;/li>
&lt;li>Stage 2: 这一阶段的目的是扩展模型的上下文长度到 32K, 训练使用了 1T token, 数据包括长视频以及 agent-oriented instruction-following 数据&lt;/li>
&lt;li>Stage 3: 这一阶段的目的是将模型的上下文长度进一步扩展到 262K, 训练使用了 100B token. 数据包括长视频以及长文本&lt;/li>
&lt;/ul>
&lt;h3 id="data">&lt;a href="#data" class="header-anchor">&lt;/a>Data
&lt;/h3>&lt;ul>
&lt;li>Image Caption Data: 作者使用了 Qwen2.5-VL 32B 来进行 re-captioning, 然后进行了 de-duplication 以及 clustering 来提高数据的质量和多样性&lt;/li>
&lt;li>Interleaved Text-Image Data: 作者对文档进行分裂，然后使用微调的 Qwen2.5-VL 7B 来进行解析，对于长文本，作者将连续页面拼接在一起。作者使用了对齐以及页数来保证数据的质量&lt;/li>
&lt;li>Knowledge Data: 作者构建了多个类别的数据，然后对这些数据进行 refine&lt;/li>
&lt;li>OCR: 作者构造了 30M 的数据以及 1M 的多语种数据&lt;/li>
&lt;li>Document Parsing Data: 作者从 CC 上收集了 3M PDF 以及处理了自有的 4M 数据，最终数据集里包含合成数据和真实数据；对于长文档理解数据，作者通过将 single-page 数据 merge 在一起得到，然后作者构造了 long document VQA 数据&lt;/li>
&lt;li>Grounding and counting Data: grounding 数据包括 box-based 和 Point-based 两种形式，均从开源数据集收集得到，前者包括 RefCOCO, Object365, 后者包括 PixMo; 对于 Counting, 作者基于 grounding 数据构造了 direct counting, box-based counting 以及 point-based counting 三种形式&lt;/li>
&lt;li>Spatial Understanding: 数据包括 spatial understanding 和 3D grounding 两类数据，前者的数据使用了相对位置关系来提高 spatial reasoning 的 robustness; 后者使用了 Omni3D 来统一数据格式&lt;/li>
&lt;li>Code: 包括 Qwen3, Qwen3-Coder 的纯文本 coding 数据，以及多模态 coding 数据，覆盖了将 UI 截图转换为 HTML/CSS 以及从图片生成 SVG 等任务&lt;/li>
&lt;li>Video: 包括 Dense Caption Synthesis 以及 Spatial-Temporal Video Grounding 两个任务。作者还对不同来源不同长度的数据进行了平衡&lt;/li>
&lt;li>STEM: 作者构造了一个合成数据 pipeline, 合成了 1M point-grounding samples, 2M perception-oriented VQA 数据，最终数据集包含 6M 标注图表数据，覆盖了 STEM 相关学科；对于多模态推理数据，作者收集了 60M 的 K12 以及本科生级别的练习题，作者还合成了 12M 的多模态推理数据。除了多模态推理数据，作者还加入了纯文本推理数据&lt;/li>
&lt;li>Agent: 这部分数据包括 GUI, function calling 以及 Search 三部分， GUI 数据通过数据合成得到，Function calling 数据通过强模型生成轨迹得到，search 数据通过收集执行搜索轨迹得到&lt;/li>
&lt;/ul>
&lt;h2 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h2>&lt;p>Post-training 包含三个阶段：&lt;/p>
&lt;ol>
&lt;li>SFT: 提高模型的指令跟随能力，SFT 又分为了两个小阶段，上下文长度分别为 32K 和 256K, 对于 instruct 和 reasoning 版本，作者设计了不同的数据格式，后者包含 CoT reasoning trace&lt;/li>
&lt;li>Strong-to-Weak Distillation: 提高小模型的能力，这里应该是和 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a> 一样，将大模型的能力蒸馏到小模型里&lt;/li>
&lt;li>RL: 提高模型的 reasoning 能力以及人类偏好对齐。这里包含了 Reasoning RL 以及 General RL 两个阶段，覆盖了 math, OCR, grounding, instruction following 等 domain&lt;/li>
&lt;/ol>
&lt;p>整体的训练 pipeline 我猜测应该是这样：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen3-vl/Qwen3-VL-post-training-pipeline.png"
width="1282"
height="279"
loading="lazy"
alt="Post-training pipeline of Qwen3-VL (guessed)"
class="gallery-image"
data-flex-grow="459"
data-flex-basis="1102px"
>&lt;/p>
&lt;h3 id="code-start-data">&lt;a href="#code-start-data" class="header-anchor">&lt;/a>Code-start Data
&lt;/h3>&lt;p>Code-start Data 分为 SFT 数据和 Long CoT SFT 数据，前者用于训练 instruct 版模型，后者用于训练 reasoning 版模型&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Data&lt;/th>
&lt;th>tasks&lt;/th>
&lt;th>samples&lt;/th>
&lt;th>training&lt;/th>
&lt;th>filtering&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>SFT&lt;/td>
&lt;td>spatial reasoning&lt;br>image-grounded reasoning&lt;br>spatio-temporal grounding&lt;br>long document understanding&lt;/td>
&lt;td>1.2M (1/3 are text-only)&lt;/td>
&lt;td>- stage 1: 32K&lt;br>- stage 2: 256K&lt;/td>
&lt;td>- query &lt;br>- rule-based&lt;br>- model-based&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Long CoT SFT&lt;/td>
&lt;td>VQA, OCR, 2D/3D grounding, &lt;br>video analysis, STEM, agent&lt;/td>
&lt;td>text:multimodal = 1:1&lt;/td>
&lt;td>&lt;/td>
&lt;td>- difficulty&lt;br>- multi-modal&lt;br>- response quality&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="strong-to-weak-distillation">&lt;a href="#strong-to-weak-distillation" class="header-anchor">&lt;/a>Strong-to-Weak Distillation
&lt;/h3>&lt;p>蒸馏过程包括两个阶段：&lt;/p>
&lt;ul>
&lt;li>off-policy Distillation: 使用教师模型的输出进行训练提高模型基本的 reasoning 能力&lt;/li>
&lt;li>On-policy Distillation: 使用教师模型输出的 logit 作为蒸馏信号提高模型的 reasoning 能力&lt;/li>
&lt;/ul>
&lt;h3 id="rl">&lt;a href="#rl" class="header-anchor">&lt;/a>RL
&lt;/h3>&lt;h4 id="reasoning-rl">&lt;a href="#reasoning-rl" class="header-anchor">&lt;/a>Reasoning RL
&lt;/h4>&lt;p>作者收集了 30K 的 RL 数据，然后对通过率超过 90% 的数据进行过滤 (16 responses per query), 对于 reward, 作者构建了一个 unified reward framework 来提供奖励&lt;/p>
&lt;p>训练时，作者使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-sapo/" target="_blank" rel="noopener"
>SAPO&lt;/a> 算法进行训练&lt;/p>
&lt;h4 id="general-rl">&lt;a href="#general-rl" class="header-anchor">&lt;/a>General RL
&lt;/h4>&lt;p>作者采用了一个 multi-task RL 的范式来提高模型在不同任务上的表现，reward 主要包含两个方面：&lt;/p>
&lt;ol>
&lt;li>instruction following: 评估模型遵循用户指令的能力，包括内容，格式，长度等&lt;/li>
&lt;li>preference alignment: 对于开放式问题，评估模型帮助性，事实准确性等方面的表现&lt;/li>
&lt;/ol>
&lt;p>基于这两个方面 reward 有两个部分组成：&lt;/p>
&lt;ol>
&lt;li>rule-based reward: 基于规则的 reward, 比如格式要求等&lt;/li>
&lt;li>model-based reward: 使用 Qwen2.5-VL 72B 和 Qwen3 作为 judge model 来提供奖励&lt;/li>
&lt;/ol>
&lt;p>为了解决模型的重复性实处，中英文混杂等问题，作者构造了一个数据集来故意触发模型这些问题然后加以改正。&lt;/p>
&lt;h3 id="thinking-with-images">&lt;a href="#thinking-with-images" class="header-anchor">&lt;/a>Thinking with Images
&lt;/h3>&lt;p>作者还够在了数据提高模型的 &amp;ldquo;thinking with images&amp;rdquo; 的能力，训练包含两个阶段：&lt;/p>
&lt;ol>
&lt;li>Stage 1: 作者构造了 10K Grounding 数据，然后对 Qwen2.5-VL 32B 进行 SFT 来模仿 agent 的行为: think -&amp;gt; act -&amp;gt; analyze feedback -&amp;gt; answer, 然后作者使用 multi-turn, tool-integrated RL 来进一步提高模型的 reasoning 能力&lt;/li>
&lt;li>Stage 2: 作者从 Qwen2.5-VL 32B 蒸馏得到 120K multi-turn agentic interactions 数据集， 然后作者使用了相似的 cold-start SFT 以及 tool-integrated RL pipeline 来训练 Qwen3-VL&lt;/li>
&lt;/ol>
&lt;p>这里 RL 训练的 reward 包含以下几部分：&lt;/p>
&lt;ol>
&lt;li>answer accuracy reward&lt;/li>
&lt;li>multi-turn reasoning reward&lt;/li>
&lt;li>tool-calling reward&lt;/li>
&lt;/ol>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;h3 id="performance">&lt;a href="#performance" class="header-anchor">&lt;/a>Performance
&lt;/h3>&lt;p>Qwen3-VL 235B-A22B 的表现如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen3-vl/Qwen3-VL-performance.png"
width="721"
height="1060"
loading="lazy"
alt="Performance of Qwen3-VL 235B-A22B"
class="gallery-image"
data-flex-grow="68"
data-flex-basis="163px"
>&lt;/p>
&lt;h3 id="ablation-study">&lt;a href="#ablation-study" class="header-anchor">&lt;/a>Ablation Study
&lt;/h3>&lt;p>作者对比了以下 Qwen3-ViT 和 SigLIP-2 的表现，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen3-vl/Qwen3-VL-ablation-on-ViT.png"
width="1349"
height="136"
loading="lazy"
alt="Ablation on Qwen3-ViT"
class="gallery-image"
data-flex-grow="991"
data-flex-basis="2380px"
>&lt;/p>
&lt;p>实验结果显示，使用 1.7B 的 Qwen3 和 1.5T tokens 进行训练之后，Qwen3-ViT 的表现超过了 SigLIP2 的表现，验证了 Qwen3-ViT 的有效性&lt;/p>
&lt;p>作者对比了 Deepseek 和 baseline 的表现，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen3-vl/Qwen3-VL-Ablation-on-Deepstack.png"
width="1355"
height="130"
loading="lazy"
alt="Ablation on DeepStack"
class="gallery-image"
data-flex-grow="1042"
data-flex-basis="2501px"
>&lt;/p>
&lt;p>可以看到，相比于 baseline, DeepStack 的表现更好，说明了 DeepStack 可以提供更丰富的视觉信息。&lt;/p>
&lt;p>作者还评估了以下 Qwen3-VL 在视频版大海捞针任务上的表现，实验结果发现，对于 30 分钟的视频，Qwen3-VL 的准确率为 $100\%$, 通过 &lt;a class="link" href="https://maosong.website/p/notes-on-yarn/" target="_blank" rel="noopener"
>YARN&lt;/a> 上下文扩展策略，模型在 2 个小时视频上的准确率为 $99.5\%$.&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 Qwen3-VL 系列多模态大模型，在架构上，作者使用了 interleaved-MRoPE, DeepStack 等改进策略，在数据上，作者扩展了训练数据的多样性，在训练上，作者分别训练了 instruct 版本和 reasoning 版本。最终评估发现，Qwen3-VL 达到了 SOTA 表现。&lt;/p>
&lt;p>作者认为，未来的工作在于&lt;/p>
&lt;ol>
&lt;li>基于 Qwen3-VL 构建具身智能 agent&lt;/li>
&lt;li>提高模型的可交互感知，tool-augmented reasoning 以及 real-time multimodal control 能力&lt;/li>
&lt;li>提高模型与人类学习，合作的能力&lt;/li>
&lt;li>统一理解与生成多模态大模型&lt;/li>
&lt;/ol>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://qwen.ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&amp;amp;from=research.latest-advancements-list" target="_blank" rel="noopener"
>blog&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2511.21631" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on SAPO</title><link>https://maosong.website/p/notes-on-sapo/</link><pubDate>Fri, 05 Dec 2025 10:09:06 +0800</pubDate><guid>https://maosong.website/p/notes-on-sapo/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>相关工作包括 GRPO 以及 &lt;a class="link" href="https://maosong.website/p/notes-on-gspo/" target="_blank" rel="noopener"
>GSPO&lt;/a>&lt;/p>
&lt;p>SAPO 的关键思想有两点：&lt;/p>
&lt;ol>
&lt;li>tokne-level soft trust region 可以保证 sequence-level coherence&lt;/li>
&lt;li>非对称的 temperature 可以针对 postive token 和 negative token 进行不同的优化&lt;/li>
&lt;/ol>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;p>作者首先给出了 SAPO 的目标函数如下：&lt;/p>
$$
\mathcal{J}_{\mathrm{SAPO}}(\theta) = \mathbb{E}_{(x, y)\sim\mathcal{D},\{y_i\}_{i=1}^G\sim \pi_{\theta_{old}}(\cdot\mid x)}\left[ \frac{1}{G}\sum_{i=1}^G\frac{1}{|y_i|}\sum_{t=1}^{|y_i|}f_{i,t}(r_{i,t}(\theta))\hat{A}_{i,t} \right],
$$&lt;p>其中，&lt;/p>
$$
f_{i,t}(x) = \sigma(\tau_{i,t}(x-1))\cdot \frac{4}{\tau_{i,t}}, \tau_{i,t} = \begin{cases}
\tau_{pos}, &amp; \text{if }\hat{A}_{i,t}>0\\
\tau_{neg}, &amp;\text{otherwise}
\end{cases}
$$&lt;p>这里&lt;/p>
$$
\hat{A}_{i,t} = \hat{A}_{i} = \frac{r(x,y_i) - \mathrm{mean}(\{r(x,y_i)\}_{i=1}^G)}{\mathrm{std}(\{r(x,y_i)\}_{i=1}^G)},\quad r_{i,t}(\theta) = \frac{\pi_{\theta}(y_{i,t}\mid x, y_{i,&lt; t})}{\pi_{\theta_{old}}(y_{i,t}\mid x, y_{i,&lt; t})}
$$&lt;p>$\tau_{pos}$ 和 $\tau_{neg}$ 分别是 positive token 以及 negative token 对应的温度, $\sigma(x)=1/(1+e^{-x})$ 是 sigmoid function.&lt;/p>
&lt;p>对 $\mathcal{J}_{\mathrm{SAPO}}(\theta)$ 求导得到&lt;/p>
$$
\nabla_\theta \mathcal{J}_{\mathrm{SAPO}}(\theta) = \mathbb{E}_{(x, y)\sim\mathcal{D},\{y_i\}_{i=1}^G\sim \pi_{\theta_{old}}(\cdot\mid q)}\left[ \frac{1}{G}\sum_{i=1}^G\frac{1}{|y_i|}\sum_{t=1}^{|y_i|}w_{i,t}(\theta)r_{i,t}(\theta)\nabla_\theta \log \pi_\theta(y_{i,t}\mid q, y_{i, &lt;t}) \right]
$$&lt;p>其中&lt;/p>
$$
w_{i,t}(\theta) = 4p_{i,t}(\theta)(1-p_{i,t}(\theta)),\quad p_{i,t}(\theta) = \sigma(\tau_{i,t} (r_{i,t}(\theta)-1)),
$$&lt;p>$\mathcal{J}_{\mathrm{SAPO}}(\theta)$ 和 $w_{i,t}(\theta)$ 与 $r_{i,t}(\theta)$ 的关系如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-sapo/SAPO-gating-illustration.png"
width="1352"
height="426"
loading="lazy"
alt="illustration of gating mechanism"
class="gallery-image"
data-flex-grow="317"
data-flex-basis="761px"
>&lt;/p>
&lt;p>为了保证当 $r_{i,t}(\theta)=1$ 时，SAPO 等价于 $r_{i,t}(\theta)\hat{A}_{i,t}$ 而与 $\tau_{i,t}$ 无关，作者在 $f_{i,t}(x)$ 加入了系数 $4/\tau_{i,t}$.&lt;/p>
&lt;h2 id="comparison">&lt;a href="#comparison" class="header-anchor">&lt;/a>Comparison
&lt;/h2>&lt;p>作者接下来对比了 GSPO 以及 GRPO 两个算法&lt;/p>
&lt;p>首先作者使用了一下统一的目标函数公式来表示三个算法&lt;/p>
$$
\mathcal{J}(\theta) = \mathbb{E}_{(x, y)\sim\mathcal{D},\{y_i\}_{i=1}^G\sim \pi_{\theta_{old}}(\cdot\mid x)}\left[ \frac{1}{G}\sum_{i=1}^G\frac{1}{|y_i|}\sum_{t=1}^{|y_i|}f_{i,t}(r_{i,t}(\theta))\hat{A}_{i,t} \right],
$$&lt;p>其中不同算法的 $f_{i,t}(\cdot)$ 不一样，三种算法的定义如下&lt;/p>
$$
\begin{aligned}
\mathrm{SAPO}&amp;:f_{i,t}^{\mathrm{SAPO}}(r_{i,t}(\theta))=\sigma(\tau_{i,t}(x-1))\cdot \frac{4}{\tau_{i,t}}, \tau_{i,t} = \begin{cases}
\tau_{pos}, &amp; \text{if }\hat{A}_{i,t}>0\\
\tau_{neg}, &amp;\text{otherwise}
\end{cases}\\
\mathrm{GRPO}&amp;:f_{i,t}^{\mathrm{GRPO}}(r_{i,t}(\theta), \hat{A}_{i,t})=\begin{cases}
\min(r_{i,t}(\theta), 1+\epsilon) &amp; \hat{A}_{i,t}>0\\
\max(r_{i,t}(\theta), 1-\epsilon) &amp; \hat{A}_{i,t}\leq0
\end{cases}\\
\mathrm{GSPO}&amp;:f_{i,t}^{\mathrm{GSPO}}(r_{i,t}(\theta), \hat{A}_{i,t})=f_{i,t}^{\mathrm{seq}}(r_{i,t}(\theta), \hat{A}_{i,t})=\begin{cases}
\min(s_{i,t}(\theta), 1+\epsilon) &amp; \hat{A}_{i,t}>0\\
\max(s_{i,t}(\theta), 1-\epsilon) &amp; \hat{A}_{i,t}\leq0
\end{cases}
\end{aligned}
$$&lt;p>其中&lt;/p>
$$
s_i(\theta) = \left(\frac{\pi_{\theta}(y_{i}\mid x)}{\pi_{\theta_{old}}(y_{i}\mid x)}\right)^{\frac{1}{|y_i|}}, s_{i,t}(\theta) = \mathrm{sg}[s_i(\theta)]\frac{\pi_{\theta}(y_{i,t}\mid x, y_{i,&lt; t})}{\mathrm{sg}[\pi_{\theta}(y_{i,t}\mid x, y_{i,&lt; t})]}
$$&lt;p>首先是与 GSPO 的对比，通过一些假设和简化，我们得到&lt;/p>
$$
\begin{aligned}
\nabla_\theta \mathcal{J}_{\mathrm{SAPO}}(\theta) &amp;=\mathbb{E}\left[\frac1G\sum_{i=1}^G g_{\tau_i}(\log s_i(\theta))\nabla_\theta \log s_i(\theta)\hat{A}_i\right]\\
\nabla_\theta \mathcal{J}_{\mathrm{GSPO}}(\theta) &amp;=\mathbb{E}\left[\frac1G\sum_{i=1}^G s_i(\theta)\nabla_\theta \log s_i(\theta)\hat{A}_i\right]
\end{aligned}
$$&lt;p>其中 $g_{\tau_i}(\log s_i(\theta)) = \mathrm{sech}^2\left(\tau_i/2\log s_i(\theta)\right)$. 相比于 GSPO, SAPO 有两个优势：&lt;/p>
&lt;ol>
&lt;li>smoothness and stability, soft gate 避免了 hard clipping 带来的不连续性&lt;/li>
&lt;li>token-level adaptivity with sequence-level coherence. 当假设不成立的时候，SAPO 退化为 token-level gating, 这样可以降低 outliers 的权重&lt;/li>
&lt;/ol>
&lt;p>GRPO 的函数可以进一步简化为&lt;/p>
$$
f_{i,t}^{\mathrm{GRPO}}(r_{i,t}(\theta), \hat{A}_{i,t})= \begin{cases}
1, &amp; \text{if }\hat{A}_{i,t}>0\text{ and }r_{i,t}(\theta)\leq 1+ \epsilon\\
0, &amp; \text{if }\hat{A}_{i,t}>0\text{ and }r_{i,t}(\theta)> 1+ \epsilon\\
1, &amp; \text{if }\hat{A}_{i,t}\leq0\text{ and }r_{i,t}(\theta)\geq 1- \epsilon\\
0, &amp; \text{if }\hat{A}_{i,t}\leq0\text{ and }r_{i,t}(\theta)&lt; 1- \epsilon
\end{cases}\\
$$&lt;p>可以看到，GRPO 对应一个 binary trust region. 与 GRPO 相比，SAPO 将对应的 hard indicator 替换未来一个 smooth kernel $f_{i,t}^{\mathrm{SAPO}}(r_{i,t}(\theta))=\mathrm{sech}^2\left(\tau_i/2r_{i,t}(\theta)-1\right)$, 这样可以避免 gradient vanishing 以及提高训练的稳定性。&lt;/p>
&lt;p>最终结论为：&lt;/p>
&lt;ul>
&lt;li>相比于 GSPO, SAPO 对于 off-policy 的数据利用率更高&lt;/li>
&lt;li>相比于 GRPO, SAPO 避免了 hard token level clipping 导致的 zero-gradient 问题&lt;/li>
&lt;/ul>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>作者对比了 SAPO, GSPO 以及 GRPO-R2(GRPO with routing replay) 三种方法，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-sapo/SAPO-performance.png"
width="1211"
height="869"
loading="lazy"
alt="Performance of SAPO"
class="gallery-image"
data-flex-grow="139"
data-flex-basis="334px"
>&lt;/p>
&lt;p>实验结果显示，SAPO 的表现超过了 GSPO 以及 GRPO&lt;/p>
&lt;p>作者还探究了超参数 $\tau_{pos}$ 和 $\tau_{neg}$ 的影响，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-sapo/SAPO-ablation-on-temperature.png"
width="1219"
height="853"
loading="lazy"
alt="Ablation study on temperature"
class="gallery-image"
data-flex-grow="142"
data-flex-basis="342px"
>&lt;/p>
&lt;p>实验结果显示，当 $\tau_{neg}>\tau_{pos}$ 时，模型训练最稳定，这说明了 negative token 是导致训练不稳定的主要原因。&lt;/p>
&lt;p>作者还在 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3-vl/" target="_blank" rel="noopener"
>Qwen3-VL&lt;/a> 上进行了验证，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-sapo/SAPO-Qwen3-VL-perfornance.png"
width="1220"
height="447"
loading="lazy"
alt="Performance of SAPO on Qwen3-VL"
class="gallery-image"
data-flex-grow="272"
data-flex-basis="655px"
>&lt;/p>
&lt;p>实验结果显示，SAPO 的表现超过了 GSPO 以及 GRPO-R2&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>在本文中，作者提出了 SAPO, 一个解决 hard-clipping 训练不稳定性以及低效率的策略优化算法，作者使用了基于温度的 soft gate 来代替 clipping, 以及对于 positive token 和 negative token 使用了不同的 temperature 这两点改进。结果验证了 SAPO 的有效性，作者认为使用 smooth 以及 adaptive gating 机制可以有效提高 RL 训练的稳健性以及有效性。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2511.20347" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on DeepStack</title><link>https://maosong.website/p/notes-on-deepstack/</link><pubDate>Thu, 04 Dec 2025 17:32:41 +0800</pubDate><guid>https://maosong.website/p/notes-on-deepstack/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>已有的 MLLM 将视觉 token 作为一个 1d sequence, 输入给 LLM. 在本文中，作者将 visual token 注入到 LLM 的不同 layer 中来提高视觉信息的利用率&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepstack/DeepStack-architecture.png"
width="1149"
height="427"
loading="lazy"
alt="Architecture of DeepStack"
class="gallery-image"
data-flex-grow="269"
data-flex-basis="645px"
>&lt;/p>
&lt;p>首先，对于输入的图片 $I$, 我们将其分为高精度图片版本 $I_{high}$ 和低精度图片版本 $I_{low}$, $I_{low}$ 通过 vision encoder 和 MLP 得到对应的视觉 token $X_v$ 作为 LLM 的输入，然后在 LLM transformer block 的第 $i$ 层，其对应的视觉 token $X_{i,v}$ 会与 stack feature $X_{v}^i$ 相加，这里 $X_v^i$ 是对高精度图片输入的一个采样，即&lt;/p>
$$
X_v^i = \mathrm{Sampling2D}(\mathrm{MLP}(\mathrm{ViT}(I_{high})))
$$&lt;p>算法伪代码如下所示&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># H0: Input embeddings for LLM (Original inputs args for traditional LMM); # vis_pos: the location of visual tokens; &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># X, Xstack: Original visual tokens, Extra high-resolution visual token list; &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># lstart, n: Index of starting layer, and layer interval for stacking.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">H0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Xstack&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">lstart&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vis_pos&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">H&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">H0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">idx&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">TransformerLayer&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">enumerate&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">layers&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># DeepStack: &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">idx&lt;/span> &lt;span class="o">&amp;gt;=&lt;/span> &lt;span class="n">lstart&lt;/span> &lt;span class="o">&amp;amp;&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">idx&lt;/span> &lt;span class="err">−&lt;/span> &lt;span class="n">lstart&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">H&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">vis_pos&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">Xstack&lt;/span>&lt;span class="p">[(&lt;/span>&lt;span class="n">idx&lt;/span> &lt;span class="err">−&lt;/span> &lt;span class="n">lstart&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">//&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Original Transformer: &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">H&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">TransformerLayer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">H&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;h3 id="ablation-study">&lt;a href="#ablation-study" class="header-anchor">&lt;/a>Ablation Study
&lt;/h3>&lt;p>作者进一步验证了不同实验配置，结果发现在 early layer 进行 deepstack 效果最好，越往后效果越差&lt;/p>
&lt;p>作者还在 ViT 上应用了 DeepStack 策略，结果发现 ViT 的效果也有所提升&lt;/p>
&lt;p>作者还发现，模型表现提升是因为加入了 high-reoslution image token 信息&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 DeepStack, 一个提高 MLLM 中视觉信息利用率的方法，作者验证了这个方法的有效性。&lt;/p>
&lt;h2 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://openreview.net/forum?id=fXDpDzHTDV" target="_blank" rel="noopener"
>paper&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on ViT</title><link>https://maosong.website/p/notes-on-vit/</link><pubDate>Thu, 04 Dec 2025 11:00:44 +0800</pubDate><guid>https://maosong.website/p/notes-on-vit/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>Transformer 架构在 NLP 领域已经成为了事实上的标准。而在 CV 领域，目前还是 CNN 架构占据了主导。&lt;/p>
&lt;p>在本文中，作者就探究是否可以应用 Transformer 架构来完成图像识别的任务。&lt;/p>
&lt;p>作者发现，当在小规模数据集上进行训练时，Transformer 架构的表现是不如 CNN 架构的，作者认为原因是 Transformer 架构缺乏如 translation equivariance 和 locality 等 inductive biases. 但是当数据集规模上去之后，作者发现这种 inductive bias 可以通过大规模训练抵消掉。其中，最好的模型在 ImageNet 上达到了 $88.55\%$ 的准确率。&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;p>ViT 的架构如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-vit/ViT-architecture.png"
width="949"
height="483"
loading="lazy"
alt="Architecture of ViT"
class="gallery-image"
data-flex-grow="196"
data-flex-basis="471px"
>&lt;/p>
&lt;p>为了能够处理图片，对于输入的图片 $x\in\mathbb{R}^{H\times W\times C}$, 其中 $(H,W)$ 是图片大小，$C$ 是 channel 的个数。作者将其展开为一个 2D patch 序列, $x_p\in\mathbb{R}^{N\times (P^2\times C)}$, 其中 $(P, P)$ 是 image patch 的大小，$N=HW/P^2$ 是 image patch 的的个数，也是 Transformer 输入 token 的个数，Transformer 的 hidden size 为 $D$, 作者使用了一个 linear layer 来将 image patch 转换为 Transformer 的输入。&lt;/p>
&lt;p>与 BERT 一致，作者使用了一个 &lt;code>[class]&lt;/code> token 来作为输入 patch 序列的 embedding, 即 $z_0^0=x_{class}$, 其对应的输出 $z_L^0$ 作为该图片的表示。作者还使用了 1D 的 position encoding. 最终，ViT 表达式如下所示&lt;/p>
$$
\begin{aligned}
z_0 &amp;= [x_{class};x_p^1\mathbf{E};x_p^2\mathbf{E};\cdots;x_p^N\mathbf{E};]+\mathbf{E}_{pos}, &amp;\mathbf{E}\in\mathbb{R}^{(P^2\cdot C)\times D},\mathbf{E}_{pos}\in\mathbb{R}^{(N+1)\times D}\\
z_{\ell}'&amp;=\mathrm{MultiHeadAttention}(\mathrm{LayerNorm}(z_{\ell-1}))+z_{\ell-1},&amp;\ell=1,\dots,L\\
z_{\ell} &amp;= \mathrm{MLP}(\mathrm{LayerNorm}(z_{\ell}'))+z_{\ell}',&amp;\ell=1,\dots,L\\
y&amp;=\mathrm{LayerNorm}(z_L^0)
\end{aligned}
$$&lt;p>作者分析认为，相比于 CNN 架构，Transformer 架构缺乏 inductive bias, 这是因为每个 patch 对其周围的 2D 临近信息用的非常少，ViT 架构必须从零开始学习这些位置以及空间关系。&lt;/p>
&lt;p>作者还介绍了混合架构，即 patch embedding 由一个 linear layer 替换为一个 CNN 架构&lt;/p>
&lt;p>在 finetune 时，作者移除了 pre-trained 的 prediction head, 然后加入了一个 $D\times K$ 的 feed forward layer, 其中 $K$ 是分类的类别数。&lt;/p>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>作者使用的数据集如下所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Dataset&lt;/th>
&lt;th>classes&lt;/th>
&lt;th>images&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>ImageNet&lt;/td>
&lt;td>1K&lt;/td>
&lt;td>1.3M&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ImageNet-21K&lt;/td>
&lt;td>21K&lt;/td>
&lt;td>14M&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>JFT&lt;/td>
&lt;td>18K&lt;/td>
&lt;td>303M&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>作者在 ImageNet, CIFAR-10/100, Oxford-IIIT Pets, Oxford Flowers-102 这些数据集上进行评测。&lt;/p>
&lt;p>模型配置如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Layers&lt;/th>
&lt;th>$D$&lt;/th>
&lt;th>$D_{FFN}$&lt;/th>
&lt;th># heads&lt;/th>
&lt;th># params&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>ViT-Base&lt;/td>
&lt;td>12&lt;/td>
&lt;td>768&lt;/td>
&lt;td>3072&lt;/td>
&lt;td>12&lt;/td>
&lt;td>86M&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ViT-Large&lt;/td>
&lt;td>24&lt;/td>
&lt;td>1024&lt;/td>
&lt;td>4096&lt;/td>
&lt;td>16&lt;/td>
&lt;td>307M&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ViT-Huge&lt;/td>
&lt;td>32&lt;/td>
&lt;td>1280&lt;/td>
&lt;td>5120&lt;/td>
&lt;td>16&lt;/td>
&lt;td>632M&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>为了简便，作者在后续实验过程中，对模型名字进行了改写，比如 ViT-L/16 就代表了 ViT-Large model, 其对应的 patch size 为 16.&lt;/p>
&lt;p>ViT 的实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-vit/ViT-performance.png"
width="1159"
height="357"
loading="lazy"
alt="Performance of ViT"
class="gallery-image"
data-flex-grow="324"
data-flex-basis="779px"
>&lt;/p>
&lt;p>可以看到相比于 CNN 架构，ViT 所需要的训练时间更少，且效果更好。&lt;/p>
&lt;p>为了验证 ViT 模型具有更高的训练效率，作者在不同的数据集上进行的实验，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-vit/ViT-transfer-performance.png"
width="545"
height="387"
loading="lazy"
alt="Transfer to ImageNet"
class="gallery-image"
data-flex-grow="140"
data-flex-basis="337px"
>&lt;/p>
&lt;p>可以看到，当数据集比较小时，大部分 ViT 模型表现都比 ResNet 差，但是当数据集规模增加之后，ViT 的表现逐渐超过了 ResNet. 并且，随着数据集规模的增加，大模型的表现也比小模型好。&lt;/p>
&lt;p>作者进一步在四个不同大小的随机数据集上进行了验证，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-vit/ViT-linear-few-shot-performance.png"
width="551"
height="384"
loading="lazy"
alt="Linear few-shot evaluation on ImageNet v.s. pre-training size"
class="gallery-image"
data-flex-grow="143"
data-flex-basis="344px"
>&lt;/p>
&lt;p>实验结果显示，inductive bias 在小规模数据集上比较有效，但是当数据集规模上去之后，直接从数据集中学习相关的 pattern 更加高效和有效。&lt;/p>
&lt;p>由于不同的模型参数可能不太一致，为了更加公平地对比，作者使用算力来进行对比，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-vit/ViT-cost-vs-performance.png"
width="1050"
height="457"
loading="lazy"
alt="Performance v.s. cost for different architectures"
class="gallery-image"
data-flex-grow="229"
data-flex-basis="551px"
>&lt;/p>
&lt;p>实验结果显示，ViT 的训练效率比 ResNet 更高，其次，hybrid model 在算力比较小的时候，其表现比 ViT 更好，但是当算力提升之后，两者表现差不多。最后，ViT 随着算力的提升，还有进一步提升的空间。&lt;/p>
&lt;p>作者还对 ViT 的 attention 进行了进一步的分析，结果有以下几点：&lt;/p>
&lt;ol>
&lt;li>row-column structure 很明显，同一行同一列的 patches 有比较相似的 embedding&lt;/li>
&lt;li>ViT 学习到的 position embedding 对于比较近的 patches 其相似度也更高&lt;/li>
&lt;li>对于 attention 来说，某些 heads 在 early layer 就已经开始关注全局信息，并且随着 layer 的提升，attention distance 也在提升，说明模型关注全局信息的能力逐渐增强&lt;/li>
&lt;/ol>
&lt;p>作者尝试了对 Transformer 进行 scaling up, 结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-vit/ViT-scaling.png"
width="1117"
height="385"
loading="lazy"
alt="Scaling of ViT"
class="gallery-image"
data-flex-grow="290"
data-flex-basis="696px"
>&lt;/p>
&lt;p>实验结果显示，使用更多的 layer 可以有效提高模型的表现，但是当 layer 数大于 16 之后，其提升有限。另一方面，提高宽度对模型表现影响不大。作者发现，降低 patch size 也可以提高模型的表现。&lt;/p>
&lt;p>作者还对比了不同类型的 position encoding, 结果发现，使用/不使用 position encoding 对模型表现影响非常大，但是不同的 position encoding 总体来说表现差别不是很大。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 ViT, 一个基于 Transformer 架构的图像识别模型，作者通过在大规模数据集上进行训练验证了 ViT 架构的有效性。&lt;/p>
&lt;p>作者发现目前还存在如下挑战：&lt;/p>
&lt;ol>
&lt;li>如何使用自监督的方式进行训练，比如类似 BERT 的 masked patch prediction&lt;/li>
&lt;li>将 ViT 应用于其他的视觉任务，比如检测和分割&lt;/li>
&lt;li>进一步 scaling ViT&lt;/li>
&lt;/ol>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://openreview.net/forum?id=YicbFdNTTy" target="_blank" rel="noopener"
>openreview&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on CoMP</title><link>https://maosong.website/p/notes-on-comp/</link><pubDate>Thu, 04 Dec 2025 10:58:30 +0800</pubDate><guid>https://maosong.website/p/notes-on-comp/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者首先提到已有的 vision foundation model (VFM) 存在两个问题：&lt;/p>
&lt;ol>
&lt;li>不能处理动态分辨率图片输入，尽管我们可以使用 Bilinear interpolation 和 multi-resolution training 等方法，但是模型对于动态分辨率图片输入处理能力仍然不足&lt;/li>
&lt;li>VFM 和 LLM 之间存在 representation gap&lt;/li>
&lt;/ol>
&lt;p>针对这两个问题，作者提出了 CoMP, 一个 continual pre-training pipeline, CoMP 包含两个模块：&lt;/p>
&lt;ol>
&lt;li>C-RoPE, 一个针对 VFM 的 continual RoPE, 用于帮助 VFM 处理动态分辨率图片输入&lt;/li>
&lt;li>Alignment Loss, 用于对齐 VFM 和 LLM 的 representation&lt;/li>
&lt;/ol>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;p>CoMP 整体的架构图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-comp/CoMP-overall-architecture.png"
width="868"
height="454"
loading="lazy"
alt="Overview of CoMP"
class="gallery-image"
data-flex-grow="191"
data-flex-basis="458px"
>&lt;/p>
&lt;p>可以看到，CoMP 本质上就是一个多模态大模型，知识我们训练的目标为 VFM&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-comp/CoMP-modules-architecture.png"
width="977"
height="461"
loading="lazy"
alt="C-RoPE and Alignment Loss"
class="gallery-image"
data-flex-grow="211"
data-flex-basis="508px"
>&lt;/p>
&lt;h3 id="c-rope">&lt;a href="#c-rope" class="header-anchor">&lt;/a>C-RoPE
&lt;/h3>&lt;p>C-RopE 的核心思想是结合绝对位置编码以及相对位置编码来使得 pre-trained ViT 可以接受任意精度图片输入，对于输入图片 $X_V\in\mathbb{R}^{H\times W}$, 首先经过 patchify 得到 $N=HW/P^2$ 个 patch, 这里 $P$ 是 patch size, 每个 patch 大小为 $x_p\in\mathbb{R}^{N\times (P^2\cdot C)}$, $C$ 是 channels. 然后 VFM 每一个 layer 的计算过程为&lt;/p>
$$
\begin{aligned}
z_0 &amp;= [x_p^1E;\dots;x_p^NE] + \mathrm{Int}(E_{pos})\\
q_i,k_i,v_i &amp;= \mathrm{Proj}_q(z_i), \mathrm{Proj}_k(z_i), \mathrm{Proj}_v(z_i)\\
y_i &amp;= z_i + \mathrm{Proj}_o(\mathrm{Softmax}\left((Rq_i)^T(Rk_i) / D_v\right)v_i)\\
z_{i+1} &amp;= y_i + \mathrm{FFN}(y_i)
\end{aligned}
$$&lt;p>这里 $E\in\mathbb{E}^{P^2\cdot C\times D_v}$, $E_{pos}\in\mathbb{R}^{N\times D_V}$ 分别是 patch embedding 和 learnable position embedding, $\mathrm{Int}(\cdot)$ 是 bilinear interpolation.&lt;/p>
&lt;p>相比于 &lt;a class="link" href="https://maosong.website/p/notes-on-vit/" target="_blank" rel="noopener"
>ViT&lt;/a>, C-RoPE 做出了两点改动：&lt;/p>
&lt;ol>
&lt;li>使用了 Interpolation 来支持动态分辨率图片输入&lt;/li>
&lt;li>使用了 RoPE 来高效利用位置信息&lt;/li>
&lt;/ol>
&lt;h3 id="text-supervised-generative-pre-training">&lt;a href="#text-supervised-generative-pre-training" class="header-anchor">&lt;/a>Text-supervised Generative Pre-training
&lt;/h3>&lt;p>作者还是用了 LLM 的 cross-entropy loss 来进行对齐，text decoding loss 定义为&lt;/p>
$$
\mathcal{L}_{dec} = -\frac1T\sum_{i=V+1}^{V+T}\log P(X_i\mid X_{&lt;i}, H_v)
$$&lt;p>其中 $H_v=\mathcal{F}_{\psi}(\mathcal{V}(X_V))$ 是视觉特征，$T, V$ 分别代表了文本 token 和视觉 token 个数&lt;/p>
&lt;h3 id="vision-language-representation-alignment">&lt;a href="#vision-language-representation-alignment" class="header-anchor">&lt;/a>Vision-language Representation Alignment
&lt;/h3>&lt;p>text-decoding loss 可以对其视觉特征和文本特征，但是对于使用自监督训练方式的 VLM, 比如 DINOv2, 其预训练目标与 text-decoding loss 之间存在较大 gap, 为了解决这个问题，作者提出了 alignment loss.&lt;/p>
&lt;p>具体做法就是先计算出文本和视觉特征：&lt;/p>
$$
F_v = \mathrm{Pool}(H_v), F_t = \mathrm{Pool}(P_{\theta}(X_t))
$$&lt;p>然后作者将 $F_v, F_t$ 映射到语言空间，&lt;/p>
$$
C_v = W^TF_v, C_t=W^TF_t
$$&lt;p>这里 $W\in\mathbb{R}^{D_t\times K}$, $D_t, K$ 分别为 LLM 的 hidden size 以及 vocabulary size.&lt;/p>
&lt;p>接下来作者使用了 iterative Sinkhorn-Knopp 算法来归一化 $C_t$&lt;/p>
$$
p_t=\mathrm{Diag}(u_W)\exp(\frac{C_t}{\epsilon})\mathrm{Diag}(v)
$$&lt;p>这里 $u_W\in\mathbb{R}^K$ 是 words 的 prior marginal distribution,$v\in\mathbb{R}^B$ 是 renormalization vector&lt;/p>
&lt;p>最终 alignment loss 定义为&lt;/p>
$$
\mathcal{L}_{align} = -p_t\log p_v
$$&lt;p>这里 $p_v=\mathrm{softmax}(C_v)$, 作者对 LLM 使用了 stop gradient 操作，仅训练 VFM&lt;/p>
&lt;h3 id="training">&lt;a href="#training" class="header-anchor">&lt;/a>Training
&lt;/h3>&lt;p>CoMP 训练分为三个阶段：&lt;/p>
&lt;ul>
&lt;li>Stage 1: warming up, 仅训练 adapter&lt;/li>
&lt;li>Stage 2: 所有模型参数参与训练，使用了 RoPE 2D 和高精度图片输入&lt;/li>
&lt;li>Stage 3: instruction tuning, 使用 RoPE 2D 和动态分辨率图片输入&lt;/li>
&lt;/ul>
&lt;p>训练的损失为&lt;/p>
$$
\mathcal{L} = \begin{cases}
\mathcal{L}_{dec} + \alpha \mathcal{L}_{align}, &amp;\text{Stage 1 and Stage 2}\\
\mathcal{L}_{dec}, &amp;\text{Stage 3}
\end{cases}
$$&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>作者基于 CLIP, SigLIP, SigLip-2, DINOv2, AIMv2 等 VFM 进行了实验，主要结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-comp/CoMP-performance.png"
width="1153"
height="499"
loading="lazy"
alt="Performance of CoMP on multimodal understanding tasks"
class="gallery-image"
data-flex-grow="231"
data-flex-basis="554px"
>&lt;/p>
&lt;p>作者还对 training recipe 进行的消融实验，结果如下，可以看到，RoPE-2D 对模型表现提升最大，数据量和动态分辨率图片输入次之&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-comp/CoMP-trianing-ablation.png"
width="1124"
height="400"
loading="lazy"
class="gallery-image"
data-flex-grow="281"
data-flex-basis="674px"
>&lt;/p>
&lt;p>作者还对 C-RoPE 以及 alignment loss 进行的消融实验，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-comp/CoMP-ablation-performance.png"
width="1164"
height="365"
loading="lazy"
alt="Ablation on C-RoPE and aligment loss"
class="gallery-image"
data-flex-grow="318"
data-flex-basis="765px"
>&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 CoMP，一个 continual pre-training VFM 的方法，通过 C-RoPE 以及 alignment loss, 作者提高了 VFM 在 MLLM 中的表现&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2503.18931" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on DeepSeek-R1</title><link>https://maosong.website/p/notes-on-deepseek-r1/</link><pubDate>Tue, 02 Dec 2025 18:21:54 +0800</pubDate><guid>https://maosong.website/p/notes-on-deepseek-r1/</guid><description/></item><item><title>Notes on DeepSeek-V2</title><link>https://maosong.website/p/notes-on-deepseek-v2/</link><pubDate>Tue, 02 Dec 2025 18:21:54 +0800</pubDate><guid>https://maosong.website/p/notes-on-deepseek-v2/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者首先提到如何提高模型的训练效率以及 inference efficiency 是两个尚未解决的问题。&lt;/p>
&lt;p>基于这两个问题，作者在本文中提出了 DeepSeek-V2，一个开源的 MoE 模型，DeepSeek-V2 的亮点在于训练和推理都非常高效。最终 DeepSeeK-V2 包含 236B 总参数，激活参数为 21B, 上下文长度为 128K. 作者还开源了 DeepSeek-V2-Lite, 一个 15.7B-A2.4B 的 MoE 模型，用于学术研究。&lt;/p>
&lt;p>DeepSeek-V2 主要改进点为：&lt;/p>
&lt;ol>
&lt;li>基于 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseekmoe/" target="_blank" rel="noopener"
>DeepSeekMoE&lt;/a>, 使用了 MoE 架构&lt;/li>
&lt;li>使用了 MLA 压缩 KV cache, 大幅度提高推理效率&lt;/li>
&lt;/ol>
&lt;p>DeepSeek-V2 预训练使用了 &lt;strong>8.1T&lt;/strong> tokens, 相比于 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-llm/" target="_blank" rel="noopener"
>DeepSeek-LLM&lt;/a>, 预训练数据主要增加了中文数据以及提高了数据的质量。&lt;/p>
&lt;p>接下来，作者收集了 &lt;strong>1.5M&lt;/strong> 对话数据来进行 SFT, 最终作者基于 DeepSeek Math 提出的 GRPO 来进行对齐。&lt;/p>
&lt;h2 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h2>&lt;p>DeepSeek-V2 的模型架构如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v2/DeepSeek-V2-architecture.png"
width="1324"
height="1064"
loading="lazy"
alt="Architecture of DeepSeek-V2"
class="gallery-image"
data-flex-grow="124"
data-flex-basis="298px"
>&lt;/p>
&lt;p>模型基于 DeepSeekMoE 开发得到，相比于 DeepSeekMoE, DeepSeek-V2 主要是使用了 MLA&lt;/p>
&lt;h3 id="mla">&lt;a href="#mla" class="header-anchor">&lt;/a>MLA
&lt;/h3>&lt;p>这部分介绍见 &lt;a class="link" href="https://maosong2022.github.io/p/notes-on-mla/" target="_blank" rel="noopener"
>MLA&lt;/a>&lt;/p>
&lt;h3 id="deepseekmoe">&lt;a href="#deepseekmoe" class="header-anchor">&lt;/a>DeepSeekMoE
&lt;/h3>&lt;h4 id="architecture-1">&lt;a href="#architecture-1" class="header-anchor">&lt;/a>Architecture
&lt;/h4>&lt;p>关于架构的介绍见 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseekmoe/" target="_blank" rel="noopener"
>DeepSeekMoE&lt;/a>&lt;/p>
&lt;h4 id="device-limited-routing">&lt;a href="#device-limited-routing" class="header-anchor">&lt;/a>Device-Limited Routing
&lt;/h4>&lt;p>由于 DeepSeek-MoE 使用了细粒度的专家，因此专家会分布在更多的设备（GPU）上，计算时，基于 routing 的 expert 所在设备，会产生不同大小的通信开销。为了降低通信开销，作者构建了 device-limited routing mechanism. 具体的做法就是，在 Routing 之前，先基于 experts 的 affinity score 挑选 $M$ 个设备，然后基于这 $M$ 个设备的专家挑选 top-K 专家进行计算。&lt;/p>
&lt;p>作者通过实验发现，当 $M\geq3$ 时，device-limited routing 可以和标准的 top-K routing 表现差不多。&lt;/p>
&lt;h4 id="auxiliary-loss-for-load-balance">&lt;a href="#auxiliary-loss-for-load-balance" class="header-anchor">&lt;/a>Auxiliary Loss for Load Balance
&lt;/h4>&lt;p>作者使用了三个 loss 来实现负载均衡。其中，expert level 和 device level 的 load balancing loss 与 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseekmoe/" target="_blank" rel="noopener"
>DeepSeekMoE&lt;/a> 相同。第三个 loss 是 communication balance loss, 这个 loss 的目的是让每个设备的通信开销保持平衡。损失函数的表达式如下所示&lt;/p>
$$
\mathcal{L}_{communication} = \alpha\sum_{i=1}^D f_iP_i
$$&lt;p>其中 $\alpha$ 是超参数，$D$ 是 expert group 的个数。&lt;/p>
$$
f_i = \frac{D}{MT}\sum_{t=1}^T\mathbb{1}(\text{Token }t \text{ is sent Device }i),\quad P_i = \sum_{j\in\mathcal{E}_i}P_j
$$&lt;p>device limited routing 让每个 device 发送至多 $MT$ 个 hidden states 到其他设备上。而 communication balancing loss 则让每个设备最多从其他设备接收 $MT$ 个 hidden states.&lt;/p>
&lt;h4 id="token-dropping-strategy">&lt;a href="#token-dropping-strategy" class="header-anchor">&lt;/a>Token-Dropping Strategy
&lt;/h4>&lt;p>尽管前面已经增加了 load balance loss, 但毕竟不是硬约束。因此，作者就从硬件层面提出了 Token dropping 策略，来提高训练效率。核心思想就是，在训练时，主动丢弃部分 token, 强制让各个设备的计算量不会超过额度限制，进而减少资源浪费。&lt;/p>
&lt;p>具体做法就是，在训练之前，先将每个设备的 capacity factor 设置为 1 （定义见 &lt;a class="link" href="https://maosong.website/p/switch-transformer/" target="_blank" rel="noopener"
>Switch Transformer&lt;/a>）. 然后按照 affinity score 来丢弃一些分数比较低的 token, 直到该设备的 token 数量刚到达到 capacity。为了避免过度学习导致模型表现较差，对于 $10\%$ 的训练数据，作者不执行 token dropping 策略。&lt;/p>
&lt;p>最终，在 inference 时，可以根据需求来决定是否丢弃 token, 比如在 low latency 场景，我们可以丢弃低价值的 token, 在高精度场景，我们就可以保留所有的 token.由于在训练阶段已经才去过 token dropping 策略，因此在推理时不管是丢弃还是全部保留模型都能比较好的适应。&lt;/p>
&lt;h2 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h2>&lt;h3 id="data">&lt;a href="#data" class="header-anchor">&lt;/a>Data
&lt;/h3>&lt;p>预训练数据与 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-llm/" target="_blank" rel="noopener"
>DeepSeek-LLM&lt;/a> 基本上差不多，作者针对中文数据，数据质量进行了改进。最终预训练数据包括 &lt;strong>8.1T&lt;/strong> token, 其中中文数据比英文数据多 $12\%$.&lt;/p>
&lt;p>tokenizer 与 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-llm/" target="_blank" rel="noopener"
>DeepSeek-LLM&lt;/a> 一致。&lt;/p>
&lt;h3 id="model-configuration">&lt;a href="#model-configuration" class="header-anchor">&lt;/a>Model Configuration
&lt;/h3>&lt;p>模型配置如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>DeepSeek-V2&lt;/th>
&lt;th>DeepSeek-V2-Lite&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Date&lt;/td>
&lt;td>2024-5&lt;/td>
&lt;td>2024-5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Total Parameters&lt;/td>
&lt;td>236B&lt;/td>
&lt;td>15.7B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Activated Parameters&lt;/td>
&lt;td>21B&lt;/td>
&lt;td>2.4B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># tokens&lt;/td>
&lt;td>8.1T&lt;/td>
&lt;td>5.7T&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Dense Layers&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># MoE Layers&lt;/td>
&lt;td>60&lt;/td>
&lt;td>26&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Hidden Dim&lt;/td>
&lt;td>5120&lt;/td>
&lt;td>2048&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Dense Intermediate Dim&lt;/td>
&lt;td>12288&lt;/td>
&lt;td>10944&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MoE Intermediate Dim&lt;/td>
&lt;td>1536&lt;/td>
&lt;td>1408&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Attention&lt;/td>
&lt;td>MLA&lt;/td>
&lt;td>MLA&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Attention Heads&lt;/td>
&lt;td>128&lt;/td>
&lt;td>16&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Key-Value Heads&lt;/td>
&lt;td>128&lt;/td>
&lt;td>16&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Routed Experts&lt;/td>
&lt;td>160&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Experts Active Per Token&lt;/td>
&lt;td>6&lt;/td>
&lt;td>6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Shared Experts&lt;/td>
&lt;td>2&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>这里比较特殊的一点在于，模型在第一层使用了 MoE layer, 这个做法的原因在后面的 &lt;a class="link" href="https://maosong.website/p/notes-on-olmoe/" target="_blank" rel="noopener"
>olmoe&lt;/a> 里有提到，核心思想是 early layer 特别是第一层 layer 收敛比较慢。&lt;/p>
&lt;p>MLA 的配置如下 (DeepSeek-V2)&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>field&lt;/th>
&lt;th>value&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$d_c$&lt;/td>
&lt;td>512&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$d_c'$&lt;/td>
&lt;td>1536&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$n_h$&lt;/td>
&lt;td>16&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$d_h$&lt;/td>
&lt;td>128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$d_h^R$&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="training-recipe">&lt;a href="#training-recipe" class="header-anchor">&lt;/a>Training Recipe
&lt;/h3>&lt;p>训练的配置也与 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-llm/" target="_blank" rel="noopener"
>DeepSeek-LLM&lt;/a> 差不多，对于 MoE，作者使用了 PP 将不同的 layers 分配在不同的 device 上，然后 MoE 的 experts 被分配在 8 个 device 上 ($D=8$), 对于 device-limited routing, 每个 token 发送到至多 3 个 device, 也就是 $M=3$.&lt;/p>
&lt;h3 id="infra">&lt;a href="#infra" class="header-anchor">&lt;/a>Infra
&lt;/h3>&lt;p>在 infra 上，DeepSeek-V2 也是用了 HAI-LLM 框架进行训练。这里面使用了 16-way zero-bubble PP, 8-way EP, ZeRO-1 DP.&lt;/p>
&lt;p>由于 DeepSeek-V2 的激活参数比较少，因此，作者没有使用 TP, 进而降低通信开销。作者还将 shared experts 的计算与 expert all-to-all 通信进行重叠来提高计算效率。作者还使用了 kernel fusion 和 flash attention 2 来加速训练。&lt;/p>
&lt;h3 id="long-context">&lt;a href="#long-context" class="header-anchor">&lt;/a>Long Context
&lt;/h3>&lt;p>在预训练阶段结束之后，作者使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-yarn/" target="_blank" rel="noopener"
>YARN&lt;/a> 来将模型的上下文从 4K 扩展到 128K. 超参数设置为&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Parameter&lt;/th>
&lt;th>Value&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$s$&lt;/td>
&lt;td>40&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\alpha$&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\beta$&lt;/td>
&lt;td>32&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>target context length&lt;/td>
&lt;td>160K&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>scaling factor&lt;/td>
&lt;td>$\sqrt{t} = 0.0707\ln s + 1$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>作者在 32K 的上下文下额外训练了 1000 步，然后在推理阶段通过 YaRN 将模型的上下文长度扩展到 128K.&lt;/p>
&lt;h3 id="evaluation">&lt;a href="#evaluation" class="header-anchor">&lt;/a>Evaluation
&lt;/h3>&lt;p>作者对比了 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-llm/" target="_blank" rel="noopener"
>DeepSeek-LLM&lt;/a>, &lt;a class="link" href="https://maosong.website/p/notes-on-qwen1.5/" target="_blank" rel="noopener"
>Qwen1.5&lt;/a>, &lt;a class="link" href="https://maosong.website/p/mixstral-8x7b/" target="_blank" rel="noopener"
>Mixtral MoE&lt;/a> 以及 &lt;a class="link" href="https://maosong.website/p/notes-on-llama3/" target="_blank" rel="noopener"
>LLaMA 3&lt;/a>, 实验结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v2/DeepSeek-V2-pretraining-performance.png"
width="1264"
height="1177"
loading="lazy"
alt="Performance of DeepSeek-V2 base"
class="gallery-image"
data-flex-grow="107"
data-flex-basis="257px"
>&lt;/p>
&lt;h3 id="efficiency">&lt;a href="#efficiency" class="header-anchor">&lt;/a>Efficiency
&lt;/h3>&lt;p>作者对比了以下 DeepSeek-MoE 和 DeepSeek-LLM 的训练效率，结果发现，对于 1T 的 token, DeepSeek-LLM 需要 300.6K GPU hours, 而 DeepSeek-V2 仅需要 127.8K GPU hours. 也就是说，DeepSeeK-V2 节省了 $42.5\%$ 的训练成本&lt;/p>
&lt;p>在推理时，作者首先将模型的精度转换为 FP8，然后作者进一步对模型进行 KV cache quantization 来进一步压缩每个 token 的 KV cache 到 6bits. 最终，DeepSeek-V2 的 throughtput 为 50K tokens/s.&lt;/p>
&lt;h2 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h2>&lt;p>post-training 分为 SFT 和 RL 两个阶段。&lt;/p>
&lt;h3 id="sft">&lt;a href="#sft" class="header-anchor">&lt;/a>SFT
&lt;/h3>&lt;p>在 SFT 阶段，作者构建了 &lt;strong>1.5M&lt;/strong> 样本，包括 1.2M 有帮助性的样本和 0.3M 安全性相关的样本。模型训练了 2 个 epoch, 学习率为 $5e-6$.&lt;/p>
&lt;h3 id="rl">&lt;a href="#rl" class="header-anchor">&lt;/a>RL
&lt;/h3>&lt;p>作者使用了 GRPO 算法来进一步对齐模型的表现。&lt;/p>
&lt;p>作者通过实验发现，在 reasoning data, 如 code 和 math 相关数据上进行训练时，可以有效提高模型的表现。因此作者将 RL 的训练分为两个阶段，第一个阶段用于提高模型的 reasoning 能力，第二个阶段用于对齐人类偏好。&lt;/p>
&lt;p>在第一个阶段，作者首先训练了一个针对 code 和 Math 的 reward model $\mathrm{RM}_{\mathrm{reasoning}}$, 然后基于这个 reward model 来训练 policy model:&lt;/p>
$$
r_i=\mathrm{RM}_{\mathrm{reasoning}}(o_i)
$$&lt;p>在第二阶段，作者使用了一个 Multi-reward 框架，包括一个 helpful reward model $\mathrm{RM}_{\mathrm{helpful}}$, 一个 safety reward model $\mathrm{RM}_{\mathrm{safety}}$ 和一个 rule-based reward model $\mathrm{RM}_{\mathrm{rule}}$, 最终的 reward 为&lt;/p>
$$
r_i = c_1\mathrm{RM}_{\mathrm{helpful}}+c_2\mathrm{RM}_{\mathrm{safety}}+c_3\mathrm{RM}_{\mathrm{rule}}
$$&lt;p>训练时，reward model 由 SFT model 初始化得到，然后基于 point-wise 或者 pair-wise loss 进行训练。&lt;/p>
&lt;h3 id="evaluation-1">&lt;a href="#evaluation-1" class="header-anchor">&lt;/a>Evaluation
&lt;/h3>&lt;p>chat 版本的模型评估结果如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v2/DeepSeek-V2-posttraining-performance.png"
width="1325"
height="923"
loading="lazy"
alt="Performance of DeepSeek-V2-chat"
class="gallery-image"
data-flex-grow="143"
data-flex-basis="344px"
>&lt;/p>
&lt;h3 id="discussion">&lt;a href="#discussion" class="header-anchor">&lt;/a>Discussion
&lt;/h3>&lt;p>作者讨论了三点发现：&lt;/p>
&lt;ol>
&lt;li>SFT data 数量。已有工作认为进需要 10K 左右的样本就可以进行 SFT，但是作者发现当数据量小于 10K 时，模型在 IFEval benchmark 上的表现大幅度下降。作者认为，这是由于数据过少导致模型很难掌握特定的技能。因此，作者认为足够的数据以及数据质量都很重要，特别是写作类任务和 open-ended QA 类任务。&lt;/li>
&lt;li>alignment tax. 作者发现通过 human preference alignment, 模型在 open-ended generation benchmark 上的保险有了很大提升。与 RLHF 一样，作者也发现了 alignment 之后模型在一些 benchmark 上表现也会下降。作者通过改进解决了这个问题，作者认为如何在不损失模型表现的情况下实现对齐是一个值得探究的方向。&lt;/li>
&lt;li>online RL. 作者发现 Online RL 比 offline RL 的表现更好。作者认为如何根据不同的任务来选取 offline RL 和 online RL 也是一个值得探究的问题。&lt;/li>
&lt;/ol>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>在本文中，作者提出了 DeepSeek-V2, 一个基于 MoE 架构的大语言模型系列，模型的上下文为 128K. 作者基于 DeepSeek-MoE, 提出了 MLA 来提高模型的 inference 效率，并大幅度降低了训练的成本。&lt;/p>
&lt;p>作者介绍了几点未来工作：&lt;/p>
&lt;ol>
&lt;li>进一步 scaling up MoE 模型，降低模型的训练以及推理成本&lt;/li>
&lt;li>进一步对齐模型和人类的价值观，然后最小化人类监督信号&lt;/li>
&lt;li>扩展模型到多模态版本&lt;/li>
&lt;/ol>
&lt;h2 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2405.04434" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://huggingface.co/deepseek-ai/DeepSeek-V2" target="_blank" rel="noopener"
>HuggingFace&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on MLA</title><link>https://maosong.website/p/notes-on-mla/</link><pubDate>Tue, 02 Dec 2025 18:21:54 +0800</pubDate><guid>https://maosong.website/p/notes-on-mla/</guid><description>&lt;p>DeepSeek 在 2024 年 5 月提出了 multi-head latent attention (MLA), 用于提高 attention 的 Inference 效率&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>传统的 multi head attention (MHA) 虽然效果好，但是在 inference 时，其 KV cache 会变成瓶颈，影响推理效率。为了解决这个问题，已有的工作如 &lt;a class="link" href="https://maosong2022.github.io/p/notes-on-mqa/" target="_blank" rel="noopener"
>MQA&lt;/a> 和 &lt;a class="link" href="https://maosong2022.github.io/p/notes-on-gqa/" target="_blank" rel="noopener"
>GQA&lt;/a> 通过共享权重来减少 KV cache 内存占用，但是结果发现模型的表现也会降低。&lt;/p>
&lt;p>为了解决这个问题，作者提出了 multi-head latent attention (MLA), 来压缩 KV cache.&lt;/p>
&lt;h2 id="related-work">&lt;a href="#related-work" class="header-anchor">&lt;/a>Related Work
&lt;/h2>&lt;h3 id="mha">&lt;a href="#mha" class="header-anchor">&lt;/a>MHA
&lt;/h3>&lt;p>令 $d$ 为 hidden size, $n_h$ 为 attention heads 的个数，$\ell$ 为 transformer layer 的层数，$d_h$ 为每个 head 的 dimension, $h_t\in\mathbb{R}^d$ 为 attention layer 中第 $t$ 个 token 对应的 hidden states。对于标准的 MHA, 我们首先计算 Q, K, V 如下：&lt;/p>
$$
q_t=W^{Q}h_t,\quad k_t=W^Kh_t,\quad v_t = W^Vh_t
$$&lt;p>其中，$W^Q,W^K,W^V\in\mathbb{R}^{d_hn_h\times d}$ 分别为 query, key, value projection layer 的权重。接下来 MHA 的计算方式如下&lt;/p>
$$
\begin{aligned}
o_{t,i} &amp;= \sum_{j=1}^t\mathrm{softmax}_j\left(\frac{q_{t,i}^Tk_{j,i}}{\sqrt{d_h}}\right)v_{j,i},\\
u_t&amp;= W^O[o_{t,1};o_{t,2};\dots,;o_{t,n_h}]
\end{aligned}
$$&lt;p>其中 $q_t=[q_{t,1};q_{t,2};\dots,;q_{t,n_h}]$, $k_t=[k_{t,1};k_{t,2};\dots,;k_{t,n_h}]$, $v_t=[v_{t,1};v_{t,2};\dots,;v_{t,n_h}]$. $W^O\in\mathbb{R}^{d\times d_hn_h}$ 为 output projection 的权重。在 inference 阶段，每个 token 需要缓存其 key 以及 value 对应的值，从而每个 token 的 kv cache 占用为 $2n_hd_h\ell$. 当序列长度过大时，KV cache 会影响整体的 inference efficiency.&lt;/p>
&lt;h3 id="mqa--gqa">&lt;a href="#mqa--gqa" class="header-anchor">&lt;/a>MQA &amp;amp; GQA
&lt;/h3>&lt;p>MQA 通过在所有的 heads 中共享 key 和 value 来实现降低 kv cache 的作用，在 MQA 中，$W^K, W^V\in\mathbb{R}^{d_h\times d}$, 在计算时，对应的 $k_t$ 和 $v_t$ 通过广播机制参与 attention 的计算。此时，KV cache 占用为 MHA 的 $1/n_h$, 即 $2d_h\ell$.&lt;/p>
&lt;p>但是，MQA 的问题是表达能力太弱（表现差），因此后续 GQA 进行了改进，GQA 在 MQA 和 MHA 之间进行了权衡，即将 heads 分为若干个 group, 每个 group 中共享 key 和 value, 即 $W^K, W^V\in\mathbb{R}^{n_gd_h\times d}$, 这里 $n_g$ 是 group 个数，在计算 attention 时，key 和 value 在 group 内部共享，此时，GQA 的 KV cache 占用是 MQA 的 $n_g$ 倍，即 $2n_gd_h\ell$.&lt;/p>
&lt;p>这部分具体介绍见 &lt;a class="link" href="https://maosong2022.github.io/p/notes-on-mqa/" target="_blank" rel="noopener"
>MQA&lt;/a> 和 &lt;a class="link" href="https://maosong2022.github.io/p/notes-on-gqa/" target="_blank" rel="noopener"
>GQA&lt;/a>.&lt;/p>
&lt;h2 id="mla">&lt;a href="#mla" class="header-anchor">&lt;/a>MLA
&lt;/h2>&lt;p>MLA 的架构图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-mla/MLA-architecture.png"
width="387"
height="438"
loading="lazy"
alt="MLA architecture (sourced from MHA2MLA)"
class="gallery-image"
data-flex-grow="88"
data-flex-basis="212px"
>&lt;/p>
&lt;p>MLA 使用 low-rank joint compression 来压缩 key 以及 value 的 KV cache:&lt;/p>
$$
c_t^{KV} = W^{DKV}h_t,\quad k_t^C = W^{UK}c_t^{KV}, v_t^C = W^{UV}c_t^{KV}
$$&lt;p>这里 $c_t^{KV}\in\mathbb{R}^{d_c}$ 为 key 以及 value 压缩后的 latent vector. $d_c&lt;&lt;d_hn_h$ 为 KV cache compression dimension. $W^{DKV}\in\mathbb{R}^{d_c\times d}$ 为 down projection matrix, 这个矩阵是 key 和 value 共享的，$W^{UK}, W^{UV}\in\mathbb{R}^{d_hn_h\times d_c}$ 为 key, value 对应的 up projection matrix.&lt;/p>
&lt;p>另外，为了减少训练时的 activation memory, 作者对于 query 同样也执行了 low-rank compression, 压缩方式如下&lt;/p>
$$
c_t^Q = W^{DQ}h_t,\quad q_t^C = W^{UQ}c_t^Q
$$&lt;p>其中 $c_t^Q\in\mathbb{R}^{d_c'}$ 为 query 压缩后的 latent vector, $d_c'&lt;&lt; d_hn_h$ 为 query compression dimension, $W^{DQ}\in\mathbb{R}^{d_c'\times d}$, $W^{UQ}\in\mathbb{R}^{d_hn_h\times d_c'}$ 分别时 down projection, up projection matrix.&lt;/p>
&lt;p>最后 attention 的计算与 MHA 保持一致：&lt;/p>
$$
\begin{aligned}
o_{t,i} &amp;= \sum_{j=1}^t\mathrm{softmax}_j\left(\frac{q_{t,i}^Tk_{j,i}}{\sqrt{d_h}}\right)v_{j,i},\\
u_t&amp;= W^O[o_{t,1};o_{t,2};\dots,;o_{t,n_h}]
\end{aligned}
$$&lt;p>在推理的时候，我们只需要缓存 $c_t^{KV}$ 即可，这样每个 token 的 KV cache 为 $d_c\ell$. 并且在 inference 时，我们可以将 $W^{UK}$ 和 $W^{Q}$ 融合在一起，将 $W^{UV}$ 和 $W^{O}$ 融合在一起，也就是说我们不需要显式的计算出 $k_t$ 以及 $v_t$, 即&lt;/p>
$$
q_t^Tk_t = (W^{UQ}c_t^Q)^T(W^{UK}c_t^{KV}) = (c_t^Q)^T((W^{UQ})^TW^{UK})\boxed{c_t^{KV}}
$$&lt;p>以及&lt;/p>
$$
\begin{aligned}
u_t &amp;= W^O[o_{t,1};o_{t,2};\dots,;o_{t,n_h}] \\
&amp;= \sum_{i=1}^tW_i^Oo_{t,i}\\
&amp;= \sum_{i=1}^tW_i^O\sum_{j=1}^t\mathrm{softmax}_j\left(\frac{q_{t,i}^Tk_{j,i}}{\sqrt{d_h}}\right)v_{j,i}\\
&amp;= \sum_{i=1}^tW_i^O\sum_{j=1}^t\mathrm{softmax}_j\left(\frac{q_{t,i}^Tk_{j,i}}{\sqrt{d_h}}\right)W_i^{KV}c_t^{KV}\\
&amp;= \sum_{i=1}^t(W_i^OW_i^{KV})\sum_{j=1}^t\mathrm{softmax}_j\left(\frac{q_{t,i}^Tk_{j,i}}{\sqrt{d_h}}\right)\boxed{c_t^{KV}}
\end{aligned}
$$&lt;p>这里 $W^O = [W^O_1,\dots,W^O_{n_h}]$, $W^{UV}=[W^{KV}_1;\dots;W^{KV}_{n_h}]$, $W_i^O\in\mathbb{R}^{d\times d_h}$, $W^{UV}_i\in\mathbb{R}^{d_h\times d_c}$.&lt;/p>
&lt;h3 id="decoupled-position-embedding">&lt;a href="#decoupled-position-embedding" class="header-anchor">&lt;/a>Decoupled Position Embedding
&lt;/h3>&lt;p>接下来，作者介绍了如何解决 RoPE 不相容的问题。如果说我们直接在 $k_t^C$ 上进行 RoPE, 那么我们有&lt;/p>
$$
q_t^Tk_t = (R_mW^{UQ}c_t^Q)^T(R_nW^{UK}c_t^{KV}) = (c_t^Q)^T((W^{UQ})^TR_{m-n}W^{UK})\boxed{c_t^{KV}}
$$&lt;p>此时，我们没有办法将 $W^{UK}$ 吸收到 $W^{UQ}$ 中，这样就导致在 inference 时我们必须重新计算所有 prefix token 对应的 key, 这显然会降低 inference efficiency&lt;/p>
&lt;p>为了解决这个问题，作者使用了partial RoPE的技巧，即将query和key拆解为NoPE以及RoPE两部分，前者由MLA产生，后者携带位置信息。RoPE部分包括query $q_{t,i}^R\in\mathbb{R}^{d_h^R}$ 以及一个共享的 key $k_t^R\in\mathbb{R}^{d_h^R}$, 其中 $d_h^R$ 是 decoupled query 以及 decoupled key 的 head dimension.&lt;/p>
&lt;blockquote>
&lt;p>[!remark]
这里 key 对应的 RoPE 共享的原因是这部分信息也需要使用 KV cache 进行缓存，通过共享可以降低 KV cache 占用；而 query 对应的 RoPE 不共享的原因是提高 head 的表达能力，与 MHA 原理一致。&lt;/p>
&lt;/blockquote>
&lt;p>对应 MLA 的计算公式如下&lt;/p>
$$
\begin{aligned}
q_t^R&amp;=\mathrm{RoPE}(W^{QR}c_t^Q)\\
k_t^R &amp;= \mathrm{RoPE}(W^{KR}h_t)\\
q_{t,i} &amp;= [q_{t,i}^C;q_{t,i}^R]\\
k_{t,i} &amp;= [k_{t,i}^C;k_{t}^R]\\
o_{t,i} &amp;= \sum_{j=1}^t\mathrm{softmax}_j\left(\frac{q_{t,i}^Tk_{j,i}}{\sqrt{d_h+d_h^R}}\right)v_{j,i}^C,\\
u_t&amp;= W^O[o_{t,1};o_{t,2};\dots,;o_{t,n_h}]
\end{aligned}
$$&lt;p>其中 $q_t^R=[q_{t,1}^R;q_{t,2}^R;\dots,;q_{t,n_h}^R]$, $W^{QR}\in\mathbb{R}^{d_h^Rn_h\times d_c'}$, $W^{KR}\in\mathbb{R}^{d_h^R\times d}$ . $\mathrm{RoPE}(\cdot)$ 只执行 RoPE 矩阵乘法的操作。&lt;/p>
&lt;p>在这种情形下，attention 的计算如下所示&lt;/p>
$$
\begin{aligned}
q_{t,i}^Tk_{t,i} &amp;= [q_{t,i}^C;q_{t,i}^R]^T[k_{t,i}^C;k_{t}^R]\\
&amp;= (q_{t,i}^C)^Tk_{t,i}^C + (q_{t,i}^R)^Tk_{t}^R
\end{aligned}
$$&lt;p>可以看到，现在 attention 的计算分为了两部分，一部分是 MLA 自身的计算，这部分计算前面已经证明可以通过矩阵吸收的方式来进行优化，第二部分是关于 RoPE 部分的计算，这部分计算量不是很大&lt;/p>
&lt;p>最终，MLA 完整的计算公式如下&lt;/p>
$$
\begin{aligned}
c_t^Q=W^{DQ}h_t\\
[q_{t,1}^C;\dots;q_{t,n_h}^C]=q_t^C&amp;= W^{UQ}c_t^Q\\
[q_{t,1}^R;\dots;q_{t,n_h}^R]=q_t^R&amp;= \mathrm{RoPE}(W^{QR}c_t^Q)\\
q_{t,i} &amp;= [q_{t,i}^C;q_{t,i}^R]\\
\boxed{c_t^{KV}} &amp;= W^{DKV}h_t\\
[k_{t,1}^C;\dots;k_{t,n_h}^C]=k_t^C&amp;= W^{UK}c_t^{KV}\\
\boxed{k_t^R} &amp;= \mathrm{RoPE}(W^{KR}h_t)\\
k_{t,i} &amp;= [k_{t,i}^C;k_{t}^R]\\
[v_{t,1}^C;\dots;v_{t,n_h}^C]=v_t^C&amp;= W^{UV}c_t^{KV}\\
o_{t,i} &amp;= \sum_{j=1}^t\mathrm{softmax}_j\left(\frac{q_{t,i}^Tk_{j,i}}{\sqrt{d_h+d_h^R}}\right)v_{j,i}^C,\\
u_t&amp;= W^O[o_{t,1};o_{t,2};\dots,;o_{t,n_h}]
\end{aligned}
$$&lt;p>在 inference 时，decoupled key 也需要被缓存，因此 DeepSeek-V2 每个 token 所需要的 KV cache 为 $(d_c+d_h^R)\ell$, 框选的部分即为 Inference 阶段需要缓存的内容&lt;/p>
&lt;p>MLA 与 MHA, MQA, GQA 的对比如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-mla/DeepSeek-V2-comparison-attention.png"
width="1336"
height="367"
loading="lazy"
alt="Comparison of different attention mechanisms"
class="gallery-image"
data-flex-grow="364"
data-flex-basis="873px"
>&lt;/p>
&lt;h3 id="comparison-of-kv-cache">&lt;a href="#comparison-of-kv-cache" class="header-anchor">&lt;/a>Comparison of KV Cache
&lt;/h3>&lt;p>接下来，作者对比了不同 attention 机制的 KV cache, 结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Attention Mechanism&lt;/th>
&lt;th>KV Cache per Token (# Element)&lt;/th>
&lt;th>Capability&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Multi-Head Attention (MHA)&lt;/td>
&lt;td>$2n_hd_h\ell$&lt;/td>
&lt;td>Strong&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Grouped-Query Attention (GQA)&lt;/td>
&lt;td>$2n_gd_h\ell$&lt;/td>
&lt;td>Moderate&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Multi-Query Attention (MQA)&lt;/td>
&lt;td>$2d_h\ell$&lt;/td>
&lt;td>Weak&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MLA (Ours)&lt;/td>
&lt;td>$(d_c+d_h^R)\ell\approx 9/2d_h\ell$&lt;/td>
&lt;td>Stronger&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>这里作者将 $d_c$ 设置为 $4d_h$, $d_h^R$ 设置为 $d_h/2$, 因此得到了上面的 $9/2d_h\ell$ 的近似。与 GQA 相比，相当于 MLA 使用了 2.25 个 group, 但是可以得到更强的效果。&lt;/p>
&lt;p>为了避免 low-rank compression 以及 fine-grained expert segmentation 对输出的 scale 产生影响，作者对 compressed latent vectors $c_t^Q, c_t^{KV}$ 进行了 normalization.&lt;/p>
&lt;h2 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h2>&lt;p>首先是代码变量与公式变量的对应关系&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>code name&lt;/th>
&lt;th>variable name&lt;/th>
&lt;th>Value&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>hidden_size&lt;/code>&lt;/td>
&lt;td>$d$&lt;/td>
&lt;td>5120&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>kv_lora_rank&lt;/code>&lt;/td>
&lt;td>$d_c$&lt;/td>
&lt;td>512&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>q_lora_rank&lt;/code>&lt;/td>
&lt;td>$d_c'$&lt;/td>
&lt;td>1536&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>qk_nope_head_dim&lt;/code>&lt;/td>
&lt;td>$d_h$&lt;/td>
&lt;td>128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>qk_rope_head_dim&lt;/code>&lt;/td>
&lt;td>$d_h^R$&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>v_head_dim&lt;/code>&lt;/td>
&lt;td>$d_h$&lt;/td>
&lt;td>128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>num_attention_heads&lt;/code>&lt;/td>
&lt;td>$n_h$&lt;/td>
&lt;td>128&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>在具体实现时，作者对计算过程进行了优化，具体就是先合并计算然后通过 &lt;code>split&lt;/code> 进行拆分，这部分策略应用于三个部分：&lt;/p>
$$
\begin{aligned}
\begin{bmatrix}
q_t^c\\
q_t^R
\end{bmatrix} &amp;= \begin{bmatrix}
W^{UQ}\\
W^{QR}
\end{bmatrix}W^{DQ}h_t\\
\begin{bmatrix}
c_t^{KV}\\
k_t^R
\end{bmatrix} &amp;= \begin{bmatrix}
W^{DKV}\\
W^{KR}
\end{bmatrix}h_t\\
\begin{bmatrix}
k_t^c\\
v_t^c
\end{bmatrix} &amp;= \begin{bmatrix}
W^{UK}\\
W^{UV}
\end{bmatrix}c_t^{KV}
\end{aligned}
$$&lt;p>代码如下所示&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;span class="lnt">49
&lt;/span>&lt;span class="lnt">50
&lt;/span>&lt;span class="lnt">51
&lt;/span>&lt;span class="lnt">52
&lt;/span>&lt;span class="lnt">53
&lt;/span>&lt;span class="lnt">54
&lt;/span>&lt;span class="lnt">55
&lt;/span>&lt;span class="lnt">56
&lt;/span>&lt;span class="lnt">57
&lt;/span>&lt;span class="lnt">58
&lt;/span>&lt;span class="lnt">59
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">DeepseekV2Attention&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">layer_idx&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># d_h + d_h^R&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_head_dim&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">qk_nope_head_dim&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">qk_rope_head_dim&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># W^{DQ}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_a_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_lora_rank&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">attention_bias&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># [W^{UQ}; W^{QR}]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_b_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_lora_rank&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_heads&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_head_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># [W^{DKV}; W^{KR}]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">kv_a_proj_with_mqa&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">kv_lora_rank&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">qk_rope_head_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">attention_bias&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># [W^{UK}; W^{UV}]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">kv_b_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">kv_lora_rank&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_heads&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_head_dim&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">qk_rope_head_dim&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">v_head_dim&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># W^O&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">o_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_heads&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">v_head_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">attention_bias&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">...&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># [q_t^c; q_t^R]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_b_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_a_layernorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_a_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">)))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">q&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">bsz&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">q_len&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_heads&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_head_dim&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transpose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q_nope&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">q_pe&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">split&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">q&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">qk_nope_head_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">qk_rope_head_dim&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># [c_t^{KV}; k_t^R]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">compressed_kv&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">kv_a_proj_with_mqa&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">compressed_kv&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k_pe&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">split&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">compressed_kv&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">kv_lora_rank&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">qk_rope_head_dim&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">k_pe&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">k_pe&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">bsz&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">q_len&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">qk_rope_head_dim&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transpose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># [k_t^c; v_t^c]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">kv&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">kv_b_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">kv_a_layernorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">compressed_kv&lt;/span>&lt;span class="p">))&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">bsz&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">q_len&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_heads&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">qk_nope_head_dim&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">v_head_dim&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transpose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">k_nope&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">value_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">split&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">kv&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">qk_nope_head_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">v_head_dim&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># q_t^R, k_t^R&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q_pe&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k_pe&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">apply_rotary_pos_emb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">q_pe&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k_pe&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">cos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sin&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">position_ids&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># q_{t, i}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">query_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">k_pe&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">new_empty&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">bsz&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_heads&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">q_len&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_head_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">query_states&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="p">:&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">qk_nope_head_dim&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">q_nope&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">query_states&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">qk_nope_head_dim&lt;/span> &lt;span class="p">:]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">q_pe&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># k_{t, i}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">key_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">k_pe&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">new_empty&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">bsz&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_heads&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">q_len&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_head_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">key_states&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="p">:&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">qk_nope_head_dim&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">k_nope&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">key_states&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">qk_nope_head_dim&lt;/span> &lt;span class="p">:]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">k_pe&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Q^TK&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attn_weights&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">matmul&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">query_states&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">key_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transpose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">softmax_scale&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># softmax(...) in FP32&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attn_weights&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">functional&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">softmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">attn_weights&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">query_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dtype&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># o_{t, i}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attn_output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">matmul&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">attn_weights&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">value_states&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># u_t&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attn_output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">o_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">attn_output&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">attn_output&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">attn_weights&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">past_key_value&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="参数量计算">&lt;a href="#%e5%8f%82%e6%95%b0%e9%87%8f%e8%ae%a1%e7%ae%97" class="header-anchor">&lt;/a>参数量计算
&lt;/h3>&lt;p>首先，我们结合 DeepSeek-V2 的 &lt;code>config&lt;/code> 计算一下 MLA 部分的参数量：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Matrix&lt;/th>
&lt;th>Parameters&lt;/th>
&lt;th>values&lt;/th>
&lt;th>ratio&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$W^{DKV}$&lt;/td>
&lt;td>$dd_c$&lt;/td>
&lt;td>2621440&lt;/td>
&lt;td>1.91%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$W^{UK}$&lt;/td>
&lt;td>$d_hn_hd_c$&lt;/td>
&lt;td>8388608&lt;/td>
&lt;td>6.12%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$W^{UV}$&lt;/td>
&lt;td>$d_hn_hd_c$&lt;/td>
&lt;td>8388608&lt;/td>
&lt;td>6.12%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$W^{DQ}$&lt;/td>
&lt;td>$d_c'd$&lt;/td>
&lt;td>7864320&lt;/td>
&lt;td>5.74%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$W^{UQ}$&lt;/td>
&lt;td>$d_hn_hd_c'$&lt;/td>
&lt;td>25165824&lt;/td>
&lt;td>18.37%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$W^{KR}$&lt;/td>
&lt;td>$d_h^Rd$&lt;/td>
&lt;td>327680&lt;/td>
&lt;td>0.24%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$W^{QR}$&lt;/td>
&lt;td>$d_h^Rd$&lt;/td>
&lt;td>327680&lt;/td>
&lt;td>0.24%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$W^{O}$&lt;/td>
&lt;td>$dd_hn_h$&lt;/td>
&lt;td>83886080&lt;/td>
&lt;td>61.24%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Total&lt;/td>
&lt;td>$d(d_c+d_c'+2h_h^R)+d_hn_h(2d_c+d_c'+d)$&lt;/td>
&lt;td>136970240&lt;/td>
&lt;td>100%&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>我们接下来对比一下各个模型架构之间 attention 部分的参数量，可以看到与 MHA 一致，大部分参数量都集中在最后的 Output projection layer 上&lt;/p>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>作者首先对比了 MHA, GQA, MQA 的表现，作者基于一个 7B 的 dense 模型，使用 1.33T token 进行训练，实验结果如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Benchmark (Metric)&lt;/th>
&lt;th># Shots&lt;/th>
&lt;th>MQA&lt;/th>
&lt;th>GQA(8 Groups)&lt;/th>
&lt;th>MHA&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td># Params&lt;/td>
&lt;td>-&lt;/td>
&lt;td>7.1B&lt;/td>
&lt;td>6.9B&lt;/td>
&lt;td>6.9B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>BBH (EM)&lt;/td>
&lt;td>3-shot&lt;/td>
&lt;td>33.2&lt;/td>
&lt;td>35.6&lt;/td>
&lt;td>&lt;strong>37.0&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MMLU (Acc.)&lt;/td>
&lt;td>5-shot&lt;/td>
&lt;td>37.9&lt;/td>
&lt;td>41.2&lt;/td>
&lt;td>&lt;strong>45.2&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>C-Eval (Acc.)&lt;/td>
&lt;td>5-shot&lt;/td>
&lt;td>30.0&lt;/td>
&lt;td>37.7&lt;/td>
&lt;td>&lt;strong>42.9&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CMMLU (Acc.)&lt;/td>
&lt;td>5-shot&lt;/td>
&lt;td>34.6&lt;/td>
&lt;td>38.4&lt;/td>
&lt;td>&lt;strong>43.5&lt;/strong>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>实验结果显示，MHA 的表现显著优于 GQA 和 MQA. 这说明了 MQA 和 GQA 虽然减少了 KV cache 的占用，但是相应地，它们对应的表现也有所降低。&lt;/p>
&lt;p>接下来，作者对比了 MLA 和 MHA 的表现，实验结果如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Benchmark (Metric)&lt;/th>
&lt;th># Shots&lt;/th>
&lt;th>MHA&lt;/th>
&lt;th>MLA&lt;/th>
&lt;th>MHA&lt;/th>
&lt;th>MLA&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td># Activated Params&lt;/td>
&lt;td>-&lt;/td>
&lt;td>2.5B&lt;/td>
&lt;td>2.4B&lt;/td>
&lt;td>25.0B&lt;/td>
&lt;td>21.5B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Total Params&lt;/td>
&lt;td>-&lt;/td>
&lt;td>15.8B&lt;/td>
&lt;td>15.7B&lt;/td>
&lt;td>250.8B&lt;/td>
&lt;td>247.4B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>KV Cache per Token (# Element)&lt;/td>
&lt;td>-&lt;/td>
&lt;td>110.6K&lt;/td>
&lt;td>15.6K&lt;/td>
&lt;td>860.2K&lt;/td>
&lt;td>34.6K&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>BBH (EM)&lt;/td>
&lt;td>3-shot&lt;/td>
&lt;td>37.9&lt;/td>
&lt;td>&lt;strong>39.0&lt;/strong>&lt;/td>
&lt;td>46.6&lt;/td>
&lt;td>&lt;strong>50.7&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MMLU (Acc.)&lt;/td>
&lt;td>5-shot&lt;/td>
&lt;td>48.7&lt;/td>
&lt;td>&lt;strong>50.0&lt;/strong>&lt;/td>
&lt;td>57.5&lt;/td>
&lt;td>&lt;strong>59.0&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>C-Eval (Acc.)&lt;/td>
&lt;td>5-shot&lt;/td>
&lt;td>&lt;strong>51.6&lt;/strong>&lt;/td>
&lt;td>50.9&lt;/td>
&lt;td>57.9&lt;/td>
&lt;td>&lt;strong>59.2&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CMMLU (Acc.)&lt;/td>
&lt;td>5-shot&lt;/td>
&lt;td>52.3&lt;/td>
&lt;td>&lt;strong>53.4&lt;/strong>&lt;/td>
&lt;td>60.7&lt;/td>
&lt;td>&lt;strong>62.5&lt;/strong>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，MLA 的表现比 MHA 的表现更好，并且 KV cache 也更少。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者提出了 MLA, 一个基于 low rank compression 的注意力机制，通过将 key value vector 压缩到低维空间，MLA 可以有效降低 Inference latency, 作者通过实现证明 MLA 的表现可以与 MHA 相比，并且 KV cache 更小。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2405.04434" target="_blank" rel="noopener"
>DeepSeek-V2&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2502.14837" target="_blank" rel="noopener"
>MHA2MLA&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.cnblogs.com/rossiXYZ/p/18827618" target="_blank" rel="noopener"
>探秘Transformer系列之（28）&amp;mdash; DeepSeek MLA&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Loss-free Balancing</title><link>https://maosong.website/p/notes-on-loss-free-balancing/</link><pubDate>Fri, 21 Nov 2025 15:38:52 +0800</pubDate><guid>https://maosong.website/p/notes-on-loss-free-balancing/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>已有的 MoE 模型往往都会使用 load balancing loss 来避免 Imbalanced routing, 但是加入这个额外的损失之后，模型训练的梯度也会受到影响。&lt;/p>
&lt;p>DeepSeek 基于这个问题提出了 Loss-Free Balancing, 该方法不引入额外的 loss item, 而是在 routing 的结果上加入一个 bias item, bias item 可以根据 expert load 来动态更新进而实现 load balancing.&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;p>MoE definition&lt;/p>
$$
\begin{aligned}
h_t &amp;= u_t + \sum_{i=1}^Ng_{i,t}\mathrm{FFN}_i(u_t),\\
g_{i,t} &amp;= \begin{cases}
s_{i,t}, &amp; s_{i,t}\in\mathrm{Topk}(\{s_{i,j}\mid 1\leq j\leq N\}, K)\\
0, &amp;\text{otherwise}
\end{cases}\\
s_{i,t} &amp;= G(u_t^Te_i),
\end{aligned}
$$&lt;p>其中 $G$ 是 gating function, $e_i$ 是第 $i$ 个专家对应 gating function 的权重&lt;/p>
&lt;p>load balancing loss 有两个作用：&lt;/p>
&lt;ol>
&lt;li>避免 routing collapse, 即模型只选择固定的少数专家完成任务&lt;/li>
&lt;li>减少通信开销&lt;/li>
&lt;/ol>
&lt;p>但是引入 load balancing loss 会对 LLM 的训练产生影响，为了避免对模型性能造成影响，我们需要小心设置 load balancing loss 的权重，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-loss-free-balancing/loss-free-balancing-hyperparameter-setting.png"
width="917"
height="575"
loading="lazy"
alt="The dilemma between load balance and model performance for auxiliary-loss-controlled training"
class="gallery-image"
data-flex-grow="159"
data-flex-basis="382px"
>&lt;/p>
&lt;p>为了解决这个问题，作者提出了 Loss-Free Balancing, 具体做法就是在每个专家的 gating score 上加入一个 bias term, 然后再决定对应的专家：&lt;/p>
$$
g_{i,t} = \begin{cases}
s_{i,t}, &amp; s_{i,t}+b_i\in\mathrm{Topk}(\{s_{i,j}+b_j\mid 1\leq j\leq N\}, K)\\
0, &amp;\text{otherwise}
\end{cases}
$$&lt;p>注意这里的 bias item 仅影响 top-K 操作，其对最终的输出没有影响。&lt;/p>
&lt;p>为了实现负载均衡，作者根据上一个 batch 的 expert load 情况来调整 bias item, 如果某一个专家的 load 太大，则对应的 bias item 会变小。 其算法实现过程如下：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-loss-free-balancing/loss-free-balancing-algorithm.png"
width="1152"
height="410"
loading="lazy"
alt="Adjusting the per-expert bias during training"
class="gallery-image"
data-flex-grow="280"
data-flex-basis="674px"
>&lt;/p>
&lt;p>作者对比不同的负载均衡算法如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Load Balancing Methods&lt;/th>
&lt;th>Balanced Expert Load&lt;/th>
&lt;th>Interference Gradients&lt;/th>
&lt;th>Future Token Leakage&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Loss-Controlled (strong auxiliary loss)&lt;/td>
&lt;td>balanced&lt;/td>
&lt;td>strong&lt;/td>
&lt;td>no leakage&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Loss-Controlled (weak auxiliary loss)&lt;/td>
&lt;td>imbalanced&lt;/td>
&lt;td>weak&lt;/td>
&lt;td>no leakage&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Expert Choice&lt;/td>
&lt;td>balanced&lt;/td>
&lt;td>none&lt;/td>
&lt;td>with leakage&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Loss-Free (Ours)&lt;/td>
&lt;td>balanced&lt;/td>
&lt;td>none&lt;/td>
&lt;td>no leakage&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>作者使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseekmoe/" target="_blank" rel="noopener"
>DeepSeekMoE&lt;/a> 来进行实验，作者使用了 sigmoid function 作为 gating function, 因为作者发现 sigmoid function 效果比 softmax 效果更好。&lt;/p>
&lt;p>作者提出了 maximal violation (MaxVio) 来量化一个 MoE layer 的负载均衡程度&lt;/p>
$$
\mathrm{MaxVio} = \frac{\max_i\mathrm{Load}_i-\overline{\mathrm{Load}_i}}{\overline{\mathrm{Load}_i}}
$$&lt;p>其中 $\mathrm{Load}_i$ 代表了分配给第 $i$ 个专家的 token 个数，$\overline{\mathrm{Load}_i}$ 代表了理想情况下的负载均衡。&lt;/p>
&lt;p>实验结果如下图所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model Size&lt;/th>
&lt;th>Load Balancing Methods&lt;/th>
&lt;th>Validation Perplexity&lt;/th>
&lt;th>MaxVio&lt;sub>global&lt;/sub>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>1B&lt;/td>
&lt;td>Loss-Controlled&lt;/td>
&lt;td>9.56&lt;/td>
&lt;td>0.72&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1B&lt;/td>
&lt;td>Loss-Free&lt;/td>
&lt;td>&lt;strong>9.50&lt;/strong>&lt;/td>
&lt;td>&lt;strong>0.04&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>3B&lt;/td>
&lt;td>Loss-Controlled&lt;/td>
&lt;td>7.97&lt;/td>
&lt;td>0.52&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>3B&lt;/td>
&lt;td>Loss-Free&lt;/td>
&lt;td>&lt;strong>7.92&lt;/strong>&lt;/td>
&lt;td>&lt;strong>0.04&lt;/strong>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>实验结果显示，本文提出的 Loss-Free Balancing 效果更好，且负载均衡更高&lt;/p>
&lt;p>作者还展示了训练过程的负载情况如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-loss-free-balancing/loss-free-training-process-load.png"
width="1145"
height="406"
loading="lazy"
alt="Load of training process"
class="gallery-image"
data-flex-grow="282"
data-flex-basis="676px"
>&lt;/p>
&lt;p>可以看到，Loss-Free 的负载一直比 load balancing loss 效果更好&lt;/p>
&lt;h3 id="ablation-study">&lt;a href="#ablation-study" class="header-anchor">&lt;/a>Ablation Study
&lt;/h3>&lt;p>作者首先探究了 bias term 更新速率对模型表现的影响，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-loss-free-balancing/loss-free-ablation-update-rate.png"
width="920"
height="508"
loading="lazy"
alt="The impact of update rate on training load balance"
class="gallery-image"
data-flex-grow="181"
data-flex-basis="434px"
>&lt;/p>
&lt;p>结果显示，使用过大的 update rate 会影响最终的负载均衡，而较小的 update rate 收敛速率比较慢。因此作者在本文中使用了 $u=0.001$ 这个设置&lt;/p>
&lt;p>作者还对比了 baseline model 使用 softmax 个 sigmoid gating 两种方式，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-loss-free-balancing/loss-free-ablation-gating-function.png"
width="917"
height="573"
loading="lazy"
alt="Ablation study on gating function"
class="gallery-image"
data-flex-grow="160"
data-flex-basis="384px"
>&lt;/p>
&lt;p>实验结果显示，Sigmoid function 对于超参数更加 robust, 且表现也更好一些。&lt;/p>
&lt;p>作者还尝试了不同的 bias 更新方式，结果显示尽管不同的更新方式有可能会提高 load balance, 但是最终模型表现提升不大。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 Loss-Free Balancing 策略，一个针对 MoE 负载均衡而不需要额外损失项的方法&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2408.15664" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Mixstral 8x7B</title><link>https://maosong.website/p/mixstral-8x7b/</link><pubDate>Sat, 01 Nov 2025 15:32:30 +0800</pubDate><guid>https://maosong.website/p/mixstral-8x7b/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者在本文中提出了 Mixtral 8x7B, 一个 MoE 模型，模型上下文为 32K. 作者还对模型进行 finetune 得到了 Mixtral 8x7B-Instruct, finetuning 包含 SFT 和 &lt;a class="link" href="https://maosong.website/p/notes-on-dpo/" target="_blank" rel="noopener"
>DPO&lt;/a> 两个阶段。&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;p>模型架构与 &lt;a class="link" href="https://maosong.website/p/mixstral-7b/" target="_blank" rel="noopener"
>Mistral-7B&lt;/a> 基本相同，参数如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Parameter&lt;/th>
&lt;th>Value&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>dim&lt;/code>&lt;/td>
&lt;td>4096&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>n_layers&lt;/code>&lt;/td>
&lt;td>32&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>head_dim&lt;/code>&lt;/td>
&lt;td>128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>hidden_dim&lt;/code>&lt;/td>
&lt;td>14336&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>n_heads&lt;/code>&lt;/td>
&lt;td>32&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>n_kv_heads&lt;/code>&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>window_size&lt;/code>&lt;/td>
&lt;td>4096&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>context_len&lt;/code>&lt;/td>
&lt;td>32768&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>vocab_size&lt;/code>&lt;/td>
&lt;td>32000&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>num_experts&lt;/code>&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>top_k_experts&lt;/code>&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>MoE 的架构与 &lt;a class="link" href="https://maosong.website/p/gshard/" target="_blank" rel="noopener"
>GShard&lt;/a> 基本一致&lt;/p>
&lt;h2 id="results">&lt;a href="#results" class="header-anchor">&lt;/a>Results
&lt;/h2>&lt;p>作者探究了专家的 specialization, 结果有三点发现：&lt;/p>
&lt;ol>
&lt;li>不同专家对于不同 domain 的数据并没有出现 specialization&lt;/li>
&lt;li>在 math domain 上，专家的分布有一个明显的区别。&lt;/li>
&lt;li>连续的 token 往往会被分配到同一个专家上&lt;/li>
&lt;/ol>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文提出了 Mistral 8x7B, 一个 MoE 大语言模型&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2401.04088" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Mixstral 7B</title><link>https://maosong.website/p/mixstral-7b/</link><pubDate>Sat, 01 Nov 2025 15:28:19 +0800</pubDate><guid>https://maosong.website/p/mixstral-7b/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者主要强调 Mistral 7B 的表现超过了 LLaMA 2 7B 和 LLaMA 34B 的表现。&lt;/p>
&lt;p>Mistral 7B 主要使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-gqa/" target="_blank" rel="noopener"
>GQA&lt;/a> 以及 SWA 两个方法来加速推理和减少内存占用，进而提高 batch size 和 throughput.&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;p>Mistral 7B 的模型配置如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Parameter&lt;/th>
&lt;th>Value&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>dim&lt;/code>&lt;/td>
&lt;td>4096&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>n_layers&lt;/code>&lt;/td>
&lt;td>32&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>head_dim&lt;/code>&lt;/td>
&lt;td>128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>hidden_dim&lt;/code>&lt;/td>
&lt;td>14336&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>n_heads&lt;/code>&lt;/td>
&lt;td>32&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>n_kv_heads&lt;/code>&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>window_size&lt;/code>&lt;/td>
&lt;td>4096&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>context_len&lt;/code>&lt;/td>
&lt;td>8192&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>vocab_size&lt;/code>&lt;/td>
&lt;td>32000&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>SWA 的示意图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/mixstral-7b/Mistral-7B-SWA.png"
width="1047"
height="387"
loading="lazy"
alt="Sliding Window Attention"
class="gallery-image"
data-flex-grow="270"
data-flex-basis="649px"
>&lt;/p>
&lt;p>对于第 $k$ 层，模型可以感知到 $W\times k$ 的 tokens, 进而可以在提高训练效率的同时保持模型的表现。作者发现，在 &lt;a class="link" href="https://maosong.website/p/notes-on-flashattention/" target="_blank" rel="noopener"
>flash attention&lt;/a> 中使用 SWA 之后，模型的效率提升了 2 倍左右。&lt;/p>
&lt;p>并且使用了 SWA 之后，我们的 kv cache 也就随之固定了，因此我们可以使用一个 rolling buff cache, 其大小为 $W$, 对于第 $i$ 个 token, 我们将其保存在 cache 中的第 $i \% M$ 个位置。&lt;/p>
&lt;p>作者还进一步将 sequence 分割为多个 chunk, 每个 chunk 的大小都是 window size $M$, 这样在计算 attention 的时候，对于当前的 chunk, 我们使用 self-attention, 对于 cache 中的 attention, 我们使用 SWA, 然后对于 past token, 由于这部分不在 sliding window 内因此不参与计算&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/mixstral-7b/Mixtral-7B-prefill-and-chunking.png"
width="748"
height="307"
loading="lazy"
alt="Prefilling and Chunking"
class="gallery-image"
data-flex-grow="243"
data-flex-basis="584px"
>&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 Mistral 7B, 一个基于 GQA 和 SWA 的大语言模型。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2310.06825v1" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on olmoe</title><link>https://maosong.website/p/notes-on-olmoe/</link><pubDate>Sat, 01 Nov 2025 15:23:58 +0800</pubDate><guid>https://maosong.website/p/notes-on-olmoe/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>已有的模型如 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseekmoe/" target="_blank" rel="noopener"
>DeepSeek-MoE&lt;/a>, &lt;a class="link" href="https://maosong.website/p/mixstral-8x7b/" target="_blank" rel="noopener"
>Mixtral MoE&lt;/a>, &lt;a class="link" href="https://maosong.website/search/?keyword=qwen1.5" target="_blank" rel="noopener"
>Qwen1.5&lt;/a> 等 MoE 模型基本只开源权重。也有一些开源的模型，比如 OpenMoE 等，但是开源信息不全。基于这个目的，作者提出了 olmoe 模型系列，包括 olmoe-7B-A1B 和 olmoe-7B-A1B-instruct 两个版本。&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="pretraining">&lt;a href="#pretraining" class="header-anchor">&lt;/a>Pretraining
&lt;/h3>&lt;p>模型的架构如下图所示，MoE 架构与 dense 架构不同的地方在于 decoder layer 中的 FFN 被替换为了 MoE layer.&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-olmoe/olmoe-MoE_architecture.png"
width="1356"
height="912"
loading="lazy"
alt="Architecture of olmoe"
class="gallery-image"
data-flex-grow="148"
data-flex-basis="356px"
>&lt;/p>
&lt;p>模型的配置如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-olmoe/olmoe-config.png"
width="900"
height="1201"
loading="lazy"
alt="Model configuration"
class="gallery-image"
data-flex-grow="74"
data-flex-basis="179px"
>&lt;/p>
&lt;p>训练的目标函数为&lt;/p>
$$
\mathcal{L} = \mathcal{L}_{CE} +\alpha\mathcal{L}_{LB} +\beta\mathcal{L}_{RZ}
$$&lt;p>其中 $\alpha,\beta$ 为系数， $\mathcal{L}_{CE}$, $\mathcal{L}_{LB}$ 以及 $\mathcal{L}_{RZ}$ 分别代表 cross-entropy loss, load balancing loss 以及 routing Z loss.&lt;/p>
&lt;p>预训练数据包括 DCLM 和 Dolma1.7 两个数据集的混合，作者将预训练数据集称为&lt;strong>olmoe-mix&lt;/strong>. 数据集的配比如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-olmoe/olmoe-composition-pretraining-data.png"
width="1147"
height="441"
loading="lazy"
alt="Composition of the pretraining data"
class="gallery-image"
data-flex-grow="260"
data-flex-basis="624px"
>&lt;/p>
&lt;h3 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h3>&lt;p>在 post-training 时，作者将训练分为 instruction tuning 和 preference tuning 两个阶段，在 instruction dataset 中，作者加入了更多的代码和数学数据来提高对应的能力。数据集如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-olmoe/olmoe-composition-post-training-data.png"
width="1208"
height="367"
loading="lazy"
alt="Post-training data"
class="gallery-image"
data-flex-grow="329"
data-flex-basis="789px"
>&lt;/p>
&lt;h2 id="ablation-study">&lt;a href="#ablation-study" class="header-anchor">&lt;/a>Ablation Study
&lt;/h2>&lt;h3 id="moe-settings">&lt;a href="#moe-settings" class="header-anchor">&lt;/a>MoE Settings
&lt;/h3>&lt;h4 id="moe-vs-dense">&lt;a href="#moe-vs-dense" class="header-anchor">&lt;/a>MoE vs. Dense
&lt;/h4>&lt;p>作者对比了 MoE 模型和 dense 模型的训练效率，为了方便对比，作者使用 olmo-7B 和 olmo-1B 作为 baseline, 最终 olmoe 的总参数为 6.9B, 激活参数为 1.3B. 实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-olmoe/olmoe-MoE-vs-Dense-training-efficiency.png"
width="1155"
height="626"
loading="lazy"
alt="MoE vs. Dense"
class="gallery-image"
data-flex-grow="184"
data-flex-basis="442px"
>&lt;/p>
&lt;p>实验结果发现，MoE 模型所需要的 token 或者 FLOPs 是 dense 模型的 $1/3$, 但是由于 MoE 模型需要额外的内存开销，因此从训练时间上来看，MoE 模型训练时间仅比 dense 模型快 $2$ 倍左右。&lt;/p>
&lt;h4 id="expert-granularity">&lt;a href="#expert-granularity" class="header-anchor">&lt;/a>Expert Granularity
&lt;/h4>&lt;p>&lt;a class="link" href="https://maosong.website/p/notes-on-deepseekmoe/" target="_blank" rel="noopener"
>DeepSeekMoE&lt;/a> 提出使用细粒度的专家来提供更多的组合可能性。作者探究了不同的粒度对模型表现的影响，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-olmoe/olmoe-expert-granularity.png"
width="1446"
height="592"
loading="lazy"
alt="Expert granularity"
class="gallery-image"
data-flex-grow="244"
data-flex-basis="586px"
>&lt;/p>
&lt;p>结果显示，当专家粒度从 8E-1A 扩展到 32E-4A 时，模型在 HellaSwag 上的表现提升了 $10\%$, 但是进一步扩展到 64E-8A 时，模型的表现提升不到 $2\%$, 这说明了无限制提升粒度对模型的提升越来越有限。在本文中，作者使用了 64 个专家。&lt;/p>
&lt;h4 id="shared-experts">&lt;a href="#shared-experts" class="header-anchor">&lt;/a>Shared Experts
&lt;/h4>&lt;p>&lt;a class="link" href="https://maosong.website/p/notes-on-deepseekmoe/" target="_blank" rel="noopener"
>DeepSeekMoE&lt;/a> 提出使用共享专家来学习 common knowledge, 作者对这种方法进行了实验，结果如下图所示。&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-olmoe/olmoe-shared-experts.png"
width="1156"
height="477"
loading="lazy"
alt="Shared experts"
class="gallery-image"
data-flex-grow="242"
data-flex-basis="581px"
>&lt;/p>
&lt;p>可以看到，加入一个 shared expert 之后，模型的表现没有变化，作者认为减少 routed expert 之后，模型的组合可能性降低为原来的 $10\%$ 左右。因此作者认为没有必要使用共享专家，因此作者在 olmoe 中没有采用共享专家这个方法。&lt;/p>
&lt;h4 id="expert-choice-vs-token-choice">&lt;a href="#expert-choice-vs-token-choice" class="header-anchor">&lt;/a>Expert Choice vs. Token Choice
&lt;/h4>&lt;p>作者探究了 routing 的策略，一个是 expert choice (EC), 另一种是 token choice (TC), 分别代表了每个 expert 选取固定的 token 数和每个 token 选取固定的 expert 数这两种情况。实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-olmoe/olmoe-routing-strategy.png"
width="1157"
height="450"
loading="lazy"
alt="Expert choice (EC) vs. token choice (TC)"
class="gallery-image"
data-flex-grow="257"
data-flex-basis="617px"
>&lt;/p>
&lt;p>可以看到，token choice 的表现明显更好。EC 虽然可以实现负载均衡。但是因为自回归模型在生成时是无法提前确定生成的 token 数的，因此 EC 很可能导致算力资源浪费或者是 token dropping. 在本文中，作者采用了 TC 这种策略。&lt;/p>
&lt;h4 id="sparse-upcycling">&lt;a href="#sparse-upcycling" class="header-anchor">&lt;/a>Sparse Upcycling
&lt;/h4>&lt;p>作者还对比了从零开始训练 MoE 与基于 dense model upcycling 的方式训练 MoE，sparse upcycling 的相关工作有 MiniCPM, &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2/" target="_blank" rel="noopener"
>Qwen2&lt;/a> 以及 &lt;a class="link" href="https://maosong.website/p/mixstral-8x7b/" target="_blank" rel="noopener"
>Mixtral MoE&lt;/a>.结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-olmoe/olmoe-sparse-upcycling.png"
width="1440"
height="680"
loading="lazy"
alt="Sparse upcycling"
class="gallery-image"
data-flex-grow="211"
data-flex-basis="508px"
>&lt;/p>
&lt;p>结果发现，upcycling 确实可以提高训练效率，但是这种方法的缺陷在于：&lt;/p>
&lt;ol>
&lt;li>upcycling 受 dense model 的超参数限制&lt;/li>
&lt;li>upcycling 的训练不是很稳定&lt;/li>
&lt;/ol>
&lt;p>因此在本文中作者没有采取 upcycling 的做法。&lt;/p>
&lt;h4 id="load-balancing-loss">&lt;a href="#load-balancing-loss" class="header-anchor">&lt;/a>Load Balancing Loss
&lt;/h4>&lt;p>作者还探究 Load Balancing loss 对模型表现的影响，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-olmoe/olmoe-load-balancing-loss.png"
width="1450"
height="532"
loading="lazy"
alt="Impact of applying a load balancing loss"
class="gallery-image"
data-flex-grow="272"
data-flex-basis="654px"
>&lt;/p>
&lt;p>可以看到，加入 load balancing loss 之后，模型的表现均超过了不加时的表现。&lt;/p>
&lt;p>作者进一步分析了不同专家在加/不加 load balancing loss 时的激活情况，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-olmoe/ollmoe-load-balancing-expert-assignment.png"
width="1443"
height="566"
loading="lazy"
alt="Expert assignment during training"
class="gallery-image"
data-flex-grow="254"
data-flex-basis="611px"
>&lt;/p>
&lt;p>结果显示，load balancing loss 确实可以让不同专家的激活概率大致相当。&lt;/p>
&lt;h4 id="router-z-loss">&lt;a href="#router-z-loss" class="header-anchor">&lt;/a>Router Z-loss
&lt;/h4>&lt;p>&lt;a class="link" href="https://maosong.website/p/st-moe/" target="_blank" rel="noopener"
>ST-MoE&lt;/a> 提出了 Router z-loss 来提高 MOE 训练的稳定性和表现。其表达式如下所示&lt;/p>
$$
\mathcal{L}_{RZ}(x) = \frac{1}{B}\sum_{i=1}^B\left(\log\sum_{j=1}^{N_E}\exp(x_i^{(i)})\right)^2
$$&lt;p>实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-olmoe/olmoe-routing-z-loss.png"
width="1162"
height="419"
loading="lazy"
alt="Router z-loss"
class="gallery-image"
data-flex-grow="277"
data-flex-basis="665px"
>&lt;/p>
&lt;p>可以看到，加入 router Z-loss 之后，模型训练的稳定性有所提升。因此在本文中作者使用了这个 loss.&lt;/p>
&lt;h3 id="general-pre-training-settings">&lt;a href="#general-pre-training-settings" class="header-anchor">&lt;/a>General Pre-training Settings
&lt;/h3>&lt;h4 id="initialization">&lt;a href="#initialization" class="header-anchor">&lt;/a>Initialization
&lt;/h4>&lt;p>作者探究了不同初始化策略对模型训练的影响，结果发现使用 truncate normal initialization 的训练稳定性更高&lt;/p>
&lt;h4 id="qk-norm">&lt;a href="#qk-norm" class="header-anchor">&lt;/a>QK-Norm
&lt;/h4>&lt;p>作者探究了 &lt;a class="link" href="https://maosong.website/p/notes-on-qk-norm/" target="_blank" rel="noopener"
>QK-norm&lt;/a> 对模型训练的影响，结果发现 QK-norm 可以提高模型训练的稳定性，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-olmoe/olmoe-QK-norm-ablation.png"
width="1157"
height="306"
loading="lazy"
alt="ablation study on QK-Norm"
class="gallery-image"
data-flex-grow="378"
data-flex-basis="907px"
>&lt;/p>
&lt;h4 id="adamw-epsilon">&lt;a href="#adamw-epsilon" class="header-anchor">&lt;/a>AdamW Epsilon
&lt;/h4>&lt;p>作者发现，在 &lt;a class="link" href="https://maosong.website/p/notes-on-adamw/" target="_blank" rel="noopener"
>AdamW&lt;/a> 优化器中，使用更小的 &lt;code>eps&lt;/code> 可以提高模型的表现，因此作者将 &lt;code>eps&lt;/code> 设置为 $1e-8$.&lt;/p>
&lt;h4 id="adaptation-settings">&lt;a href="#adaptation-settings" class="header-anchor">&lt;/a>Adaptation Settings
&lt;/h4>&lt;p>在 post-training 阶段，作者在三个方面进行了实验：&lt;/p>
&lt;ol>
&lt;li>是否加入 load balancing loss: 结论是不加，因为负载均衡在 pre-training 阶段已经实现了&lt;/li>
&lt;li>是否使用 annealing: 结论是使用，因为效果更好&lt;/li>
&lt;li>使用 DPO 还是 KTO, 结论是两种方法结果差不多&lt;/li>
&lt;/ol>
&lt;p>实验结果如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-olmoe/olmoe-adaptation-experiments.png"
width="1105"
height="588"
loading="lazy"
alt="Adaptation experiments"
class="gallery-image"
data-flex-grow="187"
data-flex-basis="451px"
>&lt;/p>
&lt;h4 id="load-balancing-precision">&lt;a href="#load-balancing-precision" class="header-anchor">&lt;/a>Load Balancing Precision
&lt;/h4>&lt;p>&lt;a class="link" href="https://maosong.website/p/switch-transformer/" target="_blank" rel="noopener"
>Switch Transformer&lt;/a> 中提出使用 &lt;code>float32&lt;/code> 精度来进行 routing 的计算，作者通过实验发现，这一方法并不能提高模型训练的稳定性，因此作者没有采用这一策略。&lt;/p>
&lt;h2 id="moe-analysis">&lt;a href="#moe-analysis" class="header-anchor">&lt;/a>MoE Analysis
&lt;/h2>&lt;h3 id="router-saturation">&lt;a href="#router-saturation" class="header-anchor">&lt;/a>Router Saturation
&lt;/h3>&lt;p>作者探究了训练过程中激活的专家和训练结束后激活的专家的匹配程度，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-olmoe/olmoe-router-saturation.png"
width="1149"
height="393"
loading="lazy"
alt="Router saturation"
class="gallery-image"
data-flex-grow="292"
data-flex-basis="701px"
>&lt;/p>
&lt;p>结果发现，训练 $1\%$ 的数据之后，就有 $40\%$ 的 routing 和训练完毕的 routing 一致，当训练 $40\%$ 的数据之后，这个比例提升到了 $80\%$.&lt;/p>
&lt;p>作者还发现，later layers 比 early layers 饱和更快，early layer, 特别是 layer 0, 饱和的非常慢。作者认为，这是 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseekmoe/" target="_blank" rel="noopener"
>DeepSeekMoE&lt;/a> 放弃在第一层使用 MoE layer 的原因，因为 load balancing loss 收敛更慢。&lt;/p>
&lt;h3 id="expert-co-activation">&lt;a href="#expert-co-activation" class="header-anchor">&lt;/a>Expert Co-activation
&lt;/h3>&lt;p>作者分析了 expert 之间的互相依赖程度，作者通过可视化发现，不同的 expert 之间 co-activation 的比例比较小，说明 expert redundancy 比较低&lt;/p>
&lt;h3 id="domain-specialization">&lt;a href="#domain-specialization" class="header-anchor">&lt;/a>Domain Specialization
&lt;/h3>&lt;p>作者还探究了不同 expert 对于不同 domain 的 specialization 程度，作者发现对于 specialized domain 的数据，expert 会出现一定程度的 specialization, 但是对于通用 domain 的数据，expert 的 specialization 程度比较低。这个结论与 &lt;a class="link" href="https://maosong.website/p/mixstral-8x7b/" target="_blank" rel="noopener"
>Mixtral MoE&lt;/a> 的结论不同，作者认为这个原因是 &lt;a class="link" href="https://maosong.website/p/mixstral-8x7b/" target="_blank" rel="noopener"
>Mixtral MoE&lt;/a> 使用了 upcycling 的方式，这会限制模型的表现。因此，作者进一步强调 MoE 从零开始训练是一个更好的训练方式。&lt;/p>
&lt;h3 id="vocabulary-specialization">&lt;a href="#vocabulary-specialization" class="header-anchor">&lt;/a>Vocabulary Specialization
&lt;/h3>&lt;p>作者还探究了 vocabulary 中不同 token index 与激活专家之间的关系，结果发现 later layers 的 specialization 程度更高，这与 saturation 的趋势一致&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 olmoe, 一个全开源的 moe 大模型系列，作者详细介绍了针对 MoE 架构和通用架构的设计，为后来的模型架构设计提供了基础。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2409.02060" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>GShard</title><link>https://maosong.website/p/gshard/</link><pubDate>Wed, 29 Oct 2025 11:22:39 +0800</pubDate><guid>https://maosong.website/p/gshard/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者认为，训练大规模的模型存在如下问题：&lt;/p>
&lt;ol>
&lt;li>缺乏有效的 Model parallelism 算法&lt;/li>
&lt;li>随着设备数的增加，训练时间与 model size 呈现超线性增长的关系&lt;/li>
&lt;li>(tensorflow) 在 nodes 数增多时，构建所需要的时间也大幅度增长&lt;/li>
&lt;li>在多个设备上 partition model 比较困难&lt;/li>
&lt;/ol>
&lt;p>作者在本文中构建了一个基于 sparse MoE 架构的 600B 模型。为了解决这些问题，作者作出了如下贡献：&lt;/p>
&lt;ol>
&lt;li>作者提出了基于 MoE 架构的模型，来减少计算和通信开销&lt;/li>
&lt;li>作者提出了 Gshard, 来自动化实现并行&lt;/li>
&lt;li>作者提出了 SPMD (single program multiple data) 来减少计算表示和编译的难度&lt;/li>
&lt;/ol>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;p>架构如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/gshard/GShard-model-architecture.png"
width="1118"
height="713"
loading="lazy"
alt="architecture of GShard"
class="gallery-image"
data-flex-grow="156"
data-flex-basis="376px"
>&lt;/p>
&lt;p>其中激活专家个数为 2 个，MoE layer 和 FFN layer 交替出现。&lt;/p>
&lt;p>作者对 GATE 函数进行了如下优化：&lt;/p>
&lt;ol>
&lt;li>expert capacity. 每个 expert 都有一个处理 token 的上限值，超过该上限值之后 GATE 直接输出 0 进入下一层，假设 batch size 为 $N$, 专家个数为 $E$, 则该阈值定义为 $N/E$&lt;/li>
&lt;li>group dispatching. 将 token 拆分为 $G$ 个 group, 每个 group 并行处理，每个 group 里每个专家处理的 token 个数上限为 $N/(GE)$.&lt;/li>
&lt;li>Auxiliary loss. 作者使用了 &lt;a class="link" href="https://maosong.website/p/load-balancing-tutorial/" target="_blank" rel="noopener"
>Load Balancing loss&lt;/a> 来实现负载均衡&lt;/li>
&lt;li>Random routing. 作者选取了 top-2 的专家，当第二个专家的权重太小是，作者直接忽略第二个专家，简化为选取 top-1 的专家&lt;/li>
&lt;/ol>
&lt;p>算法运行如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/gshard/GShard-expert-computation.png"
width="1124"
height="917"
loading="lazy"
alt="expert computation of GShard"
class="gallery-image"
data-flex-grow="122"
data-flex-basis="294px"
>&lt;/p>
&lt;h2 id="parallel-implementation">&lt;a href="#parallel-implementation" class="header-anchor">&lt;/a>Parallel Implementation
&lt;/h2>&lt;p>第一步是将算法转化为线性代数的方式，算法的代码如下所示&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">gates&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">softmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">einsum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;GSM, ME-&amp;gt;GSE&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">inputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">wg&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">combine_weights&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dispatch_mask&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Top2Gating&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">gates&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">dispatched_expert_inputs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">einsum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;GSEC, GSM-&amp;gt;EGCM&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dispatch_mask&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">reshaped_inputs&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">h&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">einsum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;EGCM, EMH-&amp;gt;EGCH&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dispatched_expert_inputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">wi&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">h&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">relu&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">h&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">expert_outputs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">einsum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;EGCH, EHM-&amp;gt;GECM&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">h&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">wo&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">outputs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">einsum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;GSEC, GECM-&amp;gt;GSM&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">combine_weights&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">expert_outputs&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>第二步是通过 API 来实现并行执行&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Partition inputs along group (G) dim. &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">inputs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">split&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">inputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">D&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Replicate the gating weights&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">wg&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">replicate&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">wg&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">gates&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">softmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">einsum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;GSM, ME-&amp;gt;GSE&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">inputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">wg&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">combine_weights&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dispatch_mask&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Top2Gating&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">gating_logits&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">dispatched_expert_inputs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">einsum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;GSEC, GSM-&amp;gt;EGCM&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dispatch_mask&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">reshaped_inputs&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Partition dispatched inputs along expert (E) dim.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">dispatched_expert_inputs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">split&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dispatched_expert_inputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">D&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">h&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">einsum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;EGCM, EMH-&amp;gt;EGCH&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dispatched_expert_inputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">wi&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>第三部就是基于 compiler infra 来基于 sharding annotation 自动化分割一个 computation graph.&lt;/p>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>作者在机器翻译的任务上训练了若干模型，结果发现：&lt;/p>
&lt;ol>
&lt;li>层数更多的模型表现更好&lt;/li>
&lt;li>提高 expert capacity 有效提高模型的表现&lt;/li>
&lt;li>使用更多的 expert 可以在 high-resourced 任务上提高表现&lt;/li>
&lt;li>dense 模型相比于 MoE 模型拥有更强的迁移能力&lt;/li>
&lt;/ol>
&lt;p>从训练效率上来看&lt;/p>
&lt;ol>
&lt;li>层数更多的模型的 sample efficiency 也更高&lt;/li>
&lt;li>600B 的模型也可以在 4 天之内训练完毕&lt;/li>
&lt;/ol>
&lt;p>从内存使用效率上来看&lt;/p>
&lt;ol>
&lt;li>层数相同时，weight memory 以及 activation memory 不随专家个数增加而增加&lt;/li>
&lt;li>专家个数比较少（128）时，模型可以达到 roofline performance 的 $70\%$, 专家个数比较多（2048）时，模型依然可以达到 roofline performance 的 $48\%$.&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://maosong.website/p/gshard/GShard-execution-time-breakdown.png"
width="1064"
height="521"
loading="lazy"
alt="execution time of GShard"
class="gallery-image"
data-flex-grow="204"
data-flex-basis="490px"
>&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者探究了如何提高大规模模型的训练效率，作者提出了基于 MoE 架构的模型，然后作者还设计了 GShard, 一个自动化分割大规模模型的深度学习模块。实现结果发现，增加模型 size 可以提高模型的表现。作者还发现，SPMD 是一个更好的提高计算效率的方法。&lt;/p>
&lt;h2 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2006.16668" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>ST-MoE</title><link>https://maosong.website/p/st-moe/</link><pubDate>Wed, 29 Oct 2025 11:19:37 +0800</pubDate><guid>https://maosong.website/p/st-moe/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>已有的工作如 &lt;a class="link" href="https://maosong.website/p/gshard/" target="_blank" rel="noopener"
>GShard&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/switch-transformer/" target="_blank" rel="noopener"
>Switch Transformer&lt;/a> 均验证了 MoE 模型的有效性，但是他们的问题在于训练不稳定。&lt;/p>
&lt;p>因此，在本文中，作者就提出了 ST-MoE 来解决这个问题。作者的主要贡献如下：&lt;/p>
&lt;ol>
&lt;li>探究了如何平衡模型的表现与训练稳定性&lt;/li>
&lt;li>提出了 router Z-loss 来解决训练的不稳定性&lt;/li>
&lt;li>探究了如何设定 MoE 模型 fine tuning 时的超参数&lt;/li>
&lt;li>对 MoE 的性质进行了分析&lt;/li>
&lt;/ol>
&lt;h2 id="training-stability">&lt;a href="#training-stability" class="header-anchor">&lt;/a>Training Stability
&lt;/h2>&lt;p>作者发现，直接训练 MoE 模型会面临训练不稳定性的问题，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/st-moe/ST-MoE-training-instabilities.png"
width="1040"
height="420"
loading="lazy"
alt="Training instabilities for sparse models."
class="gallery-image"
data-flex-grow="247"
data-flex-basis="594px"
>&lt;/p>
&lt;p>可以看到不同的实验，一次训练崩溃，一次训练正常。&lt;/p>
&lt;p>作者接下来探究了如何提高训练的稳定性，作者有三点发现：&lt;/p>
&lt;ol>
&lt;li>大多数方法都可以提高训练稳定性，但是也会是的模型表现更差&lt;/li>
&lt;li>router Z-loss 可以在不损失模型表现的情况下提高训练的稳定性&lt;/li>
&lt;li>multiplicative components 如 RMSNorm 等可以提高模型表现，但是训练稳定性更差&lt;/li>
&lt;/ol>
&lt;p>作者构建了一个 baseline 的 MoE 模型，总专家个数为 $N=32$, 激活专家个数为 $K=2$, Transformer block 按照 4 个 block 为一组，每组里包含一个 MoE layer. 为了避免初始化带来的误差，作者使用 6 个随机数种子进行训练。&lt;/p>
&lt;h3 id="multiplicative-components">&lt;a href="#multiplicative-components" class="header-anchor">&lt;/a>Multiplicative Components
&lt;/h3>&lt;p>multiplicative components 指的是 GEGLU 和 RMSNorm 这些操作，其实验结果如下所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Method&lt;/th>
&lt;th>Fraction Stable&lt;/th>
&lt;th>Quality&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Baseline&lt;/td>
&lt;td>4/6&lt;/td>
&lt;td>$-1.755 \pm0.02$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Remove GEGLU&lt;/td>
&lt;td>3/3&lt;/td>
&lt;td>$-1.849 \pm0.02$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Remove RMS Norm. Scale Param&lt;/td>
&lt;td>3/3&lt;/td>
&lt;td>$-2.020 \pm0.06$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>结果发现，移除掉 multiplicative components 之后，模型训练变得稳定，但是效果也变差了。&lt;/p>
&lt;h3 id="adding-noise">&lt;a href="#adding-noise" class="header-anchor">&lt;/a>Adding Noise
&lt;/h3>&lt;p>接下来作者尝试了 &lt;a class="link" href="https://maosong.website/p/switch-transformer/" target="_blank" rel="noopener"
>Switch Transformer&lt;/a> 中的 jitter noise 和 dropoout 方法，实验结果如下图所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Method&lt;/th>
&lt;th>Fraction Stable&lt;/th>
&lt;th>Quality&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Baseline&lt;/td>
&lt;td>4/6&lt;/td>
&lt;td>$-1.755 ± 0.02$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Input jitter ($10^{-2}$)&lt;/td>
&lt;td>3/3&lt;/td>
&lt;td>$-1.777 \pm 0.03$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Dropout (0.1)&lt;/td>
&lt;td>3/3&lt;/td>
&lt;td>$-1.822 \pm0.11$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>实验结果显示，加入噪声对模型的表现存在负面影响。&lt;/p>
&lt;h3 id="constraining-activations">&lt;a href="#constraining-activations" class="header-anchor">&lt;/a>Constraining Activations
&lt;/h3>&lt;p>作者接下来分析了以下 &lt;a class="link" href="https://maosong.website/p/switch-transformer/" target="_blank" rel="noopener"
>Switch Transformer&lt;/a> 中 router 存在的问题，作者发现尽管在 router 中使用 &lt;code>float32&lt;/code> 可以提高训练稳定性，但是这还不够，因此作者使用了如下的 router Z-loss:&lt;/p>
$$
\mathcal{L}_z(x) = \frac1B\sum_{i=1}^B\left(\log\sum_{j=1}^N e^{x_j^{(i)}}\right)^2
$$&lt;p>其中 $B, N$ 分别是 batch size 和专家个数，$x_j^{(i)}$ 代表了第 $j$ 个专家对 $i$ 个 token 的激活 logits. 通过增加这个 loss, 我们可以让 routing layer 的 logits 都在一个合理的范围内。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Method&lt;/th>
&lt;th>Fraction Stable&lt;/th>
&lt;th>Quality ($\uparrow$)&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Baseline&lt;/td>
&lt;td>$4/6$&lt;/td>
&lt;td>$-1.755 \pm 0.02$&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Update clipping (clip = 0.1)&lt;/td>
&lt;td>$3/3$&lt;/td>
&lt;td>$-4.206 \pm 0.17$&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Router Z-Loss&lt;/td>
&lt;td>$3/3$&lt;/td>
&lt;td>$-1.741 \pm 0.02$&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，加入 router Z-loss 之后，模型的稳定性和表现都有了提升。&lt;/p>
&lt;p>因此，在本文中，作者使用的损失函数为&lt;/p>
$$
\mathcal{L} = \mathcal{L}_{CE} + \alpha \mathcal{L}_B + \beta \mathcal{L}_z
$$&lt;p>其中 $\mathcal{L}_{CE}, \mathcal{L}_B$ 分别是 cross-entropy loss 和 Load Balancing loss, $\alpha,\beta$ 是对应 loss 的权重。&lt;/p>
&lt;h3 id="numerical-precision">&lt;a href="#numerical-precision" class="header-anchor">&lt;/a>Numerical Precision
&lt;/h3>&lt;p>接下来作者探究了数值精度对训练效率和稳定性的影响，作者发现使用低精度进行训练的优势有：&lt;/p>
&lt;ol>
&lt;li>通信开销更小&lt;/li>
&lt;li>计算消耗更小&lt;/li>
&lt;li>内存需求更小&lt;/li>
&lt;/ol>
&lt;p>但是低精度进行训练的问题是存在严重的 roundoff error, 其示意图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/st-moe/ST-MoE-numerical-precision-and-roundoff.png"
width="925"
height="249"
loading="lazy"
alt="Numerical precision formats and roundoff errors."
class="gallery-image"
data-flex-grow="371"
data-flex-basis="891px"
>&lt;/p>
&lt;h2 id="fine-tuning">&lt;a href="#fine-tuning" class="header-anchor">&lt;/a>Fine-tuning
&lt;/h2>&lt;p>作者在本节探究了如何 fine-tune 一个 MoE 模型。&lt;/p>
&lt;p>作者首先通过实验给出了 MoE 模型容易过拟合的结论，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/st-moe/ST-MoE-overfitting-of-MoE.png"
width="1138"
height="456"
loading="lazy"
alt="Sparse models are prone to overfit."
class="gallery-image"
data-flex-grow="249"
data-flex-basis="598px"
>&lt;/p>
&lt;p>可以看到，在两个人呢误伤，MoE 模型相比于 dense 模型都更容易过拟合。&lt;/p>
&lt;p>接下来作者探究了超参数特别是 batch size 和 learning rate 对模型表现的影响，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/st-moe/ST-MoE-hyperparameter-sensitivity.png"
width="1036"
height="416"
loading="lazy"
alt="Batch size and learning rate sensitivity"
class="gallery-image"
data-flex-grow="249"
data-flex-basis="597px"
>&lt;/p>
&lt;p>可以看到，与 dense 模型相反，MoE 模型需要更大的 batch size 以及更小的 learning rate.&lt;/p>
&lt;h2 id="design-sparse-models">&lt;a href="#design-sparse-models" class="header-anchor">&lt;/a>Design Sparse Models
&lt;/h2>&lt;p>首先作者分析了专家个数 $N$ 的设置，作者认为当专家个数超过 $256$ 之后，带来的收益就微乎其微了。并且，当专家个数增加之后，虽然算力没有变化，但是通信开销以及计算优化会变得更加困难。作者还认为，在 TPU 上，每个 core 应该进放置一个 expert&lt;/p>
&lt;p>作者还对 capacity factor 进行了实验，实验结果显示，提升 capacity factor 可以提高模型的表现。但是同时也会带来计算开销，从而让模型训练更慢，实验结果如下图所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Train CF&lt;/th>
&lt;th>Step Time (s) ($\downarrow$)&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>ST-MoE-L&lt;/td>
&lt;td>1.25&lt;/td>
&lt;td>$2.397$&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ST-MoE-L&lt;/td>
&lt;td>2.0&lt;/td>
&lt;td>$2.447 (+7\%)$&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ST-MoE-32B&lt;/td>
&lt;td>1.25&lt;/td>
&lt;td>$4.244$&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ST-MoE-32B&lt;/td>
&lt;td>2.0&lt;/td>
&lt;td>$4.819 (+14\%)$&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>最终结论如下：&lt;/p>
&lt;blockquote>
&lt;ol>
&lt;li>使用 top-2 routing, 即激活 $K=2$ 个专家，capacity factor 设置为 $1.25$, 每个 core 上最多放置一个专家&lt;/li>
&lt;li>在评估时可以动态调整 capacity factor&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;h2 id="tracing-tokens-through-the-model">&lt;a href="#tracing-tokens-through-the-model" class="header-anchor">&lt;/a>Tracing Tokens through the Model
&lt;/h2>&lt;p>在这一节里，作者分析了 MoE 模型专家的性质。&lt;/p>
&lt;p>首先，作者发现，每一层里，至少有一个专家对于 sentinel token 比较敏感。并且，encoder 里的专家 specialization 更强，比如某些专家会负责 punctuation, verbs, proper names 等&lt;/p>
&lt;p>接下来，作者发现，专家的 specialization 在 decoder 里几乎没有，作者认为这是因为 target token distribution 不同导致的，即：&lt;/p>
&lt;ol>
&lt;li>decoding 是只有一小部分 token 被一个专家处理&lt;/li>
&lt;li>decoding 过程中大部分 token 都是 sentinel token&lt;/li>
&lt;/ol>
&lt;p>因此，每个 group 相比于 encoder 来说通常只 cover 一小部分的 semantic space&lt;/p>
&lt;p>encoder 和 decoder 专家的 specialization 实验结果如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Layer 1&lt;/th>
&lt;th>Layer 2&lt;/th>
&lt;th>Layer 3&lt;/th>
&lt;th>Layer 4&lt;/th>
&lt;th>Layer 5&lt;/th>
&lt;th>Layer 6&lt;/th>
&lt;th>Uniform (32-experts)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Encoder&lt;/td>
&lt;td>2.2&lt;/td>
&lt;td>1.8&lt;/td>
&lt;td>1.6&lt;/td>
&lt;td>1.7&lt;/td>
&lt;td>1.7&lt;/td>
&lt;td>1.2&lt;/td>
&lt;td>3.5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Decoder&lt;/td>
&lt;td>3.4&lt;/td>
&lt;td>3.4&lt;/td>
&lt;td>3.4&lt;/td>
&lt;td>3.4&lt;/td>
&lt;td>3.4&lt;/td>
&lt;td>3.4&lt;/td>
&lt;td>3.5&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，decoder 的专家分布和均匀分布差不多，这说明 decoder 里的专家 specialization 更弱。&lt;/p>
&lt;p>作者还探究了不同专家对于不同语言的敏感程度，结果发现，专家对于不同语言并没有出现明显的 specialization 现象，作者分析原因认为，输入一般只含有一种语言，因此各个专家均需要去处理对应语言的 token, 这就导致了专家对于不同语种的数据处理基本上没有太大差别。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>在本文中，作者提出了 ST-MoE 系列 MoE 大语言模型，作者通过实验优化了 MoE 模型训练的不稳定性问题。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2202.08906" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Switch Transformer</title><link>https://maosong.website/p/switch-transformer/</link><pubDate>Tue, 28 Oct 2025 09:38:12 +0800</pubDate><guid>https://maosong.website/p/switch-transformer/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>&lt;a class="link" href="https://maosong.website/p/kaplan-scaling-law/" target="_blank" rel="noopener"
>Kaplan scaling law&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/chinchilla-scaling-law/" target="_blank" rel="noopener"
>Chinchilla scaling law&lt;/a> 已经给出了针对 dense transformer model 的 scaling law, 即在给定算力下如何找到最优的 model size 和 dataset size. 已有的工作如 T5 主要是扩展 dense model 来达到更高的表现, 但是他们的问题是对算力要求比较高。&lt;/p>
&lt;p>为了解决这个问题，作者尝试在不增加算力的情况下提升模型的参数量，为了达到这个目的，作者构建了基于 MoE 架构的模型 Switch Transformer.&lt;/p>
&lt;p>作者的主要贡献如下：&lt;/p>
&lt;ol>
&lt;li>提出了基于 MoE 架构的 Switch Transformer model&lt;/li>
&lt;li>探究了针对 MoE 架构的 scaling law&lt;/li>
&lt;li>将 MoE model 的能力蒸馏到 small dense model 里去&lt;/li>
&lt;li>若干提升训练效率和稳定性的技巧&lt;/li>
&lt;/ol>
&lt;h2 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h2>&lt;p>Switch Transformer 的架构如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/switch-transformer/Switch-Transformer-Architecture.png"
width="1130"
height="571"
loading="lazy"
alt="Architecture of Switch Transformer"
class="gallery-image"
data-flex-grow="197"
data-flex-basis="474px"
>&lt;/p>
&lt;h3 id="moe">&lt;a href="#moe" class="header-anchor">&lt;/a>MoE
&lt;/h3>&lt;p>MoE 的定义见 &lt;a class="link" href="https://maosong.website/p/moe-tutorial/" target="_blank" rel="noopener"
>MoE tutorial&lt;/a>, 我们假设有 $N$ 个专家，其中激活 $K$ 个专家。&lt;/p>
&lt;p>之前的工作认为我们只有在激活 $>2$ 个专家时，模型才能够比较好的训练，但是在本文中，作者决定只使用 1 个 expert, 也就是 $K=1$. 作者将激活一个专家的 layer 称为 &lt;strong>Switch layer&lt;/strong>.&lt;/p>
&lt;p>作者认为 Switch Layer 有三个优势：&lt;/p>
&lt;ol>
&lt;li>router computation 现在只需要将每个 token route 到 1 个 expert&lt;/li>
&lt;li>每个专家的 capacity 更小，负载更加均衡&lt;/li>
&lt;li>routing 的实现更简单，且通信开销也降低了&lt;/li>
&lt;/ol>
&lt;h3 id="efficient-sparse-routing">&lt;a href="#efficient-sparse-routing" class="header-anchor">&lt;/a>Efficient Sparse Routing
&lt;/h3>&lt;p>作者首先定义了&lt;strong>expert capacity&lt;/strong>, 也就是每个 expert 处理的 token 数量，其定义如下&lt;/p>
$$
\text{expert capacity} = \left(\frac{\text{tokens per batch}}{\text{number of experts}}\right) * \text{capacity factor}
$$&lt;p>其示意图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/switch-transformer/Switch-Transformer-expert-capacity.png"
width="1269"
height="479"
loading="lazy"
alt="Token routing dynamics"
class="gallery-image"
data-flex-grow="264"
data-flex-basis="635px"
>&lt;/p>
&lt;p>提升 capacity factor 可以减少 token overflow 的概率，但同时也会导致计算和内存的浪费。作者通过实验发现应该尽可能降低 dropped token 比例。&lt;/p>
&lt;p>为了平衡每个 expert 处理 token 的个数，作者设计了 load balancing loss, 见 &lt;a class="link" href="https://maosong.website/p/load-balancing-tutorial/" target="_blank" rel="noopener"
>Load Balancing loss&lt;/a> 来要求每个 expert 处理的 token 数基本一致。&lt;/p>
&lt;h2 id="parallelism">&lt;a href="#parallelism" class="header-anchor">&lt;/a>Parallelism
&lt;/h2>&lt;p>作者在本节介绍了针对 MoE 模型的并行策略，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/switch-transformer/Switch-Transformer-Parallelism.png"
width="1268"
height="746"
loading="lazy"
alt="Data and weight partitioning strategies"
class="gallery-image"
data-flex-grow="169"
data-flex-basis="407px"
>&lt;/p>
&lt;p>这里，我们给定 notation 如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Term&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$B$&lt;/td>
&lt;td>Number of tokens in the batch.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$N$&lt;/td>
&lt;td>Number of total cores.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$n$&lt;/td>
&lt;td>Number of ways for data-parallelism sharding.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$m$&lt;/td>
&lt;td>Number of ways for model-parallelism sharding.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$E$&lt;/td>
&lt;td>Number of experts in Switch layers.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$C$&lt;/td>
&lt;td>Expert capacity, the batch size of each expert.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>作者将一个 Logical mesh 分为两个维度，一个是 data-parallel sharding, 用 $n$ 表示，另一个是 model-parallel sharding, 用 $m$ 表示，从而总的 cores 数为 $N=n\times m$.&lt;/p>
&lt;h3 id="data-parallelism">&lt;a href="#data-parallelism" class="header-anchor">&lt;/a>Data Parallelism
&lt;/h3>&lt;p>对于 data prallelism 来说，每个 core 上的模型是一样的，而一个 batch 的数据被切分为了 $B/n$ 份，此时 $n=N$, $m=1$. 这种并行策略的好处是仅在 forward 和 backward 完成之后才会进行一次通信。&lt;/p>
&lt;h3 id="model-parallelism">&lt;a href="#model-parallelism" class="header-anchor">&lt;/a>Model Parallelism
&lt;/h3>&lt;p>对于 model parallelism, 每个 core 上的数据都是全部数据（通过拷贝得到），模型被 shard 到所有 core 上，此时 $n=1,m=N$, 这种情况下，每次 forward 或者 backward，都需要进行 $N$ 次通信（假设我们使用 pipeline parallelism）， 这种并行策略的好处是每个 core 上的计算压力小了，但是缺点是通信开销多了。&lt;/p>
&lt;h3 id="model-and-data-parallelism">&lt;a href="#model-and-data-parallelism" class="header-anchor">&lt;/a>Model and Data Parallelism
&lt;/h3>&lt;p>第三种总策略是同时进行 model parallelism 和 data parallelism, 这种情况下，每个 core 上保存 $B/n$ 的数据以及 shard 为 $m$ 份的 sharding weight.&lt;/p>
&lt;h3 id="expert-and-data-parallelism">&lt;a href="#expert-and-data-parallelism" class="header-anchor">&lt;/a>Expert and Data Parallelism
&lt;/h3>&lt;p>第四种策略是同时进行 expert parallelism 和 data parallelism, 作者在这里假设 $n=N$ 且 $E=n=N$, 也就是每个 core 上保留 $B/n$ 的数据, 然后还有一个对应的专家。&lt;/p>
&lt;p>首先，对于输入大小为 $[B,d]$ 的 tensor, 我们会进行拆分得到大小为 $[n, B/n, d]$ 的 tensor, 代表 $n$ 个设备上分别存储 $[B/n, d]$ 的数据，首先我们计算路由权重，得到&lt;/p>
$$
[n,B/n, d]\times [d, E] \to [n, B/n, E]
$$&lt;p>权重的每个值 $[i,j,k]$ 代表了第 $i$ 个 core 上,第 $j$ 个 token 分配一个第 $k$ 个专家的概率，我们对其进行 softmax 与 top-K 操作之后，就得到了对应的专家（注意 Switch Transformer 中的 $K=1$, 我们通过 one-hot 编码将其输出转化为一个二进制矩阵，大小为 $[n,B/n, E, C]$, 这里的每个值 $[i,j,k,l]$ 代表了第 $i$ 个 core 上,第 $j$ 个 token 分配给第 $k$ 个专家第 $l$ 个 token, 接下来我们再基于输入 &lt;code>[n,B/n,d]&lt;/code> 计算 core 到 expert 的数据，其大小为 &lt;code>[n,E,C,d]&lt;/code>, 计算方式为&lt;/p>
$$
\mathrm{einsum}([n,B/n, d], [n,B/n,E,C], \mathrm{dim}=[B/n])
$$&lt;p>里面的元素 &lt;code>[i,j,k,:]&lt;/code> 代表了第 $i$ 个设备路由到第 $j$ 个专家的第 $k$ ($k&lt;C$) 个 token.&lt;/p>
&lt;p>然后我们就可以执行 &lt;code>all-to-all&lt;/code> 通信来把对应的 token 传输给对应专家所在的设备上了，通信完成之后，每个设备上的专家对收集到的输入 token 进行计算，计算完之后，再通过 &lt;code>all-to-all&lt;/code> 通信来把输出传输给输入的设备。这样就完成了 MoE 模块的计算与通信&lt;/p>
&lt;h3 id="expert-model-and-data-parallelism">&lt;a href="#expert-model-and-data-parallelism" class="header-anchor">&lt;/a>Expert, Model and Data Parallelism
&lt;/h3>&lt;p>目前我们只是通过增加专家个数来提高模型的表现，但是 FLOPs 并没有增加，如果我们希望通过增加 FLOPs 来提高模型表现的话，我们需要增加 expert layer 的 model size, 这就涉及到了 Model parallelism, 此时的做法和前面提到的 Expert and Data Parallelism 一样，我们在收集到对应的设备的输入之后，再执行 model parallelism 就可以了。&lt;/p>
&lt;h2 id="results">&lt;a href="#results" class="header-anchor">&lt;/a>Results
&lt;/h2>&lt;p>基于以上改进，作者构建了 Switch Transformer, 模型训练的数据集为 C4, 训练的目标为 masked language modeling, 训练时，作者将 $15\%$ 的 token 替换为 &lt;code>[mask]&lt;/code> token.&lt;/p>
&lt;p>模型的配置如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/switch-transformer/Switch-Transformer-model-configuration.png"
width="1256"
height="252"
loading="lazy"
alt="Model configuration"
class="gallery-image"
data-flex-grow="498"
data-flex-basis="1196px"
>&lt;/p>
&lt;p>实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/switch-transformer/Switch-Transformer-performance.png"
width="1206"
height="526"
loading="lazy"
alt="Performance of Switch Transformer"
class="gallery-image"
data-flex-grow="229"
data-flex-basis="550px"
>&lt;/p>
&lt;p>实验结果发现&lt;/p>
&lt;ol>
&lt;li>Switch Transformer 的表现和训练效率都超过了 Dense model&lt;/li>
&lt;li>Switch transformer 的训练效率稍微优于使用 2 个 expert 的 MoE-Base 模型&lt;/li>
&lt;li>Switch transformer 在 low capacity factor 的场景下效果更好&lt;/li>
&lt;/ol>
&lt;h3 id="scaling">&lt;a href="#scaling" class="header-anchor">&lt;/a>Scaling
&lt;/h3>&lt;p>作者对比了 MoE 模型的 scaling law, 结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/switch-transformer/Switch-Transformer-scaling-law.png"
width="1244"
height="497"
loading="lazy"
alt="Scaling law of MoE model"
class="gallery-image"
data-flex-grow="250"
data-flex-basis="600px"
>&lt;/p>
&lt;p>可以看到，当我们增加专家个数的时候，模型的表现是持续提升的。并且当我们增加专家个数之后，模型的训练效率也有所提升。&lt;/p>
&lt;p>作者接下来在训练时间上进行了对比，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/switch-transformer/Switch-Transformer-speed-comparison.png"
width="839"
height="672"
loading="lazy"
alt="Speed comparison of MoE model"
class="gallery-image"
data-flex-grow="124"
data-flex-basis="299px"
>&lt;/p>
&lt;p>实验结果说明，switch transformer 的训练效率比 dense model 快 7 倍左右&lt;/p>
&lt;p>作者进一步对比 switch transformer 和更大的 dense 模型 (T5 large, $3.5\times$ FLOPs), 实验结果证明 Switch transformer 的训练效率仍然更高。&lt;/p>
&lt;h3 id="switch-for-attention">&lt;a href="#switch-for-attention" class="header-anchor">&lt;/a>Switch for Attention
&lt;/h3>&lt;p>作者还探究了在 attention layer 加入 MoE 的表现，结果发现尽管效果有提升，但是训练不稳定。&lt;/p>
&lt;h3 id="no-token-left-behind">&lt;a href="#no-token-left-behind" class="header-anchor">&lt;/a>No Token Left behind
&lt;/h3>&lt;p>由于 tensorflow 为一个静态计算框架，tensor 的形状必须预先定义好，因此必须为每个 expert 设定 capacity, 但是这样就会导致有些 token 被 drop 掉。&lt;/p>
&lt;p>因此，作者提出了 &amp;ldquo;No token left behind&amp;rdquo; 这种方法，来避免出现 token overflow 的情况。具体做法就是先按照正常的 router 逻辑进行计算，如果 router 选取出来的 top-K expert （本文中 $K=1$） 都已经满载了，则选取 &lt;code>top-(K+1)&lt;/code> expert 进行计算，作者发现这种方式可以保证大部分 token 都不会被 drop. 作者通过尝试之后发现，这种方法并没有带来提升。&lt;/p>
&lt;h3 id="encouraging-exploration-across-experts">&lt;a href="#encouraging-exploration-across-experts" class="header-anchor">&lt;/a>Encouraging Exploration Across Experts
&lt;/h3>&lt;p>作者还探究了不同选取 top-Kexpert 的方式，作者对比了以下三种方法：&lt;/p>
&lt;ol>
&lt;li>argmax&lt;/li>
&lt;li>sampling from the softmax distribution&lt;/li>
&lt;li>input dropout on the incoming representation&lt;/li>
&lt;li>multiplicative jitter noise on the incoming representation&lt;/li>
&lt;/ol>
&lt;p>实验结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model Quality&lt;/th>
&lt;th>(Neg. Log Perp.) (↑)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Argmax&lt;/td>
&lt;td>-1.471&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Sample softmax&lt;/td>
&lt;td>-1.570&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Input dropout&lt;/td>
&lt;td>-1.480&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Input jitter&lt;/td>
&lt;td>-1.468&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>作者发现，jitter 的效果最好，因此在本文中作者使用 jitter 来加入 noise.&lt;/p>
&lt;h3 id="ablation-on-few-experts">&lt;a href="#ablation-on-few-experts" class="header-anchor">&lt;/a>Ablation on Few Experts
&lt;/h3>&lt;p>作者还使用了更少的专家进行实验，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/switch-transformer/Switch-Transformer-few-experts.png"
width="846"
height="666"
loading="lazy"
alt="Switch Transformer with few experts"
class="gallery-image"
data-flex-grow="127"
data-flex-basis="304px"
>&lt;/p>
&lt;p>实验结果显示，即使我们只使用少数 experts, 其模型表现仍然超过了 dense model 的表现，说明了 MoE 架构的有效性。&lt;/p>
&lt;h3 id="downstream-model-performance">&lt;a href="#downstream-model-performance" class="header-anchor">&lt;/a>Downstream Model Performance
&lt;/h3>&lt;p>作者还进一步探究了模型的预训练表现与 downstream task 任务上表现的关系，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/switch-transformer/Switch-Transformer-downstream-performance-scaling.png"
width="1257"
height="499"
loading="lazy"
alt="Upstream pre-trained quality to downstream model quality."
class="gallery-image"
data-flex-grow="251"
data-flex-basis="604px"
>&lt;/p>
&lt;p>实验结果显示，不管是 baseline 还是 Switch Transformer 其预训练的表现与下游任务上的表现都是正相关的。但是，作者也发现，MoE 模型在微调之后，其表现并不总是与预训练的表现正相关。因此，作者认为进一步探究这个机制是有必要的。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 Switch Transformer，一个基于 MoE 架构的 Transformer 模型。作者通过改进 MoE 算法，大幅度提高了计算和通信效率，结果发现模型比 dense model 有更高的训练效率。&lt;/p>
&lt;p>作者认为，未来的工作有：&lt;/p>
&lt;ol>
&lt;li>提升大规模模型的训练稳定性&lt;/li>
&lt;li>解决 MoE 模型微调之后效果不如预期的问题&lt;/li>
&lt;li>探究针对 MoE 模型的 scaling law&lt;/li>
&lt;li>支持异构架构的 MoE 模型&lt;/li>
&lt;li>在 FFN 模块意外应用 MoE 架构&lt;/li>
&lt;li>将 Switch Transformer 扩展到其他的模态&lt;/li>
&lt;/ol>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2101.03961" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Chinchilla Scaling Law</title><link>https://maosong.website/p/chinchilla-scaling-law/</link><pubDate>Wed, 22 Oct 2025 14:39:23 +0800</pubDate><guid>https://maosong.website/p/chinchilla-scaling-law/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>本文中关注的研究问题为：&lt;/p>
&lt;blockquote>
&lt;p>给定一个 FLOPs budget, 如何平衡 model size 和 dataset size 之间的关系？&lt;/p>
&lt;/blockquote>
&lt;p>即，我们希望求解如下优化问题：&lt;/p>
$$
N_{opt}(C), D_{opt}(C) =\arg\min_{N,D,\ \mathrm{s.t.}\ FLOPs(N,D)=C} L(N,D)
$$&lt;p>作者通过训练 400 多个模型，构建了对应的 scaling law.&lt;/p>
&lt;p>已有工作如 &lt;a class="link" href="https://maosong.website/p/kaplan-scaling-law/" target="_blank" rel="noopener"
>Kaplan scaling law&lt;/a> 已经发现模型参数和大语言模型表现之间的关系，一个结论就是计算最优并不代表达到最优的 loss. 在本文中，作者也有相同结论，但是作者认为大模型应该使用比 &lt;a class="link" href="https://maosong.website/p/kaplan-scaling-law/" target="_blank" rel="noopener"
>Kaplan scaling law&lt;/a> 推荐的更多的 training token. 基于这个发现，作者训练了 Chinchilla, 一个 70B 的 LLM, Chinchilla 相比 Gopher 表现有了大幅度的提升。&lt;/p>
&lt;h2 id="scaling-law">&lt;a href="#scaling-law" class="header-anchor">&lt;/a>Scaling Law
&lt;/h2>&lt;h3 id="fix-model-size-and-very-dataset-size">&lt;a href="#fix-model-size-and-very-dataset-size" class="header-anchor">&lt;/a>Fix Model Size and Very Dataset Size
&lt;/h3>&lt;p>这个方法中，作者通过改变训练步数，来研究 FLOPs 与模型表现之间的关系，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/chinchilla-scaling-law/Chinchilla-Training-curve-envelope.png"
width="3018"
height="1304"
loading="lazy"
alt="Training Curve envelope"
class="gallery-image"
data-flex-grow="231"
data-flex-basis="555px"
>&lt;/p>
&lt;p>通过对实验结果进行拟合，作者发现存在关系 $N_{opt}(C)\propto C^a$ 以及 $D_{opt}(C)\propto C^b$, 拟合的结果为 $a=b=0.5$.&lt;/p>
&lt;h3 id="isoflops-profiles">&lt;a href="#isoflops-profiles" class="header-anchor">&lt;/a>IsoFLOPS Profiles
&lt;/h3>&lt;p>这个方法中，作者使用了不同的模型大小以及算力来构建最优模型参数量与算力之间的关系。作者给定 9 个算力配置，然后选取不同参数量的模型，训练的 token 数由算力和模型参数量决定，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/chinchilla-scaling-law/Chinchilla-IsoFLOP-curves.png"
width="3018"
height="1396"
loading="lazy"
alt="IsoFLOP curves"
class="gallery-image"
data-flex-grow="216"
data-flex-basis="518px"
>&lt;/p>
&lt;p>结果显示，不同大小的模型的表现 (loss) 随算力上升先下降后上升。因此给定算力，存在一个最优的 model size. 作者基于拟合出来的曲线得到了 Gopher 使用的算力配置下的最优 model size 和 training tokens. 同样的，作者得到 $a=0.49,b=0.51$.&lt;/p>
&lt;h3 id="fitting-a-parametric-loss-function">&lt;a href="#fitting-a-parametric-loss-function" class="header-anchor">&lt;/a>Fitting a Parametric Loss Function
&lt;/h3>&lt;p>这个方法中，作者对 $L(N,D)$ 进行建模，作者使用了如下的公式&lt;/p>
$$
L(N,D) = E + \frac{A}{N^{\alpha}} + \frac{B}{D^{\beta}}
$$&lt;p>第一项代表了建模的误差，第二项代表了数据集充分大损失与模型参数之间的关系，第三项代表了当模型充分训练时，损失与数据集大小之间的关系。&lt;/p>
&lt;p>为了求解 $(A,B,E,\alpha,\beta)$, 作者基于训练收集到的数据 $L(N_i,D_i)$, 通过 L-BFGS 算法来最小化 Huber loss 进行求解，结果得到 $(A,B,E,\alpha,\beta)=( 406.4, 410.7, 1.69, 0.34, 0.28)$.&lt;/p>
&lt;p>将结果带入带上面的表达式中，然后求出梯度为 0 的点，就得到&lt;/p>
$$
N_{opt}(C) = G\left(\frac C6\right)^a, D_{opt}(C) = G^{-1}\left(\frac C6\right)^b, \text{ where }G=\left(\frac{\alpha A}{\beta B}\right)^{1/(\alpha+\beta)}, a=\frac{\beta}{\alpha+\beta}, b=\frac{\alpha}{\alpha+\beta}
$$&lt;p>带入数值之后就得到 $a=0.46$, $b=0.54$. 作者对结果可视化如下图所示，左图是拟合曲线的 Contour plot, 右图对左图的一个切片&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/chinchilla-scaling-law/Chinchilla-Parametric-fit.png"
width="2980"
height="1284"
loading="lazy"
alt="Parametric fit"
class="gallery-image"
data-flex-grow="232"
data-flex-basis="557px"
>&lt;/p>
&lt;h3 id="optimal-model-scaling">&lt;a href="#optimal-model-scaling" class="header-anchor">&lt;/a>Optimal Model Scaling
&lt;/h3>&lt;p>作者将三种方法的结果以及 &lt;a class="link" href="https://maosong.website/p/kaplan-scaling-law/" target="_blank" rel="noopener"
>Kaplan scaling law&lt;/a> 的结果总结放在下表中，作者假设 $N_{opt}(C)\propto C^a$ 以及 $D_{opt}(C)\propto C^b$&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Approach&lt;/th>
&lt;th>$a$&lt;/th>
&lt;th>$b$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Kaplan&lt;/td>
&lt;td>0.73&lt;/td>
&lt;td>0.26&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Approach 1&lt;/td>
&lt;td>0.50&lt;/td>
&lt;td>0.50&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Approach 2&lt;/td>
&lt;td>0.49&lt;/td>
&lt;td>0.51&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Approach 3&lt;/td>
&lt;td>0.46&lt;/td>
&lt;td>0.54&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>结果表明，三种方法的结论差不多：model size 和 dataset size 增长 debility 差不多。&lt;/p>
&lt;p>作者因此给出来的不同模型大小所需要的算力以及 token, 结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Parameters&lt;/th>
&lt;th>Approach 1&lt;/th>
&lt;th>&lt;/th>
&lt;th>Approach 2&lt;/th>
&lt;th>&lt;/th>
&lt;th>Approach 3&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>FLOPs&lt;/td>
&lt;td>Tokens&lt;/td>
&lt;td>FLOPs&lt;/td>
&lt;td>Tokens&lt;/td>
&lt;td>FLOPs&lt;/td>
&lt;td>Tokens&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>400 M&lt;/td>
&lt;td>1.92e+19&lt;/td>
&lt;td>8.0 B&lt;/td>
&lt;td>1.84e+19&lt;/td>
&lt;td>7.7 B&lt;/td>
&lt;td>1.84e+19&lt;/td>
&lt;td>7.7 B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1 B&lt;/td>
&lt;td>1.21e+20&lt;/td>
&lt;td>20.2 B&lt;/td>
&lt;td>1.20e+20&lt;/td>
&lt;td>20.0 B&lt;/td>
&lt;td>1.20e+20&lt;/td>
&lt;td>20.0 B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10 B&lt;/td>
&lt;td>1.23e+22&lt;/td>
&lt;td>205.1 B&lt;/td>
&lt;td>1.32e+22&lt;/td>
&lt;td>219.5 B&lt;/td>
&lt;td>1.32e+22&lt;/td>
&lt;td>219.5 B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>67 B&lt;/td>
&lt;td>5.76e+23&lt;/td>
&lt;td>1.5 T&lt;/td>
&lt;td>6.88e+23&lt;/td>
&lt;td>1.7 T&lt;/td>
&lt;td>6.88e+23&lt;/td>
&lt;td>1.7 T&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>175 B&lt;/td>
&lt;td>3.85e+24&lt;/td>
&lt;td>3.7 T&lt;/td>
&lt;td>4.54e+24&lt;/td>
&lt;td>4.3 T&lt;/td>
&lt;td>4.54e+24&lt;/td>
&lt;td>4.3 T&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>280 B&lt;/td>
&lt;td>9.90e+24&lt;/td>
&lt;td>5.9 T&lt;/td>
&lt;td>1.18e+25&lt;/td>
&lt;td>7.1 T&lt;/td>
&lt;td>1.18e+25&lt;/td>
&lt;td>7.1 T&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>520 B&lt;/td>
&lt;td>3.43e+25&lt;/td>
&lt;td>11.0 T&lt;/td>
&lt;td>4.19e+25&lt;/td>
&lt;td>13.4 T&lt;/td>
&lt;td>4.19e+25&lt;/td>
&lt;td>13.4 T&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1 T&lt;/td>
&lt;td>1.27e+26&lt;/td>
&lt;td>21.2 T&lt;/td>
&lt;td>1.59e+26&lt;/td>
&lt;td>26.5 T&lt;/td>
&lt;td>1.59e+26&lt;/td>
&lt;td>26.5 T&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10 T&lt;/td>
&lt;td>1.30e+28&lt;/td>
&lt;td>216.2 T&lt;/td>
&lt;td>1.75e+28&lt;/td>
&lt;td>292.0 T&lt;/td>
&lt;td>1.75e+28&lt;/td>
&lt;td>292.0 T&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>作者基于发现的 scaling law, 对已有模型进行了探究，发现现有的大模型都存在 under-training 的现象，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/chinchilla-scaling-law/Chinchilla-Overlaied-predictions.png"
width="2422"
height="1284"
loading="lazy"
alt="Overlaid predictions"
class="gallery-image"
data-flex-grow="188"
data-flex-basis="452px"
>&lt;/p>
&lt;p>实验结果显示，现有的大模型的 size 应该更小（或者需要更大的算力）。作者最终的结论就是，现有的比较小的模型，需要更多的算力才能达到更好的表现。&lt;/p>
&lt;h2 id="chinchilla">&lt;a href="#chinchilla" class="header-anchor">&lt;/a>Chinchilla
&lt;/h2>&lt;p>基于上一节的发现，作者提出了 Chinchilla, 一个 70B 的模型，训练使用了 1.4T token. 训练的数据集为 MassiveText 的扩展版本，训练使用的优化器为 AdamW, tokenizer 为 SentencePiece.&lt;/p>
&lt;p>模型配置如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Layers&lt;/th>
&lt;th>Number Heads&lt;/th>
&lt;th>Key/Value Size&lt;/th>
&lt;th>dmodel&lt;/th>
&lt;th>Max LR&lt;/th>
&lt;th>Batch Size&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Gopher 280B&lt;/td>
&lt;td>80&lt;/td>
&lt;td>128&lt;/td>
&lt;td>128&lt;/td>
&lt;td>16384&lt;/td>
&lt;td>$4\times 10^{-5}$&lt;/td>
&lt;td>$3M\to6M$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Chinchilla 70B&lt;/td>
&lt;td>80&lt;/td>
&lt;td>64&lt;/td>
&lt;td>128&lt;/td>
&lt;td>8192&lt;/td>
&lt;td>$1\times 10^{-5}$&lt;/td>
&lt;td>$1.5M\to3M$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="ablation-study">&lt;a href="#ablation-study" class="header-anchor">&lt;/a>Ablation Study
&lt;/h3>&lt;p>&lt;strong>learning rate schedule&lt;/strong>
作者还通过 ablation study 发现，cosine learning rate cycle length 应该和训练步数差不多，当 cycle length 太长时，模型表现会下降。&lt;/p>
&lt;p>&lt;strong>Optimizer&lt;/strong>
作者对比了 &lt;a class="link" href="https://maosong.website/p/notes-on-adam/" target="_blank" rel="noopener"
>Adam&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/notes-on-adamw/" target="_blank" rel="noopener"
>AdamW&lt;/a> 的表现，结果发现，AdamW 的表现优于 Adam.&lt;/p>
&lt;p>&lt;strong>High Precision&lt;/strong>
训练时，作者使用了高精度也就是 &lt;code>float32&lt;/code> 来保存梯度的状态，结果显示，不管是 Adam 还是 AdamW, 使用高精度都可以提高模型的表现&lt;/p>
&lt;p>&lt;strong>Comparison with Kaplan&lt;/strong>
作者还对比了 Chinchilla 和 Kaplan 的预测结果，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/chinchilla-scaling-law/Chinchilla-Comparison-with-Kaplan.png"
width="2684"
height="1436"
loading="lazy"
alt="Comparison with Kaplan"
class="gallery-image"
data-flex-grow="186"
data-flex-basis="448px"
>&lt;/p>
&lt;p>结果显示，基于 Chinchilla 预测得到的模型训练效果比 Kaplan 的更好。&lt;/p>
&lt;p>&lt;strong>Curvature of the FLOPs-frontier&lt;/strong>
作者发现，FLOP-minimal loss frontier 存在 curvature, 也就是小模型和大模型预测出来的曲线是不一样的，作者将结果展示在下图中&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/chinchilla-scaling-law/Chinchilla-curvature-of-FLOPs-frontier.png"
width="3012"
height="1570"
loading="lazy"
alt="Curvature of the FLOPs-frontier"
class="gallery-image"
data-flex-grow="191"
data-flex-basis="460px"
>&lt;/p>
&lt;p>结果显示，从小模型拟合出来的结果比大模型拥有更高的算力使用效率，作者认为这是未来的一个研究方向。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>本文中作者重新探究了针对 LLM 的 scaling law, 作者发现已有的大模型都存在 under-training 的现象，也就是说，模型需要更多的训练 token, 具体来讲，model size scaling 和 dataset scaling 应该处于同一水平。作者基于这个结论，提出了 Chinchilla, 一个 70B 的 LLM, 其表现超过了 280B 的 LLM.&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2203.15556" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Kaplan Scaling Law</title><link>https://maosong.website/p/kaplan-scaling-law/</link><pubDate>Wed, 22 Oct 2025 14:10:52 +0800</pubDate><guid>https://maosong.website/p/kaplan-scaling-law/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者首先就总结了本文的发现，主要有以下几点：&lt;/p>
&lt;ol>
&lt;li>损失与模型的 scale , 即除开 embedding 的模型参数，数据集大小以及算力强相关，与 model shape 比如 depth 或者 width 关系不大&lt;/li>
&lt;li>scaling law 是非常光滑的，意味着 scaling law 是一个可预测的模型&lt;/li>
&lt;li>overfitting 普遍存在，当参数和数据集大小同时增加时，模型的表现会增加，但是当其中一个量固定时，提升就比较小。并且当我们将模型参数提升 8 倍时，我们只需要将数据集的大小提升 5 倍就可以避免过拟合&lt;/li>
&lt;li>训练的损失函数曲线与 model size 无关，因此我们可以预测给定大小模型的表现&lt;/li>
&lt;li>模型在测试集和训练集上的表现高度相关，因此我们可以基于训练集的损失来预测模型的表现&lt;/li>
&lt;li>大模型比小模型拥有更高的 sample efficiency, 即更小的训练步数就可以达到相同的表现&lt;/li>
&lt;li>convergence 不能说明一切，我们可以通过 early-stopping 来提高算力使用效率，避免模型花费过多的算力在较小的提升上&lt;/li>
&lt;li>最优的 batch size 与 loss 呈一个 power law 的关系&lt;/li>
&lt;/ol>
&lt;h3 id="notation">&lt;a href="#notation" class="header-anchor">&lt;/a>Notation
&lt;/h3>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Symbol&lt;/th>
&lt;th>Notation&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$L$&lt;/td>
&lt;td>cross entropy loss&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$N$&lt;/td>
&lt;td>non-embedding parametters&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$B$&lt;/td>
&lt;td>batch size&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$S$&lt;/td>
&lt;td>number of training steps&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$C\approx 6NBS$&lt;/td>
&lt;td>estimate of total training compute&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$D$&lt;/td>
&lt;td>dataset size in tokens&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$B_{crit}$&lt;/td>
&lt;td>critical batch size&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$C_{\min}$&lt;/td>
&lt;td>estimate of the minimum compute to reach a given value of loss&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$S_{\min}$&lt;/td>
&lt;td>estimate of the minimum steps to reach a given value of loss&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\alpha_X$&lt;/td>
&lt;td>power-law exponents for the scaling law of loss&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$d$&lt;/td>
&lt;td>hidden size of the model&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>为了简便，后续未经特殊说明，我们说模型参数均指的是 non-embedding 的参数&lt;/p>
&lt;h3 id="scaling-law-overview">&lt;a href="#scaling-law-overview" class="header-anchor">&lt;/a>Scaling Law Overview
&lt;/h3>&lt;ol>
&lt;li>当数据集 $D$ 足够大时，损失与模型参数大小 $N$ 之间的关系为&lt;/li>
&lt;/ol>
$$
L(N) = \left(\frac{N_C}{N}\right)^{\alpha_N}, \alpha_N\sim 0.076, N_C\sim 8.8\times 10^{13}
$$&lt;ol start="2">
&lt;li>给定模型参数大小 $N$ , 损失与数据集大小 $D$ 之间的关系为&lt;/li>
&lt;/ol>
$$
L(D) = \left(\frac{D_C}{D}\right)^{\alpha_D}, \alpha_D\sim 0.095, D_C\sim 5.4\times 10^{13}
$$&lt;ol start="3">
&lt;li>给定足够大的数据集 $D$ 和最优模型大小 $N$ 时，损失与算力 $C$ 之间的关系为&lt;/li>
&lt;/ol>
$$
L(C_{\min}) = \left(\frac{C_C^{\min}}{C_{\min}}\right)^{\alpha_D}, \alpha_C^{\min}\sim 0.050, C_C^{\min}\sim 3.1\times 10^{8}
$$&lt;p>作者在不同大小的数据集，算力，模型大小下进行了测试，结果发现 scaling 与模型的 shape, transformer 的超参数之间的关系比较小。$\alpha_N,\alpha_D,\alpha_C^{\min}$ 等决定了当我们 scale up 数据及大小，模型大小和算力时损失的变化情况。比如当我们将模型参数提升至 2 倍时，模型的损失会降低至原来的 $0.95$.&lt;/p>
&lt;p>基于发现 1 和 2, 作者发现当我们将模型的 size 提升至原来的 2 倍时，模型的数据集大小应该提升至原来的 $1.67$ 倍，具体关系为 $D\sim N^{0.74}$.&lt;/p>
&lt;p>作者使用了一个统一的公式来描述损失与数据及大小和模型参数大小之间的关系&lt;/p>
$$
L(N,D) = \left[\left(\frac{N_c}{N}\right)^{\frac{\alpha_N}{\alpha_D}}+ \frac{D_c}{D}\right]^{\alpha_D}
$$&lt;ol start="4">
&lt;li>当数据集充分大时，损失与模型参数大小以及更新步数 $S$ 的关系如下&lt;/li>
&lt;/ol>
$$
L(N, S) = \left(\frac{N_C}{N}\right)^{\alpha_N} +\left(\frac{S_C}{S_{\min}(S)}\right)^{\alpha_S}
$$&lt;p>这里 $S_C\approx 2.1\times 10^3$, $\alpha_S\approx 0.76$, $S_{\min}(S)$ 是估计出来的最小优化步数&lt;/p>
&lt;ol start="5">
&lt;li>最优的 batch size 与损失函数之间的关系如下&lt;/li>
&lt;/ol>
$$
B_{crit}(L) = \frac{B_*}{L^{1/\alpha_B}}, B_*\sim 2*10^8 \text{ tokens}, \alpha_B\sim 0.21
$$&lt;ol start="6">
&lt;li>给定算力 $C$ 且无其他限制时，模型参数，数据及大小，batch size 和更新参数与算力之间的关系如下&lt;/li>
&lt;/ol>
$$
N\propto C^{\alpha_C^{\min}/\alpha_N}, B\propto C^{\alpha_C^{\min}/\alpha_B}, S\propto C^{\alpha_C^{\min}/\alpha_S}, D=BS
$$&lt;p>其中&lt;/p>
$$
\alpha_C^{\min} = \frac{1}{\frac{1}{\alpha_S}+\frac{1}{\alpha_B}+\frac{1}{\alpha_N}}
$$&lt;p>实验的结果为 $N\propto C_{\min}^{0.73}$, $B\propto C_{\min}^{0.24}$ , $S\propto C_{\min}^{0.03}$. 也就是说，当我们提升算力时，提升模型的参数大小带来的收益是最高的。&lt;/p>
&lt;h2 id="background">&lt;a href="#background" class="header-anchor">&lt;/a>Background
&lt;/h2>&lt;p>首先，transformer 的参数量通过计算可以得到&lt;/p>
$$
N\approx 2dn(2d+d_{ff}) = 12nd^2
$$&lt;p>这里 $d$ 是 hidden size, $n$ 是 layer 个数，$d_{ff}$ 是 MLP 的 hidden size, 这里我们 假设 $d_{ff}=4d$. 计算时我们丢掉了 bias 以及 LayerNorm 的参数量。具体计算过程见 &lt;a class="link" href="https://maosong.website/p/llm-parameter-computation/" target="_blank" rel="noopener"
>LLM parameter analysis&lt;/a>&lt;/p>
&lt;p>transformer 一次前向计算的 operations 数量大概为&lt;/p>
$$
C_{forward}\approx 2N + 2nLd
$$&lt;p>这里 $L$ 是输入的 token 长度。&lt;/p>
&lt;p>由于反向传播所需要的 FLOPs 是前向传播两倍，因此 transformer 的计算量为&lt;/p>
$$
C = C_{backward} + C_{forward} = 3C_{forward}\approx 6N
$$&lt;p>具体计算过程见 &lt;a class="link" href="https://maosong.website/p/llm-flops-computation/" target="_blank" rel="noopener"
>LLM FLOPs analysis&lt;/a>。也就是说，对于参数量为 $N$ 的 transformer model, 每个 token 所需要的 FLOPs 为 $C\approx 6N$&lt;/p>
&lt;h2 id="empirical-results-and-basic-power-laws">&lt;a href="#empirical-results-and-basic-power-laws" class="header-anchor">&lt;/a>Empirical Results and Basic Power Laws
&lt;/h2>&lt;h3 id="transformer-shape-and-hyper-parameter-independence">&lt;a href="#transformer-shape-and-hyper-parameter-independence" class="header-anchor">&lt;/a>Transformer Shape and Hyper-parameter Independence
&lt;/h3>&lt;p>作者基于 $N=12nd^2$, 在保持总参数量 $N$ 不变的情况下，分别调整 $n$, $d_{ff}$ 和 number of attention heads 的个数 （变化 $d$ 用于维持总参数量不变），结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/kaplan-scaling-law/Kaplan-model-shape-ablation.png"
width="1264"
height="501"
loading="lazy"
alt="Ablation study on model shape"
class="gallery-image"
data-flex-grow="252"
data-flex-basis="605px"
>&lt;/p>
&lt;p>实验结果发现，损失对于 $d_{ff}/d$, $d/n$, $d/n_h$ 都比较 robust, 说明&lt;strong>模型的损失对模型的 shape 依赖性比较低。&lt;/strong>&lt;/p>
&lt;h3 id="non-embedding-parameter-count">&lt;a href="#non-embedding-parameter-count" class="header-anchor">&lt;/a>Non-embedding Parameter Count
&lt;/h3>&lt;p>作者探究了以下 model size 对损失的影响，作者使用了不同的 $n$ 和 $d$, 然后训练得到的损失情况如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/kaplan-scaling-law/Kaplan-ablation-model-size.png"
width="1285"
height="547"
loading="lazy"
alt="Ablation study on model size"
class="gallery-image"
data-flex-grow="234"
data-flex-basis="563px"
>&lt;/p>
&lt;p>作者发现，当包含 embedding parameter 时，损失不仅依赖于模型参数量，还依赖于 layer 层数 $n$, 但是&lt;strong>当我们排除 embedding parameter 时，模型的损失便与 layer 层数 $n$ 关系不大&lt;/strong>。这个趋势可以用以下模型来表示&lt;/p>
$$
L(N) \approx \left(\frac{N_c}{N}\right)^{\alpha_N}
$$&lt;p>最终拟合的曲线如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/kaplan-scaling-law/Kaplan-parameter-scaling-law-curve.png"
width="404"
height="383"
loading="lazy"
alt="Scaling law with respect to parameters"
class="gallery-image"
data-flex-grow="105"
data-flex-basis="253px"
>&lt;/p>
&lt;h3 id="comparing-to-lstms-and-universal-transformers">&lt;a href="#comparing-to-lstms-and-universal-transformers" class="header-anchor">&lt;/a>Comparing to LSTMs and Universal Transformers
&lt;/h3>&lt;p>作者比较了 LSTM 和 Transformer 结构的损失，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/kaplan-scaling-law/Kaplan-ablation-LSTM.png"
width="1268"
height="467"
loading="lazy"
alt="Ablation study on LSTM"
class="gallery-image"
data-flex-grow="271"
data-flex-basis="651px"
>&lt;/p>
&lt;p>可以看到，&lt;strong>transformer 比 LSTM 拥有更强的学习能力&lt;/strong>， LSTM 架构对于 early context 表现比较好，但是随着 context 增加，LSTM 的表现逐渐弱于 transformer. &lt;strong>即 transformer 的长上下文能力强于 LSTM 架构&lt;/strong>。&lt;/p>
&lt;h3 id="generalization-among-data-distributions">&lt;a href="#generalization-among-data-distributions" class="header-anchor">&lt;/a>Generalization Among Data Distributions
&lt;/h3>&lt;p>模型是在 WebText2 数据集上训练的，作者进一步在其他数据集上评估了以下模型的泛化性，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/kaplan-scaling-law/Kaplan-parameter-scaling-law-generalization.png"
width="1266"
height="591"
loading="lazy"
alt="Generalization performance"
class="gallery-image"
data-flex-grow="214"
data-flex-basis="514px"
>&lt;/p>
&lt;p>结果发现，模型在其他数据集上的泛化性很好。并且，&lt;strong>模型的泛化性能仅与训练阶段的表现相关（validation loss），而与训练阶段（是否收敛）无关&lt;/strong>。&lt;/p>
&lt;p>作者还评估了 model depth 对模型泛化性的影响，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/kaplan-scaling-law/Kaplan-ablation-depth-to-generalization.png"
width="658"
height="421"
loading="lazy"
alt="Ablation study on depth"
class="gallery-image"
data-flex-grow="156"
data-flex-basis="375px"
>&lt;/p>
&lt;p>实验结果显示，&lt;strong>model depth 对模型泛化性基本没有影响&lt;/strong>。&lt;/p>
&lt;h3 id="performance-with-data-size-and-compute">&lt;a href="#performance-with-data-size-and-compute" class="header-anchor">&lt;/a>Performance with Data Size and Compute
&lt;/h3>&lt;p>作者探究了损失与 dataset size $D$ 之间的关系。作者固定一个模型，然后当 test loss 不再下降时停止训练，结果发现 test loss 与 dataset size $D$ 之间存在如下关系&lt;/p>
$$
L(D) \approx \left(\frac{D_c}{D}\right)^{\alpha_D}
$$&lt;p>拟合结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/kaplan-scaling-law/Kaplan-dataset-size-scaling-law.png"
width="419"
height="384"
loading="lazy"
alt="Scaling law with respect to dataset"
class="gallery-image"
data-flex-grow="109"
data-flex-basis="261px"
>&lt;/p>
&lt;p>接下来，基于前面计算的结果，我们有 $C\approx 6ND=6NBS$, 这里 $B$ 是 batch size, $S$ 是训练步数。给定 $C$, 作者使用不同大小的模型进行训练，batch size $B$ 保持不懂，训练步数设置为 $S=C/6BS$,实验结果显示损失与算力 $C$ 之间满足如下关系&lt;/p>
$$
L(C) \approx \left(\frac{C_c}{C}\right)^{\alpha_C}
$$&lt;p>拟合结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/kaplan-scaling-law/Kaplan-compute-scaling-law.png"
width="451"
height="384"
loading="lazy"
alt="Scaling law with respect to compute"
class="gallery-image"
data-flex-grow="117"
data-flex-basis="281px"
>&lt;/p>
&lt;p>作者进一步探究了 sample efficiency 与 model size 之间的关系，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/kaplan-scaling-law/Kaplan-sample-efficiency-vs-model-size.png"
width="1273"
height="513"
loading="lazy"
alt="Sample efficiency with respect to model size"
class="gallery-image"
data-flex-grow="248"
data-flex-basis="595px"
>&lt;/p>
&lt;p>结果显示，&lt;strong>随着 model size 增加，sample efficiency 也在增加&lt;/strong>&lt;/p>
&lt;h2 id="charting-the-infinite-data-limit-and-overfitting">&lt;a href="#charting-the-infinite-data-limit-and-overfitting" class="header-anchor">&lt;/a>Charting the Infinite Data Limit and Overfitting
&lt;/h2>&lt;p>作者在本节探讨了同时变化 $N$ 和 $D$ 对损失变化的影响。&lt;/p>
&lt;h3 id="proposed-equation">&lt;a href="#proposed-equation" class="header-anchor">&lt;/a>Proposed Equation
&lt;/h3>&lt;p>作者基于三个原则进行建模：&lt;/p>
&lt;ol>
&lt;li>改变 vocabulary size 或者 tokenization 会 rescale loss&lt;/li>
&lt;li>固定 $D$ 并且令 $N\to\infty$, 则最终损失应该接近 $L(D)$. 反之固定 $N$, 令 $D\to\infty$, 最终损失应该接近 $L(N)$&lt;/li>
&lt;li>$L(N,D)$ 在 $D=\infty$ 处应该是可解析的&lt;/li>
&lt;/ol>
&lt;p>基于以上三条原则，将模型选择为如下形式&lt;/p>
$$
L(N,D) = \left[\left(\frac{N_c}{N}\right)^{\frac{\alpha_N}{\alpha_D}}+ \frac{D_c}{D}\right]^{\alpha_D}
$$&lt;p>作者基于不同配置进行训练，基于实验结果你和得到的参数如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Parameter&lt;/th>
&lt;th>$\alpha_N$&lt;/th>
&lt;th>$\alpha_D$&lt;/th>
&lt;th>$N_c$&lt;/th>
&lt;th>$D_c$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Value&lt;/td>
&lt;td>0.076&lt;/td>
&lt;td>0.103&lt;/td>
&lt;td>$6.4\times 10^{13}$&lt;/td>
&lt;td>$1.8\times 10^{13}$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>接下来，作者探究了模型的过拟合程度，作者定义如下 metric&lt;/p>
$$
\delta L(N,D) := \frac{L(N,D)}{L(N,\infty)} - 1
$$&lt;p>带入 $L(N,D)$ 定义就得到&lt;/p>
$$
\delta L(N,D) = \left(1 + \left(\frac{N}{N_c}\right)^{\frac{\alpha_N}{\alpha_D}}\frac{D_c}{D}\right) - 1
$$&lt;p>通过测试不同的模型，作者发现 $\delta L$ 的值在 $0.02$ 左右，将实验结果带入到上面的公式就得到&lt;/p>
$$
D \geq (5\times 10^3)N^{0.7379}
$$&lt;p>也就是说对于参数量为 $N$ 的模型，需要 data size $D \geq (5\times 10^3)N^{0.7379}$ 才能避免过拟合。&lt;/p>
&lt;h2 id="scaling-laws-with-model-size-and-training-time">&lt;a href="#scaling-laws-with-model-size-and-training-time" class="header-anchor">&lt;/a>Scaling Laws with Model Size and Training time
&lt;/h2>&lt;p>作者在本节构建了损失函数与 model size $N$ 以及训练时间的 scaling law&lt;/p>
&lt;h3 id="adjustment-for-training-at-critical-batch-size">&lt;a href="#adjustment-for-training-at-critical-batch-size" class="header-anchor">&lt;/a>Adjustment for Training at Critical Batch Size
&lt;/h3>&lt;p>已有结论说明，存在一个 critical batch size $B_{crit}$, 当 batch size 接近 $B_{crit}$ 时，增加 batch size 对计算效率影响比较小，但是当 batch size 大于 $B_{crit}$ 时，带来的提升比较小。另一方面，batch size 会影响梯度的噪声程度。因此，训练步数 $S$ 和处理的样本数 $E=BS$ 应该满足：&lt;/p>
$$
\left(\frac{S}{S_{\min}}-1\right)\left(\frac{E}{E_{\min}}-1\right) = 1
$$&lt;p>这里 $S_{\min}$ 是达到损失 $L$ 所需要的最小训练步数，而 $E_{\min}$ 是最小的训练样本数量。&lt;/p>
&lt;p>作者的实验结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/kaplan-scaling-law/Kaplan-critical-batch-size-relation.png"
width="1264"
height="499"
loading="lazy"
class="gallery-image"
data-flex-grow="253"
data-flex-basis="607px"
>&lt;/p>
&lt;p>作者将 critical batch size 定义为&lt;/p>
$$
B_{crit}(L) := \frac{E_{\min}}{S_{\min}}
$$&lt;p>使用 critical batch size 进行训练可以在计算效率和算力之间达到一个平衡。&lt;/p>
&lt;p>作者基于上面的实验结果探究了 critical batch size 和 model performance 之间的关系，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/kaplan-scaling-law/Kaplan-critical-batch-size-vs-performance.png"
width="946"
height="620"
loading="lazy"
class="gallery-image"
data-flex-grow="152"
data-flex-basis="366px"
>&lt;/p>
&lt;p>可以看到，critical batch size 与 model size 的关系不大，仅与损失 $L$ 有关。作者通过以下模型拟合 critical batch size:&lt;/p>
$$
B_{crit}(L) \approx \frac{B_*}{L^{1/\alpha_B}}
$$&lt;p>这里 $B_*\approx 2\times 10^8$, $\alpha_B\approx 0.21$.&lt;/p>
&lt;p>给定一个 target loss $L$, 当 batch size $B>> B_{crit}$ 时，作者定义最小训练步数为&lt;/p>
$$
S_{\min}(S) := \frac{S}{1+B_{crit}(L)/B}
$$&lt;p>给定 target loss $L$ 和 model size $N$, 当 batch size $B&lt;&lt; B_{crit}$ 时，作者定义最小算力为&lt;/p>
$$
C_{\min}(C) := \frac{C}{1+B_{crit}(L)/B}
$$&lt;h3 id="performance-with-model-size-and-compute">&lt;a href="#performance-with-model-size-and-compute" class="header-anchor">&lt;/a>Performance with Model Size and Compute
&lt;/h3>&lt;p>作者使用如下公式来探究损失与 model size 和 computer 之间的关系&lt;/p>
$$
L(N, S_{\min}) = \left(\frac{N_C}{N}\right)^{\alpha_N} +\left(\frac{S_C}{S_{\min}(S)}\right)^{\alpha_S}
$$&lt;p>拟合结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Parameter&lt;/th>
&lt;th>$\alpha_N$&lt;/th>
&lt;th>$\alpha_S$&lt;/th>
&lt;th>$N_c$&lt;/th>
&lt;th>$S_c$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Value&lt;/td>
&lt;td>0.077&lt;/td>
&lt;td>0.76&lt;/td>
&lt;td>$6.5\times 10^{13}$&lt;/td>
&lt;td>$2.1\times 10^{3}$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>基于这个拟合结果，作者得到了下图的结果&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/kaplan-scaling-law/Kaplan-loss-vs-model-size-and-training-steps.png"
width="613"
height="347"
loading="lazy"
class="gallery-image"
data-flex-grow="176"
data-flex-basis="423px"
>&lt;/p>
&lt;p>作者还使用了不同的可视化方式，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/kaplan-scaling-law/Kaplan-performance-vs-compute-budget-and-steps.png"
width="1273"
height="560"
loading="lazy"
class="gallery-image"
data-flex-grow="227"
data-flex-basis="545px"
>&lt;/p>
&lt;p>实验结果显示，上面的公式拟合的很好。&lt;/p>
&lt;h3 id="lower-bound-on-early-stopping-step">&lt;a href="#lower-bound-on-early-stopping-step" class="header-anchor">&lt;/a>Lower Bound on Early Stopping step
&lt;/h3>&lt;p>作者还探究了以下 early step 与模型大小以及数据集之间的关系，作者通过分析得到如下结果&lt;/p>
$$
S_{stop}(N,D) \gtrsim \frac{S_c}{[L(N,D)-L(N,\infty)]^{1/\alpha_S}}
$$&lt;p>其中 $L(N,\infty)$ 是在充分大数据集上的收敛损失。作者对实验结果进行了拟合，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/kaplan-scaling-law/Kaplan-early-stop-resultes.png"
width="1277"
height="618"
loading="lazy"
class="gallery-image"
data-flex-grow="206"
data-flex-basis="495px"
>&lt;/p>
&lt;h2 id="optimal-allocation-of-the-compute-budget">&lt;a href="#optimal-allocation-of-the-compute-budget" class="header-anchor">&lt;/a>Optimal Allocation of the Compute Budget
&lt;/h2>&lt;p>作者在本节探究了最优算力与 model size $N$ 和训练数据 $2B_{crit}S_{\min}$ 之间的关系&lt;/p>
&lt;h3 id="optimal-performance-and-allocations">&lt;a href="#optimal-performance-and-allocations" class="header-anchor">&lt;/a>Optimal Performance and Allocations
&lt;/h3>&lt;p>作者首先基于&lt;/p>
$$
C_{\min}(C) := \frac{C}{1+B_{crit}(L)/B}
$$&lt;p>绘制了如下曲线图&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/kaplan-scaling-law/Kaplan-loss-vs-optimal-compute.png"
width="664"
height="439"
loading="lazy"
class="gallery-image"
data-flex-grow="151"
data-flex-basis="363px"
>&lt;/p>
&lt;p>作者发现，相比于 loss 与算力 $C$ 之间的关系，使用 $C_{\min}$ 进行拟合效果更好。&lt;/p>
&lt;p>接下来，作者基于 $L(C_{\min})$ 进一步探究了给定算力如何决定最优的 model size $N(C_{\min})$. 其实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/kaplan-scaling-law/Kaplan-optimal-model-size-given-compute.png"
width="616"
height="405"
loading="lazy"
class="gallery-image"
data-flex-grow="152"
data-flex-basis="365px"
>&lt;/p>
&lt;p>实验结果显示，model size 和算力之间有如下关系&lt;/p>
$$
N(C_{\min}) \propto (C_{\min})^{0.73}
$$&lt;p>作者进一步探究了对于非最优模型大小与算力之间的关系，作者先构建了如下的关系&lt;/p>
$$
\frac{C(N, N_{\mathrm{eff}})}{C(N_{\mathrm{eff}}, N_{\mathrm{eff}})}
= \frac{N}{N_{\mathrm{eff}}}
\left[
1 + \frac{\alpha_S}{\alpha_N}
\left( 1 - \left( \frac{N_{\mathrm{eff}}}{N} \right)^{\alpha_N} \right)
\right]^{-\!1 / \alpha_S}.
$$&lt;p>对应的示意图为&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/kaplan-scaling-law/Kaplan-suboptimal-model-efficiency.png"
width="1256"
height="600"
loading="lazy"
class="gallery-image"
data-flex-grow="209"
data-flex-basis="502px"
>&lt;/p>
&lt;p>实现结果发现，大小为最优模型的 $0.6\sim 2.2$ 倍只需要额外 $20\%$ 的算力。作者强调，这个实验结果对于超大模型不一定适用。&lt;/p>
&lt;p>作者进一步推导了 $S_{\min}$ 和 $C_{\min}$ 之间的关系，由于 $C_{\min}=6NB_{crit}S$, 且我们前面已经有 $B\propto L^{-4.8}$ 和 $L\propto C_{\min}^{-0.05}$, 因此 我们有&lt;/p>
$$
B_{crit}\propto L^{-4.8} \propto (C_{\min})^{-0.05\times (-4.8)}\propto (C_{\min})^{0.24}
$$&lt;p>以及&lt;/p>
$$
S_{\min} \propto \frac{C_{\min}}{6B_{crit}N(C_{\min})} \propto (C_{\min})^{0.03}
$$&lt;p>拟合的结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/kaplan-scaling-law/Kaplan-optimal-step-size-vs-compute.png"
width="620"
height="403"
loading="lazy"
class="gallery-image"
data-flex-grow="153"
data-flex-basis="369px"
>&lt;/p>
&lt;p>因此，基于上面的结果，当我们增加算力时，我们的主要精力应该放在增加模型大小和提高 batch size 上，而训练步数基本可以保持不变。&lt;/p>
&lt;h3 id="another-way-of-derivation">&lt;a href="#another-way-of-derivation" class="header-anchor">&lt;/a>Another way of Derivation
&lt;/h3>&lt;p>作者还给出了另一种建模 $L(C_{\min})$ 的方式，即从 $L(N,S_{\min})$ 中进行推导，作者将 $B_{crit}$ 和 $S_{\min}$ 的表达式带入到 $L(N,S_{\min})$ 然后求解最小值就得到&lt;/p>
$$
L(C_{\min})= \left(\frac{C_C^{\min}}{C_{\min}}\right)^{\alpha_C^{\min}}
$$&lt;p>其中&lt;/p>
$$
\alpha_C^{\min} = \frac{1}{\frac{1}{\alpha_S}+\frac{1}{\alpha_B}+\frac{1}{\alpha_N}} \approx 0.054
$$&lt;p>这和前面的结果基本是吻合的，进一步进行推导得到&lt;/p>
$$
N(C_{\min})\propto (C_{\min})^{\alpha_C^{\min}/\alpha_N}\approx (C_{\min})^{0.71}
$$&lt;p>这个结果也和上面的差不多。&lt;/p>
&lt;h3 id="contradiction-and-a-conjecture">&lt;a href="#contradiction-and-a-conjecture" class="header-anchor">&lt;/a>Contradiction and a Conjecture
&lt;/h3>&lt;p>作者发现，尽管拟合的 scaling law 曲线非常好，但是由于自然语言不可能达到 zero entropy, 因此该曲线最终一定会失效。作者基于更大的模型进行了实验，结果发现，模型在某一点开始就比预测的损失曲线下降的更慢。作者认为这是因为 transformer 模型已经达到了 maximal performance 导致的。&lt;/p>
&lt;p>通过前面的分析，我们发现 $L(C_{\min})$ 比 $L(D)$ 下降的快，因此两者必然在某一点相交。&lt;/p>
&lt;p>在前面的章节中，我们基于以下关系来决定数据集大小&lt;/p>
$$
D\propto N^{0.74}\propto (C_{\min})^{0.74*0.73}\propto (C_{\min})^{0.54}
$$&lt;p>这里我们利用了 $N(C_{\min})$ 的结果&lt;/p>
&lt;p>另一方面，我们有&lt;/p>
$$
D(C_{\min}) = \frac{2C_{\min}}{6N(C_{\min})}\propto (C_{\min})^{0.26}
$$&lt;p>可以看到，基于训练最优导出的数据集大小相比于拟合出来的数据集大小，实际上存在过拟合。&lt;/p>
&lt;p>作者进一步分析出了 $L(D(C_{\min}))$ 和 $L(C_{\min})$ 这两条曲线的交点，结果得到&lt;/p>
$$
C^*\approx 10^4 \text{ PF-Days}, N^*\approx 10^{12}\text{ parameters}, D^*\approx 10^12\text{ tokens}, L^*\approx 1.7\text{1.7nats/token}
$$&lt;p>作者认为出现这种原因有以下几种情况：&lt;/p>
&lt;ol>
&lt;li>$L^*$ 给出了自然语言的 entropy 的一个估计，因此当模型充分大之后，模型可能已经获取到了数据中的所有知识&lt;/li>
&lt;li>$L(C_{\min})$ 可以作为数据集噪声的一个量化表现，其衡量了数据集的质量&lt;/li>
&lt;/ol>
&lt;h2 id="learning-rate-schedule">&lt;a href="#learning-rate-schedule" class="header-anchor">&lt;/a>Learning Rate Schedule
&lt;/h2>&lt;p>附录中，作者还探究了 learning rate 与损失之间的关系，作者使用了不同 learning rate schedule 对模型损失的影响，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/kaplan-scaling-law/Kaplan-learning-rate-schedule-results.png"
width="1278"
height="530"
loading="lazy"
class="gallery-image"
data-flex-grow="241"
data-flex-basis="578px"
>&lt;/p>
&lt;p>实验结果显示，&lt;strong>只要 learning rate 下降的不会太快，模型的表现基本上差不太多&lt;/strong>。&lt;/p>
&lt;p>作者基于实验结果得到了学习率和模型参数之间的关系如下&lt;/p>
$$
\text{lr}(N)\approx 0.003239 - 0.0001395\log N
$$&lt;p>也就是说，小模型用比较大的学习率，大模型用较小的学习率。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中训练了大量不同配置的大模型，然后构建了损失（损失）与模型参数，数据及大小以及算力之间的关系。实验结果发现，损失与架构和优化参数之间的关系比较小，主要由模型参数量决定，更大的模型拥有更高的采样效率。&lt;/p>
&lt;p>作者认为，本文的局限在于损失函数不一定能够反应模型在其他语言任务上的表现。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2001.08361" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>LLM FLOPs Computation</title><link>https://maosong.website/p/llm-flops-computation/</link><pubDate>Wed, 15 Oct 2025 16:33:39 +0800</pubDate><guid>https://maosong.website/p/llm-flops-computation/</guid><description>&lt;p>本文中，我们介绍如何计算基于 transformer 架构的 LLM 的 FLOPs, 计算完成之后，我们可以推导出算力 $C$ 与模型参数量 $N$，数据集大小 $D$ 之间的关系，即 $C\approx 6ND$.&lt;/p>
&lt;h2 id="background">&lt;a href="#background" class="header-anchor">&lt;/a>Background
&lt;/h2>&lt;h3 id="flops">&lt;a href="#flops" class="header-anchor">&lt;/a>FLOPs
&lt;/h3>&lt;p>FLOPs，floating point operations，表示浮点数运算次数，一般计算 FLOPs 仅考虑加法和乘法。&lt;/p>
&lt;blockquote>
&lt;p>假设 $A\in\mathbb{R}^{m\times p}$, $B\in\mathbb{R}^{p\times n}$, 则 计算 $C=AB$ 的过程中一共需要进行 $mnp$ 次乘法运算和 $mnp$ 次加法运算，因此总的 FLOPs 为 $2mnp$.&lt;/p>
&lt;/blockquote>
&lt;h3 id="assumption">&lt;a href="#assumption" class="header-anchor">&lt;/a>Assumption
&lt;/h3>&lt;blockquote>
&lt;p>假设 $A\in\mathbb{R}^{m\times p}$, $B\in\mathbb{R}^{p\times n}$, $C\in\mathbb{R}^{m\times n}$ 则 计算 $D=AB+C$ 的过程中一共需要进行 $mnp$ 次乘法运算和 $mnp+mn$ 次加法运算，因此总的 FLOPs 为 $2mnp+mn\approx 2mnp$.&lt;/p>
&lt;/blockquote>
&lt;p>基于上述结论，我们计算 FLOPs 时，忽略 element-wise 的操作，即我们做如下假设：&lt;/p>
&lt;ol>
&lt;li>忽略 normalization 中的小常数项运算&lt;/li>
&lt;li>忽略 residual connection 和 bias term 的加法&lt;/li>
&lt;li>忽略注意力的 MASK 和 softmax，因为这两者都是 element-wise operation.&lt;/li>
&lt;li>使用 look-up 计算 embedding layer&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>注，基于以上假设，我们的结果与 Chinchilla Scaling law 的结果稍有不同，但结论不变。&lt;/p>
&lt;/blockquote>
&lt;h3 id="notation">&lt;a href="#notation" class="header-anchor">&lt;/a>Notation
&lt;/h3>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Math Variable&lt;/th>
&lt;th>Code Variable&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$n$&lt;/td>
&lt;td>&lt;code>num_hidden_layers&lt;/code>&lt;/td>
&lt;td>Transformer block 个数&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\vert V\vert$&lt;/td>
&lt;td>&lt;code>vocab_size&lt;/code>&lt;/td>
&lt;td>词表大小&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$d$&lt;/td>
&lt;td>&lt;code>hidden_size&lt;/code>&lt;/td>
&lt;td>token embedding 的维度&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$d_{ff}$&lt;/td>
&lt;td>&lt;code>intermediate_size&lt;/code>&lt;/td>
&lt;td>MLP 的中间层的维度&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$h$&lt;/td>
&lt;td>&lt;code>num_attention_heads&lt;/code>&lt;/td>
&lt;td>query head 的个数&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$s$&lt;/td>
&lt;td>&lt;code>seq_len&lt;/code>&lt;/td>
&lt;td>length of token sequence&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;blockquote>
&lt;p>注：为了避免混淆，我们使用 $n$ 来表示 decode layer 的个数。&lt;/p>
&lt;/blockquote>
&lt;h2 id="computation">&lt;a href="#computation" class="header-anchor">&lt;/a>Computation
&lt;/h2>&lt;p>我们计算训练阶段的总 FLOPs, 记为 $C$, &lt;a class="link" href="https://maosong.website/p/kaplan-scaling-law/" target="_blank" rel="noopener"
>Kaplan scaling law&lt;/a> 用 PF-days 作为单位，$1\text{ PF-Days}=10^{15}\times 24\times 3600\ FLOPs$. 训练阶段包括前向阶段 (forward pass) 和反向传播阶段 (backward pass). 因此&lt;/p>
$$
C = FLOPs(\text{forward}) + FLOPs(\text{backward})
$$&lt;h3 id="forward">&lt;a href="#forward" class="header-anchor">&lt;/a>Forward
&lt;/h3>&lt;p>decoder-only transformer 的模型架构包含三个模块：&lt;/p>
&lt;ol>
&lt;li>1 层 embedding layer&lt;/li>
&lt;li>$n$ 层 decoder layer&lt;/li>
&lt;li>1 层 lm head layer&lt;/li>
&lt;/ol>
&lt;p>因此模型总的 FLOPs 为&lt;/p>
$$
FLOPs(\text{forward}) = FLOPs(\text{embedding}) + n*FLOPs(\mathrm{decode\_layer})+FLOPs(\mathrm{lm\_head})
$$&lt;h4 id="embedding--lm-head">&lt;a href="#embedding--lm-head" class="header-anchor">&lt;/a>Embedding &amp;amp; Lm Head
&lt;/h4>&lt;p>首先，对于 embedding layer, embedding layer 本质上是一个 look up table, 计算过程中不涉及浮点数运算，因此 $\boxed{FLOPs(\text{embedding})=0}$.&lt;/p>
&lt;p>接下来，对于 &lt;code>lm_head&lt;/code>, 这是一个 linear layer, 其权重大小为 $W\in\mathbb{R}^{d\times |V|}$, 输入为 $x\in\mathbb{R}^{s\times d}$, 因此 $\boxed{FLOPs(\mathrm{lm\_head})=2sd|V|}$.&lt;/p>
&lt;p>因此，我们有&lt;/p>
$$
FLOPs(\text{forward}) = n*FLOPs(\mathrm{decode\_layer})+ 2sd|V|
$$&lt;h4 id="decode-layer">&lt;a href="#decode-layer" class="header-anchor">&lt;/a>Decode Layer
&lt;/h4>&lt;p>对于 &lt;code>decode_layer&lt;/code>, 其又包含了四个模块：&lt;/p>
&lt;ol>
&lt;li>pre-normalization&lt;/li>
&lt;li>attention&lt;/li>
&lt;li>post-normalization&lt;/li>
&lt;li>FFN&lt;/li>
&lt;/ol>
&lt;p>pre-normalization 和 post-normalization 一般是一样的，因此&lt;/p>
$$
\begin{aligned}
FLOPs(\mathrm{decode\_layer}) &amp;= FLOPs(\mathrm{pre\_normoalization}) + FLOPs(\mathrm{Attention}) + FLOPs(\mathrm{post\_normoalization}) +FLOPs(\mathrm{FFN})\\
&amp;= 2*FLOPs(\mathrm{normoalization}) + FLOPs(\mathrm{Attention})+FLOPs(\mathrm{FFN})
\end{aligned}
$$&lt;h4 id="normalization">&lt;a href="#normalization" class="header-anchor">&lt;/a>Normalization
&lt;/h4>&lt;p>现在有两种常见的 normalization, 也就是 LayerNorm 和 RMSNorm&lt;/p>
&lt;p>LayerNorm 定义如下&lt;/p>
$$
\mathrm{LayerNorm}(x) = \frac{x-\mathbb{E}[x]}{\sqrt{\mathrm{var}[x]+\epsilon}}\odot \beta + \gamma
$$&lt;p>其中 $\beta,\gamma\in\mathbb{R}^d$ 是可学习的参数。&lt;/p>
&lt;p>对输入 $x\in\mathbb{R}^{s\times d}$, 均值需要约 $sd$ 次 FLOPs，方差 $\mathrm{var}[x]$ 只需要约 $3sd$ 次 FLOPs，这两者的计算都可以忽略。接下来就是 scaling 和 shift, 这两者都是 element-wise 操作，我们这里，因此总的 FLOPs 为&lt;/p>
$$
\boxed{FLOPs(\mathrm{normoalization}) = 4sd}
$$&lt;p>RMSNorm 的作用和 LayerNorm 是一样的，但是实现上更简单&lt;/p>
$$
\mathrm{RMSNorm}(x) = \frac{x}{\sqrt{\|x\|_2^2+\epsilon}}\odot \gamma
$$&lt;p>其中 $\gamma\in\mathbb{R}^d$ 是可学习的参数&lt;/p>
&lt;p>对于 RMSNorm，其分析方式与 LayerNorm 基本一致，因此总的 FLOPs 为&lt;/p>
$$
\boxed{FLOPs(\mathrm{normoalization}) = 4sd}
$$&lt;p>总之，不管使用哪种 normalization，其 FLOPs 都是&lt;/p>
$$
\boxed{FLOPs(\mathrm{normoalization}) = 4sd}
$$&lt;h4 id="attention">&lt;a href="#attention" class="header-anchor">&lt;/a>Attention
&lt;/h4>&lt;p>Attention 定义如下&lt;/p>
$$
\mathrm{Attention}(X) = \mathrm{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V\in\mathbb{R}^{m\times d}
$$&lt;p>其中 $X\in\mathbb{R}^{s\times d}$, $W_Q,W_K,W_V\in\mathbb{R}^{d\times d}$&lt;/p>
$$
Q = W_QX\in\mathbb{R}^{s\times d},\quad
K =W_KX\in\mathbb{R}^{s\times d},\quad
V = W_VX\in\mathbb{R}^{s\times d}
$$&lt;p>$Q,K,V$ 计算的 FLOPs 为 $6*sd^2$. $QK^T$ 的 FlOPs 为 $2s^2d$, $\mathrm{softmax}(\cdot)V$ 的 FLOPs 为 $2s^2d$, 最后对于 multi-head attention 还有一个 output projection layer, 其权重为 $W_O\in\mathbb{R}^{d\times d}$, 因此 FLOPs 为 $2sd^2$. 故 attention 最终的 FLOPs 为&lt;/p>
$$
FLOPs(\mathrm{Attention})=6sd^2+2s^2d+2s^2d+2sd^2=\boxed{8sd^2+4s^2d}
$$&lt;h4 id="ffn">&lt;a href="#ffn" class="header-anchor">&lt;/a>FFN
&lt;/h4>&lt;p>对于 FFN，有两种常见的形式，一种基于 ReLU 激活函数，其定义如下&lt;/p>
$$
y = \max(xW_1+b_1, 0)W_2 + b_2
$$&lt;p>其中 $W_1\in\mathbb{R}^{d_{ff}\times d}$, $W_2\in\mathbb{R}^{d\times d_{ff}}$. $b_1\in\mathbb{R}^{d_{ff}}$, $b_2\in\mathbb{R}^{d}$.&lt;/p>
&lt;p>对输入 $x\in\mathbb{R}^{s\times d}$, 其 FLOPs 为&lt;/p>
$$
FLOPs(\mathrm{FFN_{ReLU}}) = 2sdd_{ff} + 2sd_{ff}d =\boxed{4sdd_{ff}}
$$&lt;p>其中第一项和第二项分别为为 $xW_1$ 与 $\max(xW_1+b_1, 0)W_2$ 的 FLOPs.&lt;/p>
&lt;p>另一种基于 SwiGLU 激活函数，其定义为&lt;/p>
$$
\mathrm{SwiGLU}(x) = x\odot \sigma(x)
$$&lt;p>其中 $\sigma(\cdot)$ 是 sigmoid 函数&lt;/p>
&lt;p>FFN 的定义为&lt;/p>
$$
y = W_2(W_3x\odot \mathrm{SwiGLU}(W_1x))
$$&lt;p>其中 $W_3,W_1\in\mathbb{R}^{d_{ff}\times d}$, $W_2\in\mathbb{R}^{d\times d_{ff}}$.&lt;/p>
&lt;p>对输入 $x\in\mathbb{R}^{s\times d}$, 其 FLOPs 为&lt;/p>
$$
FLOPs(\mathrm{FFN_{SwiGLU}}) = 2sdd_{ff} + 2sdd_{ff} + 2sd_{ff}d = \boxed{6sdd_{ff}}
$$&lt;h4 id="summary">&lt;a href="#summary" class="header-anchor">&lt;/a>Summary
&lt;/h4>&lt;p>最终，decoder-only transformer 的 FLOPs 计算量为 （我们假设使用 multi-head attention, 基于 SwiGLU 的 MLP）&lt;/p>
$$
\begin{aligned}
FLOPs(\text{forward}) &amp;= FLOPs(\text{embedding}) + n*FLOPs(\mathrm{decode\_layer})+FLOPs(\mathrm{lm\_head})\\
&amp;= n*FLOPs(\mathrm{decode\_layer})+2sd|V|\\
&amp;= n*(2*FLOPs(\mathrm{normoalization}) + FLOPs(\mathrm{Attention})+FLOPs(\mathrm{FFN}))+2sd|V|\\
&amp;= n*(8sd+8sd^2+4s^2d+6sdd_{ff})+2sd|V|\\
&amp;= nsd^2\left(\frac8d + 8+\frac{4s}{d}+\frac{6d_{ff}}{d}+\frac{2|V|}{nd}\right)\\
&amp;\approx \boxed{nsd^2\left(8+\frac{4s}{d}+\frac{6d_{ff}}{d}+\frac{2|V|}{nd}\right)}
\end{aligned}
$$&lt;p>这里我们丢弃了 normalization 项，因为 normalization 的 FLOPs 是一个低阶项&lt;/p>
&lt;h3 id="backward">&lt;a href="#backward" class="header-anchor">&lt;/a>Backward
&lt;/h3>&lt;p>首先，我们有如下结论：&lt;/p>
&lt;blockquote>
&lt;p>神经网络 backward 过程的计算量（FLOPs）为 forward 过程的两倍&lt;/p>
&lt;/blockquote>
&lt;p>我们使用一个简单的例子来证明这个结论，考虑线性层 $h=Wx$, 其中 $W\in\mathbb{R}^{m\times d}$, 对于输入 $x\in\mathbb{R}^{d\times 1}$ 其 forward 过程的计算量为 $2md$.&lt;/p>
&lt;p>对于反向过程，我们需要分别计算损失 $L$ 对权重和输入的梯度，即&lt;/p>
$$
\frac{\partial L}{\partial x} = W^T\frac{\partial L}{\partial h}\in\mathbb{R}^{d\times 1}, \quad\frac{\partial L}{\partial W} = \frac{\partial L}{\partial h}\otimes x^T\in{m\times d},
$$&lt;p>这里 $\frac{\partial L}{\partial h}\in\mathbb{R}^{m\times 1}$ 为损失对输出 $h$ 的梯度。因此反向传播的总计算量为&lt;/p>
$$
2dm + 2md = 4md
$$&lt;p>这里的两项分别是对 $x$ 和 $W$ 求梯度的 FLOPs.&lt;/p>
&lt;h3 id="overall">&lt;a href="#overall" class="header-anchor">&lt;/a>Overall
&lt;/h3>&lt;p>将前向传播和反向传播的计算量汇总我们就得到一次前向传播和一次反向传播过程中，对于长度为 $s$ 的 token, 其 FLOPs 为 (multi-head attention, SwiGLU-FFN)&lt;/p>
$$
\begin{aligned}
C &amp;= FLOPs(\text{forward}) + FLOPs(\text{backward})\\
&amp;= 3FLOPs(\mathrm{forward}) \\
&amp;\approx \boxed{3nsd^2\left(8+\frac{4s}{d}+\frac{6d_{ff}}{d}+\frac{2V}{nd}\right)}
\end{aligned}
$$&lt;h3 id="extension">&lt;a href="#extension" class="header-anchor">&lt;/a>Extension
&lt;/h3>&lt;h4 id="gqa">&lt;a href="#gqa" class="header-anchor">&lt;/a>GQA
&lt;/h4>&lt;p>GQA 与 MHA 不同的地方在于通过共享 key 和 value 来降低 KV cache 的占用，我们假设 group number 为 $g$, 则 key 和 value 的 FLOPs 现在变成了&lt;/p>
$$
2sdd_h\frac{h}{g}+2sdd_h\frac{h}{g}=4sdd_h\frac{h}{g}
$$&lt;p>因此 attention 部分总的 FLOPs 变成了&lt;/p>
$$
FLOPs(\mathrm{Attention})=4sd^2+2s^2d+2s^2d+4sdd_h\frac{h}{g}=4sd^2+4s^2d+4sdd_h\frac{h}{g}
$$&lt;p>当 $g=h$ 时，GQA 就变成了 MHA, 此时的 FLOPs 也一致。&lt;/p>
&lt;h4 id="moe">&lt;a href="#moe" class="header-anchor">&lt;/a>MoE
&lt;/h4>&lt;p>MoE 是针对 Dense FFN 的一个改进，介绍见 &lt;a class="link" href="https://maosong.website/p/moe-tutorial/" target="_blank" rel="noopener"
>MoE&lt;/a>, 我们假设一共有 $e$ 个路由专家，其中激活 $k$ 个。&lt;/p>
&lt;p>Gate layer 一般是一个 linear layer, 其权重矩阵大小为 $W_{G}\in\mathbb{R}^{d\times e}$, 因此 $FLOPs(\text{router})= 2sde$.&lt;/p>
&lt;p>Expert layer 和前面提到的 FFN 一致，我们每次挑选出 $k$ 个专家进行计算，因此 expert 部分 $FLOPs(\text{expert})=6ksdd_{ff}$.&lt;/p>
&lt;p>从而对于 MoE 来说，FFN 部分的 FLOPs 为&lt;/p>
$$
FLOPs(\text{MoE}) = FLOPs(\text{router})+FLOPs(\text{expert})= \boxed{2sde+6ksdd_{ff}}
$$&lt;h3 id="simplification">&lt;a href="#simplification" class="header-anchor">&lt;/a>Simplification
&lt;/h3>&lt;p>我们已经得到了 transformer 的 FLOPs 计算表达式，但是其表达式比较繁琐，因此，在研究 scaling law 时，一般会进行简化。&lt;/p>
&lt;p>首先，在 &lt;a class="link" href="https://maosong.website/p/llm-parameter-computation/" target="_blank" rel="noopener"
>LLM parameter analysis&lt;/a> 中，我们已经给出了 LLM 参数量 $N$ （基于 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a>）的计算结果&lt;/p>
$$
N=n*(4d+3dd_{ff}+2hh_{d}d + 2h_{kv}h_dd) + d(2|V|+1)
$$&lt;p>我们这里对其进行简化，一般来说 $d_{ff}=8/3d$, $h_{kv}=h$, $h_d=d/h$, 带入就得到&lt;/p>
$$
N = n(4d+8d^2+2d^2+2d^2) + d(2|V|+1)=n(12d^2+4d)+ d(2|V|+1)
$$&lt;p>我们忽略关于 $d$ 的一阶项，并且我们假设 $|V| &lt;&lt; 12nd$, 则最终模型参数量可以近似为&lt;/p>
$$
\boxed{N \approx 12nd^2}
$$&lt;p>接下来，我们基于上面的配置简化 FLOPs 表达式&lt;/p>
$$
\begin{aligned}
C &amp;=
3nsd^2\left(8+\frac{4s}{d}+\frac{6d_{ff}}{d}+\frac{2|V|}{nd}\right) \\
&amp;= 3nsd^2\left(24+\frac{4s}{d}+\frac{2|V|}{nd}\right)\\
&amp;\approx 72nsd^2 \\
&amp;= 6sN
\end{aligned}
$$&lt;p>这里我们利用了前面的 $|V| &lt;&lt; 12nd$ 假设，为了简便我们还舍弃了 $4s/d$.&lt;/p>
&lt;p>注意到 $s$ 代表 token 序列长度，如果训练集的总 token 个数为 $D$, 则最终对于包含 $D$ tokens 的数据集和包含 $N$ 参数量的 LLM, 其训练总 FLOPs 可以近似估计为&lt;/p>
$$
\boxed{C\approx 6ND}
$$&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;h3 id="setting">&lt;a href="#setting" class="header-anchor">&lt;/a>Setting
&lt;/h3>&lt;p>接下来我们定量分析一些模型的 FLOPs. 我们基于 &lt;a class="link" href="https://maosong.website/p/chinchilla-scaling-law/" target="_blank" rel="noopener"
>Chinchilla scaling law&lt;/a> 给出的实验配置 (Table A9), 我们筛掉 &lt;code>kv_size * n_heads != d_model&lt;/code> 的配置，$|V|=32,000$.&lt;/p>
&lt;p>各部分的 FLOPs 计算代码如下&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">compute_flops&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">V&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d_ff&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">lm_head_flops&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">V&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attention_flops&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">8&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">s&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">4&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">s&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">s&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">feed_forward_flops&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">4&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">s&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d_ff&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">lm_head_flops&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">attention_flops&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">feed_forward_flops&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>首先我们看一下不同大小模型的 FLOPs 分布情况&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/llm-flops-computation/LLM-FLOPs-flops_distribution.png"
width="1200"
height="600"
loading="lazy"
alt="FLOPs distribution against model size"
class="gallery-image"
data-flex-grow="200"
data-flex-basis="480px"
>&lt;/p>
&lt;p>可以看到，当模型越来越大，FFN 层的算力占比越来越高，这也是为什么后来采用 MoE 架构的一个原因。&lt;/p>
&lt;p>接下来，我们看一下模型 FLOPs 分布情况随上下文长度变化的情况&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/llm-flops-computation/LLM-FLOPs-flops_vs_context.png"
width="1200"
height="600"
loading="lazy"
alt="FLOPs distribution aginst context length"
class="gallery-image"
data-flex-grow="200"
data-flex-basis="480px"
>&lt;/p>
&lt;p>可以看到，随 context length 增加，attention 的算力占比逐渐上升，这符合 attention 是一个平方复杂度的算法的结论。并且，当上下文足够长之后，计算还出现了 overflow (图像右端的突然下降)。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>在本文中，我们介绍了如何计算基于 decoder-only transformer LLM 的 FLOPs, 并推导除了 Kaplan scaling law 中使用的公式 $C=6ND$, 这为后面的 infra 和 scaling law 的学习提供了基础。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2203.15556" target="_blank" rel="noopener"
>Chinchilla Scaling law&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Embedding.html" target="_blank" rel="noopener"
>pytorch embedding layer&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.adamcasson.com/posts/transformer-flops" target="_blank" rel="noopener"
>transformer flops&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Keye-VL 1.5</title><link>https://maosong.website/p/notes-on-keye-vl-1.5/</link><pubDate>Thu, 11 Sep 2025 11:33:31 +0800</pubDate><guid>https://maosong.website/p/notes-on-keye-vl-1.5/</guid><description>&lt;p>快手提出了 Keye-VL 1.5, 一个强调 reasoning, video understanding 的 8B 多模态大模型。作者提出了 slow-fast video encoding strategy 来提高模型的视频理解能力，作者通过在预训练和后训练提高了模型的长上下文能力和 reasoning 能力&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者回顾了多模态大模型的进展，然后提到视频理解能力仍然是一个挑战。为了解决这个问题，作者提出了 Keye-VL-1.5, 一个 8B 的视频理解多模态大模型。&lt;/p>
&lt;p>Keye-VL-1.5 主要做了三点改进：&lt;/p>
&lt;ol>
&lt;li>在架构上，使用了 Slow-Fast Video Encoding&lt;/li>
&lt;li>在预训练阶段，使用多个 stage 来提升模型的长上下文能力&lt;/li>
&lt;li>在 post-training 阶段，提高模型的 reasoning 能力和 alignment 表现&lt;/li>
&lt;/ol>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;p>Keye-VL 1.5 的架构与 &lt;a class="link" href="https://maosong.website/p/notes-on-keye-vl/" target="_blank" rel="noopener"
>Keye-VL&lt;/a> 一致。&lt;/p>
&lt;p>Keye-VL 1.5 主要做出的改进点为针对视频的 encoding 方式&lt;/p>
&lt;p>作者回顾了已有 MLLM 处理视频的方式，比如 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5-vl/" target="_blank" rel="noopener"
>Qwen2.5-VL&lt;/a> 使用 3D convolution 来 merge 相邻的两帧，&lt;a class="link" href="https://maosong.website/p/notes-on-seed1.5-vl/" target="_blank" rel="noopener"
>Seed1.5-VL&lt;/a> 采用了 Dynamic Frame-Resolution Sampling 技巧，来根据 budget 和处理的任务来动态调整采样率 (frame) 和每一帧的图片精度 (resolution)。&lt;/p>
&lt;p>但是这些方法很难进行泛化。因此，作者在本文中就提出了 &lt;strong>SlowFast video encoding stratrgy&lt;/strong>:&lt;/p>
&lt;ol>
&lt;li>Slow Pathway: 空间信息丰富（high resolution），时间信息简略 (low number of frames)&lt;/li>
&lt;li>Fast Pathway: 时间信息丰富（high number of frames），空间信息简略 (low resolution)&lt;/li>
&lt;/ol>
&lt;p>为了区分 slow/fast frames, 作者提出了一个基于 patch similarity 的 metric:&lt;/p>
&lt;ol>
&lt;li>第一帧始终定义为 slow frame&lt;/li>
&lt;li>接下来的每一帧，如果其和上一帧的相似度超过 $95\%$, 则定义为 fast frame; 反之则定义为 slow frame.&lt;/li>
&lt;/ol>
&lt;p>得到 slow/fast frames 之后，作者将 fast-frame 的 token budget 限制为 slow frame token budget 的 $30\%$ 来平衡时间信息以及空间信息。接下来，作者使用二分搜索来决定 slow frame 的 token budget. 为了区分 slow frame 和 fast frame 的 token, 作者使用了特殊的 token 来进行分离。&lt;/p>
&lt;p>最终的处理结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl-1.5/Keye-VL-1-5-video-encoding.png"
width="1364"
height="335"
loading="lazy"
alt="SlowFast Video Encoding"
class="gallery-image"
data-flex-grow="407"
data-flex-basis="977px"
>&lt;/p>
&lt;h3 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h3>&lt;p>预训练的数据和 Keye-VL 基本一致，我们主要介绍改进的点&lt;/p>
&lt;p>对于 Image caption 数据，作者认为这批数据可能会损害模型的指令跟随和 reasoning 能力，因此作者对数据进行了增广，主要是调整了数据的格式：&lt;/p>
&lt;ol>
&lt;li>QA, 数据格式为 &lt;code>&amp;lt;image, caption, [eos], question, answer&amp;gt;&lt;/code>&lt;/li>
&lt;li>reverse QA, 数据格式为 &lt;code>&amp;lt;image, question, answer, [eos], caption&amp;gt;&lt;/code>&lt;/li>
&lt;li>instruction following: 随机给一批数据作为输入，然后让模型基于特定 image 输出 caption&lt;/li>
&lt;/ol>
&lt;p>作者还构建了一个 trap question 来提高模型的 robustness 以及 faithfulness.&lt;/p>
&lt;p>OCR 数据在 Keye-VL 的基础上加入了两点：&lt;/p>
&lt;ol>
&lt;li>Structured Document and Code Understanding: 基于 markdown 和 HTML 等数据来获取 code OCR 数据&lt;/li>
&lt;li>Instruction Following OCR: 基于特定指令进行 OCR&lt;/li>
&lt;/ol>
&lt;p>对于 grounding 数据，作者进一步加入了 temporal grounding 数据，作者首先使用 TEMPURA&lt;/p>
&lt;p>来将短视频分割成若干个 video clips. 然后作者使用 SOTA MLLM 来过滤数据，最后作者基于 Gemini2.5 来生成对应的 QA.&lt;/p>
&lt;p>预训练和 Keye-VL 一样，包含 3 个 stage&lt;/p>
&lt;p>前两个 stage，作者将模型的上下文限制为 8K, 使用了 DP 和 Zero-2 来减少内存开销。在 stage 3, 作者将模型的上下文从 8K 扩展到 128K, 对应的 base frequency 从 1M 提升到 8M. 训练数据包括长视频，长文本和大规模图片。作者将优化策略调整为 Zero-1, CP 和 PP 来支持 long-context 的训练。训练时数据分布为 video:images:text=24:50:26.&lt;/p>
&lt;h3 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h3>&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl-1.5/Keye-VL-1-5-post-training-pipeline.png"
width="1360"
height="449"
loading="lazy"
alt="Post-Training Pipeline"
class="gallery-image"
data-flex-grow="302"
data-flex-basis="726px"
>&lt;/p>
&lt;p>SFT 阶段使用了 &lt;strong>7.5M&lt;/strong> 多模态 QA 样本进行训练。&lt;/p>
&lt;p>MPO 阶段的数据相比 Keye-VL 有所减少，包含：&lt;/p>
&lt;ol>
&lt;li>250K 开源样本&lt;/li>
&lt;li>150K 纯文本数据&lt;/li>
&lt;li>26K 人类标注数据&lt;/li>
&lt;/ol>
&lt;p>对于 reward model 的训练，作者使用了 SFT 和 RL 两个阶段。SFT 阶段的数据包括 R1-Reward 和 MMPR, 训练之后作者还是用比较短的 good response 来避免产生较长的回答&lt;/p>
&lt;p>在 SFT 和 MPO 阶段之后，作者使用 LongCoT code-start 初步激活模型的 reasoning 能力。&lt;/p>
&lt;p>作者构建了一个 5 部的自动化数据生成 pipeline, 如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl-1.5/Keye-VL-1-5-LongCoT-data-generation-pipeline.png"
width="1362"
height="389"
loading="lazy"
alt="LongCoT data generation pipeline"
class="gallery-image"
data-flex-grow="350"
data-flex-basis="840px"
>&lt;/p>
&lt;p>步骤如下：&lt;/p>
&lt;ol>
&lt;li>Multi-Source Data Collection and Enhancement：收集数据&lt;/li>
&lt;li>Multi-Path Reasoning Generation with Confidence Quantification: 基于 confidence 来挑选数据&lt;/li>
&lt;li>Comprehensive Two-Level Quality Assessment: 基于答案和过程的正确性来提高数据质量&lt;/li>
&lt;li>Human-in-the-Loop Quality Enhancement: 对于中等质量的数据请人类进一步进行标注&lt;/li>
&lt;li>Dynamic Quality Scoring and Data Utilization Strategy: 对数据进行打分，高质量数据进行上采样&lt;/li>
&lt;/ol>
&lt;p>接下来就是 General RL 过程。&lt;/p>
&lt;p>对于通用的 RLVR 训练，作者使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-gspo/" target="_blank" rel="noopener"
>GSPO&lt;/a> 算法来进行训练。&lt;/p>
&lt;p>在训练过程中，作者采取了 progressive hint sampling 方式，也就是提供不同程度的 hint 来提高模型的训练效率。作者将 hint 分为五个等级：&lt;/p>
&lt;ol>
&lt;li>Level 1 (Concept / Observation)&lt;/li>
&lt;li>Level 2 (Strategy / Method)&lt;/li>
&lt;li>Level 3 (Tools / Formula)&lt;/li>
&lt;li>Level 4 (Steps / Calculation)&lt;/li>
&lt;li>Level 5 (Complete Solution)&lt;/li>
&lt;/ol>
&lt;p>来提供不同程度的辅助，作者使用 Keye-VL 1.5 来确定最小的 hint level, 然后基于这个 level 提供信息进行 RL 的训练&lt;/p>
&lt;p>为了进一步提高模型的表现，作者采用了一个和 &lt;a class="link" href="https://maosong.website/p/notes-on-seed1.5-vl/" target="_blank" rel="noopener"
>Seed1.5-VL&lt;/a> 一样的迭代式训练策略，即反复进行 SFT 和 RL 来降低训练成本，提高训练效率。&lt;/p>
&lt;p>最后，再通用 RL 阶段之后，作者加入了 alignment RL 的训练来进行对齐，reward 包括三个方面：&lt;/p>
&lt;ol>
&lt;li>rule-based reward&lt;/li>
&lt;li>generative reward&lt;/li>
&lt;li>model-based reward&lt;/li>
&lt;/ol>
&lt;p>任务主要包括三个方面：&lt;/p>
&lt;ol>
&lt;li>instruction following&lt;/li>
&lt;li>format adherence&lt;/li>
&lt;li>preference alignment&lt;/li>
&lt;/ol>
&lt;p>数据介绍如下：&lt;/p>
&lt;ol>
&lt;li>instruction following:25 类硬约束，20 类软约束，数据包括 17K 多模态数据和 23K 纯文本数据，奖励包括 rule-based reward 和 generative reward&lt;/li>
&lt;li>reasoning: 12K 数学和逻辑推理数据&lt;/li>
&lt;li>RAG: 提高模型的搜索能力，作者使用 GSPO 算法进行训练&lt;/li>
&lt;/ol>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>模型表现如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl-1.5/Keye-VL-1-5-Performance.png"
width="1366"
height="988"
loading="lazy"
alt="Performance of Keye-VL 1.5"
class="gallery-image"
data-flex-grow="138"
data-flex-basis="331px"
>&lt;/p>
&lt;p>接下来作者进行了消融实验。&lt;/p>
&lt;p>首先是不同训练阶段对模型表现的影响，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-training-stage.png"
width="1369"
height="458"
loading="lazy"
alt="Ablation study on training strategy"
class="gallery-image"
data-flex-grow="298"
data-flex-basis="717px"
>&lt;/p>
&lt;p>实验结果显示，提高 SFT 训练数据可以有效提高模型在数学推理，逻辑推理和 OCR 任务上的表现。MPO 可以进一步提高模型的表现，Long CoT cold start 可以有效提高模型的 reasoning 表现。&lt;/p>
&lt;p>作者还探究了 model merging 对模型表现的影响，作者首先基于 base model 和 OCR 数据训练得到 OCR expert, 然后进行 merge, 实验结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-model-merging.png"
width="1244"
height="340"
loading="lazy"
alt="Ablation study on model merging"
class="gallery-image"
data-flex-grow="365"
data-flex-basis="878px"
>&lt;/p>
&lt;p>实验结果显示，model merging 可以有效提高模型在 special domain 上的表现，并且还可以维持模型的通用能力&lt;/p>
&lt;p>作者还发现：&lt;/p>
&lt;ol>
&lt;li>expert model 训练时间过长会影响最终 merge model 的表现&lt;/li>
&lt;li>expert mode 训练的学习率应该要设置比较小&lt;/li>
&lt;/ol>
&lt;p>接下来，作者探究了 alignment RL 对模型表现的影响，&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-alignment-RL.png"
width="1385"
height="275"
loading="lazy"
alt="Ablation on alignment RL"
class="gallery-image"
data-flex-grow="503"
data-flex-basis="1208px"
>&lt;/p>
&lt;p>实验结果说明，alignment RL 可以在保持模型 reasoning 能力的同时提高模型的指令跟随能力&lt;/p>
&lt;p>作者还探究了 hint 对模型表现的影响，使用不同 level hint 进行训练对模型表现影响的结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-hint.png"
width="1209"
height="310"
loading="lazy"
alt="Ablation on hint level"
class="gallery-image"
data-flex-grow="390"
data-flex-basis="936px"
>&lt;/p>
&lt;p>实验结果显示，使用 high level 的 hint 可以有效提高模型输出的正确率。&lt;/p>
&lt;p>最后作者探究了 rejection sampling 对模型表现的影响，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-rejection-sampling.png"
width="1384"
height="340"
loading="lazy"
alt="Ablation on rejection sampling"
class="gallery-image"
data-flex-grow="407"
data-flex-basis="976px"
>&lt;/p>
&lt;p>结果发现，通过 rejection sampling，模型的表现有了进一步的提升。因此作者采用了 ST-RL-(RFT-SFT)-(RFT-RL) 的训练方式来进行训练&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 Keye-VL1.5, 一个强调 video understanding 和 reasoning 的 MLLM. Keye-VL 1.5 使用了 SlowFast video encoding strategy 来提高模型的效率和视频理解能力。作者详细介绍了模型的数据，训练和评估。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2509.01563" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on AdamW</title><link>https://maosong.website/p/notes-on-adamw/</link><pubDate>Thu, 04 Sep 2025 10:27:03 +0800</pubDate><guid>https://maosong.website/p/notes-on-adamw/</guid><description>&lt;p>作者提出了一个针对 &lt;a class="link" href="https://maosong.website/p/notes-on-adam/" target="_blank" rel="noopener"
>Adam&lt;/a> 优化器的 weight decay 方法&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者首先回顾了动态梯度算法如 AdaGrad, RMSProp, &lt;a class="link" href="https://maosong.website/p/notes-on-adam/" target="_blank" rel="noopener"
>Adam&lt;/a> 的进展。已有工作表明动态梯度算法的泛化性要比 SGD with momentum 要差。作者在本文中探究了在 SGD 和 Adam 中使用 L2 regularization 和 weight decay 对最终模型表现的影响。结果表明，模型泛化性较差的原因在于对于 Adam, L2 regularization 的效果要比 SGD 差。&lt;/p>
&lt;p>作者有如下发现：&lt;/p>
&lt;ol>
&lt;li>L2 regularization 和 weight decay 不等价。在 SGD 中，L2 regularization 是等价的，但是在 Adam 中这个结论不成立。具体来说，L2 regularization 对历史参数的惩罚要小于 weight decay&lt;/li>
&lt;li>L2 regularization 对 Adam 效果提升有效&lt;/li>
&lt;li>weight decay 对于 SGD 和 AdamW 都很有效，在 SGD 中，weight decay 与 L2 regularization 等价&lt;/li>
&lt;li>最优的 weight decay 取决于 batch, batch 越大，最优的 weight decay 越小&lt;/li>
&lt;li>通过 learning rate scheduler 可以进一步提高 Adam 的表现&lt;/li>
&lt;/ol>
&lt;p>作者在本文中的主要贡献是通过解耦梯度更新中的 weight decay 来提高 Adam 的 regularization.&lt;/p>
&lt;p>作者的主要 motivation 是提升 Adam 表现，让其可以和 SGD with momentum 相比&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;p>Weight decay 的定义如下&lt;/p>
$$
\theta_{t+1} = (1-\lambda)\theta_t - \alpha \nabla f_t(\theta_t) \tag{1}
$$&lt;p>其中 $\lambda$ 是 weight decay rate, $\nabla f_t(\theta_t)$ 是第 $t$ 个 batch 的梯度，$\alpha$ 是学习率。&lt;/p>
&lt;p>首先，对于标准的 SGD 来说，weight decay 与 L2 regularization 等价&lt;/p>
&lt;blockquote>
&lt;p>Proposition 1
对于标准的 SGD 来说，对损失函数 $f_t(\theta_t)$ 执行 weight decay （公式 $(1)$）与对损失函数 $f_t(\theta_t)+\lambda'/2\|\theta_t\|_2^2$ 执行梯度下降算法是等价的，这里 $\lambda'=\lambda/\alpha$。&lt;/p>
&lt;/blockquote>
&lt;p>证明比较简单，只需要写出损失函数的梯度下降更新公式即可。&lt;/p>
&lt;p>基于这个结论，大部分优化算法都将 L2 regularization 和 weight decay 看做是等价的。但实际上，这个结论对于 adaptive gradient 方法来说是不成立的。结论如下&lt;/p>
&lt;blockquote>
&lt;p>Proposition 2
令 $O$ 为一个 optimizer, 其目标函数为 $f_t(\theta)$, 当不考虑 weight decay 时，梯度更新过程为 $\theta_{t+1}\gets \theta_t-\alpha M_t\nabla f_t(\theta_t)$. 当考虑 weight decay 时，梯度更新过程为 $\theta_{t+1}\gets (1-\lambda)\theta_t-\alpha M_t\nabla f_t(\theta_t)$. 如果 $M_t\neq kI$, 则不存在 $\lambda'$, 使得 $O$ 在优化目标函数 $f_t^{reg}(\theta)=f_t(\theta)+\lambda'/2\|\theta\|_2^2$ 时，不考虑 weight decay 的梯度更新与 $O$ 在优化目标函数 $f_t(\theta)$ 时，考虑 weight decay 的梯度更新等价。&lt;/p>
&lt;/blockquote>
&lt;p>证明比较简单，只需要写出两个目标函数对应的梯度更新公式即可。&lt;/p>
&lt;p>作者通过分析发现，在 adaptive gradient 方法中，对于 L2 regularization，梯度和 regularization 是打包在一起考虑的。而 weight decay 是分开考虑的。这就导致了对于梯度比较大的权重，L2 regularization 的学习率较小，从而 regularization 效应减弱。而 weight decay 中，这种效应则不存在。因此 weight decay 的 regularization 效应更强。&lt;/p>
&lt;p>作者通过这个分析，给出了一个 weight decay 与 L2 regularization 相等的条件&lt;/p>
&lt;blockquote>
&lt;p>Proposition 3
令 $O$ 为一个 optimizer, 其目标函数为 $f_t(\theta)$, 当不考虑 weight decay 时，梯度更新过程为 $\theta_{t+1}\gets \theta_t-\alpha M_t\nabla f_t(\theta_t)$. 当考虑 weight decay 时，梯度更新过程为 $\theta_{t+1}\gets (1-\lambda)\theta_t-\alpha M_t\nabla f_t(\theta_t)$. 如果 $M_t= \mathrm{diag}(s)^{-1}$ ($s_i>0,\forall i$), 则 $O$ 在优化目标函数&lt;/p>
$$
> f_t^{reg}(\theta)=f_t(\theta)+\frac{\lambda'}{2\alpha}\|\theta\odot \sqrt{s}\|_2^2
> $$&lt;p>时，不考虑 weight decay 的梯度更新与 $O$ 在优化目标函数 $f_t(\theta)$ 时，考虑 weight decay 的梯度更新等价。&lt;/p>
&lt;/blockquote>
&lt;p>上面的结论显示，对于比较大的 preconditioner $s_i$, 其在相比于 L2 regularization 被 regularized 的效应更强。&lt;/p>
&lt;p>为了解耦这两个参数，作者提出了 SGDW 算法，其 weight decay 和梯度更新同时进行，算法如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-adamw/AdamW-SGDW-algorithm.png"
width="1163"
height="514"
loading="lazy"
alt="SGDW algorithm"
class="gallery-image"
data-flex-grow="226"
data-flex-basis="543px"
>&lt;/p>
&lt;p>在算法中，为了支持同时给 $\alpha$ 和 $\lambda$ 做 scheduling, 作者提出了一个 scaling factor $\eta_t$, $\eta_t$ 由用户定义的 scheduler &lt;code>SetScheduleMultiplier(t)&lt;/code> 决定。此时，针对 SGD with momentum 的 weight decay 与 L2 regularization 是等价的&lt;/p>
&lt;p>同理，我们也可以对 Adam 算法实行同样的操作，算法如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-adamw/AdamW-AdamW-algorithm.png"
width="1156"
height="560"
loading="lazy"
alt="AdamW algorithm"
class="gallery-image"
data-flex-grow="206"
data-flex-basis="495px"
>&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中分析了 adaptive gradient 方法中 L2 regularization 与 weight decay 的不一致性。基于分析，作者提出了 SGDW 和 AdamW 两个优化算法。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/1711.05101" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Adam</title><link>https://maosong.website/p/notes-on-adam/</link><pubDate>Thu, 04 Sep 2025 10:11:55 +0800</pubDate><guid>https://maosong.website/p/notes-on-adam/</guid><description>&lt;p>作者提出了 Adam, 一个一阶的优化方法，Adam 更加高效，且具有 scaling invariant 的性质。&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者首先回顾了一下已有优化器的进展，其中主要是 SGD. 在本文中，作者提出了 Adam, 一个针对高维参数空间的一阶优化器，Adam 基于 gradient 的一阶和二阶信息为不同的参数安排不同的学习率。Adam 的来源是 &lt;em>adaptive moment estimation&lt;/em>. Adam 主要是结合了 AdaGrad 和 RMSProp 两个算法的优点。&lt;/p>
&lt;p>Adam 与 RMSProp 的区别在于：&lt;/p>
&lt;ol>
&lt;li>RMSProp 在 rescaled gradient 上进行 momentum 的计算然后更新，而 Adam 直接使用一阶和二阶矩来进行估计&lt;/li>
&lt;li>RMSProp 没有 bias-correction 项&lt;/li>
&lt;/ol>
&lt;p>Adam 的主要优势为：&lt;/p>
&lt;ol>
&lt;li>参数更新的量级与 gradient 的 scaling 无关&lt;/li>
&lt;li>步长被 stepsize 超参数限制&lt;/li>
&lt;li>不要求目标函数 stationary&lt;/li>
&lt;li>对于稀疏梯度 work 的比较好&lt;/li>
&lt;li>优化器自带 annealing&lt;/li>
&lt;/ol>
&lt;h2 id="algorithm">&lt;a href="#algorithm" class="header-anchor">&lt;/a>Algorithm
&lt;/h2>&lt;p>Adam 的算法如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-adam/Adam-optimizer-algorithm.png"
width="1163"
height="725"
loading="lazy"
alt="Adam Algorithm"
class="gallery-image"
data-flex-grow="160"
data-flex-basis="384px"
>&lt;/p>
&lt;p>我们优化的目标函数如下&lt;/p>
$$
\min_{\theta}\quad f(\theta)
$$&lt;p>这里 $f$ 一般是一个神经网络。我们记 $f(\theta)$ 在 $\theta_t$ 处的梯度为 $g_t=\nabla_{\theta}f(\theta_t)$.&lt;/p>
&lt;p>算法运行时，会更新梯度 $m_t$ 以及梯度二阶矩 $v_t$ 的 exponential moving average. 超参数 $\beta_1,\beta_2$ 负责控制 exponential decay rates. 这里 $m_t$ 和 $v_t$ 分别是一阶动量（均值）和二阶动量（未中心化的 variance）的估计。由于 $m_t$ 和 $v_t$ 的初始化都是 0, 因此他们会引入 bias, 作者在后续通过修正解决了这个问题。&lt;/p>
&lt;p>假设 $\epsilon=0$, 如果除了当前时刻 $t$ 之外，之前所有时刻的梯度 $g_i=0,i&lt;t$, 此时&lt;/p>
$$
m_t = (1-\beta_t)g_t, v_t=(1-\beta_2)g_t^2
$$&lt;p>修正后的一阶和二阶矩分别为&lt;/p>
$$
\Delta_t = \alpha \frac{(1-\beta_1)\sqrt{1-\beta_2^t}}{(1-\beta_1^t)\sqrt{1-\beta_2}}
$$&lt;p>当 $t$ 足够大的时候， $\beta_1^t\to0, \beta_2^t\to0$, 此时&lt;/p>
$$
\Delta_t = \alpha \frac{1-\beta_1}{\sqrt{1-\beta_2}}
$$&lt;p>如果之前所有时刻的梯度不全为 0, 则依据 Cauchy-Schwarz 不等式，我们有 $(\mathbb{E}[XY])^2\leq \mathbb{E}[X^2]\mathbb{E}[Y^2]$. 令 $X=1$, $Y=g$, 则我们有&lt;/p>
$$
(\mathbb{E}[g])^2\leq \mathbb{E}[g^2] \Rightarrow \frac{|\mathbb{E}[g]|}{\sqrt{\mathbb{E}[g^2]}}\leq 1
$$&lt;p>此时，我们有&lt;/p>
$$
\mathbb{E}[g_t] = \hat{m}_t, \mathbb{E}[g_t^2] = \hat{v}_t
$$&lt;p>因此，&lt;/p>
$$
|\Delta_t| = \left|\alpha\frac{\hat{m}_t}{\sqrt{\hat{v}_t}}\right|=\alpha \left|\frac{|\mathbb{E}[g]|}{\sqrt{\mathbb{E}[g^2]}}\right|\leq\alpha
$$&lt;p>从而我们有&lt;/p>
$$
|\Delta_t| \leq\begin{cases}
\alpha \frac{1-\beta_1}{\sqrt{1-\beta_2}} &amp; \text{ if }1-\beta_1>\sqrt{1-\beta_2}\\
\alpha &amp;\text{ otherwise}
\end{cases}
$$&lt;p>实际上，$\Delta_t$ 可以理解为一个 trust region, 可以用来保证当前更新的参数不会离原始参数太远。&lt;/p>
&lt;p>作者定义 signal-noise ratio (SNR) 为&lt;/p>
$$
SNR = \frac{\hat{m}_t}{\sqrt{\hat{v}_t}}
$$&lt;p>当 SNR 较小时，说明此时的不确定性比较大，因此 $\Delta_t$ 也比较小。这就避免了模型朝错误的方向更新。也就是&lt;em>automatic annealing&lt;/em>.&lt;/p>
&lt;p>$\Delta_t$ 还对 gradient 的 scaling 有不变的性质，这是因为，&lt;/p>
$$
\frac{c\cdot\hat{m}_t}{\sqrt{c^2\cdot\hat{v}_t}} = \frac{\hat{m}_t}{\sqrt{\hat{v}_t}}
$$&lt;h2 id="bias-correction">&lt;a href="#bias-correction" class="header-anchor">&lt;/a>Bias Correction
&lt;/h2>&lt;p>上一节提到，Adam 算法的初始化是存在 bias 的，作者在本届就对齐进行了分析。令 $g$ 为目标函数 $f$ 的梯度，我们希望估计其二阶动量的期望.令 $g_1,\dots,g_T$ 分别为 $\theta_1,\dots,\theta_T$ 处的梯度估计，其中 $g_t\sim p(g_t)$ 是对应时刻梯度的分布。令 $v_0=0$, 在 $t$ 时刻，我们有&lt;/p>
$$
v_t = \beta_2 v_{t-1} + (1-\beta_2)g_t^2 = (1-\beta_2)\sum_{i=1}^t\beta_2^{t-i} g_i^2
$$&lt;p>我们希望计算 $\mathbb{E}[v_t]$ 与 $\mathbb{E}[g_t^2]$ 之间的关系，我们有&lt;/p>
$$
\begin{aligned}
\mathbb{E}[v_t] &amp;= \left[(1-\beta_2)\sum_{i=1}^t\beta_2^{t-i} g_i^2\right]\\
&amp;= \mathbb{E}[g_t^2]\cdot (1-\beta_2)\sum_{i=1}^t\beta_2^{t-i}+\zeta\\
&amp;= \mathbb{E}[g_t^2](1-\beta_2^t)+\zeta
\end{aligned}
$$&lt;p>其中当 $\mathbb{E}[g_i^2]$ 为 stationary 时，$\zeta=0$, 否则我们可以通过控制 $\beta_2$ 来让 past gradient 保持在一个较小的规模。最后，我们剩下的就是 $1-\beta_2^t$, 这也是我们在算法中进行修正的地方。&lt;/p>
&lt;p>对于一阶动量 $m_t$ 的修正也是同理。&lt;/p>
&lt;h2 id="convergence-analysis">&lt;a href="#convergence-analysis" class="header-anchor">&lt;/a>Convergence Analysis
&lt;/h2>&lt;p>作者在本节中使用了 online learning framework 来分写 Adam 的收敛性。给定一系列 convex cost function $f_1(\theta),\dots,f_T(\theta)$. 在 $t$ 时刻，我们的目标是基于上一个 cost function $f_t(\theta)$ 来预测 $\theta_t$.&lt;/p>
&lt;p>作者在这里使用 regret 来分析，记 $f_t(\theta^*)$ 为 $t$ 时刻最优的参数对应的 cost function, regret 定义为&lt;/p>
$$
R(T) = \sum_{t=1}^T [f_t(\theta_t) - f_t(\theta^*)]
$$&lt;p>其中，&lt;/p>
$$
\theta^* = \arg\min_{\theta\in\mathcal{X}}\sum_{t=1}^Tf_t(\theta)
$$&lt;p>则我们有如下的结论&lt;/p>
&lt;blockquote>
&lt;p>Theorem 1
假设&lt;/p>
&lt;ol>
&lt;li>函数 $f_t$ 的梯度是有界的，即 $\|\nabla f_t(\theta)\|_2\leq G$, $\|\nabla f_t(\theta)\|_{\infty}\leq G_{\infty}$ 对任意 $\theta\in\mathbb{R}^d$ 都成立&lt;/li>
&lt;li>$\{\theta_1,\dots,\theta_T\}$ 中任意两个参数的距离都是有界的，即 $\|\theta_m-\theta_m\|_2\leq D$, $\|\theta_m-\theta_n\|_{\infty}\leq D_{\infty}$ 对任意 $m,n\in\{1,\dots,T\}$ 都成立&lt;/li>
&lt;li>$\beta_1,\beta_2\in[0,1)$ 满足 $\frac{\beta_1^2}{\sqrt{\beta_2}}&lt;1$
令 $\alpha_t=\alpha/\sqrt{t}$, $\beta_{1,t}=\beta_1\lambda^{t-1}$, $\lambda\in(0,1)$, 则我们有&lt;/li>
&lt;/ol>
$$
> R(T)\leq \frac{D^2}{2\alpha(1-\beta_1)}\sum_{i=1}^d\sqrt{T\hat{v}_{T,i}}+\frac{\alpha(1+\beta_1)G_{\infty}}{(1-\beta_1)\sqrt{1-\beta_2}(1-\gamma)^2}\sum_{i=1}^d\|g_{1:T,i}\|_2+\sum_{i=1}^d\frac{D_{\infty}^2G_{\infty}\sqrt{1-\beta_2}}{2\alpha(1-\beta_1)(1-\lambda)^2}
> $$&lt;/blockquote>
&lt;p>结果说明，当我们的 data feature 稀疏且梯度有界时我们有&lt;/p>
$$
\sum_{i=1}^d\|g_{1:T,i}\|_2&lt;&lt; dG_{\infty}\sqrt{T}
$$&lt;p>以及&lt;/p>
$$
\sum_{i=1}^d\sqrt{T\hat{v}_{T,i}}&lt;&lt; dG_{\infty}\sqrt{T}
$$&lt;p>实际上，对于 Adam 以及 Adamgrad，这个上界可以优化到 $O(\log d\sqrt{T})$.&lt;/p>
&lt;p>最终，我们可以证明 Adam 的收敛性&lt;/p>
&lt;blockquote>
&lt;p>Corollary 1
假设&lt;/p>
&lt;ol>
&lt;li>函数 $f_t$ 的梯度是有界的，即 $\|\nabla f_t(\theta)\|_2\leq G$, $\|\nabla f_t(\theta)\|_{\infty}\leq G_{\infty}$ 对任意 $\theta\in\mathbb{R}^d$ 都成立&lt;/li>
&lt;li>$\{\theta_1,\dots,\theta_T\}$ 中任意两个参数的距离都是有界的，即 $\|\theta_m-\theta_m\|_2\leq D$, $\|\theta_m-\theta_n\|_{\infty}\leq D_{\infty}$ 对任意 $m,n\in\{1,\dots,T\}$ 都成立
则对 $T\geq1$, 我们有&lt;/li>
&lt;/ol>
$$
> \frac{R(T)}{T}=O\left(\frac{1}{\sqrt{T}}\right)
> $$&lt;/blockquote>
&lt;h2 id="experiment">&lt;a href="#experiment" class="header-anchor">&lt;/a>Experiment
&lt;/h2>&lt;p>作者在 logistic regression, MLP, CNN 等三种模型架构上进行了实验。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 Adam optimizer， 一个基于 AdaGrad 和 RMSProp 优点的优化器，作者通过理论验证了 Adam 的收敛性，然后通过实验验证了 Adam 的有效性。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/1412.6980" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on RNoPE-SWA</title><link>https://maosong.website/p/notes-on-rnope-swa/</link><pubDate>Tue, 02 Sep 2025 11:24:10 +0800</pubDate><guid>https://maosong.website/p/notes-on-rnope-swa/</guid><description>&lt;p>作者系统性分析了已有的 attention 机制，然后作者提出了混合的 attention 机制，来提高模型在长上下文的表现以及维持模型在短上下文场景下的表现。&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者首先强调了提升 LLM 上下文长度面临的问题：&lt;/p>
&lt;ol>
&lt;li>如何有效处理长上下文输入&lt;/li>
&lt;li>如何训练长上下文 LLM&lt;/li>
&lt;li>如何降低长上下文 LLM 在 inference 时的 latency 以及 memory usage&lt;/li>
&lt;/ol>
&lt;p>对于建模长上下文输入，我们可以从 attention 机制或者位置编码来入手。前者类似的工作有 Landmark Attention 和 Focused Transformer, 但是这些方法的问题在于训练不稳定。 &lt;a class="link" href="https://maosong.website/p/notes-on-qk-norm/" target="_blank" rel="noopener"
>QK norm&lt;/a> 可以比较好解决 softmax 分布过于极端的问题，但是其问题在于训练时的数值不稳定性，并且可能会影响模型的长上下文能力。&lt;/p>
&lt;p>另一方面，对于位置编码，已有的工作如 APE, AliBi, &lt;a class="link" href="https://maosong.website/p/notes-on-position-encoding/" target="_blank" rel="noopener"
>RoPE&lt;/a> 等都可以提供位置信息。但是，这些方法很有可能回影响模型最终的 attention score 分布。另外，&lt;a class="link" href="https://maosong.website/p/notes-on-nope/" target="_blank" rel="noopener"
>NoPE&lt;/a> 探究了移除 position encoding 的可能性。&lt;/p>
&lt;p>还有一些工作目的是降低 softmax attention 的时间复杂度和空间复杂度。比如 sliding window attention, sparse attention, &lt;a class="link" href="https://maosong.website/p/notes-on-streamingllm/" target="_blank" rel="noopener"
>attention sink&lt;/a> 等都可以降低整体的时间/空间复杂度。但是这些方法最终的表现都有所下降。&lt;/p>
&lt;h2 id="observation">&lt;a href="#observation" class="header-anchor">&lt;/a>Observation
&lt;/h2>&lt;p>作者首先对比了以下不同方法对模型长上下文能力的影响。&lt;/p>
&lt;p>作者训练了一个 8B 的模型，然后分别对比了三种方法：&lt;/p>
&lt;ol>
&lt;li>RoPE: base frequency 设置为 10,000, SFT 阶段扩展到 2M&lt;/li>
&lt;li>QK-Norm: 在 RoPE 的基础上，对 query 和 key 先进行 normalization 再进行 RoPE&lt;/li>
&lt;li>NoPE: 移除 attention 中的位置编码信息&lt;/li>
&lt;/ol>
&lt;p>作者分别评估了三种方法的表现，实验结果如下表&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Val Loss&lt;/th>
&lt;th>MMLU&lt;/th>
&lt;th>HellaSwag&lt;/th>
&lt;th>CommonsenseQA&lt;/th>
&lt;th>ARC-E&lt;/th>
&lt;th>ARC-C&lt;/th>
&lt;th>Needles 65k&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>RoPE&lt;/td>
&lt;td>1.52&lt;/td>
&lt;td>48.55&lt;/td>
&lt;td>73.74&lt;/td>
&lt;td>68.30&lt;/td>
&lt;td>81.05&lt;/td>
&lt;td>39.13&lt;/td>
&lt;td>9.82&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>QK-Norm&lt;/td>
&lt;td>1.53&lt;/td>
&lt;td>48.21&lt;/td>
&lt;td>73.68&lt;/td>
&lt;td>68.23&lt;/td>
&lt;td>80.54&lt;/td>
&lt;td>38.98&lt;/td>
&lt;td>7.93&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>NoPE&lt;/td>
&lt;td>1.58&lt;/td>
&lt;td>47.61&lt;/td>
&lt;td>72.16&lt;/td>
&lt;td>66.42&lt;/td>
&lt;td>76.94&lt;/td>
&lt;td>37.12&lt;/td>
&lt;td>9.03&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>实验结果显示，对于通用任务，RoPE 和 QK-Norm 的表现差不多，而 NoPE 的表现较差。对于长上下文任务，QK-Norm 的表现最差。&lt;/p>
&lt;p>接下来，作者分析了三种方法的 attention 分布情况。作者将上下文分为四个 segments：&lt;/p>
&lt;ul>
&lt;li>begin: 开始的 10 个 token&lt;/li>
&lt;li>needle: 与 needle 相关的 tokens&lt;/li>
&lt;li>context: 通用的上下文 token&lt;/li>
&lt;li>qc: question/completion token, 语文题答案相关的 token&lt;/li>
&lt;/ul>
&lt;p>作者将 needle 放置在 50% 深度的位置。评测的实验结果如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Context Length&lt;/th>
&lt;th>Model Variants&lt;/th>
&lt;th>begin&lt;/th>
&lt;th>needle&lt;/th>
&lt;th>context&lt;/th>
&lt;th>qc&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>8k&lt;/td>
&lt;td>RoPE&lt;/td>
&lt;td>0.3863&lt;/td>
&lt;td>0.0328&lt;/td>
&lt;td>0.3809&lt;/td>
&lt;td>0.2000&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>QK-Norm&lt;/td>
&lt;td>0.0242&lt;/td>
&lt;td>0.0173&lt;/td>
&lt;td>0.8020&lt;/td>
&lt;td>0.1565&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>NoPE&lt;/td>
&lt;td>0.3058&lt;/td>
&lt;td>0.0454&lt;/td>
&lt;td>0.4501&lt;/td>
&lt;td>0.1987&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>32k&lt;/td>
&lt;td>RoPE&lt;/td>
&lt;td>0.3541&lt;/td>
&lt;td>0.0201&lt;/td>
&lt;td>0.4343&lt;/td>
&lt;td>0.1915&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>QK-Norm&lt;/td>
&lt;td>0.0064&lt;/td>
&lt;td>0.0056&lt;/td>
&lt;td>0.8517&lt;/td>
&lt;td>0.1364&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>NoPE&lt;/td>
&lt;td>0.2807&lt;/td>
&lt;td>0.0325&lt;/td>
&lt;td>0.4981&lt;/td>
&lt;td>0.1886&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>128k&lt;/td>
&lt;td>RoPE&lt;/td>
&lt;td>0.3463&lt;/td>
&lt;td>0.0010&lt;/td>
&lt;td>0.4751&lt;/td>
&lt;td>0.1776&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>QK-Norm&lt;/td>
&lt;td>0.0010&lt;/td>
&lt;td>0.0004&lt;/td>
&lt;td>0.8993&lt;/td>
&lt;td>0.0994&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>NoPE&lt;/td>
&lt;td>0.0846&lt;/td>
&lt;td>0.0073&lt;/td>
&lt;td>0.8156&lt;/td>
&lt;td>0.0925&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>实验结果显示：&lt;/p>
&lt;ol>
&lt;li>NoPE 是最关注 needle token 信息的，RoPE 次之，QK-Norm 最差&lt;/li>
&lt;li>QK-Norm 更关注上下文信息，对其他的信息关注度较少&lt;/li>
&lt;/ol>
&lt;p>作者发现 QK-Norm 对初始的 token 信息关注度较少，作者认为这是因为 normalization 会让模型更关注邻近的 token 信息。为了验证这个观点，作者在不同的上下文长度下，分别计算了不同方法的 attention 分布情况，为了避免噪声，作者将开始的 10 个 token 以及最后的 $3\%$ token 排除在外，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-rnope-swa/RNoPE-SWA-8k-attention.png"
width="700"
height="496"
loading="lazy"
alt="Attention distribution on 8K context length"
class="gallery-image"
data-flex-grow="141"
data-flex-basis="338px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-rnope-swa/RNoPE-SWA-32k-attention.png"
width="692"
height="482"
loading="lazy"
alt="Attention distribution on 32K context length"
class="gallery-image"
data-flex-grow="143"
data-flex-basis="344px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-rnope-swa/RNoPE-SWA-128k-attention.png"
width="693"
height="564"
loading="lazy"
alt="Attention distribution on 128K context length"
class="gallery-image"
data-flex-grow="122"
data-flex-basis="294px"
>&lt;/p>
&lt;p>实验结果说明，相比于 softmax attention, QK-Norm 的 attention score 分布更平滑，其对于 need token 的注意力不如 RoPE, 这也说明了为什么 QK-Norm 的长上下文能力比较差。并且，QK-Norm 的 recency bias 更严重。&lt;/p>
&lt;p>作者通过计算 RoPE 和 QK-Norm 的 attention distribution 的 entropy 来进一步说明这一点，结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>8k&lt;/th>
&lt;th>32k&lt;/th>
&lt;th>128k&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>RoPE&lt;/td>
&lt;td>6.02&lt;/td>
&lt;td>6.95&lt;/td>
&lt;td>7.62&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>QK-Norm&lt;/td>
&lt;td>10.71&lt;/td>
&lt;td>12.46&lt;/td>
&lt;td>14.14&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>结果显示 QK-Norm 的熵更高，这意味着 QK-Norm 的 attention score 分布更分散，也就证明了 Qk-Norm retrieval 能力比较差。&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;p>考虑到 &lt;a class="link" href="https://maosong.website/p/notes-on-nope/" target="_blank" rel="noopener"
>NoPE&lt;/a> 和 RopE 各自的优点，作者提出了一个混合架构，来结合 NoPE 与 RoPE. 具体做法就是 NoPE layer 和 RoPE layer 交替进行。作者将这个模型架构记为 RNoPE.&lt;/p>
&lt;p>RNoPE 不同 layer 与不同 base frequency 产生的 attention score 分布如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>NoPE Layers - begin&lt;/th>
&lt;th>NoPE Layers - needle&lt;/th>
&lt;th>NoPE Layers - context&lt;/th>
&lt;th>NoPE Layers - qc&lt;/th>
&lt;th>RoPE Layers - begin&lt;/th>
&lt;th>RoPE Layers - needle&lt;/th>
&lt;th>RoPE Layers - context&lt;/th>
&lt;th>RoPE Layers - qc&lt;/th>
&lt;th>needles-128k&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>RoPE&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>0.3541&lt;/td>
&lt;td>0.0201&lt;/td>
&lt;td>0.4343&lt;/td>
&lt;td>0.1915&lt;/td>
&lt;td>7.395&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RNoPE-10k&lt;/td>
&lt;td>0.3275&lt;/td>
&lt;td>0.0765&lt;/td>
&lt;td>0.5672&lt;/td>
&lt;td>0.0287&lt;/td>
&lt;td>0.0049&lt;/td>
&lt;td>0.0004&lt;/td>
&lt;td>0.6805&lt;/td>
&lt;td>0.3142&lt;/td>
&lt;td>8.036&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RNoPE-100k&lt;/td>
&lt;td>0.3263&lt;/td>
&lt;td>0.0778&lt;/td>
&lt;td>0.5633&lt;/td>
&lt;td>0.0327&lt;/td>
&lt;td>0.0241&lt;/td>
&lt;td>0.0005&lt;/td>
&lt;td>0.6782&lt;/td>
&lt;td>0.2972&lt;/td>
&lt;td>7.461&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RNoPE-2M&lt;/td>
&lt;td>0.3250&lt;/td>
&lt;td>0.0712&lt;/td>
&lt;td>0.5735&lt;/td>
&lt;td>0.0303&lt;/td>
&lt;td>0.1111&lt;/td>
&lt;td>0.0046&lt;/td>
&lt;td>0.6233&lt;/td>
&lt;td>0.2611&lt;/td>
&lt;td>7.022&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RNoPE-4M&lt;/td>
&lt;td>0.3486&lt;/td>
&lt;td>0.0369&lt;/td>
&lt;td>0.5981&lt;/td>
&lt;td>0.0165&lt;/td>
&lt;td>0.0960&lt;/td>
&lt;td>0.0039&lt;/td>
&lt;td>0.6774&lt;/td>
&lt;td>0.2227&lt;/td>
&lt;td>6.203&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RNoPE-10k-swa&lt;/td>
&lt;td>0.3303&lt;/td>
&lt;td>0.0742&lt;/td>
&lt;td>0.5634&lt;/td>
&lt;td>0.0321&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>9.562&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>实验结果显示，在 RNoPE 架构中，&lt;/p>
&lt;ol>
&lt;li>提升 base frequency 带来的增益逐渐递减&lt;/li>
&lt;li>NoPE layer 的 retrieval 能力比较强，表现在 attention sink 现象以及对于 needle token 的 spike. 但是其 recency bias 比较弱&lt;/li>
&lt;li>RoPE 的 retrieval 能力比较弱，但是其 attention sink 现象比较小&lt;/li>
&lt;li>当 base frequency 增加止呕，RoPE 的 recency bias 会降低，表现在 qc token 的权重降低&lt;/li>
&lt;/ol>
&lt;p>作者发现，RoPE 的 receptive field 会影响 NoPE layer 的 retrieval 能力。作者总结得到两个 insight&lt;/p>
&lt;ol>
&lt;li>NoPE layer 对于 retrieval 任务比较擅长，而 RoPE layer 对于处理 local Information 比较擅长&lt;/li>
&lt;li>限制 RoPE 的 receptive field 可以保证 NoPE layer 的 retrieval 能力&lt;/li>
&lt;/ol>
&lt;p>基于这两个 insight, 作者构建了 RNoPE-SWA 架构，RNoPE-SWA 相比于 RNoPE, 将 full attention 变成了 sliding window attention, 来避免 RoPE layer 对下游的 NoPE layer 产生影响。&lt;/p>
&lt;p>最终，作者基于 Command R+ 构建了模型，作者去除了 QK-Norm, 对于 NoPE layer, 作者使用了 full attention, 对于 RoPE layer, 作者使用了 window size 为 4096 的 sliding window attention. 作者通过实验验证了 NoPE layer 和 RoPElayer 的比例，实验结果发现 $1:3$ 的比例是最优的。最终每 4 个 layer 为一组，前三组为 SWA, 最后一层为 full attention.&lt;/p>
&lt;p>最终，在通用任务上评测的结果如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>MMLU&lt;/th>
&lt;th>HellaSwag&lt;/th>
&lt;th>ARC-E&lt;/th>
&lt;th>ARC-C&lt;/th>
&lt;th>SATEn&lt;/th>
&lt;th>SATMath&lt;/th>
&lt;th>GSM8K&lt;/th>
&lt;th>Winogrande&lt;/th>
&lt;th>MBPP&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Baseline&lt;/td>
&lt;td>57.5&lt;/td>
&lt;td>75.8&lt;/td>
&lt;td>84.6&lt;/td>
&lt;td>48.5&lt;/td>
&lt;td>70.0&lt;/td>
&lt;td>30.9&lt;/td>
&lt;td>40.9&lt;/td>
&lt;td>68.5&lt;/td>
&lt;td>39.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RNope-SWA&lt;/td>
&lt;td>59.5&lt;/td>
&lt;td>76.2&lt;/td>
&lt;td>82.5&lt;/td>
&lt;td>48.8&lt;/td>
&lt;td>71.9&lt;/td>
&lt;td>30.5&lt;/td>
&lt;td>42.7&lt;/td>
&lt;td>69.5&lt;/td>
&lt;td>39.3&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>在 Ruler retrieval 上评测的结果如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>8k&lt;/th>
&lt;th>16k&lt;/th>
&lt;th>32k&lt;/th>
&lt;th>64k&lt;/th>
&lt;th>128k&lt;/th>
&lt;th>256k&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Baseline&lt;/td>
&lt;td>96.6&lt;/td>
&lt;td>94.4&lt;/td>
&lt;td>95.1&lt;/td>
&lt;td>89.1&lt;/td>
&lt;td>83.0&lt;/td>
&lt;td>57.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RNope-SWA&lt;/td>
&lt;td>96.1&lt;/td>
&lt;td>96.1&lt;/td>
&lt;td>94.9&lt;/td>
&lt;td>92.0&lt;/td>
&lt;td>90.0&lt;/td>
&lt;td>74.8&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>在 Ruler QA 上评测的结果如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>8k&lt;/th>
&lt;th>16k&lt;/th>
&lt;th>32k&lt;/th>
&lt;th>64k&lt;/th>
&lt;th>128k&lt;/th>
&lt;th>256k&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Baseline&lt;/td>
&lt;td>53.5&lt;/td>
&lt;td>50.0&lt;/td>
&lt;td>52.5&lt;/td>
&lt;td>45.5&lt;/td>
&lt;td>36.0&lt;/td>
&lt;td>30.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RNope-SWA&lt;/td>
&lt;td>55.5&lt;/td>
&lt;td>52.5&lt;/td>
&lt;td>55.5&lt;/td>
&lt;td>49.0&lt;/td>
&lt;td>46.0&lt;/td>
&lt;td>42.5&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>实验结果说明，模型在通用任务任务上与 baseline 模型表现差不多，且长上下文能力更强&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者提出了 RNope-SWA, 一个混合 NoPE, RoPE position embedding 和 sliding window attention 的 attention 机制。RNope-SWA 可以在保持模型表现的同时提高计算效率和降低 KV cache 占用。&lt;/p>
&lt;p>尽管本文和已有的工作如 YoCo, Jamba-1.5 和 &lt;a class="link" href="https://maosong.website/p/notes-on-minimax-01/" target="_blank" rel="noopener"
>MiniMax-01&lt;/a> 均证明了混合 attention 架构的有效性。但是，目前对于其工作机制尚缺乏一个比较系统性的理解。作者认为这是一个需要探究的方向。另外，作者还认为如何更好的汇总上下文信息与位置信息也是一个可以探究的方向。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2501.18795" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on InternVL3.5</title><link>https://maosong.website/p/notes-on-internvl3.5/</link><pubDate>Mon, 01 Sep 2025 11:30:50 +0800</pubDate><guid>https://maosong.website/p/notes-on-internvl3.5/</guid><description>&lt;p>上海 AI LAB 提出了 InternVL 3.5 系列多模态大模型，InternVL 3.5 主要强调了模型的 reasoning 能力以及 inference 效率。&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者首先介绍了已有多模态大模型的进展，然后提到有两个未解决的问题：&lt;/p>
&lt;ol>
&lt;li>如何构建一个 stable, effective, scalable 的 RL 框架来训练 MLLM&lt;/li>
&lt;li>如何降低 MLLM 在长上下文场景下的计算开销过高的问题&lt;/li>
&lt;/ol>
&lt;p>为了解决这两个问题，作者提出了 InternVL3.5 多模态大模型系列，相比于 InternVL3, 模型在 reasoning 能力和 efficiency 上均进行了提升。作者主要通过 Cascade RL 框架来提高模型的 reasoning 表现，以及使用 Visual Resolution Router (ViR) 和 Decoupled Vision Language Deployment (DvD) 来提高模型的推理效率。&lt;/p>
&lt;p>总结起来，InternVL3.5 模型的贡献如下：&lt;/p>
&lt;ol>
&lt;li>开源了 InternVL3.5 系列多模态大模型，模型拥有先进的 reasoning 能力以及推理效率&lt;/li>
&lt;li>提出了 Cascade RL, ViR 以及 DvD 等模块来提高模型的表现/推理效率&lt;/li>
&lt;li>系统性评估了模型的表现&lt;/li>
&lt;/ol>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;p>InternVL3.5 的架构与 InternVL3 的架构基本一致，包括一个 ViT, 一个 MLP 和一个 LLM, 模型的架构如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-internvl3.5/InternVL-3-5-architecture.png"
width="1295"
height="419"
loading="lazy"
alt="Architecture of InternVl3.5"
class="gallery-image"
data-flex-grow="309"
data-flex-basis="741px"
>&lt;/p>
&lt;p>模型配置如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-internvl3.5/InternVL-3-5-model-configuration.png"
width="1227"
height="484"
loading="lazy"
alt="Configuration of InternVL3.5"
class="gallery-image"
data-flex-grow="253"
data-flex-basis="608px"
>&lt;/p>
&lt;p>InternVl3.5 延续了 InternVL 系列的做法，即 20B 以下的模型使用 InternViT-300M, 20B 以上的模型使用 InternViT-6B 作为 Visual encoder. 大语言模型基于 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/notes-on-gpt-oss/" target="_blank" rel="noopener"
>gpt-oss&lt;/a>.&lt;/p>
&lt;p>在 InternVL3.5 的基础上，作者还构建了 InternVL3.5-Flash 模型，InternVL3.5-Flash 不同的地方在于，其在 MLP 之前加入了一个 Visual Resolution Router (ViR) 模块，来处理不同分辨率的图片。在 InternVL3.5 中，每个 image patch 由 1024 个视觉 token 来表示，然后通过 pixel shuffle 压缩至 256 个。 在 InternVL3.5-Flash 中，ViR 首先会判断每个 patch 的 semantic richness, 然后基于 semantic richness 来将对应的 token 路由到不同的 pixel shuffle 模块中，最高可以将视觉 token 压缩到 64 个，这样就可以大幅度提高模型的推理效率。&lt;/p>
&lt;h3 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h3>&lt;p>Pre-training 使用 next-token-prediction loss 来更新模型权重，给定多模态 sequence $x=(x_1,\dots,x_L)$, 损失函数定义为&lt;/p>
$$
\mathcal{L}(\theta) = -\sum_{x_i\text{ is text token}} \log p_{\theta}(x_i
\mid x_1,\dots,x_{i-1})
$$&lt;p>与 InternVL2.5 一样，作者还对不同长度的 sequence 进行了加权，避免模型产生 bias, 加权后的 loss 为&lt;/p>
$$
\mathcal{L}(\theta) = -\sum_{x_i\text{ is text token}}\frac{w_i}{\sum_j w_j} \log p_{\theta}(x_i
\mid x_1,\dots,x_{i-1}),\quad w_i = \frac{1}{N^{0.5}}
$$&lt;p>其中 $N$ 是训练样本中需要计算损失的 token 个数。&lt;/p>
&lt;p>训练数据蛀牙包含两部分：&lt;/p>
&lt;ol>
&lt;li>多模态数据，这部分数据基于 InternVL3&lt;/li>
&lt;li>纯文本数据，基于 InternLM 系列和开源的数据集&lt;/li>
&lt;/ol>
&lt;p>最终，预训练数据一共包含&lt;strong>116M&lt;/strong> 样本，&lt;strong>250B&lt;/strong> token. 纯文本数据和多模态数据的比例为 $1:2.5$. 模型上下文长度为 $32K$.&lt;/p>
&lt;h3 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h3>&lt;p>Post-training 包含三个阶段：&lt;/p>
&lt;ol>
&lt;li>SFT： 使用高质量对话数据提高模型表现&lt;/li>
&lt;li>Cascade RL: 提高模型的 reasoning 能力&lt;/li>
&lt;li>Visual Consistency Learning: 训练 ViR 模块，提高模型处理动态分辨率图片的能力&lt;/li>
&lt;/ol>
&lt;p>训练 pipeline 如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-internvl3.5/InternVL-3-5-post-training-pipeline.png"
width="1290"
height="335"
loading="lazy"
alt="Post-training pipeline of InternVL3.5"
class="gallery-image"
data-flex-grow="385"
data-flex-basis="924px"
>&lt;/p>
&lt;p>SFT 阶段的训练数据包括三个方面：&lt;/p>
&lt;ol>
&lt;li>InternVL3 的指令跟随数据&lt;/li>
&lt;li>多模态 reasoning 数据&lt;/li>
&lt;li>能力扩展数据，包括 GUI 交互，具身交互以及 SVG 理解与生成的数据&lt;/li>
&lt;/ol>
&lt;p>Cascade RL 阶段结合了 offline RL 和 online RL 的优点。基本思想是先使用 offline RL 来提高模型的能力，降低训练成本。然后再使用 online RL 进一步提高模型的 reasoning 表现。&lt;/p>
&lt;p>offline RL 阶段使用的是 InternVL-MPO 算法，其损失函数为&lt;/p>
$$
\mathcal{L}_{MPO} = w_p\mathcal{L}_p+w_q\mathcal{L}_q+w_g\mathcal{L}_g
$$&lt;p>其中，$\mathcal{L}_p$ 为 &lt;a class="link" href="https://maosong.website/p/notes-on-dpo/" target="_blank" rel="noopener"
>DPO&lt;/a> 的损失函数，$\mathcal{L}_q$ 为 Quality loss, $\mathcal{L}_g$ 为 SFT 使用的 next-token-prediction loss.&lt;/p>
&lt;p>online RL 阶段使用的是 &lt;a class="link" href="https://maosong.website/p/notes-on-gspo/" target="_blank" rel="noopener"
>GSPO&lt;/a> 算法，核心思想是在 sample 层面做归一化。这个阶段的目的是进一步提高模型的表现。&lt;/p>
&lt;p>Cascade RL 的优势在于：&lt;/p>
&lt;ol>
&lt;li>训练更稳定：offline RL 可以避免模型产生 reward hacking 现象，online RL 阶段可以进一步提高模型的表现&lt;/li>
&lt;li>训练效率更高：offline RL 可以有效提高采样效率&lt;/li>
&lt;li>表现更好：先 MPO 再 RL 可以达到更好的表现&lt;/li>
&lt;/ol>
&lt;p>Visual Consistency Learning (ViCO) 的目的是降低 InternVL3.5 的推理开销。这个阶段主要训练 ViR 模块&lt;/p>
&lt;p>ViCO 包括两个 stage:&lt;/p>
&lt;p>Stage 1: Consistency training&lt;/p>
&lt;p>这一阶段的目的是使得不同压缩率处理的视觉 token 对应的输出与 ground truth 尽可能一致，作者使用另外一个 InternVL3.5 模型作为 reference model, 然后损失函数为&lt;/p>
$$
\mathcal{L}_{ViCO} = \mathbb{E}_{\xi\sim\mathcal{R}}\left[\frac{1}{N}\sum_{i=1}^N\mathrm{KL}\left(\pi_{\mathrm{ref}}(y_i\mid y_{&lt;i}, I)\ \Vert\ \pi_{\theta}(y_i\mid y_{&lt;i}, I_{\xi})\right)\right]
$$&lt;p>$\xi\in\{1/4,1/16\}$ 为 compression rate, 训练是随机采样。$\xi=1/4$ 代表最终会有 256 个视觉 token, $\xi=1/16$ 代表最终会有 64 个 token.&lt;/p>
&lt;p>Stage 2: Router training&lt;/p>
&lt;p>这个阶段的目的是训练 ViR 来选取合适的精度/压缩率。ViR 此时作为一个 binary classifier 来进行训练，训练的损失函数为 cross-entropy loss. 模型其他部分参数冻结，仅训练 ViR 模块，首先，我们将计算压缩 16 倍后的损失与压缩 4 倍的损失的比例&lt;/p>
$$
r_i = \frac{\mathcal{L}_{ViCO}(y_i\mid I_{1/16})}{\mathcal{L}_{ViCO}(y_i\mid I_{1/4})}
$$&lt;p>该比例衡量了压缩 token 之后带来的性能下降程度，接下来，作者设定了一个阈值 $\tau$, 当性能下降超过阈值 $\tau$, 则认为应该使用高精度，也就是 $\xi=1/4$, 反之则说明不需要过度视觉 token, 可以使用低精度，也就是 $\xi=1/16$. 总结得到&lt;/p>
$$
y_i^{router} = \begin{cases}
0, &amp;r_i&lt;\tau\\
1, &amp;r_i>\tau
\end{cases}
$$&lt;p>训练时，作者基于 sliding window 中的 $r_i$ 来动态调整 $\tau$ 的值。&lt;/p>
&lt;p>post-training 的训练数据如下：&lt;/p>
&lt;ol>
&lt;li>SFT 训练数据包括&lt;strong>56M&lt;/strong> 样本，&lt;strong>130B&lt;/strong> token, 纯文本数据与多模态数据的比例为 $1:3.5$.&lt;/li>
&lt;li>Cascade RL 的训练数据主要基于 MMPR, 包含 200K 左右的样本，通过过滤最终得到&lt;strong>70K&lt;/strong>左右的样本。online RL 同样使用这批数据进行训练&lt;/li>
&lt;li>ViCO 的训练数据与 SFT 阶段的训练数据一致。主要是 OCR 以及 VQA 数据&lt;/li>
&lt;/ol>
&lt;h3 id="test-time-scaling">&lt;a href="#test-time-scaling" class="header-anchor">&lt;/a>Test-time Scaling
&lt;/h3>&lt;p>test time scaling 主要基于两个方面：&lt;/p>
&lt;ol>
&lt;li>deep thinking: 就是 reasoning mode&lt;/li>
&lt;li>parallel thinking: 基于 VisualPRM 进行多次采样，然后基于 BoN 策略得到最终的输出。&lt;/li>
&lt;/ol>
&lt;h3 id="infra">&lt;a href="#infra" class="header-anchor">&lt;/a>Infra
&lt;/h3>&lt;p>模型使用 XTuner 框架进行训练，使用了包括 FSDP, data packing, FP8, flashattention3 等策略。&lt;/p>
&lt;p>作者还提出了 DvD 的策略来提高推理效率，核心思想就是将 ViT-MLP 模块放在一个 server 上，然后将 LLM 放在另一个 server 上，这样就也可以提高整体的通信效率。框架示意图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-internvl3.5/InternVL-3-5-DvD.png"
width="1297"
height="580"
loading="lazy"
alt="DvD of InternVL3.5"
class="gallery-image"
data-flex-grow="223"
data-flex-basis="536px"
>&lt;/p>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>模型整体表现如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-internvl3.5/InternVL-3-5-model-performance.png"
width="964"
height="1113"
loading="lazy"
alt="Performance of InternVL3.5"
class="gallery-image"
data-flex-grow="86"
data-flex-basis="207px"
>&lt;/p>
&lt;h3 id="ablation-study">&lt;a href="#ablation-study" class="header-anchor">&lt;/a>Ablation Study
&lt;/h3>&lt;p>作者首先探究了 Cascade RL 对模型表现的影响&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-internvl3.5/InternVL-3-5-ablation-cascade-RL.png"
width="1299"
height="331"
loading="lazy"
alt="Ablation study on Cascade RL"
class="gallery-image"
data-flex-grow="392"
data-flex-basis="941px"
>&lt;/p>
&lt;p>实验结果显示，SFT, MPO 和 offline RL 每个阶段都可以提高模型的多模态 reasoning 表现。&lt;/p>
&lt;p>作者进一步探究了不同阶段的投入产出比，也就是训练时间与最终模型表现的变化情况，如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-internvl3.5/InternVL-3-5-training-efficiency-effectiveness.png"
width="1237"
height="213"
loading="lazy"
alt="Training efficiency and effectiveness"
class="gallery-image"
data-flex-grow="580"
data-flex-basis="1393px"
>&lt;/p>
&lt;p>实验结果显示，尽管 GSPO 的效果提升比较明显，但是训练所需要的时间比较长。如果先 MPO 再进行 GSPO 的话，我们可以在较短的时间里取得较好的表现。&lt;/p>
&lt;p>接下来，作者探究了 ViR 的有效性和效率。作者首先对比了 InternVL3.5 以及 InternVL3.5-flash 的表现。结果显示，大部分情况下，这两个模型的表现没有较大差距。说明 ViR 不会损害模型的性能。&lt;/p>
&lt;p>作者进一步探究了 ViR 和 DvD 对提升模型效率的影响，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-internvl3.5/InternVL-3-5-ablation-DvD.png"
width="877"
height="397"
loading="lazy"
alt="Ablation study on DvD and ViR"
class="gallery-image"
data-flex-grow="220"
data-flex-basis="530px"
>&lt;/p>
&lt;p>实验结果说明，对于高精度图片输入，DvD 和 ViR 均可以提高模型的推理效率。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>在本文中，作者提出了 InternVL3.5 系列多模态大模型，作者提出了 Cascade RL 框架，该框架结合了 offline RL 以及 online RL 来提高模型的 reasoning 表现以及训练效率。作者还提出了 ViR 以及 DvD 模块来提高模型处理高分辨率图片的效率。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2508.18265" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Ovis2.5 MLLM with stronger perception and reasoning capability</title><link>https://maosong.website/p/ovis2.5-mllm-with-stronger-perception-and-reasoning-capability/</link><pubDate>Sat, 30 Aug 2025 17:34:44 +0800</pubDate><guid>https://maosong.website/p/ovis2.5-mllm-with-stronger-perception-and-reasoning-capability/</guid><description>&lt;p>作者提出了 Ovis2.5, 一个基于 Ovis 改进的多模态大模型系列，包括 2B 和 9B 两个 size，Ovis2.5 主要强调了支持不同分辨率图片输入以及深度思考这两个 feature&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者首先回顾了 &lt;a class="link" href="https://maosong.website/p/ovis-discrete-visual-embedding/" target="_blank" rel="noopener"
>Ovis&lt;/a>, Ovis 主要是解决 text embedding 以及 visual embedding 对齐程度比较低的问题。&lt;/p>
&lt;p>接下来，作者介绍了以下 Ovis 的两个问题：&lt;/p>
&lt;ol>
&lt;li>只能支持固定大小的图片输入&lt;/li>
&lt;li>缺乏深度思考能力&lt;/li>
&lt;/ol>
&lt;p>为了解决这两个问题，作者提出了 Ovis 2.5, Ovis 主要做出了两点改进：&lt;/p>
&lt;ol>
&lt;li>使用了 NaViT 来处理不同分辨率图片的输入&lt;/li>
&lt;li>作者通过训练提高了模型的深度思考能力&lt;/li>
&lt;/ol>
&lt;p>最终 Ovis2.5 主要有以下 feature&lt;/p>
&lt;ol>
&lt;li>支持动态分辨率图片输入&lt;/li>
&lt;li>深度思考能力&lt;/li>
&lt;li>SOTA 的表现&lt;/li>
&lt;li>高效的训练方式&lt;/li>
&lt;/ol>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;p>Ovis2.5 的架构如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/ovis2.5-mllm-with-stronger-perception-and-reasoning-capability/Ovis-2-5-architecture.png"
width="855"
height="895"
loading="lazy"
alt="Architecture of Ovis2.5"
class="gallery-image"
data-flex-grow="95"
data-flex-basis="229px"
>&lt;/p>
&lt;p>Ovis 包括三个模块：&lt;/p>
&lt;ol>
&lt;li>visual tokenizer： ViT 架构，&lt;/li>
&lt;li>visual embedding table: 类似 LLM 中的 text embedding table, 见 &lt;a class="link" href="https://maosong.website/p/ovis-discrete-visual-embedding/" target="_blank" rel="noopener"
>Ovis&lt;/a>&lt;/li>
&lt;li>LLM: 基于 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a>&lt;/li>
&lt;/ol>
&lt;p>作者在架构上进行了如下改进：&lt;/p>
&lt;ol>
&lt;li>动态分辨率图片输入处理：作者使用了 NaViT 来支持动态分辨率图片输入&lt;/li>
&lt;li>LLM: 作者使用了 Qwen3 来进一步提高模型的表现&lt;/li>
&lt;/ol>
&lt;h3 id="training">&lt;a href="#training" class="header-anchor">&lt;/a>Training
&lt;/h3>&lt;p>模型训练包括 pre-training 和 post-training 两个大的 stage, 其中 pre-training 又包含 3 个小的 stage, post-training 包含 2 个 stage. 训练过程如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/ovis2.5-mllm-with-stronger-perception-and-reasoning-capability/Ovis-2-5-training-process.png"
width="930"
height="292"
loading="lazy"
alt="Training Process of Ovis2.5"
class="gallery-image"
data-flex-grow="318"
data-flex-basis="764px"
>&lt;/p>
&lt;p>pre-training 阶段的数据包括 COYO, Laion, Wukong, DataComp, SAM 等。作者介绍了几个部分的数据：&lt;/p>
&lt;ol>
&lt;li>OCR 数据，作者基于 MLLM 来标注数据和合成 QA&lt;/li>
&lt;li>Grounding 数据，作者使用了 RefCoCo 等数据集以及先进的 MLLM 来标注数据&lt;/li>
&lt;li>Reasoning 数据，作者收集了数据然后使用 MLLM 来合成 Reasoning path&lt;/li>
&lt;/ol>
&lt;p>训练时，&lt;/p>
&lt;ol>
&lt;li>VET pretraining: 训练 VET, 作者基于 SigLIP 来初始模型的参数，然后仅训练最后一层 ViT layer, visual head 以及 VET, 图片精度为 448-896. 作者采用了动态 position embedding&lt;/li>
&lt;li>Multimodal pretraining: 这阶段全量微调所有参数，主要目的是使用对话格式的数据。图片精度为 448-1792&lt;/li>
&lt;li>multimodal instruction tuning: 这阶段训练所有参数，主要提高模型跟随多模态指令的能力&lt;/li>
&lt;/ol>
&lt;p>post-training 包括 &lt;a class="link" href="https://maosong.website/p/notes-on-dpo/" target="_blank" rel="noopener"
>DPO&lt;/a> 和 GRPO 两个阶段。&lt;/p>
&lt;ol>
&lt;li>DPO: 训练所有参数，使用 pre-training checkpoint 来多次采样&lt;/li>
&lt;li>GRPO: 使用 RLVR 数据集进行训练&lt;/li>
&lt;/ol>
&lt;h2 id="infra">&lt;a href="#infra" class="header-anchor">&lt;/a>Infra
&lt;/h2>&lt;p>infra 方面，作者主要强调了 data packing 以及多种并行策略融合。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 Ovis2.5, 一个基于 Ovis 架构的多模态大模型，作者主要强调了模型的动态图片输入处理能力以及深度思考能力。&lt;/p>
&lt;p>作者提出了几个未来的方向：&lt;/p>
&lt;ol>
&lt;li>将输入图片精度提升到 4K&lt;/li>
&lt;li>处理长视频输入并进行 temporal reasoning&lt;/li>
&lt;li>在 Reasoning 过程中加入 tool-use.&lt;/li>
&lt;/ol>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2508.11737" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Ovis-discrete visual embedding</title><link>https://maosong.website/p/ovis-discrete-visual-embedding/</link><pubDate>Sat, 30 Aug 2025 17:32:22 +0800</pubDate><guid>https://maosong.website/p/ovis-discrete-visual-embedding/</guid><description>&lt;p>作者提出了 Ovis，一个离散化表示 visual encder 输出特征的方法，来更好对齐 LLM 的视觉输入和文本输入&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者分析了已有多模态大模型的架构，已有多模态大模型的输入对于文本来说是离散的 (text token), 对于图片来说是连续的 (visual embedding)。作者认为这种连续 - 离散的输入可能会影响模型最终的表现。&lt;/p>
&lt;p>为了解决这个问题，作者构建了一个 visual embedding table, 将 visual embedding 也转换成离散的 token 表示形式，进而统一 LLM 输出的粒度。&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;p>模型的架构如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/ovis-discrete-visual-embedding/Ovis-architecure.png"
width="1174"
height="964"
loading="lazy"
alt="Architecture of Ovis"
class="gallery-image"
data-flex-grow="121"
data-flex-basis="292px"
>&lt;/p>
&lt;p>我们首先会构建一个 visual vocabulary $\{e_k\}_{k=1}^K$, 其大小为 $K$, 然后对于 ViT 输出的 $n$ 个 visual feature $\{r_i\}_{i=1}^n$, 我们会加入一个 linear head 以及一个 softmax 来构建一个 vocabulary 上的分布，即&lt;/p>
$$
v_i = \mathrm{softmax}(Wr_i), W\in\mathbb{R}^{K\times d}
$$&lt;p>这里 $v_i\in\Delta^K$ 是 visual vocabulary 上的概率分布。最终，视觉模块的输入是 vocabulary 中 visual token 的一个加权求和&lt;/p>
$$
V_i = \sum_{k=1}^K v_{i,k}e_k\in\mathbb{R}^{d'}
$$&lt;p>训练分为三个阶段：&lt;/p>
&lt;ul>
&lt;li>Stage 1: 训练 $W$, visual encoder 最后一个 block 以及 visual vocabulary&lt;/li>
&lt;li>Stage 2: 训练 $W$, visual vocabulary 以及 visual encoder&lt;/li>
&lt;li>Stage 3: multimodal SFT, 提高模型的指令跟随能力，模型所有参数都参与训练&lt;/li>
&lt;/ul>
&lt;p>训练数据分布如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/ovis-discrete-visual-embedding/Ovis-training-dataset-distribution.png"
width="1023"
height="750"
loading="lazy"
alt="Statistics of Ovis training dataset"
class="gallery-image"
data-flex-grow="136"
data-flex-basis="327px"
>&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者提出了 Ovis，一个离散化表示 visual encder 输出特征的方法，来更好对齐 LLM 的视觉输入和文本输入&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2405.20797" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on DeepSeekMoE</title><link>https://maosong.website/p/notes-on-deepseekmoe/</link><pubDate>Fri, 29 Aug 2025 11:03:12 +0800</pubDate><guid>https://maosong.website/p/notes-on-deepseekmoe/</guid><description>&lt;p>DeepSeek 在 2024 年 1 月发布了 DeepSeekMoE, 一个解决 MoE 模型 specialization 不足以及 redundancy 问题的大模型系列。&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者首先回顾了已有 MoE 模型的不足，主要有两点：&lt;/p>
&lt;ol>
&lt;li>knowledge hybridity: 已有 MoE 模型的专家个数比较少，这就导致每个专家需要掌握更多样化的知识，提高了训练难度&lt;/li>
&lt;li>knowledge redundancy: 不同专家掌握的知识可能有重叠，从而导致了模型参数的 redundancy&lt;/li>
&lt;/ol>
&lt;p>为了解决这两个问题，作者提出了 DeepSeek-MoE, DeepSeek-MoE 主要做出了两点改变：&lt;/p>
&lt;ol>
&lt;li>Fine-Grained Expert Segmentation: 作者使用了更多的专家，来提高每个专家的 specialization, 降低训练成本&lt;/li>
&lt;li>Shared Expert Isolation: 作者在 Routing expert 的基础上，加入了 shared expert 来学习 common knowledge.&lt;/li>
&lt;/ol>
&lt;p>作者在 2B-A0.6B 的模型进行了实验，结果显示模型表现超过了 &lt;a class="link" href="https://maosong.website/p/gshard/" target="_blank" rel="noopener"
>GShard&lt;/a>, 说明了 DeepSeekMoE 模型架构的有效性。&lt;/p>
&lt;p>作者还进一步将模型 scale 到了 16B-A2.8B 和 145B-A22B, 实验结果均验证了模型的 scaling 效果。&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="preliminary">&lt;a href="#preliminary" class="header-anchor">&lt;/a>Preliminary
&lt;/h3>&lt;p>作者首先回顾了 transformer 架构，transformer 的第 $\ell$ 层 decode layer 可以表示为&lt;/p>
$$
\begin{aligned}
u_{1:T}^{\ell} &amp;= \mathrm{self\_attention}(h_{1:T}^{\ell-1}) + h_{1:T}^{\ell}\\
h_t^\ell &amp;= \mathrm{FFN}(u_t^{\ell}) + u_t^{\ell}
\end{aligned}
$$&lt;p>其中 $T$ 是 sequence length, $h_{1:T}^{\ell-1}$ 是第 $\ell-1$ 层 decoder layer 输出的 hidden states.&lt;/p>
&lt;p>接下来，我们可以将 dense 架构转换为 MoE 架构，MoE 架构与 dense 架构不同的地方在与 $\mathrm{FFN}$ 不再是 MLP, 而是一个 MoE 模块，其表达式如下&lt;/p>
$$
\begin{aligned}
h_t^\ell &amp;= \sum_{i=1}^N\left(g_{i,t}\mathrm{FFN}_i(u_t^{\ell})\right) + u_t^{\ell}\\
g_{i,t} &amp;= \begin{cases}
s_{i,t,}, &amp;s_{i,t}\in\mathrm{Topk}(\{s_{j,t}\mid 1\leq j \leq N\},K)\\
0, &amp;\text{otherwise}
\end{cases}\\
s_{i,t,} &amp;= \mathrm{softmax}_i({u_t^{\ell}}^Te_i^{\ell})
\end{aligned}
$$&lt;p>这里 $N$ 是专家的总个数， $K$ 是激活专家个数， $e_{i}^{\ell}$ 是 routing layer 的权重矩阵，$\mathrm{FFN}_i$ 是每个专家对应的 FFN.&lt;/p>
&lt;h3 id="deepseekmoe-architecutre">&lt;a href="#deepseekmoe-architecutre" class="header-anchor">&lt;/a>DeepSeekMoE Architecutre
&lt;/h3>&lt;p>DeepSeekMoE 架构如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseekmoe/DeepSeekMoE-architecture.png"
width="1200"
height="586"
loading="lazy"
alt="Architecture of DeepSeekMoE"
class="gallery-image"
data-flex-grow="204"
data-flex-basis="491px"
>&lt;/p>
&lt;p>相比于其他 MoE 架构，DeepSeekMoE 主要做了以下几点改变。&lt;/p>
&lt;h4 id="fine-grained-expert-segmentation">&lt;a href="#fine-grained-expert-segmentation" class="header-anchor">&lt;/a>Fine-Grained Expert Segmentation
&lt;/h4>&lt;p>作者首先解决了每个专家学习内容过多的问题。作者的做法就是将每个 expert FFN 分割为 $m$ 个更小的专家，具体做法就是将 FFN intermediate hidden size 降低为原来的 $1/m$. 这样的话就可以在不增加模型参数量的情况下提高模型的表现。修正后的 MoE 模块为&lt;/p>
$$
\begin{aligned}
h_t^\ell &amp;= \sum_{i=1}^{mN}\left(g_{i,t}\mathrm{FFN}_i(u_t^{\ell})\right) + u_t^{\ell}\\
g_{i,t} &amp;= \begin{cases}
s_{i,t,}, &amp;s_{i,t}\in\mathrm{Topk}(\{s_{j,t}\mid 1\leq j \leq mN\},mK)\\
0, &amp;\text{otherwise}
\end{cases}\\
s_{i,t,} &amp;= \mathrm{softmax}_i({u_t^{\ell}}^Te_i^{\ell})
\end{aligned}
$$&lt;p>可以看到，现在我们一共有 $mN$ 个专家，激活专家个数为 $mK$ 个。作者认为，通过提高专家的粒度，我们可以有效增加专家组合的可能性，这就提高了最终的组合多样性。&lt;/p>
&lt;h4 id="shared-expert-isolation">&lt;a href="#shared-expert-isolation" class="header-anchor">&lt;/a>Shared Expert Isolation
&lt;/h4>&lt;p>接下来，作者介绍了解决不同专家学习到重复知识的问题，作者的做法是在 routing Expert 的基础上加入 Shared expert. 也就是说，有固定几个专家始终都会被激活，这部分专家复杂学习通用知识，从而减少知识冗余。增加 shared expert 之后的 MoE 模块为&lt;/p>
$$
\begin{aligned}
h_t^\ell &amp;= \sum_{i=1}^{K_s}\mathrm{FFN}_i(u_t^{\ell})+\sum_{i=K_s+1}^{mN}\left(g_{i,t}\mathrm{FFN}_i(u_t^{\ell})\right) + u_t^{\ell}\\
g_{i,t} &amp;= \begin{cases}
s_{i,t,}, &amp;s_{i,t}\in\mathrm{Topk}(\{s_{j,t}\mid K_s+1\leq j \leq mN\},mK-K_s)\\
0, &amp;\text{otherwise}
\end{cases}\\
s_{i,t,} &amp;= \mathrm{softmax}_i({u_t^{\ell}}^Te_i^{\ell})
\end{aligned}
$$&lt;p>此时，模型中一共包含 $K_s$ 个共享专家，$mN-K_s$ 个 routing expert, 其中激活专家个数为 $mK-K_s$.&lt;/p>
&lt;h4 id="load-balancing-loss">&lt;a href="#load-balancing-loss" class="header-anchor">&lt;/a>Load Balancing Loss
&lt;/h4>&lt;p>接下来，作者解决了训练时的 load imbalance 问题，作者提出了两个 loss 来分别解决不同层面的 load imbalance 问题。&lt;/p>
&lt;p>首先，在 expert 层面，作者使用了如下的 load balancing loss:&lt;/p>
$$
\mathcal{L} = \alpha_1\sum_{i=1}^{N'}f_iP_i
$$&lt;p>其中 $\alpha_1$ 是超参数，&lt;/p>
$$
f_i = \frac{N'}{K'T}\sum_{i=1}^{N'}\mathbb{1}(\text{Token }i \text{ selects Expert }i),\quad P_i = \frac{1}{T}\sum_{t=1}^Ts_{i,t}
$$&lt;p>分别为以及分配给第 $i$ 个专家的 token 比例以及概率之和。$N'=mN-K_s$, $K'=mK-K_s$. $\mathbb{1}(\cdot)$ 是 indicator function.&lt;/p>
&lt;p>其次，在 device 层面，作者也是用了 load balancing loss 来减少不同设备之间不必要的通信。作者将 routed experts 分为 $D$ 个 group $\mathcal{E}_1,\dots,\mathcal{E}_D$, 然后每个设备部署一个 group, group level 的 load balancing loss 定义如下：&lt;/p>
$$
\mathcal{L} = \alpha_2\sum_{i=1}^D f_i' P_i'
$$&lt;p>其中 $\alpha_2$ 是超参数，&lt;/p>
$$
f_i' = \frac{1}{|\mathcal{E}_i|}\sum_{j\in\mathcal{E}_i}f_i,\quad P_i' = \sum_{j\in\mathcal{E}_i}P_i
$$&lt;p>实际中，作者使用了一个较小的 $\alpha_1$ 来避免 routing collapse, 使用了一个较大的 $\alpha_2$ 来提高 Device 层面的负载均衡。&lt;/p>
&lt;h3 id="training">&lt;a href="#training" class="header-anchor">&lt;/a>Training
&lt;/h3>&lt;p>作者使用了中英文数据进行训练，tokenizer 基于 BPE 算法，vocab size 为 8K. 模型训练基于 HAI-LLM.训练时使用了 TP, DP, PP, EP 等并行策略。&lt;/p>
&lt;p>2B, 16B, 145B 模型的参数如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>2B&lt;/th>
&lt;th>16B&lt;/th>
&lt;th>145B&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>total params&lt;/td>
&lt;td>2B&lt;/td>
&lt;td>16.4B&lt;/td>
&lt;td>144.6B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>activated params&lt;/td>
&lt;td>0.3B&lt;/td>
&lt;td>2.8B&lt;/td>
&lt;td>22.2B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>hidden size&lt;/td>
&lt;td>1280&lt;/td>
&lt;td>2048&lt;/td>
&lt;td>4096&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>layers&lt;/td>
&lt;td>9&lt;/td>
&lt;td>28&lt;/td>
&lt;td>62&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>attention heads&lt;/td>
&lt;td>10&lt;/td>
&lt;td>16&lt;/td>
&lt;td>32&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>head dimension&lt;/td>
&lt;td>128&lt;/td>
&lt;td>128&lt;/td>
&lt;td>128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>routed experts&lt;/td>
&lt;td>63&lt;/td>
&lt;td>64&lt;/td>
&lt;td>128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>activated experts&lt;/td>
&lt;td>7&lt;/td>
&lt;td>6&lt;/td>
&lt;td>12&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>shared experts&lt;/td>
&lt;td>1&lt;/td>
&lt;td>2&lt;/td>
&lt;td>4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>training tokens&lt;/td>
&lt;td>100B&lt;/td>
&lt;td>2T&lt;/td>
&lt;td>245B&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;h3 id="alignment">&lt;a href="#alignment" class="header-anchor">&lt;/a>Alignment
&lt;/h3>&lt;p>作者针对 DeepseekMoE 16B 进行了微调，微调使用了 &lt;strong>1.4M&lt;/strong> 的训练样本，覆盖了 math, code, QA, reasoning 等任务。&lt;/p>
&lt;h3 id="ablation-study">&lt;a href="#ablation-study" class="header-anchor">&lt;/a>Ablation Study
&lt;/h3>&lt;p>作者在 2B 的模型上进行了 ablation study.&lt;/p>
&lt;p>首先，作者探究了细粒度专家和共享专家的有效性，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseekmoe/DeepSeekMoE-ablation-experts.png"
width="1197"
height="552"
loading="lazy"
alt="Ablation on experts"
class="gallery-image"
data-flex-grow="216"
data-flex-basis="520px"
>&lt;/p>
&lt;p>实验结果显示，与 &lt;a class="link" href="https://maosong.website/p/gshard/" target="_blank" rel="noopener"
>GShard&lt;/a> 相比，&lt;strong>使用共享专家可以有效提高模型的表现&lt;/strong>。并且，&lt;strong>使用更细粒度的专家也可以进一步提高模型的表现&lt;/strong>&lt;/p>
&lt;p>作者还探究了共享专家与路由专家的比例，作者分别使用不同的比例进行实验，结果发现共享专家：路由专家个数为 1：3 的时候模型效果最好。&lt;/p>
&lt;p>作者还探究了模型的泛化性，作者 mask 掉一部分概率最高的 routing expert, 然后从剩下的专家里进行 topK 的挑选，然后作者比较模型和 GShard 的表现，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseekmoe/DeepSeekMoE-ablation-expert-specialization.png"
width="737"
height="526"
loading="lazy"
alt="Ablation study on expert specialization"
class="gallery-image"
data-flex-grow="140"
data-flex-basis="336px"
>&lt;/p>
&lt;p>实验结果显示，&lt;strong>DeepSeekMoE 对于 mask 操作更敏感，这说明 DeepSeekMoE 模型中专家的 specialization 更强。&lt;/strong>&lt;/p>
&lt;p>作者还探究了 mask 掉共享专家对模型表现的影响，结果显示&lt;strong>共享专家与路由专家之间的 overlap 很小，去掉共享专家之后，模型表现会变差。&lt;/strong>&lt;/p>
&lt;p>作者进一步分析了共享专家与路由专家组合的有效性。作者探究 DeepSeekMoE 是否可以使用更少的路由专家来获取知识。作者通过使用不同的 activated routed experts 来进行实验，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseekmoe/DeepSeekMoE-ablation-number-activated-experts.png"
width="730"
height="524"
loading="lazy"
alt="Ablation study on activated routed experts"
class="gallery-image"
data-flex-grow="139"
data-flex-basis="334px"
>&lt;/p>
&lt;p>实验结果显示，DeepSeekMoE 仅需激活 4 个路由专家，就可以达到与 GShard 相同的表现。&lt;strong>这说明了 DeepSeekMoE 模型中每个专家可以学习到更准确的知识。&lt;/strong>&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 DeepSeekMoE 架构，来提高 MoE 模型中专家的利用效率。为了达到这一点，作者首先使用了更细粒度的专家，降低每个专家的学习成本，然后，作者使用了共享专家，来降低不同专家之间的知识冗余。作者先在 2B 的模型上验证了方法的有效性，然后作者将模型 scale 到了 16B 和 125B。结果显示模型的效果均超过了以前的工作。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2401.06066" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on DeepSeek-LLM</title><link>https://maosong.website/p/notes-on-deepseek-llm/</link><pubDate>Tue, 26 Aug 2025 10:53:10 +0800</pubDate><guid>https://maosong.website/p/notes-on-deepseek-llm/</guid><description>&lt;p>DeepSeek 在 2024 年 1 月 5 日发布了 DeepSeek LLM, 包括 7B 和 67B 两个 size, 作者主要强调了对于 scaling law 的探究&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>已有的 scaling law 如 &lt;a class="link" href="https://maosong.website/p/kaplan-scaling-law/" target="_blank" rel="noopener"
>Kaplan&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/chinchilla-scaling-law/" target="_blank" rel="noopener"
>Chinchilla&lt;/a> 介绍了 model size, dataset size, compute budget 与模型表现之间的关系。在本文中，作者进一步探究了 learning rate 和 batch size 等超参数与模型表现之间的关系。基于发现的 scaling law, 作者为不同大小的模型设置了最优的超参数。并且，作者还发现不同数据集与模型表现之间的关系。&lt;/p>
&lt;p>最终，基于这些实验结果，作者提出了 DeepSeek LLM, 模型使用 &lt;strong>2T token&lt;/strong> 进行预训练，使用 1M samples 进行后训练，后训练包括 SFT 以及 &lt;a class="link" href="https://maosong.website/p/notes-on-dpo/" target="_blank" rel="noopener"
>DPO&lt;/a>.&lt;/p>
&lt;h2 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h2>&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;p>DeepSeek-LLM 的架构与 LLaMA 基本相同，作者在 67B 的模型上使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-gqa/" target="_blank" rel="noopener"
>GQA&lt;/a> 来提高 inference 效率。最终模型的配置如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Params&lt;/th>
&lt;th>7B&lt;/th>
&lt;th>67B&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$n_{\text{layers}}$&lt;/td>
&lt;td>$30$&lt;/td>
&lt;td>$95$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$d_{\text{model}}$&lt;/td>
&lt;td>$4096$&lt;/td>
&lt;td>$8192$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$n_{\text{heads}}$&lt;/td>
&lt;td>$32$&lt;/td>
&lt;td>$64$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$n_{\text{kv\_heads}}$&lt;/td>
&lt;td>$32$&lt;/td>
&lt;td>$8$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Context Length&lt;/td>
&lt;td>$4096$&lt;/td>
&lt;td>$4096$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Sequence Batch Size&lt;/td>
&lt;td>$2304$&lt;/td>
&lt;td>$4608$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Learning Rate&lt;/td>
&lt;td>$4.2e-4$&lt;/td>
&lt;td>$3.2e-4$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Tokens&lt;/td>
&lt;td>2T&lt;/td>
&lt;td>2T&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="data">&lt;a href="#data" class="header-anchor">&lt;/a>Data
&lt;/h3>&lt;p>作者主要从 Common Crawl 构建预训练数据，数据处理过程包括：去重，过滤以及 remixing 三个步骤。&lt;/p>
&lt;p>对于 tokenizer, 作者使用了 &lt;a class="link" href="https://maosong.website/p/hands-on-llm1-tokenizer/" target="_blank" rel="noopener"
>BBPE&lt;/a> 算法，tokenizer 的大小设置为 100,000, 最终的 tokenizer 大小为 102400.&lt;/p>
&lt;h3 id="hyper-parameters">&lt;a href="#hyper-parameters" class="header-anchor">&lt;/a>Hyper Parameters
&lt;/h3>&lt;p>作者主要对比了一下不同 learning rate schedule 的表现：&lt;/p>
&lt;ol>
&lt;li>cosine learning schedule&lt;/li>
&lt;li>multi-step learning rate schedule: 包含三个 Stage, 第一个 stage 保持最大学习率，第二个 stage 将学习率降低为最大学习率的 $31.6\%$, 第三个 stage 降低为最大学习率的 $10\%$.&lt;/li>
&lt;/ol>
&lt;p>对比的实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-llm/DeepSeek-LLM-learning-rate-ablation.png"
width="1077"
height="381"
loading="lazy"
alt="Comparison of different learning schedulers"
class="gallery-image"
data-flex-grow="282"
data-flex-basis="678px"
>&lt;/p>
&lt;p>实验结果显示，multi-step learning rate scheduler 的表现与 cosine learning rate 表现差不多。并且，multi-step learning rate scheduler 对于 continue pretraining 支持更好。因此在本文中作者使用了 multi-step learning rate scheduler.&lt;/p>
&lt;h3 id="infra">&lt;a href="#infra" class="header-anchor">&lt;/a>Infra
&lt;/h3>&lt;p>作者使用了数据并行，张量并行，序列并行以及 1F1B pipeline 并行。作者还使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-flashattention/" target="_blank" rel="noopener"
>flash attention&lt;/a> 来提高硬件利用率。&lt;/p>
&lt;h2 id="scaling-law">&lt;a href="#scaling-law" class="header-anchor">&lt;/a>Scaling Law
&lt;/h2>&lt;p>本节中，作者分析了 scaling law, 主要有以下三点：&lt;/p>
&lt;ol>
&lt;li>构建了针对 learning rate 和 batch size 的 scaling law&lt;/li>
&lt;li>作者使用 non-embedding FLOPs/token $M$ 来表示 model scale&lt;/li>
&lt;li>预训练数据的质量对最后中的 scaling 影响很大&lt;/li>
&lt;/ol>
&lt;p>作者首先构建了针对 batch size 和 learning rate 的 scaling law, 结果显示最优的 learning rate 和 batch size 范围都比较广，这个结论与 &lt;a class="link" href="https://maosong.website/p/kaplan-scaling-law/" target="_blank" rel="noopener"
>Kaplan&lt;/a> 一致。&lt;/p>
&lt;p>接下来，作者构建了 batch size $B$, learning rate $\eta$ 与 compute budget $C 之间的关系，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-llm/DeepSeek-LLM-scaling-curve-bsz-lr.png"
width="1066"
height="379"
loading="lazy"
alt="Scaling curves of batch size and learning rate"
class="gallery-image"
data-flex-grow="281"
data-flex-basis="675px"
>&lt;/p>
&lt;p>拟合得到的曲线为&lt;/p>
$$
\begin{aligned}
\eta_{opt} &amp;= 0.3118* C^{-0.1250}\\
B_{opt} &amp;= 0.2920 * C^{0.3271}
\end{aligned}
$$&lt;p>可以看到，随着 compute budget 增加，$B_{opt}$ 也逐渐增加，而 $\eta_{opt}$ 逐渐减小。并且，最优参数的范围都比较广。&lt;/p>
&lt;p>接下来，作者进一步探究了 batch size 与 generalization error $L$ 之间的关系。作者希望找到 model scale $N$, data scale $D$ 与 compute budget $C$ 之间的关系，即&lt;/p>
$$
N_{opt} \varpropto C^a,D_{opt} \varpropto C^b
$$&lt;p>compute budget 与 model scale, data scale 之间的关系可以近似表示为 $C=6ND$, 这个公式的推导见 &lt;a class="link" href="https://maosong.website/p/llm-flops-computation/" target="_blank" rel="noopener"
>LLM FLOPs computation&lt;/a>。我们用 $N_1,N_2$ 分别表示模型的 non-embedding parameter 以及 complete parameters, 则我们可以用 $6N_1$ 或者 $6N_2$ 来近似 model scale, 但是 $6N_1$ 和 $6N_2$ 均没有考虑 attention 的计算开销，因此这两种近似的误差都比较大。&lt;/p>
&lt;p>为了解决这个问题，作者提出了一个新的 model scale 表示形式，即 non-embedding FLOPS/token $M$, 其中 $M$ 包含 attention 的计算开销但是不包含 vocabulary computation. 基于这种表示，compute budget 可以近似表示为 $C=MD$. $M$ 与 $6N_1,6N_2$ 的区别表示如下所示&lt;/p>
$$
\begin{aligned}
6N_1 &amp;= 72nd^2\\
6N_2 &amp;= 72nd^2 + 6Vd\\
M &amp;= 72nd^2+12ndl
\end{aligned}
$$&lt;p>其中, $d$ 是 hidden size, $n$ 是 layers 个数, $V$ 是 vocabulary size, $l$ 是 sequence length. 作者在不同 scale 的模型上比较了三种表示方式，结果发现 $6N_1$ 和 $6N_2$ 要么低估，要么高估了模型的参数量。&lt;/p>
&lt;p>基于 model scale 的表示方式，作者构建了如下的优化问题&lt;/p>
$$
M_{opt}(C), D_{opt}(C) = {\arg\min}_{M,D\ s.t.\ C=MD} L(N,D)
$$&lt;p>作者使用了 &lt;a class="link" href="https://maosong.website/p/chinchilla-scaling-law/" target="_blank" rel="noopener"
>Chinchilla&lt;/a> 提出来的 IsoFLOP 曲线进行拟合，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-llm/DeepSeek-LLM-IsoFLOP-curve.png"
width="1070"
height="398"
loading="lazy"
alt="IsoFLOP curve and optimal model/data allocation"
class="gallery-image"
data-flex-grow="268"
data-flex-basis="645px"
>&lt;/p>
&lt;p>拟合的曲线为&lt;/p>
$$
M_{opt}(C) = 0.1715*C^{0.5243}, D_{opt}(C) = 5.8316*C^{0.4757}
$$&lt;p>作者还进一步拟合了 compute budget 与 optimal generalization error 之间的关系，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-llm/DeepSeek-LLM-performance-scaling-curve.png"
width="662"
height="414"
loading="lazy"
alt="Performance Scaling curve"
class="gallery-image"
data-flex-grow="159"
data-flex-basis="383px"
>&lt;/p>
&lt;p>实验结果显示，作者提出的 scaling law 可以很好预测模型的表现。&lt;/p>
&lt;p>最后，作者探究了以下不同数据集的 scaling law, 作者使用 early in-house data, current in-house data 以及 OpenWebText2 来将进行实验，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-llm/DeepSeek-LLM-dataset-ablation.png"
width="746"
height="285"
loading="lazy"
alt="Comparison of different dataset scaling"
class="gallery-image"
data-flex-grow="261"
data-flex-basis="628px"
>&lt;/p>
&lt;p>结果显示，scaling law 与数据质量高度相关。当数据质量提升时，model scaling exponent $a$ 逐步提升，data scaling exponent $b$ 逐步下降，说明 compute budget 更多由模型参数量决定。因此，作者认为提升 compute budget 之后，我们应该优先提高模型的 model size.&lt;/p>
&lt;h2 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h2>&lt;p>作者构建了 1.5M 的中英文指令数据。其中安全性的数据有 300K, 有帮助性的数据有 1.2M, 其中包括 $31.2\%$ 的通用数据，$46.6\%$ 的数学相关数据，$22.2\%$ 的代码数据。&lt;/p>
&lt;p>post-training 包含两个阶段：&lt;/p>
&lt;ol>
&lt;li>SFT:7B 的模型训练了 4 个 epoch, 67B 的模型训练了 2 个 epoch, 作者发信进一步训练 67B 的模型会导致过拟合。作者发现，模型在训练过程中会出现重复输出的情况，特别是数学 SFT 数据，为了解决这个问题，作者使用了一个两阶段的 SFT 以及 DPO.&lt;/li>
&lt;li>&lt;a class="link" href="https://maosong.website/p/notes-on-dpo/" target="_blank" rel="noopener"
>DPO&lt;/a>: 提高模型的能力，作者发现 DPO 可以提高模型 open-ended generation skill.&lt;/li>
&lt;/ol>
&lt;h2 id="evaluation">&lt;a href="#evaluation" class="header-anchor">&lt;/a>Evaluation
&lt;/h2>&lt;p>我们主要关注一下消融实验。&lt;/p>
&lt;p>首先作者探究了分阶段 SFT 对模型表现的影响。作者发现，小模型在 math 和 code 数据集上需要训练更长时间，但是这也损害了模型的对话能力。为了解决这个问题，作者使用两阶段的训练模式，第一个阶段使用所有的数据进行训练，第二个阶段仅使用对话数据进行训练，实验结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>HumanEval&lt;/th>
&lt;th>GSM8K&lt;/th>
&lt;th>Repetition&lt;/th>
&lt;th>IFEval&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>DeepSeek LLM 7B Chat Stage1&lt;/td>
&lt;td>48.2&lt;/td>
&lt;td>63.9&lt;/td>
&lt;td>0.020&lt;/td>
&lt;td>38.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DeepSeek LLM 7B Chat Stage2&lt;/td>
&lt;td>48.2&lt;/td>
&lt;td>63.0&lt;/td>
&lt;td>0.014&lt;/td>
&lt;td>41.2&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，经过第二阶段训练之后，模型的表现有所提升&lt;/p>
&lt;p>接下来，作者探究了 Multi-choice question 对模型表现的影响，MCQ 要求模型不仅需要有相关的知识，还要理解每个选项的含义。作者使用 20M 中文 MCQ 来进行消融实验，结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>MMLU&lt;/th>
&lt;th>CEval&lt;/th>
&lt;th>CMMLU&lt;/th>
&lt;th>TriviaQA&lt;/th>
&lt;th>ChineseQA&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>DeepSeek LLM 7B Chat&lt;/td>
&lt;td>49.4&lt;/td>
&lt;td>47.0&lt;/td>
&lt;td>49.7&lt;/td>
&lt;td>57.9&lt;/td>
&lt;td>75.0&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DeepSeek LLM 7B Chat + MC&lt;/td>
&lt;td>60.9&lt;/td>
&lt;td>71.3&lt;/td>
&lt;td>73.8&lt;/td>
&lt;td>57.9&lt;/td>
&lt;td>74.4&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>实验结果显示，MCQ 确实可以提高模型在上述几个 benchmark 上的表现，但是其泛化性会下降。因此，作者在 pre-training 和 fine-tuning 阶段并没有使用 MCQ 数据进行训练。&lt;/p>
&lt;p>作者还探究了在 pre-training 阶段加入 instruction data, 来提高 base model 在下游 benchmark 上的表现。结果发现，base model 的表现提升优先。作者认为，尽管 instruction data 可以提高 base model 表现，但是如果 Instruction data 数量过少，则模型表现不太可能学习到有用的知识。因此，作者的做法是不在 pretraining 阶段加入 Instruction data.&lt;/p>
&lt;p>最后，作者探究了 system prompt 对模型表现的影响。受 LLaMA 2 启发，作者也尝试在输入中加入 system prompt. 实验结果如下所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>MT Bench&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>DeepSeek LLM 7B Chat&lt;/td>
&lt;td>7.15&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DeepSeek LLM 7B Chat + System Prompt&lt;/td>
&lt;td>7.11&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DeepSeek LLM 67B Chat&lt;/td>
&lt;td>8.35&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DeepSeek LLM 67B Chat + System Prompt&lt;/td>
&lt;td>8.58&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，7B 的模型加入 system prompt 之后，模型表现有所下降；67B 的模型加入 system prompt 之后，模型表现有所提升。作者认为，大模型更容易理解 system prompt 的意图，而小模型的指令跟随能力则较差，因此 system prompt 反而会影响模型表现。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 DeepSeek LLM 系列大语言模型，作者详细介绍了超参数的选择以及 scaling law 等。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2401.02954" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on MFA</title><link>https://maosong.website/p/notes-on-mfa/</link><pubDate>Sat, 23 Aug 2025 16:04:34 +0800</pubDate><guid>https://maosong.website/p/notes-on-mfa/</guid><description>&lt;p>阶跃星辰等提出了 Multi-matrix Factorization Attention (MFA), 一个新型注意力机制，用于在 KV cache 限制下最大化模型的表现。&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>multi-head attention (MHA) 的问题在于，其 KV cache 的内存占用（memory footprint）随 sequence length 以及 batch size 线性增长，从而成为了 LLM 在 decoding 阶段的瓶颈。&lt;/p>
&lt;p>为了解决 MHA 的内存占用过高问题，已有的工作如 &lt;a class="link" href="https://maosong.website/p/notes-on-mqa/" target="_blank" rel="noopener"
>MQA&lt;/a>, &lt;a class="link" href="https://maosong.website/p/notes-on-gqa/" target="_blank" rel="noopener"
>GQA&lt;/a> 等通过共享 key, value projection 来降低 KV cache size. 而 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v3/" target="_blank" rel="noopener"
>DeepSeek-V3&lt;/a> 提出的 MLA 则是通过对 key, value projection 进行 low-rank compression, 然后只存储 latents 的方法来降低 KV cache size.&lt;/p>
&lt;p>但是，已有的这些方法的问题在于，当我们设置 KV cache budget 之后，它们的表现就比标准的 MHA 要差。&lt;/p>
&lt;p>基于以上这些发现，作者首先分析了已有 attention 机制的 modeling capacity, 然后使用一个统一的框架来表示这些 attention 机制。作者发现，attention heads 的个数以及 dimension 对模型表现有较大影响。&lt;/p>
&lt;p>基于这个发现，作者提出了 &lt;strong>Multi-matrix Factorization Attention (MFA)&lt;/strong>, 以及其变体 &lt;strong>MFA-Key-Reuse (MFA-KR)&lt;/strong>. MFA 的主要目的是在有限的 KV cache size 下提高模型的表现。&lt;/p>
&lt;h2 id="background">&lt;a href="#background" class="header-anchor">&lt;/a>Background
&lt;/h2>&lt;p>作者首先介绍了 GMHA 的概念，GMHA 由三部分组成：&lt;/p>
&lt;ol>
&lt;li>QK circuit: 决定了信息之间如何交互&lt;/li>
&lt;li>valueoutput (VO) circuits：决定了信息如何传递&lt;/li>
&lt;li>per-head softmax attention.&lt;/li>
&lt;/ol>
&lt;p>接下来，作者介绍了 Fully Parameterized Bilinear Attention (FPBA), FPBA 的定义如下：&lt;/p>
$$
O = \sum_{c=1}^d\left(\sum_{j=1}^N\phi\left(\frac{xW_cx_j}{H}\right)x_jU_c\right)
$$&lt;p>其中 $\phi$ 是 softmax 函数，$d$ 是模型的 hidden dimension, $N$ 是 sequence length, $W_c,U_c\in\mathbb{R}^{d\times d}$ 每个 channel 上的参数矩阵&lt;/p>
&lt;ol>
&lt;li>每个 channel 都有各自的参数 $W_c, U_c$ 来获取 $x_i$ 与 $x_j$ 之间的信息&lt;/li>
&lt;li>提高泛化性，所有 channel 的 $U_c$ 组合起来可以遍历 $d$ 维空间中的任意一个 permutation, 这样就避免来的信息损失&lt;/li>
&lt;li>利用率高，FPBA 获取了 $x_i$ 与 $x_j$ 之间 $d$ 维空间可能的表示&lt;/li>
&lt;/ol>
&lt;p>基于以上这三个特点，作者认为 FPBA 是 GMHA 框架的一个 capacity upper bound. 此时每个 token 的 KV cache 占用为 $2d^2$ (key and value).&lt;/p>
&lt;p>然后，作者分析了 MHA 及其变体与 GMHA 的关系，MHA 可以写作如下形式&lt;/p>
$$
\begin{aligned}
O &amp;= \sum_{c=1}^h\left(\sum_{j=1}^N\phi\left(\frac{xQ_c(x_jK_c)^T}{\sqrt{d}}\right)x_jV_c\right)O_c^T\\
&amp;= \sum_{c=1}^h\left(\sum_{j=1}^N\phi\left(\frac{x(Q_cK_c^T)x_j^T}{\sqrt{d}}\right)x_jV_cO_c^T\right)
\end{aligned}
$$&lt;p>其中 $Q_c,K_c,V_c\in\mathbb{R}^{d\times h_d}$, $O_c\in\mathbb{R}^{d\times h_d}$ 分别是 query, key, value, output projection layer 对应的权重矩阵，$n$ 是 attention head 的个数，令 $h_d$ 为每个 attention 的 head dimension，则我们有 $nh_d=d$.&lt;/p>
&lt;p>可以看到，MHA 实际上是一个特殊的 FPBA, 其中，$W_c$ 和 $U_c$ 分别由秩为 $h_d$ 的低秩分解 $Q_cK_c^T$ 以及 $V_cO_c^T$ 近似。此时每个 token 的 KV cache 占用为 $2d$ (key and value).&lt;/p>
&lt;p>MQA 可以看作是 GQA 的一个特殊情况。对于 GQA 来说，我们有一个 group size $g\in[1, h]$, 当 $g=1$ 时，GQA 就是 MHA. 当 $g=h$ 时，GQA 就是 MQA, 通常 $g$ 满足 $h\ \%\ g=0$. GQA 的表达式与 MHA 基本相同，只是多个 head 会共享一个 $K_c$ 以及 $V_c$. 此时，每个 token 的 KV cache 占用为 $2gh_d$. 对于 MQA，其每个 token 的 KV cache 占用为 $2h_d$.&lt;/p>
&lt;p>对于 MLA, 其表达式如下所示&lt;/p>
$$
\begin{aligned}
O &amp;= \sum_{c=1}^m\left(\sum_{j=1}^N\phi\left(\frac{xS_QQ_c(x_jS_KK_c)^T}{\sqrt{d}}\right)x_jS_VV_c\right)O_c^T\\
&amp;= \sum_{c=1}^m\left(\sum_{j=1}^N\phi\left(\frac{x(S_QQ_cK_c^TS_K^T)x_j^T}{\sqrt{d}}\right)x_jS_VV_cO_c^T\right)
\end{aligned}
$$&lt;p>其中，$S_Q,S_K,S_V\in\mathbb{R}^{d\times C}$ 在所有的 heads 中是共享的，$Q_c,K_c,V_c\in\mathbb{R}^{C\times h_d}$ 是每个 head 的 query, key, value projection layer 的参数， 是 latent factorization 的维度。与 FPBA 相比，我们可以看到，MLA 实际上是在 $d/m$ 个 head 上共享了参数，其中，$W_c$ 和 $U_c$ 分别由秩为 的低秩分解 $S_QQ_cK_c^TS_K^T$ 以及 $S_VV_cO_c^T$ 近似。尽管模型中 $C>h_d$, 但是最终的 rank 仍然是 $h_d$, 因此模型的表现也就受到了限制。&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;p>对已有的 attention 分析之后，作者认为，要提高模型的表现，attention 需要做到亮点：&lt;/p>
&lt;ol>
&lt;li>最小化 KV cache 占用和参数量&lt;/li>
&lt;li>attention 的 capacity 尽可能接近 FPBA&lt;/li>
&lt;/ol>
&lt;p>基于这两个原则，作者提出了 MFA, MFA 主要依赖三个策略：&lt;/p>
&lt;ol>
&lt;li>提升 attention heads 的 head dimension, 通过提高 head dimension, 我们可以有效提高 attention head 的表达能力&lt;/li>
&lt;li>使用矩阵分解来降低参数量&lt;/li>
&lt;li>使用单一的 KV head 来降低 KV cache 内存占用&lt;/li>
&lt;/ol>
&lt;p>最终，MFA 的表达式如下所示&lt;/p>
$$
\begin{aligned}
O &amp;= \sum_{c=1}^m\left(\sum_{j=1}^N\phi\left(\frac{xS_QQ_c(x_jS_K)^T}{\sqrt{d}}\right)x_jS_V\right)O_c^T\\
&amp;= \sum_{c=1}^m\left(\sum_{j=1}^N\left(\frac{x(S_QQ_cS_K^T)x_j^T}{\sqrt{d}}\right)x_jS_VO_c^T\right)
\end{aligned}
$$&lt;p>其中 $S_Q,S_K,S_V\in\mathbb{R}^{d\times C}$ 是所有的 attention head 所共享的，$Q_c,O_c\in\mathbb{R}^{C\times C}$ 是每个 head 的 query up projection 和 output projection, $C$ 是 latent factorization 的维度。&lt;/p>
&lt;p>在 inference 的时候，由于我们只需要保存 $x_jS_K$ 和 $x_jS_V$, 因此所需要的 KV cache size 为 $2C$. 与 FPBA 相比，MFA 分别使用 $S_QQ_cS_K^T$ 和 $S_VO_c^T$ 来近似 $W_c$ 和 $U_c$, 近似矩阵的 rank 为 $C$. 由于 $C>d$, 因此其表达能力也更强，MFA 有如下优势：&lt;/p>
&lt;ol>
&lt;li>scalable head count: MFA 可以支持使用更多的 attention heads, 每增加一个 heads, 所需要的额外参数为 $2C^2$. 并且，增加 attention heads 个数不会增加 KV cache 占用&lt;/li>
&lt;li>enhanced head expressiveness: MFA 近似矩阵的 rank 为 $C>d$, 因此表达能力更强&lt;/li>
&lt;li>Compatibility with position encodings: MFA 可以无缝集成 position encoding.&lt;/li>
&lt;/ol>
&lt;p>为了进一步降低 MFA 的 KV cache 占用，作者提出了 MFA-Key-Reuse (MFA-KA). 核心思想是使用 $S_K$ 来表示 $S_V$, 这样可以额外降低 $50\%$ 的 KV cache 占用，表示方法如下所示&lt;/p>
$$
S_V = S_K + \alpha\odot NS_K = (I +\mathrm{diag}(\alpha)N)S_K
$$&lt;p>其中 $N\in\mathbb{R}^{N\times N}$, $\alpha\in\mathbb{R}^C$.&lt;/p>
&lt;p>最终，MFA, MFA-KR 与 GQA 的对比如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-mfa/MFA-illustration.png"
width="1352"
height="632"
loading="lazy"
alt="Comparison of MFA with GQA"
class="gallery-image"
data-flex-grow="213"
data-flex-basis="513px"
>&lt;/p>
&lt;p>不同 attention 的量化对比如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Method&lt;/th>
&lt;th>KV Cache&lt;/th>
&lt;th>Parameter&lt;/th>
&lt;th>Heads&lt;/th>
&lt;th>Factor. rank per head&lt;/th>
&lt;th>Shared latent subspace Dim.&lt;/th>
&lt;th>Total effec. rank&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>FPBA&lt;/td>
&lt;td>$2d^2$&lt;/td>
&lt;td>$2d^3$&lt;/td>
&lt;td>$d$&lt;/td>
&lt;td>$d$&lt;/td>
&lt;td>$d$&lt;/td>
&lt;td>$d^2$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MHA&lt;/td>
&lt;td>$2d$&lt;/td>
&lt;td>$4d^2$&lt;/td>
&lt;td>$n$&lt;/td>
&lt;td>$h_d$&lt;/td>
&lt;td>$d$&lt;/td>
&lt;td>$nh_d$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MQA&lt;/td>
&lt;td>$2h_d$&lt;/td>
&lt;td>$(2 + 2/n)d^2$&lt;/td>
&lt;td>$n$&lt;/td>
&lt;td>$h_d$&lt;/td>
&lt;td>$h_d$&lt;/td>
&lt;td>$nh_d$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GQA&lt;/td>
&lt;td>$2gh_d$&lt;/td>
&lt;td>$(2 + 2g/n)d^2$&lt;/td>
&lt;td>$n$&lt;/td>
&lt;td>$h_d$&lt;/td>
&lt;td>$gh_d$&lt;/td>
&lt;td>$nh_d$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MLA&lt;/td>
&lt;td>$2C$&lt;/td>
&lt;td>$5dC + d^2$&lt;/td>
&lt;td>$m$&lt;/td>
&lt;td>$h_d$&lt;/td>
&lt;td>$C$&lt;/td>
&lt;td>$mh_d$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MFA&lt;/td>
&lt;td>$2C$&lt;/td>
&lt;td>$3Cd + 2mC^2$&lt;/td>
&lt;td>$m$&lt;/td>
&lt;td>$C$&lt;/td>
&lt;td>$C$&lt;/td>
&lt;td>$mC$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h2>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Step3vAttention&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Step3VLConfig&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">layer_idx&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="fm">__init__&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">config&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">config&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">layer_idx&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">layer_idx&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">getattr&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;head_dim&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_attention_heads&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_key_value_heads&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">total_num_kv_heads&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_key_value_heads&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_attention_heads&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_attention_heads&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_key_value_groups&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_attention_heads&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_key_value_heads&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">getattr&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;share_q_dim&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">kv_size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_key_value_heads&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">scaling&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="o">**-&lt;/span>&lt;span class="mf">0.5&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">is_causal&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">True&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_size&lt;/span> &lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">k_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">v_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">o_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_attention_heads&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># query down projection normalization&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">inter_norm&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Step3vRMSNorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">eps&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">rms_norm_eps&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># query up projection&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">wq&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_attention_heads&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">position_embeddings&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Tuple&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attention_mask&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">past_key_value&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">Cache&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">cache_position&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">LongTensor&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">**&lt;/span>&lt;span class="n">kwargs&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Unpack&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">FlashAttentionKwargs&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">Tuple&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">Tuple&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">]]]:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">input_shape&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[:&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">query_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">key_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">k_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">input_shape&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">))&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transpose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">value_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">v_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">input_shape&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">))&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transpose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">query_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">inter_norm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">query_states&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">query_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">wq&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">query_states&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">input_shape&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">))&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transpose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">cos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sin&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">position_embeddings&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">query_states&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">key_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">apply_rotary_pos_emb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">query_states&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">key_states&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">cos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sin&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">...&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 MFA 以及 MFA-KR, 一个在 KV cache 有限的条件下最大限度提高 attention 表达能力的 attention 机制。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2412.19255" target="_blank" rel="noopener"
>Multi-matrix Factorization Attention&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://huggingface.co/stepfun-ai/step3/blob/main/modeling_step3.py" target="_blank" rel="noopener"
>Step v3 code&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on MX-format</title><link>https://maosong.website/p/notes-on-mx-format/</link><pubDate>Thu, 21 Aug 2025 18:23:03 +0800</pubDate><guid>https://maosong.website/p/notes-on-mx-format/</guid><description>&lt;p>MX format 是一个表示数据的数据格式，在 LLM 中主要用于量化。相比于直接对整个张量进行量化，MX format 可以在更细粒度的层面控制量化，从而提高模型的表现&lt;/p>
&lt;h2 id="microscaling">&lt;a href="#microscaling" class="header-anchor">&lt;/a>Microscaling
&lt;/h2>&lt;p>Microscaling (MS) format 如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-mx-format/MX-format-illustration.png"
width="1048"
height="490"
loading="lazy"
alt="illustration of MX format"
class="gallery-image"
data-flex-grow="213"
data-flex-basis="513px"
>&lt;/p>
&lt;p>MX format 包括三个部分：&lt;/p>
&lt;ol>
&lt;li>elements $P_1,\dots,P_k$ 未 scale 的数据，要求 $P_1,\dots,P_k$ 的数据类型相同&lt;/li>
&lt;li>shared scale $X$, 对 element 进行的 scale 参数，所有的 $k$ 个 bits 共享一个 $X$&lt;/li>
&lt;li>block size, 决定 element block 的大小&lt;/li>
&lt;/ol>
&lt;p>在存储时，我们只需要存储 $X$ 以及 $P_1,\dots,P_k$, 我们假设 $X$ 需要 $w$ bits 来表示，$P_i$ 需要 $d$ bits 来表示，则我们一共需要 $w+kd$ bits 来表示这 $k$ 个元素。&lt;/p>
&lt;h2 id="concrete-mx-compliant-formats">&lt;a href="#concrete-mx-compliant-formats" class="header-anchor">&lt;/a>Concrete MX-compliant Formats
&lt;/h2>&lt;p>MX-format 包含了一下几种数据格式：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Format Name&lt;/th>
&lt;th>Element Data Type&lt;/th>
&lt;th>Element Bits(d)&lt;/th>
&lt;th>Scaling Block Size(k)&lt;/th>
&lt;th>Scale Data Type&lt;/th>
&lt;th>Scale Bits(w)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>MXFP8&lt;/td>
&lt;td>FP8 (E5M2)&lt;/td>
&lt;td>8&lt;/td>
&lt;td>32&lt;/td>
&lt;td>E8M0&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MXFP8&lt;/td>
&lt;td>FP8 (E4M3)&lt;/td>
&lt;td>8&lt;/td>
&lt;td>32&lt;/td>
&lt;td>E8M0&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MXFP6&lt;/td>
&lt;td>FP6 (E3M2)&lt;/td>
&lt;td>6&lt;/td>
&lt;td>32&lt;/td>
&lt;td>E8M0&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MXFP6&lt;/td>
&lt;td>FP6 (E2M3)&lt;/td>
&lt;td>6&lt;/td>
&lt;td>32&lt;/td>
&lt;td>E8M0&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MXFP4&lt;/td>
&lt;td>FP4 (E2M1)&lt;/td>
&lt;td>4&lt;/td>
&lt;td>32&lt;/td>
&lt;td>E8M0&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MXINT8&lt;/td>
&lt;td>INT8&lt;/td>
&lt;td>8&lt;/td>
&lt;td>32&lt;/td>
&lt;td>E8M0&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="gpt-oss-quantization">&lt;a href="#gpt-oss-quantization" class="header-anchor">&lt;/a>GPT-oss Quantization
&lt;/h2>&lt;p>&lt;a class="link" href="https://maosong.website/p/notes-on-gpt-oss/" target="_blank" rel="noopener"
>gpt-oss&lt;/a> 中使用了 MXFP4 来表示 MoE 中的 down projection 以及 up projection weight matrix 的权重。&lt;/p>
&lt;p>其具体操作过程如下：&lt;/p>
&lt;ol>
&lt;li>我们将参数分为大小为 32 的 block&lt;/li>
&lt;li>每个 block 由一个 scale $X$ 来表示，其精度为 E8M0, 即 8bits, 表示范围为 $[-127,127]$, 以及 $32$ 个元素 $P_i$ 来表示，每个元素的精度为 E2M1, 即 4bits, 表示范围为 $[-6.0,6.0]$.&lt;/li>
&lt;li>由于每个元素由 4bits 来表示，因此我们将两个元素合并在一起来表示&lt;/li>
&lt;/ol>
&lt;p>在加载时，我们可以用如下代码来恢复 $P_i$ 的值到 FP8&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">FP4_VALUES&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mf">0.0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mf">0.5&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mf">1.0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mf">1.5&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mf">2.0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mf">3.0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mf">4.0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mf">6.0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mf">0.0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mf">0.5&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mf">1.0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mf">1.5&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mf">2.0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mf">3.0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mf">4.0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mf">6.0&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">convert_moe_packed_tensors&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">blocks&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">scales&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">*&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dtype&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dtype&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">bfloat16&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">rows_per_chunk&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">32768&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">1024&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kn">import&lt;/span> &lt;span class="nn">math&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># scales are represented with uini8&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">scales&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">scales&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">int32&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">127&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">assert&lt;/span> &lt;span class="n">blocks&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[:&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">scales&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">blocks&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="si">=}&lt;/span>&lt;span class="s2"> does not match &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">scales&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="si">=}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">lut&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">FP4_VALUES&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">dtype&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">device&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">blocks&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">device&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">*&lt;/span>&lt;span class="n">prefix_shape&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">G&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">B&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">blocks&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">rows_total&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">math&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">prod&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">prefix_shape&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">G&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">blocks&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">blocks&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reshape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">rows_total&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">B&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">scales&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">scales&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reshape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">rows_total&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># each byte representing 2 elements and represented with unit8&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">out&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">empty&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">rows_total&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">B&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">dtype&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">device&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">blocks&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">device&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">r0&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">rows_total&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">rows_per_chunk&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">r1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">r0&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">rows_per_chunk&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">rows_total&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">blk&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">blocks&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">r0&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">r1&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">exp&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">scales&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">r0&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">r1&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># nibble indices -&amp;gt; int64&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">idx_lo&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">blk&lt;/span> &lt;span class="o">&amp;amp;&lt;/span> &lt;span class="mh">0x0F&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">long&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">idx_hi&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">blk&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="mi">4&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">long&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sub&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">out&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">r0&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">r1&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sub&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">::&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">lut&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">idx_lo&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sub&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">::&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">lut&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">idx_hi&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ldexp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">sub&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">exp&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">out&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">sub&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">del&lt;/span> &lt;span class="n">idx_lo&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">idx_hi&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">blk&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">exp&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">out&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">out&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reshape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">prefix_shape&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">G&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">B&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">prefix_shape&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">G&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">B&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># to match for now existing implementation&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">out&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float8_e5m2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://github.com/huggingface/transformers/blob/242bb2cafccec9f90479f5f688bca9d240b1031f/src/transformers/models/gpt_oss/convert_gpt_oss_weights_to_hf.py#L78" target="_blank" rel="noopener"
>gpt-oss code&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf#page=4.11" target="_blank" rel="noopener"
>report&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on flashattention</title><link>https://maosong.website/p/notes-on-flashattention/</link><pubDate>Thu, 21 Aug 2025 11:32:53 +0800</pubDate><guid>https://maosong.website/p/notes-on-flashattention/</guid><description>&lt;p>作者提出了 flashattention, 一个通过降低 multi head attention 内存访问开销来提高 attention 计算效率的方法&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>Transformer 的 attention 是一个平方度复杂度的算法，这个平方复杂度既体现在时间复杂度上（矩阵乘法），也体现在空间复杂度上（需要存储中间结果）。因此，要降低 attention 的复杂度，我们有两种思路：&lt;/p>
&lt;ol>
&lt;li>从时间复杂度上入手，比如使用稀疏 attention 机制或者线性注意力机制&lt;/li>
&lt;li>从空间复杂度上入手，比如使用 &lt;a class="link" href="https://maosong.website/p/notes-on-gqa/" target="_blank" rel="noopener"
>GQA&lt;/a>, &lt;a class="link" href="https://maosong.website/p/notes-on-mqa/" target="_blank" rel="noopener"
>MQA&lt;/a> 等减少内存的访问开销&lt;/li>
&lt;/ol>
&lt;p>本文提出的 flashattention 就属于降低空间复杂度的一种做法。作者认为，我们应该设计一种 &lt;strong>IO-aware&lt;/strong> 的 attention 算法，来减少 attention 计算式的内存访问开销，进而提高 attention 的计算效率。&lt;/p>
&lt;p>作者首先提到，一个未解决的问题就是：&lt;/p>
&lt;blockquote>
&lt;p>降低 attention 的内存访问开销是否可以提高 attention 的计算效率？&lt;/p>
&lt;/blockquote>
&lt;p>作者发现，已有的一些工作虽然在理论上降低了 attention 的计算效率，但是在实际中，他们的效果并没有提升太多。作者分析原因认为，已有工作主要关注于降低 FLOPs, 但是忽略了内存访问开销。&lt;/p>
&lt;p>因此，作者在本文中就提出了 flashattention, 一个 IO-aware 的 attention 算法，作者通过尽可能降低内存访问开销来提高模型的计算效率。具体做法就是，避免从内存中读写 attention matrix, 作者认为这个目标有两个挑战：&lt;/p>
&lt;ol>
&lt;li>计算 softmax 的时候不访问所有的输入&lt;/li>
&lt;li>在反向传播时不存储中间的 attention matrix&lt;/li>
&lt;/ol>
&lt;p>作者提出了两个方法来分别解决这两个问题：&lt;/p>
&lt;ol>
&lt;li>作者使用了 &lt;strong>tiling&lt;/strong> 技巧，将 input 分成多个 block, 然后分别进行处理，进而降低 softmax 的内存访问开销&lt;/li>
&lt;li>作者使用了 &lt;strong>recompute&lt;/strong> 技巧，在反向传播时，重新计算 softmax normalization factor&lt;/li>
&lt;/ol>
&lt;p>通过这些改进，我们可以让 attention 运行更快，并且降低内存访问开销。&lt;/p>
&lt;p>作者还从理论上分析了 flashattention 的复杂度，提供了理论基础。&lt;/p>
&lt;p>作者通过实验验证了 flashattention 的有效性，主要是三点：&lt;/p>
&lt;ol>
&lt;li>训练效率更高：相比于 Huggingface 和 Megatron, flashattention 的训练效率提升了 2-3 倍&lt;/li>
&lt;li>模型的表现更好：相比于 GPT-2, 模型的 perplexity 提升了 0.7 个点左右&lt;/li>
&lt;li>速度更快：flashattention 比标准的 attention 实现快 3 倍以上&lt;/li>
&lt;/ol>
&lt;h2 id="background">&lt;a href="#background" class="header-anchor">&lt;/a>Background
&lt;/h2>&lt;h3 id="hardware-performance">&lt;a href="#hardware-performance" class="header-anchor">&lt;/a>Hardware Performance
&lt;/h3>&lt;p>作者首先介绍了以下 GPU 的内存架构，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-flashattention/flashattention-GPU-hierarchy.png"
width="430"
height="383"
loading="lazy"
alt="Memory Hierarchy of GPU"
class="gallery-image"
data-flex-grow="112"
data-flex-basis="269px"
>&lt;/p>
&lt;p>可以看到，GPU 内存可以分为三个层级：&lt;/p>
&lt;ol>
&lt;li>SRAM: GPU 的寄存器，容量小，但是访问速度极快&lt;/li>
&lt;li>High bandwith memory (HBM): GPU 的高速内存，访问速度较快，容量中等&lt;/li>
&lt;li>DRAM: CPU 内存，容量最大，但是访问速度较慢&lt;/li>
&lt;/ol>
&lt;p>接下来作者介绍了 Execution model 的概念，GPU 有多个线程来执行同一个操作（SPMD），这个操作也被称为 kernel, kernel 会从 HBM 中加载输入到 SRAM 中进行计算，然后写回 HBM.&lt;/p>
&lt;p>对一个算法，我们可以将其归类为 compute-bound 和 memory-bound 两类， 我们可以用 arithmetic intensity 来进行区分，arithmetic intensity 定义为 arithmetic operations 与 memory access 的比率。&lt;/p>
&lt;ol>
&lt;li>compute bound: 算法的瓶颈在于算力，由于算力不足导致运行时间慢，比如矩阵乘法&lt;/li>
&lt;li>memory-bound: 算法的瓶颈在于内存访问效率，比如 element-wise 操作或者是 reduction&lt;/li>
&lt;/ol>
&lt;p>为了提高 memory-bound 类型算法的效率，我们进行 kernel fusion, 即把多个访问同一片内存的操作放在一起处理，避免多次读写内存&lt;/p>
&lt;h3 id="standard-attention-implementation">&lt;a href="#standard-attention-implementation" class="header-anchor">&lt;/a>Standard Attention Implementation
&lt;/h3>&lt;p>作者还回顾了一下标准化的 attention 实现。&lt;/p>
&lt;h4 id="forward-pass">&lt;a href="#forward-pass" class="header-anchor">&lt;/a>Forward Pass
&lt;/h4>&lt;p>给定 $Q,K,V\in\mathbb{R}^{N\times d}$, 其中 $N$ 是序列长度, $d$ 是 head dimension, attention 的定义如下&lt;/p>
$$
S = QK^T\in\mathbb{R}^{N\times N},\quad P = \mathrm{softmax}(S)\in\mathbb{R}^{N\times N},\quad O = PV\in\mathbb{R}^{N\times d}
$$&lt;p>这里 $\mathrm{softmax}$ 是逐行计算的。&lt;/p>
&lt;p>算法的执行过程如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-flashattention/flashattention-standard-attention-implementation.png"
width="1022"
height="251"
loading="lazy"
alt="Algorithm 0: Standard Attention Implementation"
class="gallery-image"
data-flex-grow="407"
data-flex-basis="977px"
>&lt;/p>
&lt;p>我们有第一个结论&lt;/p>
&lt;blockquote>
&lt;p>Proposition 1
标准化 attention 前向传播时访问 HBM 的内存访问开销为 $\mathcal{O}(Nd+N^2)$.&lt;/p>
&lt;/blockquote>
&lt;p>证明：对于 attention, 我们需要从 HBM 中加载 $Q,K,V\in\mathbb{R}^{N\times d}$, 然后输出 $O\in\mathbb{R}^{N\times d}$ 并保存到内存中。&lt;/p>
&lt;p>首先我们需要计算 $S = QK^T$, 这一步需要加载 $Q,K$ 并将 $S$ 保存到 HBM 中，内存访问量为 $\mathcal{O}(Nd+N^2)$.&lt;/p>
&lt;p>接下来，我们需要计算 $P = \mathrm{softmax}(S)$, 这一步需要加载 $S$ 然后将 $P$ 保存到 HBM 中，内存访问量为 $\mathcal{O}(N^2)$.&lt;/p>
&lt;p>最后，我们需要计算 $O = PV$, 这一步需要加载 $P$ 和 $V$ 然后将 $O$ 保存到 HBM 中，内存访问量为 $\mathcal{O}(Nd+N^2)$.&lt;/p>
&lt;p>总的来说，标准化 attention 的内存访问开销为 $\mathcal{O}(Nd+N^2)$.&lt;/p>
&lt;h4 id="backward-pass">&lt;a href="#backward-pass" class="header-anchor">&lt;/a>Backward Pass
&lt;/h4>&lt;p>标准 attention 反向传播过程如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-flashattention/flashattention-standard-attention-backward-pass.png"
width="1187"
height="281"
loading="lazy"
alt="standard attention backward pass"
class="gallery-image"
data-flex-grow="422"
data-flex-basis="1013px"
>&lt;/p>
&lt;blockquote>
&lt;p>Proposition 2
标准化 attention 反向传播时访问 HBM 的内存访问开销为 $\mathcal{O}(Nd+N^2)$.&lt;/p>
&lt;/blockquote>
&lt;p>证明：对于标准化 attention 的反向传播，我们需要从 HBM 中加载 $Q,K,V,dO\in\mathbb{R}^{N\times d}$ , 然后输出 $dQ,dK,dV$ 并保存到 HBM 中。&lt;/p>
&lt;p>首先我们计算 $dV=P^TdO$, 这一步需要加载 $P,dO$ 并将 $dV$ 保存到 HBM 中，内存访问开销为 $\mathcal{O}(Nd+N^2)$.&lt;/p>
&lt;p>接下来我们计算 $dP=dOV^T$, 这一步需要加载 $dO, V$ 并将 $dP$ 保存到 HBM 中，内存访问开销为 $\mathcal{O}(Nd)$.&lt;/p>
&lt;p>然后我们计算 $dS$, 这一步需要加载 $P$ 并将 $dS$ 保存到 HBM 中，内存访问开销为 $\mathcal{O}(N^2)$.&lt;/p>
&lt;p>对于 $dQ$ 和 $dK$ 的计算，内存访问开销都是 $\mathcal{O}(Nd+N^2)$.&lt;/p>
&lt;p>因此，标准化 attention 的内存访问开销为 $\mathcal{O}(Nd+N^2)$.&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;p>作者在本节首先介绍了 flashattention 算法，然后作者证明了 flashattention 的正确性以及分析了复杂度。最后作者对 flashattention 进行扩展得到了 Block-sparse Flashattention.&lt;/p>
&lt;h3 id="flashattention">&lt;a href="#flashattention" class="header-anchor">&lt;/a>Flashattention
&lt;/h3>&lt;p>attention 模块的输入是 $Q,K,V\in\mathbb{R}^{N\times d}$, 输出是 $O\in\mathbb{R}^{N\times d}$, 作者的目标是减少计算过程中的 HBM 访问次数&lt;/p>
&lt;p>作者分别使用了 tiling 和 recomputation 来解决 attention 前向传播和反向传播中的内存访问开销。flashattention 的核心思想是，我们将 $Q,K,V$ 分割成 block, 然后在 block 层面进行加载和计算。&lt;/p>
&lt;h4 id="tiling">&lt;a href="#tiling" class="header-anchor">&lt;/a>Tiling
&lt;/h4>&lt;p>首先作者介绍了一下如何使用 tiling 来计算 softmax.&lt;/p>
&lt;p>给定一个向量 $x\in\mathbb{R}^{B}$, 其 softmax 计算方式如下&lt;/p>
$$
m(x) = \max_i x_i,\ f(x) = [e^{x_1-m(x)},\dots,e^{x_B-m(x)}], \ \ell(x)=\sum_if(x)_i, \ \mathrm{softmax}(x) = \frac{f(x)}{\ell(x)}
$$&lt;p>如果我们现在有两个向量 $x^{(1)}, x^{(2)}\in\mathbb{R}^{B}$, 记 $x=[x^{(1)}, x^{(2)}]^T\in\mathbb{R}^{2B}$, 我们可以将 $\mathrm{softmax}(x)$ 的计算分解为&lt;/p>
$$
\begin{aligned}
m(x) &amp;= \max(m(x^{(1)}), m(x^{(2)}))， f(x) = [e^{m(x^{(1)})-m(x)}f(x^{(1)}),e^{m(x^{(2)})-m(x)}f(x^{(2)})]\\
\ell(x) &amp;= e^{m(x^{(1)})-m(x)}\ell(x^{(1)}) + e^{m(x^{(2)})-m(x)}\ell(x^{(2)}), \mathrm{softmax}(x) = \frac{f(x)}{\ell(x)}
\end{aligned}
$$&lt;p>因此，如果我们额外记录 $m(x)$ 以及 $\ell(x)$ 这两个量，那么我们可以每次仅计算 softmax 的一个 block. 具体细节见&lt;a class="link" href="https://maosong.website/p/notes-on-softmax/" target="_blank" rel="noopener"
>softmax&lt;/a>.&lt;/p>
&lt;h4 id="recomputation">&lt;a href="#recomputation" class="header-anchor">&lt;/a>Recomputation
&lt;/h4>&lt;p>在反向传播过程中，一般我们需要存储 $S,P\in\mathbb{R}^{N\times N}$, 需要的空间复杂度为 $\mathcal{O}(N^2)$. 但是，通过存储 $O\in\mathbb{R}^{N\times d}$ 以及 $(m,\ell)$, 我们可以避免重新计算 $S,P$,这可以看做是 gradient checkpointing. 但是与 checkpointing 相比，因为 flashattention 减少了内存访问开销，因此其反向过程并没有变得更慢。&lt;/p>
&lt;h4 id="algorithm">&lt;a href="#algorithm" class="header-anchor">&lt;/a>Algorithm
&lt;/h4>&lt;p>最终，flashattention 的算法如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-flashattention/flashattention-algorithm.png"
width="1207"
height="700"
loading="lazy"
alt="Flashattention algorithm"
class="gallery-image"
data-flex-grow="172"
data-flex-basis="413px"
>&lt;/p>
&lt;h3 id="analysis">&lt;a href="#analysis" class="header-anchor">&lt;/a>Analysis
&lt;/h3>&lt;h4 id="correctness">&lt;a href="#correctness" class="header-anchor">&lt;/a>Correctness
&lt;/h4>&lt;p>算法的正确性由定理 1 给出&lt;/p>
&lt;blockquote>
&lt;p>Theorem 1
flashattention (即算法 1) 输出 $O=\mathrm{softmax}(QK^T)V$, 其时间复杂度为 $\mathcal{O}(N^2d)$, 空间复杂度为 $\mathcal{O}(N)$.&lt;/p>
&lt;/blockquote>
&lt;p>证明：时间复杂度主要由矩阵乘法决定。在计算 $S_{ij}=Q_iK_j^T$ 时，所花费的 FLOPS 为 $\mathcal{O}(B_rB_cd)$. 在计算 $\tilde{P}_{ij}V_j$ 时，所花费的 FLOPS 为 $\mathcal{O}(B_rB_cd)$. 循环一共执行了&lt;/p>
$$
T_cT_r = \left\lceil\frac{N}{B_c}\right\rceil\left\lceil\frac{N}{B_r}\right\rceil
$$&lt;p>从而总的 FLOPS 为&lt;/p>
$$
\mathcal{O}\left(\frac{N^2}{B_rB_c}B_rB_cd\right) = \mathcal{O}(N^2d)
$$&lt;p>在 flashattention 的计算过程中，我们只需要保存 $(\ell, m)$ 即可，因此需要的额外内存空间为 $\mathcal{O}(N)$.&lt;/p>
&lt;p>接下来，我们可以证明 flashattention 的正确性，我们使用归纳法来证明。令 $j$ 满足 $0\leq j\leq T_c$, $K_{:j}\in\mathbb{R}^{jB_c\times d}$, $V_{:j}\in\mathbb{R}^{jB_c\times d}$ 分别为 $K$ 和 $V$ 的前 $jB_c$ 行。 $S_{:, :j}=QK_{:j}^T\in\mathbb{R}^{N\times jB_c}$, $P_{:,:j}=\mathrm{softmax}(S_{:,:j})\in\mathbb{R}^{N\times jB_c}$, $m^{(j)}, \ell^{(j)}, O^{(j)}$ 分别为 $m,\ell, O$ 的第 $j$ 个元素。我们证明经过第 $j$ 次迭代后，HBM 中保存的是&lt;/p>
$$
m^{(j)}=\mathrm{rowmax}(S_{:,:j})\in\mathbb{R}^N, \ell^{(j)}=\mathrm{rowsum}(\exp(S_{:,:j}-m^{(j)}))\in\mathbb{R}^N, O^{(j)} = P_{:,:j}V_{:j}\in\mathbb{R}^{N\times d}
$$&lt;p>当 $j=0$ 时，上面的结果显然成立。现在我们假设对某个 $j=0,\dots, T_c-1$ 上面的结果成立，我们需要证明对 $j+1$ 也成立。&lt;/p>
&lt;p>首先&lt;/p>
$$
m^{(j+1)}=\max(m^{(j)}， \tilde{m}) = \max(\mathrm{rowmax}(S_{:,:j}), \mathrm{rowmax}(S_{:,j:j+1}))=\mathrm{rowmax}(S_{:,:j+1})
$$&lt;p>接下来&lt;/p>
$$
\begin{aligned}
\ell^{(j+1)} &amp;= \exp(m^{(j)}-m^{(j+1)})\ell^{(j)} + \exp(\tilde{m}-m^{(j+1)})\tilde{\ell}\\
&amp;=\exp(m^{(j)}-m^{(j+1)})\mathrm{rowsum}(\exp(S_{:,:j}-m^{(j)})) + \exp(\tilde{m}-m^{(j+1)})\mathrm{rowsum}(\exp(S_{:,j:j+1}-\tilde{m}))\\
&amp;= \mathrm{rowsum}(\exp(S_{:,:j}-m^{(j+1)})) + \mathrm{rowsum}(\exp(S_{:,j:j+1}-m^{(j+1)}))\\
&amp;= \mathrm{rowsum}(\exp(S_{:,:j+1}-m^{(j+1)}))
\end{aligned}
$$&lt;p>最后，我们计算 $O^{(j+1)}$ 得到：&lt;/p>
$$
\begin{aligned}
O^{(j+1)} &amp;= \mathrm{diag}(\ell^{(j+1)})^{-1}(\mathrm{diag}(\ell^{(j)})\exp(m^{(j)}-m^{(j+1)})O^{(j)}+\exp(\tilde{m}-m^{(j+1)})\exp(S_{:,j:j+1}-\tilde{m})V_{:,j:j+1})\\
&amp;= \mathrm{diag}(\ell^{(j+1)})^{-1}(\mathrm{diag}(\ell^{(j)})\exp(m^{(j)}-m^{(j+1)})P_{:,:j}V_{:,:j}+\exp(-m^{(j+1)})\exp(S_{:,j:j+1})V_{:,j:j+1})\\
&amp;= \mathrm{diag}(\ell^{(j+1)})^{-1}(\mathrm{diag}(\ell^{(j)})\exp(m^{(j)}-m^{(j+1)})\mathrm{diag}(\ell^{(j)})^{-1}\exp(S_{:,:j}-m^{(j)})V_{:,:j}+\exp(-m^{(j+1)})\exp(S_{:,j:j+1})V_{:,j:j+1})\\
&amp;= \mathrm{diag}(\ell^{(j+1)})^{-1}(\exp(-m^{(j+1)})\exp(S_{:,:j}))V_{:,:j}+\exp(-m^{(j+1)})\exp(S_{:,j:j+1})V_{:,j:j+1})\\
&amp;= \mathrm{diag}(\ell^{(j+1)})^{-1}(
\begin{bmatrix}
\exp(S_{:,:j}-m^{(j+1)}) &amp; \exp(S_{:,:j}-m^{(j+1)})
\end{bmatrix}\begin{bmatrix}
V_{:,:j} \\
V_{:,j:j+1}
\end{bmatrix}\\
&amp;= \mathrm{softmax}(S_{:,:j+1})V_{:,:j+1}
\end{aligned}
$$&lt;p>因此上面的结果对 $j+1$ 也成立，从而 flashattention 的结果对 $j=0,\dots,T_c$ 都成立。&lt;/p>
&lt;h4 id="forward-pass-of-flashattention">&lt;a href="#forward-pass-of-flashattention" class="header-anchor">&lt;/a>Forward Pass of flashattention
&lt;/h4>&lt;p>第一个问题是如何提高 softmax 计算的效率，作者的做法先先计算 normalization constant 然后再分别计算不同的 column.&lt;/p>
&lt;p>给定 $Q,K,V\in\mathbb{R}^{N\times d}$, 其中 $N$ 是序列长度, $d$ 是 head dimension, attention 的定义如下&lt;/p>
$$
S = QK^T\in\mathbb{R}^{N\times N},\quad P = \mathrm{softmax}(S)\in\mathbb{R}^{N\times N},\quad O = PV\in\mathbb{R}^{N\times d}
$$&lt;p>我们有 $S_{ij}=q_i^Tk_j$, 这里 $q_i$ 和 $k_j$ 分别是 $Q$ 和 $K$ 的第 $i$ 列以及第 $j$ 列， normalization constant 定义为：&lt;/p>
$$
L_i = \sum_{j=1}^N \exp\left(q_i^Tk_j\right)
$$&lt;p>对任意 $i$, 计算 $L_i$ 只需要 $\mathcal{O}(N)$ 的空间复杂度。&lt;/p>
&lt;p>令 $v_j$ 是 $V$ 的第 $i$ 列，则输出 $O$ 的第 $i$ 列 $o_i$ 为&lt;/p>
$$
o_i = P_{i:}V = \sum_{j=1}^N P_{ij}v_j = \sum_{j=1}^N\frac{\exp(q_i^Tk_j)}{L_i}v_j
$$&lt;p>这个过程中，对任意 $i$, 计算 $o_i$ 也只需要 $\mathcal{O}(N)$ 的空间复杂度。&lt;/p>
&lt;p>因此，在 $L_i$ 已经计算好的情况下，我们可以在 $\mathcal{O}(N)$ 的空间复杂度下计算 $o_i$.&lt;/p>
&lt;p>最终，flashattention 的 forward pass 过程如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-flashattention/flashattention-forward-pass-algorithm.png"
width="1207"
height="847"
loading="lazy"
alt="flashattention forward pass"
class="gallery-image"
data-flex-grow="142"
data-flex-basis="342px"
>&lt;/p>
&lt;p>接下来，作者分析了 flashattention 的内存访问开销。结论如下&lt;/p>
&lt;blockquote>
&lt;p>Theorem 2
令 $N$ 为 sequence length, $d$ 为 head dimension, $M$ 是 SRAM 的 size, 且满足 $d\leq M\leq Nd$. 则 flashattention 前向传播的内存访问开销为 $\Theta(N^2d^2M^{-1})$.&lt;/p>
&lt;/blockquote>
&lt;p>证明：由 Algorithm 1（或者 Algorithm 2）可以知道，$K$ 和 $V$ 的每一个元素都只需要从 HBM 中加载一次，而每一次外层循环都会从 HBM 中加载一次 $O$ 和 $Q$, 因此总的 HBM 访问次数为 $\mathcal{O}(Nd+NdT_c)=\mathcal{O}(NdT_c)$.&lt;/p>
&lt;p>接下来，我们给出每一次内层循环的内存访问开销，这是由 SRAM 的大小决定的。由于我们需要 SRAM 可以存储 $K_j\in\mathbb{R}^{B_c\times d}$ 以及 $V_j\in\mathbb{R}^{B_c\times d}$ ，我们的 block size 需要满足&lt;/p>
$$
B_cd = \mathcal{O}(M) \Rightarrow B_c = \mathcal{O}\left(\frac{M}{d}\right)
$$&lt;p>同理，对于 $O$ 和 $Q$, 我们有&lt;/p>
$$
B_rd = \mathcal{O}(M) \Rightarrow B_r = \mathcal{O}\left(\frac{M}{d}\right)
$$&lt;p>最后，我们还需要 SRAM 可以存储 $S_{ij}\in\mathbb{R}^{B_r\times B_c}$, 因此&lt;/p>
$$
B_rB_c=\mathcal{O}(M)
$$&lt;p>这样，&lt;/p>
$$
B_c = \mathcal{O}\left(\frac{M}{d}\right), B_r=\mathcal{O}\left(\min\left(\frac{M}{d},\frac{M}{B_c}\right)\right)=\mathcal{O}\left(\min\left(\frac{M}{d},d\right)\right)
$$&lt;p>从而&lt;/p>
$$
T_c = \frac{N}{B_c} = \mathcal{O}\left(\frac{Nd}{M}\right)
$$&lt;p>最终，总的内存访问开销为&lt;/p>
$$
\mathcal{O}(NdT_c) = \mathcal{O}\left(\frac{N^2d^2}{M}\right)
$$&lt;p>一般来说, $d$ 的大小为 $64-128$, $M$ 的大小为 $100 KB$ 左右, $d^2&amp;laquo; M, 因此 flashattention 的内存访问开销远小于标准化 attention 的内存访问开销。&lt;/p>
&lt;p>作者还证明 flashattention 的内存访问开销是一个下界，即&lt;/p>
&lt;blockquote>
&lt;p>Proposition 3
令 $N$ 为 sequence length, $d$ 为 head dimension, $M$ 是 SRAM 的 size, 且满足 $d\leq M\leq Nd$. 则不存在一个对任意 $M\in[d,Nd]$ 都可以在 内存访问开销为 $\Theta(N^2d^2M^{-1})$ 的条件下完成 attention 计算的算法。&lt;/p>
&lt;/blockquote>
&lt;p>证明可以用反证法，基本思想是加载 $Q,K,V$ 的 HBM 访问次数至少为 $\mathcal{O}(Nd)$.&lt;/p>
&lt;h4 id="backward-pass-of-flashattention">&lt;a href="#backward-pass-of-flashattention" class="header-anchor">&lt;/a>Backward Pass of flashattention
&lt;/h4>&lt;p>第二个问题是能否在线性空间复杂度下计算 attention 的反向传播过程。&lt;/p>
&lt;p>首先我们记损失函数为 $\phi$, 然后令 $\phi$ 对 $O,Q,K,V$ 的梯度分别为 $dO,dQ,dK, dV\in\mathbb{R}^{N\times d}$, 我们的目标是计算 $dQ, dK, dV$.&lt;/p>
&lt;p>$dV$ 的计算是最容易的，我们有 $dV=P^TdO$, 因此&lt;/p>
$$
dv_j = \sum_{i=1}^N P_{ij}do_i = \sum_{i=1}^N\frac{\exp(q_i^Tk_j)}{L_i}do_i
$$&lt;p>由于我们已经计算了 $L_i$, 因此，$dv_j$ 只需要 $\mathcal{O}(d)$ 的空间复杂度。&lt;/p>
&lt;p>接下来，注意到 $dP=dOV^T$, 因此我们有&lt;/p>
$$
dP_{ij} = do_i^Tv_j
$$&lt;p>计算的空间复杂度也是要 $\mathcal{O}(N)$ 的&lt;/p>
&lt;p>注意到 $P_{i:}=\mathrm{softmax}(s_{i:})$, 且 $y=\mathrm{softmax}(x)$ 的 Jacobian 是 $\mathrm{diag}(y)-yy^T$ (推导过程见 &lt;a class="link" href="https://maosong.website/p/notes-on-softmax/" target="_blank" rel="noopener"
>softmax&lt;/a>), 我们有&lt;/p>
$$
dS_{i:} = (\mathrm{diag}(P_{i:})-P_{i:}P_{i:}^T)dP_{i:} = P_{i:} \odot dP_{i:} - (P_{i:}^TdP_{i:})P_{i:}
$$&lt;p>我们定义&lt;/p>
$$
D_i = P_{i:}^TdP_{i:}= \sum_{j=1}^N\frac{\exp(q_i^Tk_j)}{L_i}do_i^Tv_j = do_i^T\sum_{j=1}^N\frac{\exp(q_i^Tk_j)}{L_i}v_j = do_i^To_i
$$&lt;p>$D_i$ 的空间复杂度也只需要 $\mathcal{O}(N)$.&lt;/p>
&lt;p>则&lt;/p>
$$
dS_{i:} =P_{i:} \odot dP_{i:} - D_iP_{i:}
$$&lt;p>我们有&lt;/p>
$$
dS_{ij} = P_{ij}dP_{ij} - D_iP_{ij} = P_{ij}(dP_{ij}-D_i)
$$&lt;p>注意到 $S_{ij}=q_i^Tk_j$, 我们有&lt;/p>
$$
dq_i = \sum_{j=1}^N dS_{ij}k_j = \sum_{j=1}^NP_{ij}(dP_{ij}-D_i)k_j = \sum_{j=1}^N\frac{\exp(q_i^Tk_j)}{L_i}(do_i^Tv_j-D_i)k_j
$$&lt;p>因此计算 $dq_i$ 的空间复杂度为 $\mathcal{O}(d)$.&lt;/p>
&lt;p>同样的，&lt;/p>
$$
dk_j = \sum_{j=1}^N dS_{ij}q_i = \sum_{j=1}^NP_{ij}(dP_{ij}-D_i)q_i = \sum_{j=1}^N\frac{\exp(q_i^Tk_j)}{L_i}(do_i^Tv_j-D_i)q_i
$$&lt;p>其空间复杂度为 $\mathcal{O}(N)$.&lt;/p>
&lt;p>总之，attention 的反向传播过程所需要的空间复杂度为 $\mathcal{O}(N)$.&lt;/p>
&lt;p>作者发现有两点可以改进：&lt;/p>
&lt;ol>
&lt;li>attention mask 不需要存储，我们只需要保存 forward pass 时的输入，然后在 backward pass 时重新生成即可，这样只需要 $\mathcal{O}(N)$ 的空间复杂度。&lt;/li>
&lt;li>计算 softmax 的梯度是，如果使用公式 $D_i=P_{i:}^TdP_{i:}$ 来计算的话，由于 $P_{i:}\in\mathbb{R}^N$, 可能会导致超过 SRAM 的内存使用限制，因此，我们可以使用 $D_i=do_i^To_i$ 来避免这个问题，其中 $o_i\in\mathbb{R}^d$.&lt;/li>
&lt;/ol>
&lt;p>最终，flashattention 的 backward pass 过程如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-flashattention/flashattention-backward-pass.png"
width="1206"
height="1203"
loading="lazy"
alt="flashattention backward pass"
class="gallery-image"
data-flex-grow="100"
data-flex-basis="240px"
>&lt;/p>
&lt;p>经过前面的分析，flashattention 的反向传播的时间复杂度为 $\mathcal{O}(N^2)$, 空间复杂度为 $\mathcal{O}(N)$.&lt;/p>
&lt;blockquote>
&lt;p>Theorem 5
令 $N$ 为 sequence length, $d$ 为 head dimension, $M$ 是 SRAM 的 size, 且满足 $d\leq M\leq Nd$. 则 flashattention 反向传播的内存访问开销为 $\Theta(N^2d^2M^{-1})$.&lt;/p>
&lt;/blockquote>
&lt;p>定理的证明与 Theorem 2 基本一致，我们此处不再赘述。&lt;/p>
&lt;h3 id="block-sparse-flashattention">&lt;a href="#block-sparse-flashattention" class="header-anchor">&lt;/a>Block-sparse Flashattention
&lt;/h3>&lt;p>当 attention 具有 block sparsity 的性质时，作者提出了 blck-sparse flashattention 来进一步提高 attention 的计算效率。&lt;/p>
&lt;p>给定 $Q,K,V\in\mathbb{R}^{N\times d}$, 以及一个 mask $M\in\{0,1\}^{N\times N}$, 我们需要计算&lt;/p>
$$
S = QK^T\in\mathbb{R}^{N\times N},\quad P = \mathrm{softmax}(S\odot \mathbb{1}_{M})\in\mathbb{R}^{N\times N},\quad O = PV\in\mathbb{R}^{N\times d}
$$&lt;p>其中当 $M_{kl}=1$ 时， $(S\odot \mathbb{1}_ {M})_ {kl}=S_ {kl}$, 否则 $(S\odot \mathbb{1}_ {M})_{kl}=0$.&lt;/p>
&lt;p>Block-sparse attention 的算法如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-flashattention/flashattention-block-sparse-algorithm.png"
width="1205"
height="905"
loading="lazy"
alt="Block-sparse flashattention forward pass"
class="gallery-image"
data-flex-grow="133"
data-flex-basis="319px"
>&lt;/p>
&lt;blockquote>
&lt;p>Proposition 4
令 $N$ 为 sequence length, $d$ 为 head dimension, $M$ 是 SRAM 的 size, 且满足 $d\leq M\leq Nd$. 则 block-sparse attention 的内存访问开销为 $\Theta(Nd+N^2d^2M^{-1}s)$, 其中 $s$ 是 block-sparse mask 中的非零 block 的比例&lt;/p>
&lt;/blockquote>
&lt;p>证明与 Theorem 2 的证明是类似的，总的内存访问开销为 $\mathcal{O}(Nd+NdT_c)$, 但是在计算的过程中，由于 mask 矩阵的 block-sparsity, 我们实际上只需要计算一小部分 $M_{ij}\neq0$ 的情况，因此最终的内存访问开销为&lt;/p>
$$
\mathcal{O}\left(Nd+\frac{N^2d^2}{M}s\right)
$$&lt;p>可以看到，attention mask 的 sparsity 越高，block-sparse flashattention 的效率也就越高。当 $N$ 非常大时，$s 通常为 $1/\sqrt{N}$ 或者 $N^{-1}\log N$, 从而最终的内存访问开销为 $\mathcal{O}(N\sqrt{N})$ 或者 $\mathcal{O}(N\log N)$.&lt;/p>
&lt;p>作者对比了以下 block-sparse flashattention 和 flashattention 的效率对比，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-flashattention/flashattention-block-sparse-efficiency.png"
width="287"
height="214"
loading="lazy"
alt="Efficiency of block-sparse flashattention"
class="gallery-image"
data-flex-grow="134"
data-flex-basis="321px"
>&lt;/p>
&lt;h2 id="experiment">&lt;a href="#experiment" class="header-anchor">&lt;/a>Experiment
&lt;/h2>&lt;p>作者通过实验验证了 flashattention 的有效性，如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Attention&lt;/th>
&lt;th>Standard&lt;/th>
&lt;th>FlashAttention&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>GFLOPs&lt;/td>
&lt;td>66.6&lt;/td>
&lt;td>75.2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>HBM R/W (GB)&lt;/td>
&lt;td>40.3&lt;/td>
&lt;td>4.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Runtime (ms)&lt;/td>
&lt;td>41.7&lt;/td>
&lt;td>7.3&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，尽管 flashattention 相比于标准化 attention 需要更多的算力，但是由于其内存访问开销更少，所以最终的运行时间大有了大幅度降低&lt;/p>
&lt;p>作者还探究了 block size 对 flashattention 性能对的影响，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-flashattention/flashattention-ablation-block-size.png"
width="315"
height="216"
loading="lazy"
alt="Ablation on block size"
class="gallery-image"
data-flex-grow="145"
data-flex-basis="350px"
>&lt;/p>
&lt;p>可以看到，随着 block size 增加，循环次数降低，内存访问开销也逐渐降低。但是当 block size 充分大 ( $> 256$) 之后，运行时间就会被别的因素所限制，并且过大的 block size 可能会导致 SRAM 的内存溢出&lt;/p>
&lt;p>作者首先在 BERT 和 GPT-2 上验证了 flashattention 的表现，BERT 的实验结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>BERT Implementation&lt;/th>
&lt;th>Training time (minutes)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Nvidia MLPerf 1.1&lt;/td>
&lt;td>$20.0\pm1.5$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FlashAttention (ours)&lt;/td>
&lt;td>$17.4\pm1.4$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>GPT-2 的实验结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model implementations&lt;/th>
&lt;th>OpenWebText (ppl)&lt;/th>
&lt;th>Training time (speedup)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>GPT-2 small - Huggingface&lt;/td>
&lt;td>18.2&lt;/td>
&lt;td>9.5 days (1.0 )&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GPT-2 small - Megatron-LM&lt;/td>
&lt;td>18.2&lt;/td>
&lt;td>4.7 days (2.0 )&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GPT-2 small - FlashAttention&lt;/td>
&lt;td>18.2&lt;/td>
&lt;td>2.7 days (3.5 )&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GPT-2 medium - Huggingface&lt;/td>
&lt;td>14.2&lt;/td>
&lt;td>21.0 days (1.0 )&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GPT-2 medium - Megatron-LM&lt;/td>
&lt;td>14.3&lt;/td>
&lt;td>11.5 days (1.8 )&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GPT-2 medium - FlashAttention&lt;/td>
&lt;td>14.3&lt;/td>
&lt;td>6.9 days (3.0 )&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>实验结果显示，flashattention 比 Huggingface 快 3 倍左右，比 Megatron 快 1.7 倍左右&lt;/p>
&lt;ol>
&lt;li>训练速度：实验显示，flashattention 在 BERT 上，比 MLPerf 1.1 快 $15\%$, 在 GPT-2 上比 HuggingFace 快 3 倍，比 Megatron 快 1.8 倍&lt;/li>
&lt;li>准确率：flashattention 是第一个在 Path-X 上比随机表现更好的 transformer 模型；block-sparse flashattention 是第一个在 Path-256 上比随机表现更好的的 sequence model&lt;/li>
&lt;/ol>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者提出了 flashattention, 一个通过优化标准 attention 内存访问效率来提高 attention 计算效率的方法，作者详细介绍了算法设计的原理与证明，并通过实验证明了结果的有效性。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2205.14135" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on StreamingLLM</title><link>https://maosong.website/p/notes-on-streamingllm/</link><pubDate>Wed, 20 Aug 2025 10:16:35 +0800</pubDate><guid>https://maosong.website/p/notes-on-streamingllm/</guid><description>&lt;p>作者提出了 StreamingLLM, 一个基于 attention sink 来提高 sliding window attention 在超长上下文场景下表现的方法。&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>已有的基于 softmax attention 的架构的问题在于很难扩展到长上下文的场景，主要原因有两点：&lt;/p>
&lt;ol>
&lt;li>KV cache 会随着序列长度增加而商城，从而提高 decoding 的 latency&lt;/li>
&lt;li>序列长度超过预训练的 context length 之后，模型表现会急剧下降&lt;/li>
&lt;/ol>
&lt;p>为了解决这个问题，已有的方法可以分为三类：&lt;/p>
&lt;ol>
&lt;li>length extrapolation: 使用 RoPE 或者 AliBi 等方法来扩展 LLM 的 context length, 这类方法的问题是扩展的上下文长度仍然有限，对于 streaming 的场景作用有限&lt;/li>
&lt;li>context window attention: 扩展 LLM 的上下文长度，如 &lt;a class="link" href="https://maosong.website/p/notes-on-flashattention/" target="_blank" rel="noopener"
>flash attention&lt;/a> 等来降低 attention 的计算和内存开销。这类方法也是只在有限的上下文场景下 work&lt;/li>
&lt;li>Improving LLMs’ Utilization of Long Text: 更好利用长上下文的数据&lt;/li>
&lt;/ol>
&lt;p>基于已有的工作的发现，作者提出了本文研究的核心问题：&lt;/p>
&lt;blockquote>
&lt;p>如何在不损失模型表现和效率的情况下，提高模型在无限长上下文场景下的表现。&lt;/p>
&lt;/blockquote>
&lt;p>为了解决这个问题，作者首先分析了 sliding window attention 的不足，作者发现，sliding window attention 在超过 KV cache size 之后，表现也会急剧下降。作者通过实验发现，sliding window attention 表现急剧下降的原因在于 &lt;strong>attention sink&lt;/strong>, 也就是模型损失了对于初始 token 的关注，从而导致模型表现下降。&lt;/p>
&lt;p>基于 attention sink, 作者设计了 StreamingLLM, 用于提高 sliding window attention 在长上下文场景下的表现，结果发现，模型的表现有了大幅度的提升。&lt;/p>
&lt;p>作者还进一步在预训练阶段加入了 sink token 充当初始 token, 进一步提高模型的表现。&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="attention-sink">&lt;a href="#attention-sink" class="header-anchor">&lt;/a>Attention Sink
&lt;/h3>&lt;p>作者首先探究了一下 softmax attention 以及 sliding window attention 性能下降的节点，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-streamingllm/attention_sink_perplexity.png"
width="1155"
height="216"
loading="lazy"
alt="Perplexity of different attention with 20K tokens"
class="gallery-image"
data-flex-grow="534"
data-flex-basis="1283px"
>&lt;/p>
&lt;p>可以看到，softmax attention 性能急剧下降的节点为 pre-training 的 context length; 而 sliding window attention 性能急剧下降的节点为 KV cache size.&lt;/p>
&lt;p>接下来，作者分析了一下不同 layer 的 attention 分布情况，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-streamingllm/attention_sink-attention-logits-visualization.png"
width="1099"
height="256"
loading="lazy"
alt="Visualization of attention logits"
class="gallery-image"
data-flex-grow="429"
data-flex-basis="1030px"
>&lt;/p>
&lt;p>可以看到，初始的 2 层 layer 里 attention logits 的分布比较均匀。但是在后续的 layer 里，第一个 token 的权重都大幅度上升。&lt;/p>
&lt;p>作者分析原因认为，sliding window attention 在超过 KV cache size 之后性能急剧下降的主要原因是初始 token 不再参与 softmax 的计算，这导致了 softmax 的计算出现了比较大的变化，从而模型的表现开始下降。&lt;/p>
&lt;p>为了探究初始 token 对最终模型表现的影响因素是语义层面还是位置层面的，作者将初始的 token 替换为 &lt;code>\n&lt;/code>, 并比较了模型的表现，结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Llama-2-13B&lt;/th>
&lt;th>PPL (↓)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>0 + 1024(Window)&lt;/td>
&lt;td>5158.07&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4 + 1020&lt;/td>
&lt;td>5.40&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4&amp;quot;\n&amp;quot;+1020&lt;/td>
&lt;td>5.60&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，把初始的四个 token 替换为 &lt;code>\n&lt;/code>, 并不影响模型最终的表现，这说明是初始 token 的位置信息在发挥作用。&lt;/p>
&lt;p>作者接下来探究了一下模型架构的影响，实验结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Cache Config&lt;/th>
&lt;th>0+2048&lt;/th>
&lt;th>1+2047&lt;/th>
&lt;th>2+2046&lt;/th>
&lt;th>4+2044&lt;/th>
&lt;th>8+2040&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Falcon-7B&lt;/td>
&lt;td>17.90&lt;/td>
&lt;td>12.12&lt;/td>
&lt;td>12.12&lt;/td>
&lt;td>12.12&lt;/td>
&lt;td>12.12&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MPT-7B&lt;/td>
&lt;td>460.29&lt;/td>
&lt;td>14.99&lt;/td>
&lt;td>15.00&lt;/td>
&lt;td>14.99&lt;/td>
&lt;td>14.98&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Pythia-12B&lt;/td>
&lt;td>21.62&lt;/td>
&lt;td>11.95&lt;/td>
&lt;td>12.09&lt;/td>
&lt;td>12.09&lt;/td>
&lt;td>12.02&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Cache Config&lt;/td>
&lt;td>0+4096&lt;/td>
&lt;td>1+4095&lt;/td>
&lt;td>2+4094&lt;/td>
&lt;td>4+4092&lt;/td>
&lt;td>8+4088&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Llama-2-7B&lt;/td>
&lt;td>3359.95&lt;/td>
&lt;td>11.88&lt;/td>
&lt;td>10.51&lt;/td>
&lt;td>9.59&lt;/td>
&lt;td>9.54&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，不同的模型架构都存在这个问题，这说明 sliding window attention 的影响与架构无关。并且，作者认为，使用初始 4 个 token 就可以有效的避免模型的性能下降，进一步增加初始 token 的数量不会有进一步提升。&lt;/p>
&lt;p>作者分析 attention sink 出现的原因在于，&lt;/p>
&lt;ol>
&lt;li>初始的 token 对于后续所有的 token 都是可见的，因此其会携带一些信息&lt;/li>
&lt;li>在预训练阶段，模型并没有一个一致的初始 token 来标注起始信息，这导致模型会默认使用第一个 token 来储存一些信息。&lt;/li>
&lt;/ol>
&lt;p>为了解决这个问题，作者就提出了缓存初始 token 的方法，具体做法就是，在 sliding window attention 的基础上，我们还会加上初始 token 的信息，作者展示示意图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-streamingllm/attention_sink-KV_cache-rolling.png"
width="481"
height="190"
loading="lazy"
alt="Rolling KV cache of StramingLLM"
class="gallery-image"
data-flex-grow="253"
data-flex-basis="607px"
>&lt;/p>
&lt;p>也就是说，我们初始 token 始终会参与计算（论文中初始 token 数量为 4），然后我们会维持一个大小为 3 的 KV cache 队列来进行最终 sliding window attention 的计算，这样，每次计算 attention 的时候，我们就会使用 $\# \text{iniital token} + \# \text{sliding window token}$ 这么多的 token 来计算 attention. 作者对比了不同 attention 的计算方式，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-streamingllm/attention_sink-illustration-streamingLLM.png"
width="1148"
height="389"
loading="lazy"
alt="Illustration of StreamingLLM"
class="gallery-image"
data-flex-grow="295"
data-flex-basis="708px"
>&lt;/p>
&lt;p>前面是在 inference 阶段进行优化的，作者现在进一步探究在 pre-training 阶段加入 attention sink 参与训练对模型表现的影响。&lt;/p>
&lt;p>[[softmax-off-by-one]] 提出了我们应该加入一个 zero sink token, 其计算公式如下&lt;/p>
$$
\mathrm{softmax}_1(x)_i = \frac{\exp(x_i)}{1 + \sum_{j=1}^N \exp(x_j)}
$$&lt;p>这里 $x\in\mathbb{R}^N$ 是输入的序列。我们可以将 sink token 视为一个 key 以及 value 都是 0 向量的特殊 token.&lt;/p>
&lt;p>在本文中，作者使用了一个可学习的 sink token. 作者对比了原始 softmax attention, 使用 zero sink attention, learnable sink attention 三种方法的表现，结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Cache Config&lt;/th>
&lt;th>0+1024&lt;/th>
&lt;th>1+1023&lt;/th>
&lt;th>2+1022&lt;/th>
&lt;th>4+1020&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Vanilla&lt;/td>
&lt;td>27.87&lt;/td>
&lt;td>18.49&lt;/td>
&lt;td>18.05&lt;/td>
&lt;td>18.05&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Zero Sink&lt;/td>
&lt;td>29214&lt;/td>
&lt;td>19.90&lt;/td>
&lt;td>18.27&lt;/td>
&lt;td>18.01&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Learnable Sink&lt;/td>
&lt;td>1235&lt;/td>
&lt;td>18.01&lt;/td>
&lt;td>18.01&lt;/td>
&lt;td>18.02&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到 zero sink 仍然需要一部分初始 token 来维持模型的表现。作者在论文中推荐使用 learnable sink.&lt;/p>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>作者首先验证了 StreamlingLLM 在不同架构上的表现，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-streamingllm/attention_sink-preplexsity-4M-tokens.png"
width="1151"
height="172"
loading="lazy"
alt="Perplexity of StreamingLLM on 4M tokens"
class="gallery-image"
data-flex-grow="669"
data-flex-basis="1606px"
>&lt;/p>
&lt;p>实验结果显示，StreamingLLM 可以扩展到 4M 的上下文&lt;/p>
&lt;p>接下来，作者探究了以下在 Pretraining 阶段加入 learnable sink token 对模型表现的影响，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-streamingllm/attention_sink-pre-training-sink-token.png"
width="358"
height="208"
loading="lazy"
alt="Pre-training loss curves of models w/ sink tokens"
class="gallery-image"
data-flex-grow="172"
data-flex-basis="413px"
>&lt;/p>
&lt;p>可以看到，加入 sink token 之后对模型的表现没有显著影响。并且，模型在下游任务上的表现与标准的 softmax attention 表现差不多。&lt;/p>
&lt;p>作者还对 StreamlingLLM 进行了可视化，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-streamingllm/attention_sink-visualization-attention-logits.png"
width="1153"
height="214"
loading="lazy"
alt="Visualization of attention logits with StreamingLLM"
class="gallery-image"
data-flex-grow="538"
data-flex-basis="1293px"
>&lt;/p>
&lt;p>作者进一步评估了 StreamingLLM 在下游任务上的表现，我们主要关注一下 ARC 上的表现，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-streamingllm/attention_sink-ARC-performance.png"
width="808"
height="255"
loading="lazy"
alt="Accuracy on the ARC"
class="gallery-image"
data-flex-grow="316"
data-flex-basis="760px"
>&lt;/p>
&lt;p>可以看到，full attention 出现了 OOM error, 而 sliding window attention 虽然避免了 OOM 的问题，但是其表现非常差。而 StreamingLLM 则进一步提高了 Sliding Window attention 的表现。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 StreamingLLM, 一个在 Sliding window attention 中加入 sink token 来避免超过 cache size 之后模型表现急剧下降的问题。作者详细介绍了 attention sink 现象以及解决方法。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://openreview.net/forum?id=NG7sS51zVF" target="_blank" rel="noopener"
>Efficient Streaming Language Models with Attention Sinks&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on gpt-oss</title><link>https://maosong.website/p/notes-on-gpt-oss/</link><pubDate>Tue, 19 Aug 2025 16:14:56 +0800</pubDate><guid>https://maosong.website/p/notes-on-gpt-oss/</guid><description>&lt;p>openAI 发布了 gpt-oss 大语言模型，包含 120B-A5.1B 以及 20.9B-A3.6B 两个 size, 作者强调了模型的 instruction following, tool use, 以及 adaptive thinking 能力&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;p>gpt-oss 系列是一个基于 MoE transformer 架构的 LLM. 架构中交替使用 sliding window attention 和 full attention, sliding window size 为 128 token, 架构图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gpt-oss/gpt-oss-architecture.png"
width="876"
height="789"
loading="lazy"
alt="gpt-oss-architecture"
class="gallery-image"
data-flex-grow="111"
data-flex-basis="266px"
>&lt;/p>
&lt;p>模型的配置如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>120B&lt;/th>
&lt;th>20B&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Date&lt;/td>
&lt;td>2025/8/5&lt;/td>
&lt;td>2025/8/5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Total Parameters&lt;/td>
&lt;td>116B&lt;/td>
&lt;td>20B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Activated Parameters&lt;/td>
&lt;td>5.13B&lt;/td>
&lt;td>3.61B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># MoE Layers&lt;/td>
&lt;td>36&lt;/td>
&lt;td>24&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Hidden Dim&lt;/td>
&lt;td>2880&lt;/td>
&lt;td>2880&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MoE Intermediate Dim&lt;/td>
&lt;td>2880&lt;/td>
&lt;td>2880&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Attention&lt;/td>
&lt;td>GQA+sliding window&lt;/td>
&lt;td>GQA+sliding window&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Attention Head Dim&lt;/td>
&lt;td>64&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Attention bias&lt;/td>
&lt;td>True&lt;/td>
&lt;td>True&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Attention Heads&lt;/td>
&lt;td>64&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Key-Value Heads&lt;/td>
&lt;td>8&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Experts (total)&lt;/td>
&lt;td>128&lt;/td>
&lt;td>32&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Experts Active Per Token&lt;/td>
&lt;td>4&lt;/td>
&lt;td>4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Shared Experts&lt;/td>
&lt;td>0&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>QK-Norm&lt;/td>
&lt;td>No&lt;/td>
&lt;td>No&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>在架构上，gpt-oss 做的主要改变有：&lt;/p>
&lt;ol>
&lt;li>Q, K, V projection layer, expert layer, routing layer 都使用了 bias&lt;/li>
&lt;li>修改了 expert layer 中 SwiGLU 的定义&lt;/li>
&lt;li>attention 中额外使用了一个 attention sink&lt;/li>
&lt;/ol>
&lt;h4 id="swiglu">&lt;a href="#swiglu" class="header-anchor">&lt;/a>SwiGLU
&lt;/h4>&lt;p>大多数模型使用的基于 SwiGLU 的 MLP 定义如下&lt;/p>
$$
y = W_2(W_3x \odot \mathrm{SwiGLU}(W_1x))
$$&lt;p>其中 $\mathrm{SwiGLU}(x)=x\odot\mathrm{sigmoid}(x)$, 在 gpt-oss 模型中，作者首先定义了两个常数 $\alpha=1.702$, $\mathrm{limit}=7.0$, 然后 SwiGLU MLP 的定义如下&lt;/p>
$$
\begin{aligned}
o_1&amp;=W_1x+b_1,\\
o_3&amp;=W_3x+b_3\\
o_1&amp;=\mathrm{clamp}(o_1,\max=\mathrm{limit})\\
o_3&amp;=\mathrm{clamp}(o_3,\min=-\mathrm{limit},\max=\mathrm{limit})\\
o_3&amp;= o_3\odot \mathrm{sigmoid}(\alpha\cdot o_3)\\
o_3&amp;= (o_1+1)\odot o_3\\
y &amp;= W_2o_3
\end{aligned}
$$&lt;h4 id="attention-sink">&lt;a href="#attention-sink" class="header-anchor">&lt;/a>Attention Sink
&lt;/h4>&lt;p>作者使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-streamingllm/" target="_blank" rel="noopener"
>attention sink&lt;/a> 来避免 window attention 在超过 kv cache size 之后，表现大幅度下降的问题。&lt;/p>
&lt;h3 id="quantization">&lt;a href="#quantization" class="header-anchor">&lt;/a>Quantization
&lt;/h3>&lt;p>为了降低模型的内存占用量，作者使用了 PTQ 来训练 MoE 的权重，使用的精度为 MXFP4, 这样每个参数由 4.25 bits 来表示。最终，模型的参数存储格式如下：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gpt-oss/gpt-oss-precision-format.png"
width="1152"
height="1235"
loading="lazy"
alt="precision format of gpt-oss"
class="gallery-image"
data-flex-grow="93"
data-flex-basis="223px"
>&lt;/p>
&lt;p>通过这个流程，gpt-oss-120B 可以部署在 80GB 内存的 GPU 上，gpt-oss-20B 可以部署在 16GB 内存的 GPU 上。模型各部分参数量如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Component&lt;/th>
&lt;th>120b&lt;/th>
&lt;th>20b&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>MLP&lt;/td>
&lt;td>114.71B&lt;/td>
&lt;td>19.12B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Attention&lt;/td>
&lt;td>0.96B&lt;/td>
&lt;td>0.64B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Embed + Unembed&lt;/td>
&lt;td>1.16B&lt;/td>
&lt;td>1.16B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Active Parameters&lt;/td>
&lt;td>5.13B&lt;/td>
&lt;td>3.61B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Total Parameters&lt;/td>
&lt;td>116.83B&lt;/td>
&lt;td>20.91B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Checkpoint Size&lt;/td>
&lt;td>60.8GiB&lt;/td>
&lt;td>12.8GiB&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>这里在计算激活参数的时候，没有没有考虑 embedding 的参数量。&lt;/p>
&lt;h3 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h3>&lt;p>预训练细节不多，主要是使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-flashattention/" target="_blank" rel="noopener"
>flash attention&lt;/a> 进行加速计算，使用了 Triton 进行了 kernel 的优化，gpt-oss-120b 训练了 120M H100-hours, gpt-oss-20B 的训练时间是 gpt-oss-120Bd 的十分之一左右&lt;/p>
&lt;h3 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h3>&lt;p>post-training 的数据包括 coding, math 以及 science 等，主要使用 RL 进行训练&lt;/p>
&lt;p>作者介绍了以下 post-training 使用的格式，即 &lt;code>harmony chat format&lt;/code>. 角色的优先级如下所示&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">System &amp;gt; Developer &amp;gt; User &amp;gt; Assistant &amp;gt; Tool
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>作者还加入了 channels 来限制可以使用的信息，比如使用 &lt;code>analysis&lt;/code> 来表示 CoT tokens, 使用 &lt;code>commentary&lt;/code> 来表示 function calling 等，一个具体的例子如下：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gpt-oss/gpt-oss-chat-template.png"
width="1351"
height="821"
loading="lazy"
alt="gpt-oss-chat-template"
class="gallery-image"
data-flex-grow="164"
data-flex-basis="394px"
>&lt;/p>
&lt;h2 id="evaluation">&lt;a href="#evaluation" class="header-anchor">&lt;/a>Evaluation
&lt;/h2>&lt;p>gpt-oss 系列的表现如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gpt-oss/gpt-oss-performance.png"
width="1179"
height="1106"
loading="lazy"
alt="performance of gpt-oss"
class="gallery-image"
data-flex-grow="106"
data-flex-basis="255px"
>&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者提出了 gpt-oss 系列大语言模型，gpt-oss 在架构上与已有的主流模型架构如 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a>, &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v3/" target="_blank" rel="noopener"
>DeepSeek-V3&lt;/a> 等都有一定区别&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://openai.com/index/introducing-gpt-oss/" target="_blank" rel="noopener"
>technical report&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on QK-Norm</title><link>https://maosong.website/p/notes-on-qk-norm/</link><pubDate>Wed, 13 Aug 2025 16:12:11 +0800</pubDate><guid>https://maosong.website/p/notes-on-qk-norm/</guid><description>&lt;p>作者提出了 QK norm, 一个解决 softmax 注意力权重不稳定的 scaling 算法。&lt;/p>
&lt;h2 id="problem-definition">&lt;a href="#problem-definition" class="header-anchor">&lt;/a>Problem Definition
&lt;/h2>&lt;p>Softmax 可以用于将 logits 转化为一个概率分布，但是 softmax 问题是输入的微小差别会对输出产生巨大影响，甚至会 mask 掉其他信号。因此我们需要选取合适的缩放因子，来解决 softmax 的极端值问题。SDPA 的可视化结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qk-norm/QK-Norm-SDPA-attention.png"
width="1351"
height="398"
loading="lazy"
alt="SDPA attention visualization"
class="gallery-image"
data-flex-grow="339"
data-flex-basis="814px"
>&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;p>作者首先提出了 QKNorm, 其使用了一个可学习的 scaling 参数来控制 $QK^T$ 的范围，进而让 attention 的 pattern 更加分散。&lt;/p>
&lt;p>作者首先回顾了以下已有的进展，主要是三点：&lt;/p>
&lt;ol>
&lt;li>FixNorm: 将 word embedding 限制为单位长度&lt;/li>
&lt;li>PerNorm: 使用 Pre-norm 替换 Post-norm&lt;/li>
&lt;li>ScaleNorm: 使用 $\ell_2$ normalization 替换 LayerNorm, 并乘以一个可学习的 scaling 参数。&lt;/li>
&lt;/ol>
&lt;p>作者基于这三点进行了改进，改进后的 attention 定义如下&lt;/p>
$$
\mathrm{softmax}\left(g\cdot \hat{Q}\hat{K}^T\right)V
$$&lt;p>其中,&lt;/p>
$$
\hat{Q} = [\frac{q_1}{\|q_1\|_2},\dots,\frac{q_m}{\|q_m\|_2}], \hat{K} = [\frac{k_1}{\|k_1\|_2},\dots,\frac{k_n}{\|k_n\|_2}]
$$&lt;p>是对原始的 $Q, K$ 按列进行 $\ell_2$ normalization 得到的结果， $g$ 是一个可学习的参数，其初始化值为&lt;/p>
$$
g_0 = \log_2(L^2-L)
$$&lt;p>这里 $L$ 是训练数据 $97.5$ 分位。&lt;/p>
&lt;p>使用这种动态缩放之后，attention 的分布变得更加分散了，结果如下图所示：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qk-norm/QK-norm-normalized-attention.png"
width="1366"
height="406"
loading="lazy"
alt="QK normalized attention"
class="gallery-image"
data-flex-grow="336"
data-flex-basis="807px"
>&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://aclanthology.org/2020.findings-emnlp.379.pdf" target="_blank" rel="noopener"
>Query-Key Normalization for Transformers&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on GLM-4.5</title><link>https://maosong.website/p/notes-on-glm-4.5/</link><pubDate>Wed, 13 Aug 2025 12:27:48 +0800</pubDate><guid>https://maosong.website/p/notes-on-glm-4.5/</guid><description>&lt;p>智谱 AI 提出了 GLM4.5, 包含 GLM4.5 和 GLM-4.5-Air,两个 MoE LLM. 模型大小分别为 355B-A22B 和 106B-A12B, GLM4.5 主要关注 agentic, reasoning 以及 coding 三个领域。&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者认为，通用模型有三个关键能力，即 ARC：&lt;/p>
&lt;ol>
&lt;li>Agent: 与外部工具以及真实世界进行交互&lt;/li>
&lt;li>Reasoning: 解决数学和科学领域的复杂问题&lt;/li>
&lt;li>Coding: 解决真实世界软件工程相关问题&lt;/li>
&lt;/ol>
&lt;p>已有的商业模型如 o1/o3, Claude Sonnet 4 已经在 ARC 上达到了非常好的表现，但是开源模型仍然比较稀缺&lt;/p>
&lt;p>基于这个目标，作者就提出了 GLM4.5 和 GLM-4.5-Air, 来统一完成三个不同的目标。&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h2 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h2>&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;p>GLM-4.5 是一个基于 MoE 架构的 LLM, 架构与 DeepSeek-MoE 相似，作者做了如下几点改变：&lt;/p>
&lt;ol>
&lt;li>在 MoE layer 中，使用了 loss-free balance routing, 然后使用了 sigmoid function 作为 routing score 的 normalization.&lt;/li>
&lt;li>与 &lt;a class="link" href="https://maosong.website/p/notes-on-kimi-k2/" target="_blank" rel="noopener"
>Kimi-k2&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v3/" target="_blank" rel="noopener"
>DeepSeek-V3&lt;/a> 相比，作者降低了 head dimension, 提升了 number of layers. 作者认为更深的模型更有利于提高模型的 Reasoning 表现&lt;/li>
&lt;li>attention 上，作者使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-gqa/" target="_blank" rel="noopener"
>GQA&lt;/a>, 对于 #RoPE, 作者使用了 partial RoPE, 只旋转每个 token 的前半部分， 作者还将 attention heads 的个数增加到了 2.5 倍，作者发现增加 attention heads 可以提高模型的 Reasoning 表现&lt;/li>
&lt;li>作者还使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-qk-norm/" target="_blank" rel="noopener"
>QK-Norm&lt;/a> 来防止 attention logits 爆炸&lt;/li>
&lt;li>作者还使用了一个 MoE layer 作为 MTP layer 来支持 speculative decoding.&lt;/li>
&lt;/ol>
&lt;p>模型与 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v3/" target="_blank" rel="noopener"
>DeepSeek-V3&lt;/a> 和 Kimi-k2 的对比如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>GLM-4.5&lt;/th>
&lt;th>GLM-4.5-Air&lt;/th>
&lt;th>Step 3&lt;/th>
&lt;th>Kimi K2&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Date&lt;/td>
&lt;td>2025/8/8&lt;/td>
&lt;td>2025/8/8&lt;/td>
&lt;td>2025/7/25&lt;/td>
&lt;td>2025/7/28&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Total Parameters&lt;/td>
&lt;td>355B&lt;/td>
&lt;td>106B&lt;/td>
&lt;td>316B&lt;/td>
&lt;td>1043B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Activated Parameters&lt;/td>
&lt;td>32B&lt;/td>
&lt;td>12B&lt;/td>
&lt;td>38B&lt;/td>
&lt;td>32B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Dense Layers&lt;/td>
&lt;td>3&lt;/td>
&lt;td>1&lt;/td>
&lt;td>5&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># MoE Layers&lt;/td>
&lt;td>89&lt;/td>
&lt;td>45&lt;/td>
&lt;td>56&lt;/td>
&lt;td>60&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># MTP Layers&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;td>0&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Hidden Dim&lt;/td>
&lt;td>5120&lt;/td>
&lt;td>4096&lt;/td>
&lt;td>7168&lt;/td>
&lt;td>7168&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Dense Intermediate Dim&lt;/td>
&lt;td>12288&lt;/td>
&lt;td>10944&lt;/td>
&lt;td>18432&lt;/td>
&lt;td>18432&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MoE Intermediate Dim&lt;/td>
&lt;td>1536&lt;/td>
&lt;td>1408&lt;/td>
&lt;td>5120&lt;/td>
&lt;td>2048&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Attention&lt;/td>
&lt;td>GQA&lt;/td>
&lt;td>GQA&lt;/td>
&lt;td>MFA&lt;/td>
&lt;td>MLA&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Attention Head Dim&lt;/td>
&lt;td>128&lt;/td>
&lt;td>128&lt;/td>
&lt;td>256&lt;/td>
&lt;td>192&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Attention Heads&lt;/td>
&lt;td>96&lt;/td>
&lt;td>96&lt;/td>
&lt;td>64&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Key-Value Heads&lt;/td>
&lt;td>8&lt;/td>
&lt;td>8&lt;/td>
&lt;td>1&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>scoring&lt;/td>
&lt;td>sigmoid&lt;/td>
&lt;td>sigmoid&lt;/td>
&lt;td>softmax&lt;/td>
&lt;td>softmax&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Experts (total)&lt;/td>
&lt;td>160&lt;/td>
&lt;td>128&lt;/td>
&lt;td>48&lt;/td>
&lt;td>384&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Experts Active Per Token&lt;/td>
&lt;td>8&lt;/td>
&lt;td>8&lt;/td>
&lt;td>3&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Shared Experts&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>QK-Norm&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>No&lt;/td>
&lt;td>No&lt;/td>
&lt;td>No&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="pre-training-data">&lt;a href="#pre-training-data" class="header-anchor">&lt;/a>Pre-training Data
&lt;/h3>&lt;p>预训练数据包括四个方面&lt;/p>
&lt;ol>
&lt;li>Web: 过滤低质量数据和使用模版产生的数据&lt;/li>
&lt;li>Multilingual: 基于 webpages 和 Fineweb-2&lt;/li>
&lt;li>Code: 基于 GitHub 和其他代码平台，作者使用了 [[Fill in the middle]] 来训练模型。&lt;/li>
&lt;li>Math &amp;amp; Scirence: 训练一个 classifier 来给数据进行打分。&lt;/li>
&lt;/ol>
&lt;p>最终，预训练数据一共包括 &lt;strong>23T token&lt;/strong>.&lt;/p>
&lt;h3 id="pre-training-recipe">&lt;a href="#pre-training-recipe" class="header-anchor">&lt;/a>Pre-training Recipe
&lt;/h3>&lt;p>&lt;img src="https://maosong.website/p/notes-on-glm-4.5/GLM4_5-pre-training.png"
width="1173"
height="406"
loading="lazy"
alt="Pre-training recipe of GLM4.5"
class="gallery-image"
data-flex-grow="288"
data-flex-basis="693px"
>&lt;/p>
&lt;p>预训练包括 2 个阶段:&lt;/p>
&lt;ol>
&lt;li>Pre-training: 使用网页数据进行训练&lt;/li>
&lt;li>Mid-training: 加入 code, math, science 数据进行训练，在这个阶段，作者使用了 repo-level 的 code 数据，合成的 reasoning 数据以及长上下文数据。作者将模型上下文从 4K 扩展到 32K，然后在扩展到 128K.&lt;/li>
&lt;/ol>
&lt;p>作者在 pre-training 的时候使用了 random truncation, 在 mid-training 的时候使用了 best-fit packing 技巧&lt;/p>
&lt;p>训练时，与 Kimi-k2 一样，作者使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-moonlight/" target="_blank" rel="noopener"
>Muon&lt;/a> 作为优化器。作者使用了 cosine decay schedule. batch size 从 16M token 到 64M token.&lt;/p>
&lt;h2 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h2>&lt;p>Post-training 分为两个阶段：&lt;/p>
&lt;ul>
&lt;li>Stage 1, Expert Training. 构建 agent, reasoning, General chat 三个 domain 的专家模型&lt;/li>
&lt;li>Stage 2, Unified Training. 使用 self-distillation 来汇总多个模型的能力&lt;/li>
&lt;/ul>
&lt;p>训练框架如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-glm-4.5/GLM4.5-post-training-framework.png"
width="1337"
height="1001"
loading="lazy"
alt="Post-training of GLM4.5"
class="gallery-image"
data-flex-grow="133"
data-flex-basis="320px"
>&lt;/p>
&lt;h3 id="sft">&lt;a href="#sft" class="header-anchor">&lt;/a>SFT
&lt;/h3>&lt;p>两个 stage 都由 SFT 开始，&lt;/p>
&lt;ul>
&lt;li>在 Stage 1 里，SFT 的目标是让 expert model 掌握初步的 chat, reasoning 以及 tool-use 的能力。作者使用了一小部分包含 CoT 的 SFT 数据进行训练&lt;/li>
&lt;li>在 Stage 2 中，SFT 的目标是将不同的 expert model 蒸馏到一个模型中，作者使用了百万级的数据，包含 reasoning 任务和通用的 chat 数据，来训练模型的 hybrid reasoning 能力&lt;/li>
&lt;/ul>
&lt;p>在训练模型的 tool-use 能力是，作者发现，function call 在 code 场景下会出现混淆，提高了模型的学习成本。因此，作者的解决方法是使用了类似 XML 的 special token tags&lt;/p>
&lt;blockquote>
&lt;p>Recall
与之相反，Kimi-K2 认为模板应该尽可能简洁，因此 Kimi 采取了 TypeScript 作为 function call 的语言&lt;/p>
&lt;/blockquote>
&lt;p>从专家模型进行采样是，作者进行了数据过滤。还对数据进行了分级结果发现，使用难题进行训练可以提升模型 $2\%\sim4\%$ 的表现，多次采样也可以提高模型的表现&lt;/p>
&lt;p>Agentic SFT 数据的构建包括四个步骤：&lt;/p>
&lt;ol>
&lt;li>Agentic Framework and Tool Collection: 收集 MCP 和 tool API&lt;/li>
&lt;li>Task Synthesis: 合成不同的 agentic 任务&lt;/li>
&lt;li>Trajectory Generation: 采样生成的 rollout&lt;/li>
&lt;li>Quality Filtering: 过滤低质量的数据&lt;/li>
&lt;/ol>
&lt;h3 id="rl">&lt;a href="#rl" class="header-anchor">&lt;/a>RL
&lt;/h3>&lt;h4 id="reasoning-rl">&lt;a href="#reasoning-rl" class="header-anchor">&lt;/a>Reasoning RL
&lt;/h4>&lt;p>这个阶段使用了 GRPO 算法进行训练，与 &lt;a class="link" href="https://maosong.website/p/notes-on-dapo/" target="_blank" rel="noopener"
>DAPO&lt;/a> 一样，作者去除了损失函数中的 KL divergence。&lt;/p>
&lt;p>首先，作者探究了课程学习对模型表现的影响，结果发现，课程学习可以有效提高模型的性能。因此，作者构建了一个 2 阶段的课程学习框架。实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-glm-4.5/GLM4.5-curriculum-RL.png"
width="995"
height="504"
loading="lazy"
alt="Performance of curriculum RL"
class="gallery-image"
data-flex-grow="197"
data-flex-basis="473px"
>&lt;/p>
&lt;p>可以看到，在第二个阶段，模型可以进一步通过更难的题目获得提升。&lt;/p>
&lt;p>其次，作者探究了以下渐进式扩展模型上下文对模型表现的影响。DeepScaleR 认为，逐步提高模型的上下文长度，可以有效提高模型的表现。但是，本文确认为这种方法会损害模型的性能，原因在于，模型在 SFT 阶段的上下文长度就是 64K, 如果我们降低模型的上下文长度，这会导致训练数据分布不一致，从而影响模型的长上下文表现。因此作者直接在 64K 的上下文上进行训练。&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-glm-4.5/GLM4.5-multi-stage-context-extension.png"
width="1159"
height="453"
loading="lazy"
alt="Ablation on progressive context extension"
class="gallery-image"
data-flex-grow="255"
data-flex-basis="614px"
>&lt;/p>
&lt;p>接下来，作者探究了以下采样温度对模型表现的影响，温度太低会导致模型探索能力下降，太高的话会导致输出质量下降。因此作者动态调整采样温度来平衡模型的性能以及探索能力。&lt;/p>
&lt;blockquote>
&lt;p>[!tip]
Kimi-K2 认为随着 RL 训练的进行，我们应该逐步降低采样温度来稳定模型的表现&lt;/p>
&lt;/blockquote>
&lt;p>最后，作者分析了以下 code 以及 Science RL 中的一些问题。对于 code RL, 作者发现，我们应该在 sequence 层面而不是 token 层面进行平均。对于 Science RL, 作者强调了高质量数据的重要性。实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-glm-4.5/GLM4.5-code-science-RL.png"
width="1146"
height="450"
loading="lazy"
alt="Ablation on Code and Science RL"
class="gallery-image"
data-flex-grow="254"
data-flex-basis="611px"
>&lt;/p>
&lt;h4 id="agent-rl">&lt;a href="#agent-rl" class="header-anchor">&lt;/a>Agent RL
&lt;/h4>&lt;p>作者主要关注 web-search 以及 code generation 两个任务。对于 web-search, 作者构建了一个数据合成 pipeline, 用于生成 multi-step reasoning 的 QA 数据。构建过程包括基于知识图谱的 multi-hop reasoning 和 human-in-the-loop 的内容提取。对于 code generation, 作者基于 GitHub 的 PR 以及 issues 构建了 benchmark&lt;/p>
&lt;p>RL 的训练目标如下&lt;/p>
$$
\mathcal{L}(\theta) = \mathbb{E}_{x\sim\mathcal{D}}\left[\frac1K\sum_{i=1}^K(r(x,y_i) - \bar{r}(x))\right]
$$&lt;p>其中 $(x,y_i)$ 是基于 $\pi_{\mathrm{old}}$ 采样的 trace, $\bar{r}(x) = 1/k\sum_{i=1}^Kr(x,y_i)$ 是平均的 reward. 计算损失时，只有模型的回答参与计算。&lt;/p>
&lt;p>作者发现，通过训练模型的 web-search 以及 code generation 能分，模型在 tool-use 以及 coding 任务上的表现也有了提升。作者还是用了 format penalty 来保证模型输出格式的正确性。如果格式不对的话，模型获得的奖励是 0&lt;/p>
&lt;blockquote>
&lt;p>Recall
在 &lt;a class="link" href="https://maosong.website/p/notes-on-glm-4.1v-thinking/" target="_blank" rel="noopener"
>GLM-4.V-Thinking&lt;/a> 中，作者认为应该在 cold-start SFT 阶段让模型学会格式要求，而不是在 RL 阶段加入 format reward&lt;/p>
&lt;/blockquote>
&lt;p>由于 agent RL 的训练比较耗时，为了提高训练效率。作者首先基于 SFT 模型进行 agent RL 训练，训练到一定步数之后，作者使用 self-distillation 来将能力蒸馏回 SFT model, 接下来再基于 Self-distillation 后的 SFT 模型来进行 agent RL 训练&lt;/p>
&lt;blockquote>
&lt;p>Recall
&lt;a class="link" href="https://maosong.website/p/notes-on-seed1.5-vl/" target="_blank" rel="noopener"
>Seed1.5-VL&lt;/a> 和 LlaMA3.2 都提到了使用 multi-round SFT-RL 的形式来提高模型的表现&lt;/p>
&lt;/blockquote>
&lt;p>作者还发现，随着交互轮数的提升，模型的表现也有相应提升。实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-glm-4.5/GLM4.5-interaction-turns.png"
width="696"
height="428"
loading="lazy"
alt="Interaction turn scaling"
class="gallery-image"
data-flex-grow="162"
data-flex-basis="390px"
>&lt;/p>
&lt;h4 id="general-rl">&lt;a href="#general-rl" class="header-anchor">&lt;/a>General RL
&lt;/h4>&lt;p>General RL 用于提高模型的整体表现，解决潜在的问题以及提升关键能力，作者主要使用了 RLHF 和 RLAIF 两种方法&lt;/p>
&lt;p>对于 Holistic RL, 作者收集了 5000 条 prompt, reward 基于人类反馈和 AI 反馈。人类反馈用于训练一个 reward model, 对于 AI 反馈，作者构建了 scoring rubrics. 然后作者将两种反馈结合在一起&lt;/p>
&lt;p>对于 Instruction following RL, 作者构建了基于规则的奖励，reward model 的奖励以及 critical model 的奖励。实验结果显示，这种奖励方式可以有效降低模型的 reward hacking&lt;/p>
&lt;p>对于 function calling RL, 作者使用了 step-wise rule-based RL 来提高模型表现。对于 end-to-end multi-turn RL, 作者训练了一个 expert model 来蒸馏专家到模型。&lt;/p>
&lt;p>最后，对于 Pathology RL, 作者希望通过 RL 来解决潜在的问题，比如语言混合输出，重复输出以及格式错误等。作者构建了一批模型容易出错的数据，然后来训练模型。&lt;/p>
&lt;h4 id="infra">&lt;a href="#infra" class="header-anchor">&lt;/a>Infra
&lt;/h4>&lt;p>作者针对不同任务分别构建了不同的 scheduling 模式：&lt;/p>
&lt;ul>
&lt;li>对于通用 RL 任务，作者将 training engine 和 inference engine 放在一个 worker 来提高效率&lt;/li>
&lt;li>对于 agentic RL 任务，作者将 training 和 inference engine 分开，来提高 data throughput&lt;/li>
&lt;/ul>
&lt;p>在训练时，作者使用了 BF16 精度，在推理时，作者使用了 FP8 精度来提高推理效率。&lt;/p>
&lt;p>针对 agentic RL 任务，作者还进行了优化。与 Kimi-k2 类似，作者让 inference engine 持续产出 rollout, 然后让 training engine 来更新模型权重，最后同步到 inference engine 上&lt;/p>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>整体表现如下图所示，GLm4.5 在 ARC benchmark 上的平均表现达到了第三名。&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-glm-4.5/GLM4.5-average_performance.png"
width="1071"
height="776"
loading="lazy"
alt="Average performance on ARC benchmarks"
class="gallery-image"
data-flex-grow="138"
data-flex-basis="331px"
>&lt;/p>
&lt;p>具体来看，&lt;/p>
&lt;ol>
&lt;li>在 agentic benchmark 上, GLM4.5 仅次于 o3 的表现&lt;/li>
&lt;li>在 coding benchmark 上，GLM4.5 次于 Claude Opus 4 和 Claude Sonnet 4, 排第三名&lt;/li>
&lt;li>在通用能力上，GLM&lt;/li>
&lt;/ol>
&lt;p>人工对比 coding agent 能力的结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-glm-4.5/GLM4.5-agent-coding-performance.png"
width="1143"
height="514"
loading="lazy"
alt="Comparison of GLM4.5 aginst other models"
class="gallery-image"
data-flex-grow="222"
data-flex-basis="533px"
>&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者提出了 GLM4.5， 一个基于 MoE 架构的大语言模型系列，包含 GLM4.5(355B-A22B) 和 GLM4.5-Air(106B-A12B) 两个模型，作者详细介绍了模型的架构，训练，数据和评估。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2508.06471" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on ARC-Hunyuan-Video-7B</title><link>https://maosong.website/p/notes-on-arc-hunyuan-video-7b/</link><pubDate>Tue, 12 Aug 2025 10:57:57 +0800</pubDate><guid>https://maosong.website/p/notes-on-arc-hunyuan-video-7b/</guid><description>&lt;p>腾讯 ARC LAB 提出了 ARC-Hunyuan-Video-7B, 一个针对短视频理解和推理的视频多模态大模型。&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者首先提出了 Structured video comprehension 的概念&lt;/p>
&lt;blockquote>
&lt;p>the ability to decompose a video into its constituent events and narrative elements with temporal precision.&lt;/p>
&lt;/blockquote>
&lt;p>视频信息包含了 dense visual elements, 比如文字等信息，还有 rich audio streams, 比如音效，音乐，再就是 rapid narrative pacing, 用于强调情感。&lt;/p>
&lt;p>已有的多模态大模型对于视频的细粒度理解和推理能力存在不足。尽管 &lt;a class="link" href="https://maosong.website/p/notes-on-keye-vl/" target="_blank" rel="noopener"
>Keye-VL&lt;/a> 也是一个短视频理解多模态大模型，但是其并没有利用 audio 模态的信息。其他的 audio-visual LLM 虽然可以处理 audio 模态的信息，但是他们主要集中于通用场景的视频，这类视频的特点是：叙述节奏慢，信息密度低。因此，对于叙述节奏快，信息丰富的短视频，这些模型表现就比较差。&lt;/p>
&lt;p>基于以上这些问题，作者提出了 ARC-Hunyuan-Video-7B, 一个基于 Hunyuan-7B vision-language model 的短视频多模态大模型，其主要做了两点改进：&lt;/p>
&lt;ol>
&lt;li>加入了一个 audio encoder 用于提取 audio 信息，然后对 audio 信息与视觉信息进行同步&lt;/li>
&lt;li>使用了一个 timestamp overlap 机制，将时间戳印在视频的帧上，帮助模型理解事件的时序信息&lt;/li>
&lt;/ol>
&lt;p>作者还涉及了多阶段的训练策略，来提高模型的表现。最后在测试时，对于一个 1 分钟的视频，模型在 H20 GPU 上进需要 10 秒就可以处理完毕。&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;p>&lt;img src="https://maosong.website/p/notes-on-arc-hunyuan-video-7b/ARC-Hunyuan-Video-architecture.png"
width="682"
height="467"
loading="lazy"
alt="Architecture of ARC-Hunyuan-Video-7B"
class="gallery-image"
data-flex-grow="146"
data-flex-basis="350px"
>&lt;/p>
&lt;p>模型基于 Hunyuan-7B-VLM 开发得到，&lt;/p>
&lt;ul>
&lt;li>Visual Encoding: 作者将时间戳以 &lt;code>HH:MM:SS&lt;/code> 的形式印在视频的帧上。视频的采样率为 1FPS, 超过 150s 的视频会进行均匀采样降低至 150s. 每一帧会 resize 到 $640\times 640$, 最后每一帧输出 112 token&lt;/li>
&lt;li>Audio Encoding: 使用 Whisper 作为编码器。小于 300s 的视频，会按照 30s 为单位切分为不同的 chunk, 大于 300s 的视频，会分割为 150 个 chunk, 然后每个 chunk 只保留开头 2s 的内容。最后，使用 Whisper 进行特征提取，最后再通过 MLP 与 LLM 的特征空间进行对齐&lt;/li>
&lt;li>Visual-audio Synchronization: 先对 audio token 进行 zero padding 与 Visual token 的个数进行对齐，然后将两者相加&lt;/li>
&lt;/ul>
&lt;h3 id="training">&lt;a href="#training" class="header-anchor">&lt;/a>Training
&lt;/h3>&lt;p>&lt;img src="https://maosong.website/p/notes-on-arc-hunyuan-video-7b/ARC-Hunyuan-Video-training-overall.png"
width="682"
height="467"
loading="lazy"
alt="Training pipeline of ARC-Hunyuan-Video-7B"
class="gallery-image"
data-flex-grow="146"
data-flex-basis="350px"
>&lt;/p>
&lt;h4 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h4>&lt;p>&lt;img src="https://maosong.website/p/notes-on-arc-hunyuan-video-7b/ARC-Hunyuan-Video-annotation.png"
width="1362"
height="289"
loading="lazy"
alt="Boostrapped annotation pipeline of ARC-Hunyuan-Video-7B"
class="gallery-image"
data-flex-grow="471"
data-flex-basis="1131px"
>&lt;/p>
&lt;p>作者首先保住了一些数据。具体流程就是，使用 Whisper-v3 来转录带时间戳的音频，然后使用 InternVL-2.5-8B [[InternVL-2.5]] 来生成细粒度的 caption. 作者还使用 CoT prompt 策略，让模型一步步生成时间的描述，创作者的想法以及如何吸引用户的 tag 等。&lt;/p>
&lt;p>预训练数据如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Category&lt;/th>
&lt;th>data&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Video description and summary&lt;/td>
&lt;td>4.5M short-form video&lt;br>0.2M public video&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Image caption and OCR&lt;/td>
&lt;td>4.7M image-text pairs&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ASR&lt;/td>
&lt;td>3.2M audio-text pairs&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Video temporal grounding&lt;/td>
&lt;td>0.5M temporally grounding instances&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Video multi-granular caption&lt;/td>
&lt;td>50K high-quality samples&lt;br>80K in-house videos&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>训练包含两个 stage:&lt;/p>
&lt;ol>
&lt;li>Stage 1: 使用 ASR 数据训练模型接受音频输入的能力，为了避免视觉模态能力下降，作者还加入了 Image-text pair data 来训练，缺失的模态用 zero padding 来补齐&lt;/li>
&lt;li>Stage 2: 使用全部数据进行训练，仅训练 MLP 和 LLM&lt;/li>
&lt;/ol>
&lt;h4 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h4>&lt;p>作者首先进行了消融实验，探究 human-annotated 数据对模型表现的影响。作者收集了 140 条人类标注的数据，然后基于这些数据进行消融实验：&lt;/p>
&lt;ol>
&lt;li>直接使用人类标注数据进行 SFT, 模型表现变化不大&lt;/li>
&lt;li>直接使用人类标注数据作为正样本，合成数据作为负样本进行 DPO, 模型表现变化也不大&lt;/li>
&lt;/ol>
&lt;p>作者分析原因认为，&lt;strong>人类标注数据和合成数据之间存在 distribution shift&lt;/strong>.&lt;/p>
&lt;p>受 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-r1/" target="_blank" rel="noopener"
>DeepSeek-R1&lt;/a> 启发，作者决定使用 GRPO 算法来提高模型的表现，训练任务包含两个：&lt;/p>
&lt;ol>
&lt;li>multi-dimensional Multi-choide QA: 提高模型的视频理解能力&lt;/li>
&lt;li>Temporal video grounding: 提高模型的时序感知能力&lt;/li>
&lt;/ol>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Stage&lt;/th>
&lt;th>Data&lt;/th>
&lt;th>Module&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>SFT&lt;/td>
&lt;td>MCQ:&lt;br> - 460K open-ended QA&lt;br> - 70K MCQA&lt;br> - 20K QA&lt;br>Grounding:&lt;br> - 10K academic&lt;br> - 5K real-world&lt;br>General:&lt;br> - 45K description&lt;br> - 12K caption&lt;/td>
&lt;td>MLP+LLM&lt;/td>
&lt;td>提高指令跟随能力&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Cold Start SFT&lt;/td>
&lt;td>- 90K MCQA&lt;br>- 18K temporal grounding&lt;br>- 20K open-ended QA&lt;br>- 15K summarization&lt;br>- 3K chapter-level captioning&lt;/td>
&lt;td>MLP+LLM&lt;/td>
&lt;td>初步激活模型的 reas 能力&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RL&lt;/td>
&lt;td>- 100K MCQ&lt;br>- 35K temporal grounding&lt;/td>
&lt;td>LLM&lt;/td>
&lt;td>提升模型的 reasoning 能力&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SFT&lt;/td>
&lt;td>- 25K human-annotated subjective question&lt;br>- 100K MCQ with CoT&lt;br>- 50K temporal grounding with reasoning traces&lt;/td>
&lt;td>-&lt;/td>
&lt;td>使用人类标注数据进一步提高模型的能力&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>作者构建了一个评估短视频理解能力的 benchmark: ShortVid-Bench, 评估以下方面：&lt;/p>
&lt;ol>
&lt;li>Temporal Reasoning and Localization&lt;/li>
&lt;li>Affective Intent Classification&lt;/li>
&lt;li>Creator Intent Taxonomy&lt;/li>
&lt;li>Narrative Comprehension&lt;/li>
&lt;li>Humor &amp;amp; Meme Deconstruction&lt;/li>
&lt;li>Creative Innovation Analysis&lt;/li>
&lt;/ol>
&lt;p>对比的模型包括 Qwen2.5-VL-7B, Qwen2.5-omni-7B 和 Keye-VL-8B&lt;/p>
&lt;p>评估结果如下：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-arc-hunyuan-video-7b/ARC-Hunyuan-Video-performance.png"
width="1367"
height="244"
loading="lazy"
alt="Performance of ARC-Hunyuan-Video-7B"
class="gallery-image"
data-flex-grow="560"
data-flex-basis="1344px"
>&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者提出了 ARC-Hunyuan-Video-7B, 一个针对短视频的视频理解多模态大模型。作者详细介绍了模型的架构，训练数据以及评估。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2507.20939" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/TencentARC/ARC-Hunyuan-Video-7B" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on GQA</title><link>https://maosong.website/p/notes-on-gqa/</link><pubDate>Thu, 07 Aug 2025 18:08:36 +0800</pubDate><guid>https://maosong.website/p/notes-on-gqa/</guid><description>&lt;p>Google Research 在 23 年 12 月份提出了 Group Query Attention (GQA), 一个提升 multi-head attention 效率的方法。GQA 自 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2/" target="_blank" rel="noopener"
>Qwen2&lt;/a> 系列开始被应用。&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>Multi-head attention (MHA) 的问题在于 inference 阶段，每次 decoding，都需要重新加载 attention 模块中 query layer, key layer 和 value layer 的权重，而加载权重会受带宽限制。&lt;/p>
&lt;p>已有的工作有 &lt;a class="link" href="https://maosong.website/p/notes-on-mqa/" target="_blank" rel="noopener"
>MQA&lt;/a>, 也就是我们把多个 head 的 key layer 以及 value layer 压缩成一个，这样对于 $h$ 个 head 的 attention，我们有 $h$ 个 query layer，$1$ 个 key layer 以及 1 个 value layer. 但是 MQA 的问题在于其会导致性能下降，而且训练过程会不稳定。&lt;/p>
&lt;p>因此，在本文中作者就作出了两点贡献：&lt;/p>
&lt;ol>
&lt;li>如何将一个 MHA 模型转化为一个一个 MQA 模型&lt;/li>
&lt;li>提出了 Group Query Attention (GQA)，在保持模型性能的同时，提高计算效率&lt;/li>
&lt;/ol>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="uptraining">&lt;a href="#uptraining" class="header-anchor">&lt;/a>Uptraining
&lt;/h3>&lt;p>将 MHA 模型转化为 MQA 模型分为两步：&lt;/p>
&lt;ol>
&lt;li>将 MHA 权重转化为 MQA 权重&lt;/li>
&lt;li>额外的预训练&lt;/li>
&lt;/ol>
&lt;p>具体来讲，作者使用了一个 mean pooling 的方法，来将不同 head 的 query layer 以及 key layer 的权重转化为 MQA 对应 layer 的权重。然后作者 pre-training 若干步来让模型适应新的结构。&lt;/p>
&lt;h3 id="gqa">&lt;a href="#gqa" class="header-anchor">&lt;/a>GQA
&lt;/h3>&lt;p>GQA 的思路在于在 MHA 和 MQA 之间达到一个平衡，也就是说我们将 key layer 和 value layer 进行分组，每个组内共享一个 key layer 和 value layer, 我们假设有 $h$ 个 head，$G$ 个 group，那么&lt;/p>
&lt;ol>
&lt;li>$G=1$ 时，所有的 head 共享一个 key layer 和一个 value layer, 此时 GQA 等价于 MQA&lt;/li>
&lt;li>$G=H$ 时，每个 head 都有一个 key layer 和一个 value layer, 此时 GQA 等价于 MHA&lt;/li>
&lt;li>$1&lt;G&lt;H$ 时，GQA 时 MQA 和 MHA 的一个 trade-off，兼顾两者的性能与效率&lt;/li>
&lt;/ol>
&lt;p>三者的示意图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gqa/MQA_comparison_group_query.png"
width="2080"
height="784"
loading="lazy"
alt="Overview of grouped-query methods"
class="gallery-image"
data-flex-grow="265"
data-flex-basis="636px"
>&lt;/p>
&lt;h2 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h2>&lt;p>MQA 的代码也比较好理解，我们首先定义 group size，即 &lt;code>num_key_value_heads&lt;/code>, 然后基于 group size 定义对应的 key layer &lt;code>self.k_proj&lt;/code> 和 value layer &lt;code>self.v_proj&lt;/code>.&lt;/p>
&lt;p>计算得到 &lt;code>key_states&lt;/code> 和 &lt;code>value_states&lt;/code> 之后，在计算 attention，即 &lt;code>eager_attention_forward&lt;/code> 的时候，我们对 &lt;code>key_states&lt;/code> 和 &lt;code>value_states&lt;/code> 进行复制，即 &lt;code>repeat_kv&lt;/code>&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;span class="lnt">49
&lt;/span>&lt;span class="lnt">50
&lt;/span>&lt;span class="lnt">51
&lt;/span>&lt;span class="lnt">52
&lt;/span>&lt;span class="lnt">53
&lt;/span>&lt;span class="lnt">54
&lt;/span>&lt;span class="lnt">55
&lt;/span>&lt;span class="lnt">56
&lt;/span>&lt;span class="lnt">57
&lt;/span>&lt;span class="lnt">58
&lt;/span>&lt;span class="lnt">59
&lt;/span>&lt;span class="lnt">60
&lt;/span>&lt;span class="lnt">61
&lt;/span>&lt;span class="lnt">62
&lt;/span>&lt;span class="lnt">63
&lt;/span>&lt;span class="lnt">64
&lt;/span>&lt;span class="lnt">65
&lt;/span>&lt;span class="lnt">66
&lt;/span>&lt;span class="lnt">67
&lt;/span>&lt;span class="lnt">68
&lt;/span>&lt;span class="lnt">69
&lt;/span>&lt;span class="lnt">70
&lt;/span>&lt;span class="lnt">71
&lt;/span>&lt;span class="lnt">72
&lt;/span>&lt;span class="lnt">73
&lt;/span>&lt;span class="lnt">74
&lt;/span>&lt;span class="lnt">75
&lt;/span>&lt;span class="lnt">76
&lt;/span>&lt;span class="lnt">77
&lt;/span>&lt;span class="lnt">78
&lt;/span>&lt;span class="lnt">79
&lt;/span>&lt;span class="lnt">80
&lt;/span>&lt;span class="lnt">81
&lt;/span>&lt;span class="lnt">82
&lt;/span>&lt;span class="lnt">83
&lt;/span>&lt;span class="lnt">84
&lt;/span>&lt;span class="lnt">85
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">repeat_kv&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n_rep&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">batch&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_key_value_heads&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">slen&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">head_dim&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">n_rep&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">hidden_states&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="p">:]&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">expand&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">batch&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_key_value_heads&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n_rep&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">slen&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">head_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reshape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">batch&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_key_value_heads&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">n_rep&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">slen&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">head_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">eager_attention_forward&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">module&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">query&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">key&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">value&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attention_mask&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">scaling&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">float&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dropout&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">float&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">0.0&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">**&lt;/span>&lt;span class="n">kwargs&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Unpack&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">TransformersKwargs&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">key_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">repeat_kv&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">key&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">module&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_key_value_groups&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">value_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">repeat_kv&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">value&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">module&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_key_value_groups&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attn_weights&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">matmul&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">query&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">key_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transpose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">scaling&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">attention_mask&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">causal_mask&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">attention_mask&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="p">:&lt;/span> &lt;span class="n">key_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">]]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attn_weights&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">attn_weights&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">causal_mask&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attn_weights&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">functional&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">softmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">attn_weights&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">query&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dtype&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attn_weights&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">functional&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dropout&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">attn_weights&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">dropout&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">training&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">module&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">training&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attn_output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">matmul&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">attn_weights&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">value_states&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attn_output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">attn_output&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transpose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">contiguous&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">attn_output&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">attn_weights&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Qwen3Attention&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Qwen3Config&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">layer_idx&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="fm">__init__&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">getattr&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;head_dim&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_attention_heads&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">scaling&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="o">**-&lt;/span>&lt;span class="mf">0.5&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_attention_heads&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">attention_bias&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">k_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_key_value_heads&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">attention_bias&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">v_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_key_value_heads&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">attention_bias&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">o_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_attention_heads&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">attention_bias&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">position_embeddings&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">tuple&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attention_mask&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">past_key_value&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">Cache&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">cache_position&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">LongTensor&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">**&lt;/span>&lt;span class="n">kwargs&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Unpack&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">FlashAttentionKwargs&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">tuple&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">tuple&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">]]]:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">input_shape&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[:&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_shape&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">input_shape&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">query_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_norm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_shape&lt;/span>&lt;span class="p">))&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transpose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">key_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">k_norm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">k_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_shape&lt;/span>&lt;span class="p">))&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transpose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">value_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">v_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_shape&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transpose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">cos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sin&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">position_embeddings&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">query_states&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">key_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">apply_rotary_pos_emb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">query_states&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">key_states&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">cos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sin&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attn_output&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">attn_weights&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">eager_attention_forward&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">query_states&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">key_states&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">value_states&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attention_mask&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dropout&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.0&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">scaling&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">scaling&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sliding_window&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sliding_window&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="c1"># diff with Llama&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">**&lt;/span>&lt;span class="n">kwargs&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attn_output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">attn_output&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reshape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">input_shape&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">contiguous&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attn_output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">o_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">attn_output&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">attn_output&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">attn_weights&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>本文中，作者提出了一个解决 multi-query attention 的 uptraining 方法，以及提出了 GQA，一个结合 MHA 表现和 MQA 效率的新型注意力机制。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2305.13245" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on MQA</title><link>https://maosong.website/p/notes-on-mqa/</link><pubDate>Thu, 07 Aug 2025 18:06:37 +0800</pubDate><guid>https://maosong.website/p/notes-on-mqa/</guid><description>&lt;p>Google 在 2019 年提出了 multi-query attention (MQA), 用于解决 MQA 内存带宽瓶颈问题。&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="background">&lt;a href="#background" class="header-anchor">&lt;/a>Background
&lt;/h3>&lt;p>对于 multi-head attention, 我们假设其 hidden size 为 $d$, 有 $h$ 个 heads, 每个 head 的 size 为 $d_h=d/h$, 输入 sequence 长度为 $n$, batch size 为 $d$. 则总的 arithmetic operations 为 $O(bnd^2)$. 总的内存访问量为 $O(bnd + bhn^2+d^2)$, 第一项是 $Q,K,V$ 的内存占用（$Q,K,V$ 分别是 query, key 和 value layer 的输出），第二项是 attention score 的占用，第三项是 query, key 和 value layer 的权重。&lt;/p>
&lt;p>因此，其 &lt;strong>Memory Access Ratio&lt;/strong> (MAR), 也就是内存访问量 与 arithmetic operations 之比为&lt;/p>
$$
O\left(\frac1k + \frac{1}{bn}\right)
$$&lt;p>对于现代的 GPU 来说，其一般算力比较强，但是内存访问带宽相对较慢，因此我们希望 MAR 越低越好，以充分发挥 GPU 的算力。&lt;/p>
&lt;h3 id="mha-analysis">&lt;a href="#mha-analysis" class="header-anchor">&lt;/a>MHA Analysis
&lt;/h3>&lt;p>在训练的时候，由于我们知道 ground truth sequence, 因此我们可以并行计算。但是在 inference 的时候，我们只能 token-by-token 进行计算，因此我们分析一下 token-by-token 场景下的 MAR&lt;/p>
&lt;p>我们整体的 arithmetic operations 还是 $O(bnd^2)$.&lt;/p>
&lt;p>但是，现在我们要调用 $n$ 次 multi-head attention, 因此我们总的内存访问量为 $O(bn^2d + nd^2)$, 第一项是 $K$ 和 $V$ , 第二项是 query, key 和 value layer 的权重。&lt;/p>
&lt;p>这种情况下，MAR 就变成了&lt;/p>
$$
O\left(\frac{n}{d} + \frac{1}{b}\right)
$$&lt;p>当 $n\approx d$ 或者 $b\approx 1$ 时，MAR 就非常接近于 1，意味着内存带宽成了一个主要的瓶颈。为了解决这个问题，我们有两种做法：&lt;/p>
&lt;ol>
&lt;li>提升 batch size $b$, 也就是同时 inference 多次&lt;/li>
&lt;li>降低 $K$ 和 $V$ 的大小&lt;/li>
&lt;/ol>
&lt;h3 id="mqa">&lt;a href="#mqa" class="header-anchor">&lt;/a>MQA
&lt;/h3>&lt;p>MQA 的做法就是第二种，也就是降低 $K$ 和 $V$ 的大小，但是 $K,V$ 分别是 key 和 value layer 的输出，要降低输出大小，我们就必须改变 key 和 value layer 的 size。基于这个考虑，作者在所有的 head 上共享了一个 key 和 value layer，也就是说，原来&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">k_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_heads&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">head_dim&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># (d, n*d_h)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">v_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_heads&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">head_dim&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># (d, n*d_h)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>现在在 MQA 里，其变成了&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">k_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">head_dim&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># (d, n*d_h)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">v_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">head_dim&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># (d, n*d_h)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="mqa-analysis">&lt;a href="#mqa-analysis" class="header-anchor">&lt;/a>MQA Analysis
&lt;/h3>&lt;p>我们还是在 token-by-token 的场景下进行分析。&lt;/p>
&lt;p>我们整体的 arithmetic operations 还是 $O(bnd^2)$.&lt;/p>
&lt;p>调用 $n$ 次 multi-query attention 的总的内存访问量为 $O(bnd +bn^2d_h+ nd^2)$, 第一项是 $q$ , 第二项是 $K$ 和 $V$ , 第三项是是 query, key 和 value layer 的权重。&lt;/p>
&lt;p>此时，MAR 变成了&lt;/p>
$$
O\left(\frac{1}{d} + \frac{n}{dh}+\frac{1}{b}\right)
$$&lt;p>现在，我们就将 $n/d$ 这一项给降低了 $h$ 倍。如果我们的 batch size 足够大的话，理论上 MQA 应该能极大提高整体的计算效率。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>MQA 为了追求极致的内存带宽占用，选择使用单一的 key 和 value, 来极大提高 inference 的 decoding 效率，但是后来在 GQA 中验证发现，MQA 虽然非常高效，但是其表现比较差，这也是后来没有得以应用的原因。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/1911.02150" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Moonlight</title><link>https://maosong.website/p/notes-on-moonlight/</link><pubDate>Thu, 07 Aug 2025 10:49:32 +0800</pubDate><guid>https://maosong.website/p/notes-on-moonlight/</guid><description>&lt;p>Kimi 提出了 Moonlight, 一个基于 Muon optimizer 训练得到的 16B-A3B MoE LLM. 作者详细介绍了如何 scale up muon optimizer.&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>&lt;a class="link" href="https://maosong.website/p/notes-on-muon-blog/" target="_blank" rel="noopener"
>Muon&lt;/a> 验证了 Muon optimizer 在小语言模型 nanoGPT 上的表现，但是对于更大规模 LLM 的表现，尚未有人探究。因此 Kimi 就希望在大规模 LLM 上验证 Muon optimizer 的表现。作者主要进行了两点改进：&lt;/p>
&lt;ol>
&lt;li>加入 weight decay&lt;/li>
&lt;li>调整了不同参数更新的 scale&lt;/li>
&lt;/ol>
&lt;p>基于改进后的 Muon optimizer, 其训练效率相比于 AdamW 提升了 2 倍。作者基于 Muon Optimizer 训练得到了 Moonlight, 一个 16B-A3B 的 MoE LLM.&lt;/p>
&lt;p>作者主要作出了三点贡献：&lt;/p>
&lt;ol>
&lt;li>探究了 weight decay 在 scaling Muon 时的作用&lt;/li>
&lt;li>分布式 Muon optimizer 的实现&lt;/li>
&lt;li>验证了 Muon optimizer 的 scaling law&lt;/li>
&lt;/ol>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="background">&lt;a href="#background" class="header-anchor">&lt;/a>Background
&lt;/h3>&lt;p>作者首先介绍了一下 Muon optimizer, 给定步数 $t$, 参数矩阵 $W_{t-1}$, momentum $\mu$, 学习率 $\eta_t$ 以及目标函数 $\mathcal{L}_t$, Muon optimizer 的更新方式如下：&lt;/p>
$$
\begin{aligned}
M_t &amp;= \mu M_{t-1} + \nabla\mathcal{L}_t(W_{t-1})\\
O_t &amp;= \mathrm{Newton-Schulz}(M_t)\\
W_t &amp;= W_{t-1} - \eta_t O_t
\end{aligned}
$$&lt;p>这里 $M_t$ 是 gradient 的 momentum, 初始化为 $M_0=0$. 在上面的更新公式中，Newton-Schulz 的作用是求解 $(M_tM_t^T)^{-1/2}M_t$. 令 $M_t=U\Sigma V^T$ 为 SVD 分解， 我们有&lt;/p>
$$
(M_tM_t^T)^{-1/2}M_t = UV^T
$$&lt;p>这是一个半正交矩阵，即 $(UV^T)^T(UV^T)=I$.&lt;/p>
&lt;p>Newton-Schulz 迭代的具体公式如下：&lt;/p>
$$
X_0 = \frac{M_t}{\|M_t\|_F},\quad X_k = aX_{k-1} + b(X_{k-1}X_{k-1}^T)X_{k-1} + c(X_{k-1}X_{k-1}^T)^2X_{k-1}
$$&lt;p>其中，normalization 是为了保证 Newton-Schulz 的收敛性。 $a,b,c$ 是三个超参数，在 Muon 中设置为 $(a,b,c)=(3.4445, 4.7750, 2.0315)$.&lt;/p>
&lt;h3 id="scaling-up-muon">&lt;a href="#scaling-up-muon" class="header-anchor">&lt;/a>Scaling up Muon
&lt;/h3>&lt;p>作者发现，尽管 Muon 在小规模场景下 work 的很好，但是大规模性场景下的收益就非常有限了。作者发现，这是因为模型的参数以及每一层输出的 RMS 变得很大，这可能会影响模型的性能。因此，作者就和 AdamW 一样使用 weight dacay 来避免这个问题，即&lt;/p>
$$
W_t =W_{t-1} - \eta_t(O_t + \lambda W_{t-1})
$$&lt;p>作者通过实验对比了 AdamW, vanilla Muon 和 Muon w/ weigth decay 三者的表现，实验结果如下图所示&lt;/p>
&lt;p>实验结果显示，尽管 vanilla Muon 手链最快，但是由于其权重增长很快，因此最后模型的表现不如 AdamW 和 Muon w/ weigth decay.&lt;/p>
&lt;p>接下来，作者分析了以下更新矩阵的 Root Mean Square (RMS), 结论是 Muon optimizer 的 RMS 与参数矩阵的形状相关：&lt;/p>
&lt;blockquote>
&lt;p>Lemma
For a full-rank matrix parameter of shape $[A, B]$, its theoretical Muon update RMS is $\sqrt{1/\max(A, B)}$.&lt;/p>
&lt;/blockquote>
&lt;p>证明如下：通过 Newton-Schulz 迭代，我们得到 $O_t=UV^T$, 其中 $M_t=U\Sigma V^T$ 是 SVD 分解，我们有&lt;/p>
$$
\mathrm{RMS}(O_t) = \sqrt{\frac{\sum_{i=1}^A\sum_{j=1}^BO_{t,i,j}^2}{AB}}=\sqrt{\frac{r}{AB}}
$$&lt;p>其中, $r=\mathrm{rank}(M_t)$ , 这样就完成了证明。&lt;/p>
&lt;p>而 Adam 和 AdamW 的 RMS 都在 $1$ 附近。作者认为 RMS 也会影响模型表现：&lt;/p>
&lt;ol>
&lt;li>当 $\max(A,B)$ 过大时，如 dense MLP matrix, 其更新就会变得很小，限制了模型的表现&lt;/li>
&lt;li>当 $\max(A,B)$ 过小时，如 &lt;a class="link" href="https://maosong.website/p/notes-on-gqa/" target="_blank" rel="noopener"
>GQA&lt;/a> 中的 KV head 或者 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v3/" target="_blank" rel="noopener"
>DeepSeek-V3&lt;/a> 中的 MLA, 更新又会变得很大，导致训练不稳定。&lt;/li>
&lt;/ol>
&lt;p>因此，作者就提出了一个 rescaling 的技巧，来消除 Muon optimizer 的影响。&lt;/p>
&lt;p>作者通过实验发现，AdamW 的 RMS 通常在 $0.2\sim0.4$ 左右，因此，作者将 Muon optimizer 的更新设置如下&lt;/p>
$$
W_t = W_{t-1} - \eta_t(0.2\cdot O_t\cdot \sqrt{\max(A,B)} + \lambda W_{t-1})
$$&lt;p>基于这个改变， Muon 和 AdamW 可以共享学习率以及 weight decay 参数。&lt;/p>
&lt;h3 id="distributed-muon">&lt;a href="#distributed-muon" class="header-anchor">&lt;/a>Distributed Muon
&lt;/h3>&lt;p>ZeRO-1 天然适合 AdamW, 因为 AdamW 都是 element-wise 进行计算的。但是 Muon 则需要梯度矩阵的全部信息。因此，作者就针对 ZeRO-1 进行适配， 提出了 &lt;strong>Distributed Muon&lt;/strong>, 分布式版本将优化器的状态进行切分，然后加入了两个额外的操作：&lt;/p>
&lt;ol>
&lt;li>DP gather: 将 ZeRO-1 切分的梯度矩阵 gather 为一个完整的矩阵&lt;/li>
&lt;li>Calculate Full Update: 对完整的梯度矩阵执行 Newton-Schulz 迭代&lt;/li>
&lt;/ol>
&lt;p>最终，Distributed Muon 的算法如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-moonlight/Moonlight-Distributed-muon.png"
width="1365"
height="553"
loading="lazy"
alt="Distributed Muon"
class="gallery-image"
data-flex-grow="246"
data-flex-basis="592px"
>&lt;/p>
&lt;p>最后，作者分析了一下 distributed Muon 和 distributed AdamW 的内存和算力占用：&lt;/p>
&lt;ol>
&lt;li>内存开销：Muon 只有一阶矩，而 AdamW 有二阶矩，因此 Muon 的额外内存开销为 AdamW 的一半。&lt;/li>
&lt;li>通信开销：对于 ZeRO-1，通信开销来源于三个过程：All-Gather 参数 $P$ 用于前向传播, Reduce-Scatter 梯度 $G$ 用于反向传播, All-Gather 更新后的参数 $P$ 用于下一轮的前向传播。AdamW 不引入额外通信，所以其每个参数的通信量为 $4+4=8$, 分别代表 $G$ 和 $P$ 的通信量。而 Muon 则需要额外的一次通信来得到 full matrix, 因此每个参数通信量为 $4+4+2=10$, 分别代表 $P, G$ 和 full matrix. 也就是说，分布式 Muon 的通信量最高为 AdamW 的 $1.25$ 倍。实际上由于我们使用 multiple DP, 这个比例会更接近于 $1.0$.&lt;/li>
&lt;li>latency：Distributed Muon 相比于 AdamW latency 更高，这是因为 Muon 需要进行 DP gather 以及计算 Newton-Schulz 迭代。但实际上，latency 很小，因为 Newton-Schulz 迭代只需要迭代 5 次，并且 optimizer 的 end-to-end latency 相比于 forward-backward 过程是可以忽略的。一些额外的技巧也可以降低 latency.&lt;/li>
&lt;/ol>
&lt;p>实际在训练的过程中，作者发现 Distributed Muon 相比于 AdamW 并没有太明显的 latency.&lt;/p>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;h3 id="scaling-law-of-muon">&lt;a href="#scaling-law-of-muon" class="header-anchor">&lt;/a>Scaling Law of Muon
&lt;/h3>&lt;p>作者分析了一下 Muon Optimizer 的 scaling law, 实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-moonlight/Moonlight-scaling-law.png"
width="825"
height="729"
loading="lazy"
alt="Scaling law for Muon and AdamW"
class="gallery-image"
data-flex-grow="113"
data-flex-basis="271px"
>&lt;/p>
&lt;p>实验结果表明，在最优设置下，Muon Optimizer 只需要 $52\%$ 的 FLOPs 就可以达到 AdamW 的表现&lt;/p>
&lt;h3 id="pretraining-with-muon">&lt;a href="#pretraining-with-muon" class="header-anchor">&lt;/a>Pretraining with Muon
&lt;/h3>&lt;p>作者分贝使用 AdamW 和 Muon 训练模型，然后评测了以下模型在不同 benchmark 上的表现，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-moonlight/Moonlight-pre-training-performance.png"
width="970"
height="577"
loading="lazy"
alt="Pretraining performance of different optimizer"
class="gallery-image"
data-flex-grow="168"
data-flex-basis="403px"
>&lt;/p>
&lt;p>可以看到，在相同的设置下，Muon optimizer 的表现更好。&lt;/p>
&lt;h3 id="dynamics-of-singular-spectrum">&lt;a href="#dynamics-of-singular-spectrum" class="header-anchor">&lt;/a>Dynamics of Singular Spectrum
&lt;/h3>&lt;p>Muon optimizer 的核心思想就是让比较难更新的方向也能被更新到，本节作者就探究了 Muon 是否满足这个性质，作者对参数矩阵进行 SVD 分解，然后定义 SVD entropy 如下&lt;/p>
$$
H(\sigma) = -\frac{1}{\log n}\sum_{i=1}^n\frac{\sigma_i^2}{\sum_{j=1}^n\sigma_j^2}\log\frac{\sigma_i^2}{\sum_{j=1}^n\sigma_j^2}
$$&lt;p>作者对 SVD entropy 可视化如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/Muonlight-SVD-entropy.png"
loading="lazy"
alt="Visualization of SVD entropy"
>&lt;/p>
&lt;p>可以看到，Muon optimizer 的 SVD entropy 比 AdamW 更大，这说明 AdamW 的更新方向更多更广，验证了 Muon optimizer 的核心思想&lt;/p>
&lt;h3 id="sft-with-muon">&lt;a href="#sft-with-muon" class="header-anchor">&lt;/a>SFT with Muon
&lt;/h3>&lt;p>作者还在 SFT 阶段验证了 Muon optimizer 的有效性。实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-moonlight/Moonlight-SFT-performance.png"
width="922"
height="262"
loading="lazy"
alt="Performance of Muon on SFT stage"
class="gallery-image"
data-flex-grow="351"
data-flex-basis="844px"
>&lt;/p>
&lt;p>结论主要有两个：&lt;/p>
&lt;ol>
&lt;li>预训练阶段与 SFT 阶段使用不同的优化器时，模型表现没有明显区别&lt;/li>
&lt;li>SFT 阶段使用 Muon 可以达到与 AdamW 差不多的表现，但是最好还是在 pre-training 阶段使用 Muon&lt;/li>
&lt;/ol>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者探究了如何 scale up Muon Optimizer. 通过改进，作者在 16B-A3B 的 MoE LLM 上验证了 Muon Optimizer 的性能。实验结果发现，Muon Optimizer 的训练效率比 AdamW 提升了 2 倍左右。&lt;/p>
&lt;p>作者提出了三个未来可行的研究方向：&lt;/p>
&lt;ol>
&lt;li>目前 Muon 只能针对 2D 参数进行优化，其他参数仍然依赖于 AdamW 优化器，是否可以使用 Muon 优化所有参数？&lt;/li>
&lt;li>Muon optimizer 可以理解是 spectral norm 下的 steepest descent 方法，如何将其扩展到 Schatten norm 是一个可以研究的方向&lt;/li>
&lt;li>实验里提到，预训练和 SFT 阶段使用不同的 optimizer, 表现不是最优的，如何解决这个因为不同 optimizer 导致的性能差距是一个需要解决的问题。&lt;/li>
&lt;/ol>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2502.16982" target="_blank" rel="noopener"
>Muon is Scalable for LLM Training&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Hunyuan-Large</title><link>https://maosong.website/p/notes-on-hunyuan-large/</link><pubDate>Wed, 06 Aug 2025 16:46:32 +0800</pubDate><guid>https://maosong.website/p/notes-on-hunyuan-large/</guid><description>&lt;p>腾讯混元提出了 Hunyuan-Large, 一个 389B-A52B 的 MoE LLM, 上下文长度为 256K.&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>Hunyuan-Large 主要在三个方向进行了改进：&lt;/p>
&lt;ol>
&lt;li>使用了更高质量的合成数据：模型使用了 7T 的预训练数据，其中包含了 1.5T 的合成数据&lt;/li>
&lt;li>优化了模型的架构：作者提出了 KV cache compression, recycle routing, expert-specific learning rate scaling 策略来提高模型的表现&lt;/li>
&lt;li>探究了 MoE 模型的 scaling law: 作者探究了 MoE 模型的 scaling law&lt;/li>
&lt;/ol>
&lt;h2 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h2>&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;p>Hunyuan-Large 是一个基于 MoE 的 transformer 架构，attention 部分使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-gqa/" target="_blank" rel="noopener"
>GQA&lt;/a>, position encoding 使用了 RoPE, MLP 的激活函数为 SwiGLU. 在 MoE layer 中，Hunyuan-Large 使用了 shared experts. 最终，模型的配置如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-hunyuan-large/hunyuan-large-architecture-config.png"
width="654"
height="399"
loading="lazy"
alt="Configuration of Hunyuan-Large"
class="gallery-image"
data-flex-grow="163"
data-flex-basis="393px"
>&lt;/p>
&lt;h4 id="kv-cache-compression">&lt;a href="#kv-cache-compression" class="header-anchor">&lt;/a>KV Cache Compression
&lt;/h4>&lt;p>为了减少 KV cache 的内存开销，作者使用了两个技巧：&lt;/p>
&lt;ol>
&lt;li>GQA: 通过共享 KV projection 的参数，来减少内存访问次数&lt;/li>
&lt;li>[[CLA]]: 在相邻的 layer 中共享 KV cache, 来进一步压缩 KV cache&lt;/li>
&lt;/ol>
&lt;p>在 Hunyuan-Large 中，作者将 GQA 的 group size 设置为 8, 然后相邻的 2 层 layer 共享 KV cache.&lt;/p>
&lt;p>假设输入的 batch size 为 $B$, sequence 长度为 $L$, layers 个数为 $\ell$, attention heads 个数为 $h$, KV heads 个数为 $h_{kv}$, 每个 head 的 hidden size 为 $d_h$, 则每一层的 GQA 需要缓存 $K,V\in\mathbb{R}^{B\times _{kv}\times L\times d_h}$， KV cache 的总占用为&lt;/p>
$$
2\times B\times h_{kv}\times L\times d_h \times \ell \times 2=4BLh_{kv}d_h\ell
$$&lt;p>第一个 $2$ 是因为同时缓存 K 和 V, 第二个 $2$ 是因为一般使用 &lt;code>bfloat16&lt;/code> 数据格式。&lt;/p>
&lt;p>对于 CLA, 因为连续两层共享相同的 KV cache，因此结果除以 2; 对于 MHA, $h_{kv}=h$; 对于 &lt;a class="link" href="https://maosong.website/p/notes-on-mqa/" target="_blank" rel="noopener"
>MQA&lt;/a>, $h_{kv}=1$. 最后，KV cache 的内存占用如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Attention Mechanism&lt;/th>
&lt;th>KV Cache Memory&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>MHA&lt;/td>
&lt;td>$4BLhd_h\ell$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GQA&lt;/td>
&lt;td>$4BLh_{kv}d_h\ell$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MQA&lt;/td>
&lt;td>$4BLd_h\ell$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CLA&lt;/td>
&lt;td>$2BLhd_h\ell$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GQA+CLA&lt;/td>
&lt;td>$2BLh_{kv}d_h\ell$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，使用 GQA+CLA 之后，模型的 kv cache 占用相比于 MHA 变成了&lt;/p>
$$
\frac{2BLh_{kv}d_h\ell}{4BLhd_h\ell}=\frac{1}{16}
$$&lt;p>也就是说，Hunyuan-Large 的 KV cache 内存占用下降到了 MHA 的 1/16.&lt;/p>
&lt;h4 id="expert-routing-strategy">&lt;a href="#expert-routing-strategy" class="header-anchor">&lt;/a>Expert Routing Strategy
&lt;/h4>&lt;p>作者采用了 shared expert + activated expert 的形式，其中包含 1 个 shared expert, 然后从 16 个专家里激活 1 个专家。&lt;/p>
&lt;p>为了解决 MoE 中 expert capacity 难以设定的问题，作者提出了一个 recycle routing 的策略，基本思想就是，当 activated expert 的容量超出限制时，会从其他没有超出容量限制的专家里重新进行激活。&lt;/p>
&lt;h4 id="expert-specific-learning-rate-scaling">&lt;a href="#expert-specific-learning-rate-scaling" class="header-anchor">&lt;/a>Expert Specific Learning Rate Scaling
&lt;/h4>&lt;p>作者使用 AdamW 作为优化器，作者探讨了如何设定学习率。基于之前的工作，最优的学习率与 batch size 相关：&lt;/p>
$$
\epsilon_{\mathrm{opt}}(B) = \frac{2\epsilon_{\max}}{\sqrt{\frac{\mathcal{B}_{\mathrm{noise}}}{B}}+\sqrt{\frac{B}{\mathcal{B}_{\mathrm{noise}}}}}
$$&lt;p>这里 $\epsilon_{\max}$ 是 AdamW 的学习率, $\mathcal{B}_{\mathrm{noise}}$ 是训练速度与数据使用效率的一个平衡因子。&lt;/p>
&lt;p>但是，在 MoE 模型中，不同专家处理的 token 是不一样的。基于 load balancing loss, shared expert 和 activated expert 处理的 token 个数比例大概是 $n :1$, 其中 $n=16$ 是总的专家个数。因此，对于 shared expert, 作者使用 $\epsilon_{\mathrm{opt}}(B)$ 作为学习率，然后对于 activated expert, 作者使用 $\epsilon_{\mathrm{opt}}(B/n)$ 作为学习率。&lt;/p>
&lt;h3 id="data">&lt;a href="#data" class="header-anchor">&lt;/a>Data
&lt;/h3>&lt;p>预训练数据包括收集和合成。收集的数据主要来自互联网，覆盖中英文两种语言。&lt;/p>
&lt;p>合成数据包括 4 个步骤：&lt;/p>
&lt;ol>
&lt;li>instruction generation: 作者使用高质量的语料作为 seed, 然后生成多样的 instruction 覆盖不同的 domain&lt;/li>
&lt;li>Instruction evolution: refine 上一步生成的 instruction&lt;/li>
&lt;li>Response generation: 使用 specialized model 来生成回答&lt;/li>
&lt;li>response filtering: 对生成的回答进行过滤&lt;/li>
&lt;/ol>
&lt;p>数据合成的流程如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-hunyuan-large/Hunyuan-large-data-synthesis.png"
width="1126"
height="565"
loading="lazy"
alt="Data synthesis pipeline"
class="gallery-image"
data-flex-grow="199"
data-flex-basis="478px"
>&lt;/p>
&lt;p>tokenizer 大小为 128K, 由 tittoken tokenizer 和额外的 28K token 组成。&lt;/p>
&lt;h3 id="pre-training-recipe">&lt;a href="#pre-training-recipe" class="header-anchor">&lt;/a>Pre-training Recipe
&lt;/h3>&lt;p>作者首先探究了一个针对 MoE 模型的 scaling law. 结果发现，最优的激活参数量为 58.1B, training token 个数为 5.6T. 经过平滑之后，作者最终将模型的激活参数两定为 &lt;strong>52B&lt;/strong>, 训练 token 数定为 $7T$.&lt;/p>
&lt;p>在训练时，作者将学习率分为了 3 个 stage:&lt;/p>
&lt;ol>
&lt;li>warmup phase&lt;/li>
&lt;li>gradual decay phase&lt;/li>
&lt;li>concise annealing phase&lt;/li>
&lt;/ol>
&lt;p>上面的三个 stage 结束之后，作者加入了两个 stage 来扩展模型的上下文长度从 32K 扩展到 256K. 训练的数据包括 75% 的短文本和 25% 的长文本。两个 stage 训练的 token 数均为 $10B$ 左右。&lt;/p>
&lt;h2 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h2>&lt;p>post-training 分为 SFT 和 RLHF 两个阶段。&lt;/p>
&lt;h3 id="sft">&lt;a href="#sft" class="header-anchor">&lt;/a>SFT
&lt;/h3>&lt;p>SFT 数据副高 math, coding, logical reasoning 等 domain, 包含超过 1M 的数据。&lt;/p>
&lt;p>SFT 训练了 3 个 epoch, 学习率从 2e-5 降低到 2e-6, 为了避免 overfitting, 作者使用了 0.1 的 attention dropout 和 0.2 的 hidden dropout.&lt;/p>
&lt;blockquote>
&lt;p>[!tip]
作者发现，MoE 模型可以从 dropout 中学习到更多&lt;/p>
&lt;/blockquote>
&lt;h3 id="rlhf">&lt;a href="#rlhf" class="header-anchor">&lt;/a>RLHF
&lt;/h3>&lt;p>作者使用 &lt;a class="link" href="https://maosong.website/p/notes-on-dpo/" target="_blank" rel="noopener"
>DPO&lt;/a> 来进行 RLHF, 作者同时使用了 offline 和 online 的数据来进行训练，前者是收集的数据，后者是当前 policy 生成的数据。与 LLaMA 3 和 Nemotron-4 一样，为了提高训练稳定性，对于 chosen reponse, 作者使用了 SFT loss.&lt;/p>
&lt;p>作者还是用了 exponential moving average 策略来减少 reward hacking 现象，以及降低 alignment tax.&lt;/p>
&lt;h2 id="experiment">&lt;a href="#experiment" class="header-anchor">&lt;/a>Experiment
&lt;/h2>&lt;p>对于 base 版本，作者对比了 &lt;a class="link" href="https://maosong.website/p/notes-on-llama3/" target="_blank" rel="noopener"
>LLaMA 3&lt;/a>, &lt;a class="link" href="https://maosong.website/p/mixstral-8x7b/" target="_blank" rel="noopener"
>Mixtral&lt;/a>, &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v2/" target="_blank" rel="noopener"
>DeepSeek-V2&lt;/a>, 实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-hunyuan-large/Hunyuan-large-base-performance.png"
width="1135"
height="761"
loading="lazy"
alt="Performance of Hunyuan-Large-Base"
class="gallery-image"
data-flex-grow="149"
data-flex-basis="357px"
>&lt;/p>
&lt;p>Instruction 版本的表现如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-hunyuan-large/Hunyuan-large-intruction-performance.png"
width="1178"
height="538"
loading="lazy"
alt="Performance of Hunyuan-Large Instuct"
class="gallery-image"
data-flex-grow="218"
data-flex-basis="525px"
>&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者提出了 Hunyuan-Large, 一个 389B-A52B 的 LLM, 上下文长度为 256K. 作者详细介绍了模型的架构，数据和训练方式。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2411.02265" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on GSPO</title><link>https://maosong.website/p/notes-on-gspo/</link><pubDate>Wed, 06 Aug 2025 11:26:26 +0800</pubDate><guid>https://maosong.website/p/notes-on-gspo/</guid><description>&lt;p>Qwen 提出了 Group Sequence Policy Optimization (GSPO), 一个针对 GRPO 进行改进的 RL 算法。GSPO 在 sequence 层面计算 importance ratio, 避免了 token-level 计算带来的训练不稳定性。&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>GRPO 的问题在于训练超大规模的 LLM 时，会出现 model collapse, &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/notes-on-minimax-01/" target="_blank" rel="noopener"
>MiniMax-01&lt;/a> 均对 GRPO 算法进行了改进。&lt;/p>
&lt;p>在本文中，作者认为 GRPO 算法的问题在于 Importance sampling weight 的计算是有问题的，这导致了误差随生成回答长度累积，最后被 clipping 机制放大，从而导致模型崩溃。&lt;/p>
&lt;p>为了解决这个问题，作者提出了 GSPO, 一个在 sequence 层面进行 Importance sampling 的 RL 算法，GSPO 还对 reward 进行 normalization 来保持训练的稳定性&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="preliminary">&lt;a href="#preliminary" class="header-anchor">&lt;/a>Preliminary
&lt;/h3>&lt;p>对于一个大语言模型 $\pi_{\theta}$, 我们将 $x$ 记为 query, 将 $y$ 记为 $\pi_{\theta}$ 针对 $x$ 的 response, 即&lt;/p>
$$
\pi_{\theta}(y\mid x) = \prod_{t=1}^{|y|}\pi_{\theta}(y_t\mid x, y_{&lt;t})
$$&lt;p>其中 $|y|$ 代表 response $y$ 的长度, 一般我们还有一个 reward model $r$, 用于生成奖励 $r(x,y)\in[0, 1]$.&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Note
本文中没有提及 KL divergence, 作者认为这不是重点，所以没有在公式中标出。&lt;/p>
&lt;/blockquote>
&lt;p>PPO 算法包含四个模型：reference model, 也就是 $\pi_{old}$, policy model, 也就是 $\pi_{\theta}$, value model, 用于计算 advantage, 以及 reward model, 用于计算奖励。 PPO 算法如下面公式所示&lt;/p>
$$
\mathcal{J}_{\mathrm{PPO}}(\theta) = \mathbb{E}_{(x,y)\sim\mathcal{D},y_{\leq t}\sim \pi_{\theta_{old}}(\cdot\mid q)}\left[ \min\left(r_t(\theta)\hat{A}_t,\mathrm{clip}\left(r_t(\theta), 1-\epsilon, 1+\epsilon\right)\hat{A}_t\right) \right]
$$&lt;p>其中&lt;/p>
$$
r_t(\theta) = \frac{\pi_{\theta}(y_t\mid x, y_{&lt; t})}{\pi_{\theta_{old}}(y_t\mid x, y_{&lt; t})}
$$&lt;p>是 token $y_t$ 的 importance ratio, $\hat{A}_t$ 是 $y_t$ 的 advantage estimation.&lt;/p>
&lt;p>&lt;strong>PPO&lt;/strong> 算法的问题在于其严重依赖 value model, 一般 value model 与 policy model 的大小相当，以保持内存和算力的负载均衡。&lt;/p>
&lt;p>为了解决 PPO 的这个问题，GRPO 通过多次采样然后计算 relative advantage 来避免使用 value model. 其目标函数为&lt;/p>
$$
\mathcal{J}_{\mathrm{GRPO}}(\theta) = \mathbb{E}_{(x, y)\sim\mathcal{D},\{y_i\}_{i=1}^G\sim \pi_{\theta_{old}}(\cdot\mid q)}\left[ \frac{1}{G}\sum_{i=1}^G\frac{1}{|y_i|}\sum_{t=1}^{|y_i|}\min\left(r_{i,t}(\theta)\hat{A}_{i,t},\mathrm{clip}\left(r_{i,t}(\theta), 1-\epsilon, 1+\epsilon\right)\hat{A}_{i,t}\right) \right]
$$&lt;p>其中，$G$ 是针对 query $x$ 的多次采样 response,&lt;/p>
$$
r_{i,t}(\theta) = \frac{\pi_{\theta}(y_{i,t}\mid x, y_{i,&lt; t})}{\pi_{\theta_{old}}(y_{i,t}\mid x, y_{i,&lt; t})}
$$&lt;p>是 importance ratio,&lt;/p>
$$
\hat{A}_{i,t} = \hat{A}_{i} = \frac{r(x,y_i) - \mathrm{mean}(\{r(x,y_i)\}_{i=1}^G)}{\mathrm{std}(\{r(x,y_i)\}_{i=1}^G)}
$$&lt;p>是使用 group response 估计得到的 advantage.&lt;/p>
&lt;h3 id="motivation">&lt;a href="#motivation" class="header-anchor">&lt;/a>Motivation
&lt;/h3>&lt;p>在 Qwen3 中已经提到，对于稀疏的 MoE 模型，我们必须使用更大的 batch size 来最大化内存和算力使用效率。但是，更大的 batch 意味着有一些数据必须是 off-policy 的，因此 PPO 和 GRPO 就需要使用 clipping 来降低 off-policy sample 对模型表现产生过大影响。&lt;/p>
&lt;p>基于 clipping 机制，作者认为 GRPO 的目标函数是 &amp;ldquo;ill-pose&amp;rdquo; 的，其原因在于 GRPO 计算的 Importance ratio 是与 RL 训练目标不匹配的。&lt;/p>
&lt;p>通常，importance sampling 的公式如下：&lt;/p>
$$
\mathbb{E}_{z\sim\pi_{\mathrm{tar}}}[f(z)] = \mathbb{E}_{z\sim\pi_{\mathrm{beh}}}\left[\frac{\pi_{\mathrm{tar}}(z)}{\pi_{\mathrm{beh}}(z)}f(z)\right]
$$&lt;p>其中 $\pi_{\mathrm{tar}}$ 是目标分布， $\pi_{\mathrm{beh}}$ 是采样分布, importance ratio $\pi_{\mathrm{tar}}(z)/\pi_{\mathrm{beh}}(z)$ 负责对采样进行修正。&lt;/p>
&lt;p>对于 GRPO, 其 importance ratio 是在 token 层面定义的，而一次采样是在 sequence 层面定义的，因此这种区别就导致了 GRPO 存在 high-variance noise.&lt;/p>
&lt;p>因此，作者认为，&lt;strong>Importance ratio 的关键在于优化目标应该与 reward 的粒度是一致的&lt;/strong>。&lt;/p>
&lt;h3 id="gspo">&lt;a href="#gspo" class="header-anchor">&lt;/a>GSPO
&lt;/h3>&lt;p>基于上面的想法，作者就提出了 GSPO, 作者首先针对 LLM 改写了上面的 importance sampling 的公式&lt;/p>
$$
\mathbb{E}_{x\sim \mathcal{D}, y\sim\pi_{\theta}(\cdot\mid x)}[r(x,y)] = \mathbb{E}_{x\sim\mathcal{D}, y\sim\pi_{\theta_{old}}(\cdot\mid x)}\left[\frac{\pi_{\theta}(y\mid x)}{\pi_{\theta_{old}}(y\mid x)}r(x,y)\right]
$$&lt;p>这里的 Importance ratio 表现了采样的回答 $\pi_{old}(y\mid x)$ 与目标回答 $\pi_{\theta}(y\mid x)$ 之间的差距，这是从 sequence 层面上体现的。&lt;/p>
&lt;p>因此，作者基于上面的式子，构建出了 GSPO 的目标函数&lt;/p>
$$
\mathcal{J}_{\mathrm{GSPO}}(\theta) = \mathbb{E}_{(x, y)\sim\mathcal{D},\{y_i\}_{i=1}^G\sim \pi_{\theta_{old}}(\cdot\mid q)}\left[ \frac{1}{G}\sum_{i=1}^G\min\left(s_{i}(\theta)\hat{A}_{i},\mathrm{clip}\left(s_{i}(\theta), 1-\epsilon, 1+\epsilon\right)\hat{A}_{i}\right) \right]
$$&lt;p>这里 $\hat{A}_{i}$ 与 GRPO 一致， $s_i(\theta)$ 是 normalize 之后的 importance ratio, 定义如下&lt;/p>
$$
s_i(\theta) = \left(\frac{\pi_{\theta}(y_{i}\mid x)}{\pi_{\theta_{old}}(y_{i}\mid x)}\right)^{\frac{1}{|y_i|}} = \exp\left(\frac{1}{|y_i|}\sum_{i=1}^{|y_i|}\frac{\pi_{\theta}(y_{i,t}\mid x, y_{i,&lt; t})}{\pi_{\theta_{old}}(y_{i,t}\mid x, y_{i,&lt; t})}\right)
$$&lt;p>这里作者使用了 length normalization 来控制 variance.&lt;/p>
&lt;p>作者对比了以下 GSPO 和 GRPO 两者的梯度，我们忽略期望的计算，直接写出内部的梯度，有&lt;/p>
$$
\begin{aligned}
\nabla_\theta \mathcal{J}_{\mathrm{GSPO}}(\theta) &amp;= \frac{1}{G}\sum_{i=1}^G\left(\frac{\pi_{\theta}(y_{i}\mid x)}{\pi_{\theta_{old}}(y_{i}\mid x)}\right)^{\frac{1}{|y_i|}}\hat{A}_i\cdot \frac{1}{|y_i|}\sum_{t=1}^{|y_i|} \nabla_{\theta} \log\pi_{\theta}(y_{i,t}\mid x, y_{&lt;t})\\
\nabla_\theta \mathcal{J}_{\mathrm{GRPO}}(\theta) &amp;= \frac{1}{G}\sum_{i=1}^G\hat{A}_i\cdot \frac{1}{|y_i|}\sum_{t=1}^{|y_i|} \frac{\pi_{\theta}(y_{i,t}\mid x, y_{i,&lt; t})}{\pi_{\theta_{old}}(y_{i,t}\mid x, y_{i,&lt; t})}\nabla_{\theta} \log\pi_{\theta}(y_{i,t}\mid x, y_{&lt;t})\\
\end{aligned}
$$&lt;p>可以看到，两者不同的地方在于 token 的 reweight 方式，GRPO 中先加权再求和；而 GSPO 中则是先求和，然后在 sequence 层面进行加权&lt;/p>
&lt;h3 id="gspo-token">&lt;a href="#gspo-token" class="header-anchor">&lt;/a>GSPO-token
&lt;/h3>&lt;p>为了支持 multi-turn RL 等需要细粒度 advantage 的场景，作者对 GSPO 的目标函数进行了改进，得到了 GSPO-token, 其目标函数为&lt;/p>
$$
\mathcal{J}_{\mathrm{GSPO-token}}(\theta) = \mathbb{E}_{(x, y)\sim\mathcal{D},\{y_i\}_{i=1}^G\sim \pi_{\theta_{old}}(\cdot\mid q)}\left[ \frac{1}{G}\sum_{i=1}^G\frac{1}{|y_i|}\sum_{t=1}^{|y_i|}\min\left(s_{i,t}(\theta)\hat{A}_{i},\mathrm{clip}\left(s_{i,t}(\theta), 1-\epsilon, 1+\epsilon\right)\hat{A}_{i}\right) \right]
$$&lt;p>其中，&lt;/p>
$$
s_{i,t}(\theta) = \mathrm{sg}[s_i(\theta)]\frac{\pi_{\theta_{old}}(y_{i,t}\mid x, y_{i,&lt; t})}{\mathrm{sg}[\pi_{\theta_{old}}(y_{i,t}\mid x, y_{i,&lt; t})]}
$$&lt;p>这里 $\mathrm{sg}$ 是 stop gradient operation. 通过这种改写方式，GSPO-token 与 GSPO 的优化目标一致，但是可以在更细粒度的 token 层面进行优化。&lt;/p>
&lt;h2 id="evaluation">&lt;a href="#evaluation" class="header-anchor">&lt;/a>Evaluation
&lt;/h2>&lt;p>作者使用 Qwen3-30B-A3B 进行测试。对于 GRPO，作者发现必须使用 Routing Replay training strategy 才能提升 MoE RL 的收敛性， Routing Replay 的具体做法就是保留 $\pi_{\theta_{old}}$ 的激活专家，在 $\pi_{\theta}$ 中使用相同的专家来保持稳定性。但是，GSPO 则不需要这个技巧。&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gspo/GSPO-GRPO-routing-replay.png"
width="1218"
height="388"
loading="lazy"
alt="Routing Replay strategy for GRPO"
class="gallery-image"
data-flex-grow="313"
data-flex-basis="753px"
>&lt;/p>
&lt;p>实验结果如下图所示。&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gspo/GSPO-performance.png"
width="1214"
height="701"
loading="lazy"
alt="Comparison of GSPO and GRPO"
class="gallery-image"
data-flex-grow="173"
data-flex-basis="415px"
>&lt;/p>
&lt;p>从实验结果可以看到，GSPO 比 GRPO 更加高效，效果也更好。&lt;/p>
&lt;p>作者带对比了 GSPO 与 GRPO 的 clipping 比例，结果发现，GSPO clipping 的 token 比例比 GRPO 高几个数量级，作者认为，尽管 GSPO clip 了更多 token, 但是 GSPO 更加高效，这也说明 GRPOtoken 层面的梯度估计是存在噪声的。&lt;/p>
&lt;p>作者还介绍了以下 MoE 模型中 RL 训练的 expert-activation volatility 现象，也就是说，MoE 模型参数更新之后，对于同一个 response, 激活的专家可能飞铲个不同。作者举例说明，对于 Qwen3-30B-A3B, 更新一次梯度之后，有 $10\%$ 左右的激活专家变得不一样了。&lt;/p>
&lt;p>作者最后介绍了以下 GSPO 的两个优势，一个是解决了 GRPO 依赖于 Routing Replay 的问题；第二个是支持 SGLang 和 vLLM 等推理框架。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者提出了 GSPO，一个在 sequence 层面计算 importance ratio 的 RL 算法，解决了 GRPO 在训练大规模 MoE LLM 时出现的训练不稳定性以及 mode collapse 现象。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2507.18071" target="_blank" rel="noopener"
>Group Sequence Policy Optimization&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Muon blog</title><link>https://maosong.website/p/notes-on-muon-blog/</link><pubDate>Tue, 05 Aug 2025 11:10:51 +0800</pubDate><guid>https://maosong.website/p/notes-on-muon-blog/</guid><description>&lt;p>Muon (MomentUm Orthogonalized by Newton-Schulz) 是一个针对二维神经网络的优化器，它基于 SGD-momentum 改进，增加了一个 Newton-Schulz 的后处理步骤&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;p>Newton-Schulz (NS) 的目的是用一个正交矩阵近似一个给定矩阵，即&lt;/p>
$$
\mathrm{Ortho}(G) = \arg\min_{O} \{\|O-G\|_F: \text{either } O^TO=I\text{ or } OO^T=I\}
$$&lt;p>也就是说，NS iteration 将 SDG-moment 的更新矩阵替换为了“最近的” semi-orthogonal matrix. 这等价于将更新矩阵替换为 $UV^T$, 其中 $USV^T$ 是更新矩阵的 SVD 分解。&lt;/p>
&lt;blockquote>
&lt;p>[!tip]
作者观察到，对于 SGD-momentum 和 Adam 来说，其在基于 transformer 的神经网络里有非常高的 condition number, 也就是 optimizer 仅在少数几个方向上进行优化。作者认为，通过正交化，可以有效提高模型在其他方向上的更新速度，进而提高模型表现&lt;/p>
&lt;/blockquote>
&lt;h3 id="newton-schulz">&lt;a href="#newton-schulz" class="header-anchor">&lt;/a>Newton-Schulz
&lt;/h3>&lt;p>作者提到，正交化矩阵的方法有很多，比如 SVD 分解，但是其问题是非常慢，还有 Coupled Newton iteration, 但是其精度要求非常高，必须要在 &lt;code>float32&lt;/code> 以上。&lt;/p>
&lt;p>作者因此使用了 Newton-Schulz iteration.&lt;/p>
&lt;p>令 $G=USV^T$ 是 SGD-momentum 更新矩阵的 SVD 分解，则基于系数 $(a,b,c)$ 的 NS iteration 定义如下：&lt;/p>
$$
\begin{aligned}
G' &amp;= aG + b(GG^T)G + c(GG^T)^2G\\
&amp;= (aI+b(GG^T)+c(GG^T)^2)G\\
&amp;= (aI+bUS^2U^T+cUS^4U^T)USV^T\\
&amp;= U(aS+bS^3+cS^5)V^T
\end{aligned}
$$&lt;p>也就是说，如果我们定义五次多项式函数 $\phi(x)=ax+bx^3+cx^5$, 然后执行 $N$ 次 NS iteration, 则我们得到 $U\phi^N(S)V^T$, 其中 $\phi^N$ 代表 $\phi$ 复合 $N$ 次。&lt;/p>
&lt;p>为了保证 NS iteration 收敛到 $\mathrm{Ortho}(G) = UV^T$, 我们必须保证两点：&lt;/p>
&lt;ol>
&lt;li>$S$ 的值，也就是 $G$ 的奇异值必须在区间 $[0,1]$ 上&lt;/li>
&lt;li>$\phi$ 必须满足 $\phi^N\to 1$, $N\to\infty$, $\forall x\in[0,1]$.&lt;/li>
&lt;/ol>
&lt;p>为了满足第一个条件，我们可以对 $G$ 进行 rescale, 即 $G\gets G/\|G\|_F$, rescale 不影响最终的结果，即 $\mathrm{Ortho}(G) = \mathrm{Ortho}(cG)$.&lt;/p>
&lt;p>对于 $\phi(x)$, 我们有很多选择，比如我们定义 $(a,b,c):=(2,-1.5,0.5)$ 就得到如下结果&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-muon-blog/Muon_naive_phi_x.png"
width="1080"
height="660"
loading="lazy"
alt="plot of "
class="gallery-image"
data-flex-grow="163"
data-flex-basis="392px"
>&lt;/p>
&lt;h3 id="coefficient-optimization">&lt;a href="#coefficient-optimization" class="header-anchor">&lt;/a>Coefficient Optimization
&lt;/h3>&lt;p>尽管 $(a,b,c):=(2,-1.5,0.5)$ 已经满足了第二个条件，但是我们还是想进一步优化，优化的方向主要有两个：&lt;/p>
&lt;ol>
&lt;li>让 $a$ 尽可能大，这是因为 $\phi'(0)=a$ 控制了较小奇异值的收敛速率。&lt;/li>
&lt;li>对于所有的 $x\in[0,1]$, 我们希望 $\phi^N(x)\in[1-\epsilon, 1+\epsilon]$, $N\to\infty$. 这样 NS iteration 的结果与 $\mathrm{Ortho}(G)$ 不会相差太远。&lt;/li>
&lt;/ol>
&lt;p>作者发现， $\epsilon$ 可以设置为 $0.3$ 而不影响 Muon optimizer 的收敛性。因此，作者的目标现在是&lt;/p>
$$
\begin{aligned}
\max\quad &amp;a\\
\mathrm{s.t.}\quad &amp;\lim_{N\to\infty}\phi^N(x)\in[0.7, 1.3]
\end{aligned}
$$&lt;p>作者通过 ad-hoc gradient 方法求解得到一组数值解为 $(a,b,c)=(3.4445, 4.7750, 2.0315)$, 作者将这组数值应用于 Muon optimizer 中。迭代结果如下图，可以看到，当 $x\approx0$ 时，函数变得更加陡峭。&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-muon-blog/Muon-optimized-phi.png"
width="1067"
height="648"
loading="lazy"
alt="Plot of "
class="gallery-image"
data-flex-grow="164"
data-flex-basis="395px"
>&lt;/p>
&lt;p>实验中，作者发现，仅需迭代五次，最终的结果就 work 的很好。作者还尝试了不同的多项式，结果发现并没有太大的提升。&lt;/p>
&lt;h3 id="algorithm">&lt;a href="#algorithm" class="header-anchor">&lt;/a>Algorithm
&lt;/h3>&lt;p>最终，Muon Optimizer 的算法如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-muon-blog/Muon-Algorithm.png"
width="1242"
height="840"
loading="lazy"
alt="Muon Algorithm"
class="gallery-image"
data-flex-grow="147"
data-flex-basis="354px"
>&lt;/p>
&lt;p>其中, &lt;code>NewtonSchulz5&lt;/code> 算法伪代码定义如下&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">newtonschulz5&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">G&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">steps&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">eps&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">1e-7&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">assert&lt;/span> &lt;span class="n">G&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ndim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">2&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">a&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">c&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mf">3.4445&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mf">4.7750&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">2.0315&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">X&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">G&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">bfloat16&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">X&lt;/span> &lt;span class="o">/=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">norm&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">eps&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">G&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">G&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">X&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">X&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">T&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">steps&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">A&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">X&lt;/span> &lt;span class="o">@&lt;/span> &lt;span class="n">X&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">T&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">B&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">b&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">A&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">c&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">A&lt;/span> &lt;span class="o">@&lt;/span> &lt;span class="n">A&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">X&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">a&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">X&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">B&lt;/span> &lt;span class="o">@&lt;/span> &lt;span class="n">X&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">G&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">G&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">X&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">X&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">T&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">X&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="analysis">&lt;a href="#analysis" class="header-anchor">&lt;/a>Analysis
&lt;/h2>&lt;p>本节作者分析了以下 Muon 的内存占用和算力开销。&lt;/p>
&lt;p>在 NS iteration 之前，Muon optimizer 和 SGD-moment 是一样的。&lt;/p>
&lt;p>对于 $n\times m$ 的矩阵（假设 $m\leq n$）， 首先 NS iteration 会进行转置，NS iteration 的每一步需要 $2(2nm^2+m^3)$ FLOPs, 其中括号前面的系数 $2$ 代表精度。因此，Muon 相比于 SGD momentum 需要的额外 FLOPs 为 $2T(2nm^2+m^3)$, 其中 $T$ 是迭代次数。&lt;/p>
&lt;p>使用 baseline 进行一次训练（前向 + 后向），所需要的 FLOPS 为 $6nmB$, 其中 $B$ 是 batch size. 因此，Muon 的 FLOP 开销至多为 $Tm/B$, 其中 $m$ 是模型的 hidden size, $B$ 是 batch size, $T$ 是 NS iteration 的步数。&lt;/p>
&lt;p>作者分别基于 nanoGPT 和 LLaMA-405B 进行验证，结果发现，Muon optimizer 带来的额外开销不足 $1\%$.&lt;/p>
&lt;p>作者发信啊，使用 Nesterov-style momentum 可以比普通的 SGD-momentum 效果更好，因此作者在 muon 中使用了前者。&lt;/p>
&lt;p>作者还发现，对于 QKV layer，分别进行优化效果会更好。&lt;/p>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>&lt;img src="https://maosong.website/p/notes-on-muon-blog/Muon-nanoGPT-loss-vs-training-tokens.png"
width="1400"
height="970"
loading="lazy"
alt="Optimizer comparison by tokens"
class="gallery-image"
data-flex-grow="144"
data-flex-basis="346px"
>&lt;/p>
&lt;h2 id="limitation-and-future-work">&lt;a href="#limitation-and-future-work" class="header-anchor">&lt;/a>Limitation and Future Work
&lt;/h2>&lt;p>Muon 仅被设计用于优化 2D 参数（因为涉及矩阵计算），其余的参数仍然需要 AdamW 等优化器参与。&lt;/p>
&lt;p>作者认为未来的工作有：&lt;/p>
&lt;ol>
&lt;li>能否 scale up Muon Optimizer&lt;/li>
&lt;li>分布式优化&lt;/li>
&lt;li>在 fine-tuning 和 RL 阶段使用 Muon Optimizer&lt;/li>
&lt;/ol>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者提出了 Muon optimizer，该优化器在 nanoGPT speedrun 上取得了 SOTA 的结果，作者详细介绍了优化器的工作原理。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://kellerjordan.github.io/posts/muon/" target="_blank" rel="noopener"
>Muon: An optimizer for hidden layers in neural networks&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on AFM2025</title><link>https://maosong.website/p/notes-on-afm2025/</link><pubDate>Tue, 29 Jul 2025 12:36:28 +0800</pubDate><guid>https://maosong.website/p/notes-on-afm2025/</guid><description>&lt;p>Apple 在 7 月份发布了 AFM 技术报告，包括两个多语种多模态大模型，分别为 3B 和 xB, 一个面向 device, 另一个面向 server， 前者主要集中于效率，后者集中于表现。&lt;/p>
&lt;h2 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h2>&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;h4 id="on-device-model">&lt;a href="#on-device-model" class="header-anchor">&lt;/a>On-Device Model
&lt;/h4>&lt;p>对于 on-device model, 作者将模型分为两个 block, Block1 占 $62.5\%$ 的 transformer layers, Block2 占 $37.5\%$ 的 transformer layers. 但是，对于 Block2, 作者移除了 key, value projection, 对应的 KV cache 则直接从 Block1 中获取。通过这种方式，作者将 KV cache memory usage 减少了 $37.5\%$. 并且，由于 Block2 不产生任何 key values, prefill stage 可以跳过这些计算，这样 TTFT 也可以减少 $37.5\%$.&lt;/p>
&lt;h4 id="server-model">&lt;a href="#server-model" class="header-anchor">&lt;/a>Server Model
&lt;/h4>&lt;p>对于 server model, 作者对架构进行了改进，来提高效率。架构如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-afm2025/AFM-2025-PT-MoE-architecture.png"
width="1271"
height="525"
loading="lazy"
alt="Diagram of the PT-MoE architecture"
class="gallery-image"
data-flex-grow="242"
data-flex-basis="581px"
>&lt;/p>
&lt;p>&lt;strong>Parallel Track Transformer&lt;/strong>
作者提出了 Parallel Track (PT) Transformer 架构，PT-Transformer 将 transformer 模型分割多个小的 transformer, 作者将这些小的 transformer 称之为 &lt;em>track&lt;/em>. 每个 track 包含多个 transformer block. 不同的 track 只会在输入和输出的时候进行交互，这样就能够减少同步的开销。作者讲这种模式称为 &lt;strong>track parallelism&lt;/strong>.&lt;/p>
&lt;p>&lt;strong>PT-MoE&lt;/strong>
为了进一步提高 server model 的效率，作者将 MoE 和 PT-transformer 结合在一起。具体的做法就是，每两个 transformer block 为一组，每组里包含一个 dense layer 和一个 MoE layer.&lt;/p>
&lt;p>&lt;strong>Interleaving Global and Local Attention Layers&lt;/strong>
作者还设计额 interleaved attention 机制，也就是，将 transformer block 按照四个为 1 组，前面 3 个 block 使用 window attention, window size 为 4096 和 RoPE. 最后一个 block 使用 global attention layer 以及 &lt;a class="link" href="https://maosong.website/p/notes-on-nope/" target="_blank" rel="noopener"
>NoPE&lt;/a>. 作者认为，使用 NoPE 可以提高模型对长上下文的泛化性。&lt;/p>
&lt;blockquote>
&lt;p>Recall
Qwen2.5-VL 的 ViT 使用的类似的做法，即 8 个 block 为一组，前面 7 个 block 使用 window attention, 最后一个 block 使用 full self attention.&lt;/p>
&lt;/blockquote>
&lt;h4 id="vision-encoder">&lt;a href="#vision-encoder" class="header-anchor">&lt;/a>Vision Encoder
&lt;/h4>&lt;p>Vision encoder 包含 ViT 和 adapter 两个模块&lt;/p>
&lt;p>对于 ViT 来说，作者使用了 ViT 架构：&lt;/p>
&lt;ul>
&lt;li>server model 使用了 1B 参数的 ViT-g&lt;/li>
&lt;li>on-device model 使用了 300M 参数的 ViTDet-L backbone&lt;/li>
&lt;/ul>
&lt;p>作者在 ViTDet 的基础上加入了 Register-Window 机制，这个机制用于编码一个 global register token 来与不同的 loca windows 进行交互。&lt;/p>
&lt;p>对于 adapter 来说，其包含了一个 transformer layer, 一个 linear projection layer, 一个 $3\times 3$ 的 convolutional layer. 其中， linear projection 用于将 visual token 映射到 LLM 的特征空间，pooling layer 用于压缩 visual token 个数。&lt;/p>
&lt;h3 id="data">&lt;a href="#data" class="header-anchor">&lt;/a>Data
&lt;/h3>&lt;p>主要包括 web data 和 image data 两部分&lt;/p>
&lt;p>image data 部分：&lt;/p>
&lt;ol>
&lt;li>Image-Text Crawl Data: 包含 &lt;strong>175M&lt;/strong> 图文交错数据，包含 &lt;strong>550M&lt;/strong> images&lt;/li>
&lt;li>Synthetic Image Caption data: &lt;strong>5B&lt;/strong> image caption 数据&lt;/li>
&lt;li>Text-Rich Image Data&lt;/li>
&lt;li>High-quality Domain-Specific Image-text Data: 包括 caption 数据， grounding 数据，table, chart, plots 数据以及 knowledge-required domains 的数据&lt;/li>
&lt;/ol>
&lt;h3 id="training-recipe">&lt;a href="#training-recipe" class="header-anchor">&lt;/a>Training Recipe
&lt;/h3>&lt;p>text tokenizer 大小为 150K.&lt;/p>
&lt;p>Vision encoder 的训练包含两个 stage:&lt;/p>
&lt;ol>
&lt;li>基于 CLIP 的方法，使用 &lt;strong>6B&lt;/strong>的 image-text pair 数据进行训练，图片精度为 448, 作者还使用了 FLIP 来提高训练效率&lt;/li>
&lt;li>使用一个 compact LLM, 同时训练 vsion encoder, adapter 和 compact LLM. 加入了更高质量的数据，图片精度为 672.&lt;/li>
&lt;/ol>
&lt;p>LLM 的训练使用了 &lt;strong>13.4T&lt;/strong> token&lt;/p>
&lt;h2 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h2>&lt;h3 id="sft">&lt;a href="#sft" class="header-anchor">&lt;/a>SFT
&lt;/h3>&lt;p>SFT 数据包括：&lt;/p>
&lt;ol>
&lt;li>General knowledge&lt;/li>
&lt;li>Reasoning: 纯文本包括 math 和 reasoning, 多模态包括 STEM, math, CoT 数据&lt;/li>
&lt;li>Text-Rich Image understanding: chart, table 数据&lt;/li>
&lt;li>Multilingual OCR: OCR 相关数据&lt;/li>
&lt;li>Text and visual grounding: grounding 数据&lt;/li>
&lt;li>Multi-image reasoning: 多图推理数据&lt;/li>
&lt;/ol>
&lt;p>作者还基于 retrieval-based 方法来收集数据，具体做法就是给定一些 prompt, 然后通过一个 Image search pipeline 来进行检索。&lt;/p>
&lt;p>训练的时候，作者将图片精度从 672 提升到 1344, 处理方式就是将图片切分为四个子图，然后作者还加入了一个总蓝图。这样，vision encoder 的输入包括四个子图和一个 thumbnail 图.&lt;/p>
&lt;p>为了提高 on-device model 的效率，作者设置了三种模式：&lt;/p>
&lt;ul>
&lt;li>rapid mode: 图片精度为 224&lt;/li>
&lt;li>balanced mode: 只有 thumbnail 图&lt;/li>
&lt;li>high-resolution mode: 四个子图和一个 thumbnail 图&lt;/li>
&lt;/ul>
&lt;p>对于不同的 mode, 如果输入的是低精度图片，则 $50\%$ 概率为 rapid mode; 如果输入的是高精度图片，则 $1\%$ 的概率为 rapid mode. 对于其他数据，作者将 $20\%$ 的数据设置为 balanced mode.&lt;/p>
&lt;h3 id="rlhf">&lt;a href="#rlhf" class="header-anchor">&lt;/a>RLHF
&lt;/h3>&lt;p>作者使用 RLOO 作为 RLHF 的算法。&lt;/p>
&lt;p>RL 的 infra 如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-afm2025/AFM-2025-RL-infra.png"
width="1277"
height="350"
loading="lazy"
alt="AFM2025 RL Infra"
class="gallery-image"
data-flex-grow="364"
data-flex-basis="875px"
>&lt;/p>
&lt;p>infra 主要由两个部分组成：&lt;/p>
&lt;ol>
&lt;li>Trajectory Generators: 生成轨迹并提供反馈&lt;/li>
&lt;li>Policy updater: 更新 policy&lt;/li>
&lt;/ol>
&lt;p>训练时，作者首先训练了一个 reward model, 与 AFM-2024 相似，作者使用了一个 preference loss function 以及一个 single-sided grading 作为 regularization.&lt;/p>
&lt;p>数据包括以下类别:&lt;/p>
&lt;ul>
&lt;li>text-only prompts&lt;/li>
&lt;li>Image-text prompts&lt;/li>
&lt;li>Math prompts&lt;/li>
&lt;li>Image-text STEM reasoning prompts&lt;/li>
&lt;/ul>
&lt;p>其中，前面两个使用 reward function 进行打分，后面两个基于 ruled-based verifier 进行打分&lt;/p>
&lt;p>作者还发现，人类的打分和 reward model 的发奋可能会出现 $20\%\sim30\%$ 的偏差。为了解决这个问题，作者训练了一个单独的 reward model, 专门用于 prompt selection.&lt;/p>
&lt;h2 id="tool-use">&lt;a href="#tool-use" class="header-anchor">&lt;/a>Tool Use
&lt;/h2>&lt;p>工具调用数据由于 Multi-turn 和依赖软件工具，比较难以收集。为了解决这个问题，作者设计了一个交互式标注平台，包括一个 agent 和一个工具执行环境。环境包括了工具和数据库，能执行工具调用并反馈。&lt;/p>
&lt;p>标注时，用户发起一个请求，然后 agent 自动执行工具调用，最后平台返回反正的轨迹。&lt;/p>
&lt;h2 id="multilingual">&lt;a href="#multilingual" class="header-anchor">&lt;/a>Multilingual
&lt;/h2>&lt;p>作者逐步增加模型对于新语言的理解能力。默认情形下，输入和输出的语种一致，但是包含 $0.4\%$ 的跨语种数据。在 SFT 和 RLHF 阶段，英语和多语种数据的比例为 $80\%:20\%$.&lt;/p>
&lt;h2 id="optimization">&lt;a href="#optimization" class="header-anchor">&lt;/a>Optimization
&lt;/h2>&lt;p>作者使用了 QAT 来将 on-device model 压缩到 2 bits-per-weight, 使用 Adaptive Scalable Texture Compression (ASTC) 来 post-training 3.56 bits-per-weight 版本的 server model.&lt;/p>
&lt;h3 id="qat">&lt;a href="#qat" class="header-anchor">&lt;/a>QAT
&lt;/h3>&lt;p>QAT 是一个在模型训练过程中模拟量化误差，从而提升模型量化后表现的方法。它解决了传统后量化方法精度损失较大的问题，是平衡模型性能用户效率的关键手段。&lt;/p>
&lt;p>训练时，作者通过修改权重 $W$ 来模仿量化：&lt;/p>
$$
\tilde{W} = s\left(\mathrm{clamp}(\lfloor \frac{W}{s}+z\rceil, q_{\min}, q_{\max}) - z\right)
$$&lt;p>其中, $s$ 是 scaling factor, $z$ 是 zero point, $q_{\min}$, $q_{\max}$ 是 quantization 的 range. 为了解决 rounding operation 不可微的问题，作者使用了 straight-through estimator 的方法来近似梯度。&lt;/p>
&lt;p>作者还提出了一个可学习的 scaling factor $f$ 用于计算 quantization scale, 计算方法如下所示&lt;/p>
$$
s = \frac{f\cdot \max(|W|)}{q_{\max}}
$$&lt;p>作者通过精细设计 $f$ 的初始化来保证模型训练的 robust.&lt;/p>
&lt;h3 id="astc">&lt;a href="#astc" class="header-anchor">&lt;/a>ASTC
&lt;/h3>&lt;p>对于 server model, 作者使用了 ASTC, 一个针对 GPU 图形纹理压缩的技术，来压缩模型权重。具体做法就是，模型训练好之后，作者对模型权重应用 ASTC, 然后对每个块进行预处理。存储时，每个块用 ASTC-HAR-ch 模式压缩为 128 位。最小值单独存储为 float16.&lt;/p>
&lt;p>推理时，GPU 硬件自动解压缩 ASTC 块，然后解压的权重最小值相加参与矩阵计算&lt;/p>
&lt;h3 id="quality-recovery-adapters">&lt;a href="#quality-recovery-adapters" class="header-anchor">&lt;/a>Quality Recovery Adapters
&lt;/h3>&lt;p>作者还是用 LoRA 来恢复量化模型的精度，并通过选择性压缩策略优化 ASTC 过程，在极小的算力开销下实现了接近全量微调的性能。&lt;/p>
&lt;h2 id="evaluation">&lt;a href="#evaluation" class="header-anchor">&lt;/a>Evaluation
&lt;/h2>&lt;p>On-device model 表现如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>MMLU&lt;/th>
&lt;th>MMMLU&lt;/th>
&lt;th>MGSM&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>AFM On-Device&lt;/td>
&lt;td>67.85&lt;/td>
&lt;td>60.60&lt;/td>
&lt;td>74.91&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen-2.5-3B&lt;/td>
&lt;td>66.37&lt;/td>
&lt;td>56.53&lt;/td>
&lt;td>64.80&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen-3-4B&lt;/td>
&lt;td>&lt;strong>75.10&lt;/strong>&lt;/td>
&lt;td>&lt;strong>66.52&lt;/strong>&lt;/td>
&lt;td>&lt;strong>82.97&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Gemma-3-4B&lt;/td>
&lt;td>62.81&lt;/td>
&lt;td>56.71&lt;/td>
&lt;td>74.74&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Gemma-3n-E4B&lt;/td>
&lt;td>57.84&lt;/td>
&lt;td>50.93&lt;/td>
&lt;td>77.77&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>server model 表现如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>MMLU&lt;/th>
&lt;th>MMMLU&lt;/th>
&lt;th>MGSM&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>AFM Server&lt;/td>
&lt;td>80.20&lt;/td>
&lt;td>74.60&lt;/td>
&lt;td>87.09&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LLaMA 4 Scout&lt;/td>
&lt;td>84.88&lt;/td>
&lt;td>80.24&lt;/td>
&lt;td>90.34&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen-3-235B&lt;/td>
&lt;td>&lt;strong>87.52&lt;/strong>&lt;/td>
&lt;td>&lt;strong>82.95&lt;/strong>&lt;/td>
&lt;td>&lt;strong>92.00&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GPT-4o&lt;/td>
&lt;td>85.70&lt;/td>
&lt;td>84.00&lt;/td>
&lt;td>90.30&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者提出了 AFM-2025 多模态多语种大语言模型系列，包括 on-device 和 server 两个版本，作者介绍了模型的架构，训练数据和训练方式。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://machinelearning.apple.com/papers/apple_intelligence_foundation_language_models_tech_report_2025.pdf" target="_blank" rel="noopener"
>Publication&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://machinelearning.apple.com/research/apple-foundation-models-tech-report-2025" target="_blank" rel="noopener"
>blog&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Kimi-k2</title><link>https://maosong.website/p/notes-on-kimi-k2/</link><pubDate>Thu, 24 Jul 2025 10:56:50 +0800</pubDate><guid>https://maosong.website/p/notes-on-kimi-k2/</guid><description>&lt;p>Kimi-k2 是一个总参数为 1T, 激活参数为 32B 的 MoE 大语言模型，模型使用 15.5T token 进行训练，optimizer 使用了 MuonClip. 作者主要关注模型的 agent 能力&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者首先强调现在 LLM 发展主要是 Agentic Intelligence, 也就是让模型自主感知，规划，思考和与环境交互。&lt;/p>
&lt;p>基于这个目标，作者就提出了 Kimi K2, 一个 1.04T 总参数，32B 激活参数的 MoE 模型，用于解决实现 agent Intelligence 中遇到的问题。作者主要进行了三点改进：&lt;/p>
&lt;ol>
&lt;li>MuonClip, 一个基于 &lt;a class="link" href="https://maosong.website/p/notes-on-moonlight/" target="_blank" rel="noopener"
>Muon&lt;/a> 的优化算法，来提高 Kimi K2 对 token 的利用效率以及提高训练的稳定性&lt;/li>
&lt;li>大规模的 agentic 数据合成 pipeline: 作者构建了一个用于合成工具调用，agent 数据的 pipeline&lt;/li>
&lt;li>通用的 RL 框架，作者将 RLVR 和 self-critic rubric reward mechanism 结合起来，用于提升模型的表现&lt;/li>
&lt;/ol>
&lt;h2 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h2>&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;p>Kimi-K2 的架构与 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v3/" target="_blank" rel="noopener"
>DeepSeek-V3&lt;/a> 相似，配置如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>指标&lt;/th>
&lt;th>DeepSeek-V3&lt;/th>
&lt;th>Kimi K2&lt;/th>
&lt;th>$\Delta$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td># Layers&lt;/td>
&lt;td>61&lt;/td>
&lt;td>61&lt;/td>
&lt;td>=&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Total Parameters&lt;/td>
&lt;td>$671\text{B}$&lt;/td>
&lt;td>$1.04\text{T}$&lt;/td>
&lt;td>$\uparrow 54\%$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Activated Parameters&lt;/td>
&lt;td>$37\text{B}$&lt;/td>
&lt;td>$32.6\text{B}$&lt;/td>
&lt;td>$\downarrow 13\%$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Experts (total)&lt;/td>
&lt;td>256&lt;/td>
&lt;td>384&lt;/td>
&lt;td>$\uparrow 50\%$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Experts Active per Token&lt;/td>
&lt;td>8&lt;/td>
&lt;td>8&lt;/td>
&lt;td>=&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Shared Experts&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;td>=&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Attention Heads&lt;/td>
&lt;td>128&lt;/td>
&lt;td>64&lt;/td>
&lt;td>$\downarrow 50\%$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Number of Dense Layers&lt;/td>
&lt;td>3&lt;/td>
&lt;td>1&lt;/td>
&lt;td>$\downarrow 67\%$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Expert Grouping&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>No&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>与 DeepSeek-V3 相比，模型主要进行了以下改动：&lt;/p>
&lt;ol>
&lt;li>作者认为提高专家的稀疏性，可以有效提高模型表现。因此作者将专家个数提升了 50%.&lt;/li>
&lt;li>为了降低模型在 inference 阶段的算力开销，作者将 attention heads 的个数降低了 50%.&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Sparsity Scaling Law&lt;/strong>
作者首先构建了基于 Muon 的 sparsity law, 这里 sparsity 定义为激活专家个数与总专家个数之比，即：&lt;/p>
$$
\mathrm{sparsity} = \frac{\# \mathrm{activated\ experts}}{\# \mathrm{total\ experts}}
$$&lt;p>作者在小规模上的实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2/Kimi-K2-sparsity-scaling-law.png"
width="673"
height="678"
loading="lazy"
alt="Sparsity Scaling Law"
class="gallery-image"
data-flex-grow="99"
data-flex-basis="238px"
>&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Observation&lt;/strong>
实验结果表明，在相同算力（激活专家个数）下，模型的表现随 sparsity 提高而增加。&lt;/p>
&lt;/blockquote>
&lt;p>但是，进一步提高 sparsity, 会让 infra 变的难以优化，因此在 Kimi-K2 里，将 sparsity 定义为 $48$.&lt;/p>
&lt;p>&lt;strong>Number of attention heads&lt;/strong>
作者还分析了探究了 attention heads 的最优配置。DeepSeek-V3 将 attention heads 的个数设置为 layers 层数的 2 倍，来提高带宽利用率以及提高计算效率。但是，当上下文长度增加之后，attention head 变成了 computational bound. 作者对比了不同配置模型的表现，实验结果如下：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2/Kimi-K2-attention-heads.png"
width="671"
height="675"
loading="lazy"
alt="Scaling curves for attention heads"
class="gallery-image"
data-flex-grow="99"
data-flex-basis="238px"
>&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Observation&lt;/strong>
实验结果表明，使用双倍 attention head 带来的收益是比较小的，只有 $0.5\%$ 到 $1.2\%$ 左右&lt;/p>
&lt;/blockquote>
&lt;p>因此，作者在 Kimi-K2 中，奖 attention head 的个数设置为 $64$.&lt;/p>
&lt;h3 id="data">&lt;a href="#data" class="header-anchor">&lt;/a>Data
&lt;/h3>&lt;p>Kimi-K2 主要强调了 token efficiency, 即每个 token 对模型表现提升的贡献。token efficiency 越高，说明每个 token 对最终模型的贡献也就越高&lt;/p>
&lt;p>相比于 &lt;a class="link" href="https://maosong.website/p/notes-on-kimi-k1.5/" target="_blank" rel="noopener"
>Kimi-k1.5&lt;/a>, Kimi-K2 使用了一个 rephrasing pipeline, 来提高高质量 token 的利用率，作者在 knowledge 和 mathematics domain 上进行了实验。&lt;/p>
&lt;p>最终，Kimi-K2 的预训练数据包括了 &lt;strong>15.5T&lt;/strong> token, 主要覆盖 Web text, code, mathmatics 以及 Knowledge 四个 domain. 大部分的数据处理与 Kimi-k1.5 相同。&lt;/p>
&lt;p>&lt;strong>Knowledge Data Rephrasing&lt;/strong>
作者构建了一个 synthetic rephrasing 框架来提高 knowledge token 的利用率，框架主要包含以下几个模块：&lt;/p>
&lt;ol>
&lt;li>Style and perspective-diverse prompting: 作者构建了一系列 prompt, 来让 LLM 从多个角度和风格来重新表述原始文本&lt;/li>
&lt;li>Chunk-wise auto-regressive generation: 为了保持模型在长文档中的 coherence 以及避免信息损失，作者使用了一个 chunk-based rewriting 策略，也就是对每个 chunk 分别进行改写，然后将它门汇总在一起&lt;/li>
&lt;li>Fidelity verification: 作者对原始文本和改写文本进行了 fidelity 检查来保证语义的一致性。&lt;/li>
&lt;/ol>
&lt;p>作者对比了以下三个设置对模型在 SimpleQA benchmark 上表现的影响，这三个设置分别是：&lt;/p>
&lt;ol>
&lt;li>原始数据集训练 10 epoch&lt;/li>
&lt;li>改写数据一次，然后训练 10 epoch&lt;/li>
&lt;li>改写数据一次，训练 1 epoch&lt;/li>
&lt;/ol>
&lt;p>实验结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th># Rephrasings&lt;/th>
&lt;th># Epochs SimpleQA&lt;/th>
&lt;th>Accuracy&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>0 (raw wiki-text)&lt;/td>
&lt;td>10&lt;/td>
&lt;td>23.76&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>10&lt;/td>
&lt;td>27.39&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>1&lt;/td>
&lt;td>28.94&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，改写的策略可以有效提高模型在 SimpleQA benchmark 上的表现&lt;/p>
&lt;p>&lt;strong>Math Data Rephrasing&lt;/strong>
对于数学相关数据，作者基于 SwallowMath, 将数据改写成了学习笔记 (learning-note) 的风格。然后，作者还将其他语言的高质量文本翻译为英文。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Recall&lt;/strong>
个人认为，数据改写在一定程度上也算是合成数据，对于合成数据这一块，微软提出的 phi 系列是一个比较好的参考&lt;/p>
&lt;/blockquote>
&lt;h3 id="muonclip-optimizer">&lt;a href="#muonclip-optimizer" class="header-anchor">&lt;/a>MuonClip Optimizer
&lt;/h3>&lt;p>Kimi-K2 使用了 Muon optimizer, 之前的实验结果说明了在相同的 compute budget 下，Muon optimizer 的表现超过了 AdamW. 也就是说，Muon optimizer 更加高效。&lt;/p>
&lt;p>但是，Muon optimizer 的问题是训练不稳定。为了解决这个问题，作者提出了 QK-clip, 一个 weight clipping 机制，来显示约束 attention logits. QK-clip 的机制是&lt;strong>当输出的 logits 超过某一个阈值之后，就对齐进行截断&lt;/strong>。&lt;/p>
&lt;p>每个 head 的 attention 的计算公式如下&lt;/p>
$$
O = \mathrm{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V
$$&lt;p>其中, $d$ 是 hidden size, $Q, K,V$ 分别是 query, key, value, 定义如下：&lt;/p>
$$
Q = XW_Q, K=XW_K, V=XW_V
$$&lt;p>这里 $W_Q,W_K,W_V$ 是模型可学习的参数。&lt;/p>
&lt;p>作者定义每个 head 的 max logit 如下：&lt;/p>
$$
S_{\max}^h = \frac{1}{\sqrt{d}} \max_{X\in\mathcal{B}}\max_{i,j} [QK^T]_{ij}
$$&lt;p>最简单的做法就是直接进行截断，也就是&lt;/p>
$$
W_Q\gets \gamma^\alpha W_q, W_K\gets \gamma^{1-\alpha}W_K
$$&lt;p>其中 $\gamma=\min(1, \tau S_{\max})$, 这里 $S_{\max}=\max_h S_{\max}^h$ 是所有 head 对应 $S_{\max}^h$ 的最大值。&lt;/p>
&lt;p>但是，实际中，作者发现只有少部分 head 会出现 logits 爆炸现象。为了提高计算效率，作者针对每个 head 单独进行 scaling, 也就是 $\gamma_{h}=\min(1, \tau S_{\max}^h)$. 对于 MLA 架构，作者仅在 unshared 模块使用 clipping:&lt;/p>
&lt;ul>
&lt;li>$q^c$ 以及 $k^c$, scaling factor 为 $\sqrt{\gamma_h}$&lt;/li>
&lt;li>$q^R$, scaling factor 为 $\gamma_h$&lt;/li>
&lt;/ul>
&lt;p>最后，作者将 Muon, weight decay, RMS matching 和 QK-clip 汇总在一起，得到 Kimi-k2 使用的 MuonClip optimizer, 算法如下所示：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2/Kimi-k2-muonclip.png"
width="1385"
height="668"
loading="lazy"
alt="MuonClip Optimizer"
class="gallery-image"
data-flex-grow="207"
data-flex-basis="497px"
>&lt;/p>
&lt;p>接下来，作者对比了以下 Muon 和 MuonClip 两个优化器的表现，作者分别使用这两个优化器训练 53B-A9B 的 MoE 模型，实验结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2/Kimi-k2-muonclip-performance.png"
width="1365"
height="551"
loading="lazy"
alt="Comparison between Muon and MuonClip"
class="gallery-image"
data-flex-grow="247"
data-flex-basis="594px"
>&lt;/p>
&lt;p>实验结果表明，Muon 优化器在 1000 步左右就出现了 logits 爆炸的现象，但是 MuonClip 通过优化可以避免这个问题。作者还发现，MuonClip 可以减少 loss spike 的产生，整体的 loss 变化情况如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2/Kimi-k2-model-training-dynamics.png"
width="977"
height="561"
loading="lazy"
alt="Training loss curve"
class="gallery-image"
data-flex-grow="174"
data-flex-basis="417px"
>&lt;/p>
&lt;p>作者还在附录中说明 QK-clip 并不影响最终的收敛性，并且，为了尽可能减少对模型训练的干扰，作者发现，仅在训练初期 QK-clip 被激活:&lt;/p>
&lt;ol>
&lt;li>在初始的 70, 000 步里，有 12.7% 的 attention heads 至少触发了一次 QK-clip, 并且将它们的 $S_\max$ 降到了 100 以下&lt;/li>
&lt;li>接下来的 70, 000 步里，QK-clip 就不再被激活&lt;/li>
&lt;/ol>
&lt;h3 id="infra">&lt;a href="#infra" class="header-anchor">&lt;/a>Infra
&lt;/h3>&lt;p>Kimi-k2 使用了 16-way 的 PP, 16-way 的 EP 以及 ZeRO-1 Data Parallelism. 具体流程如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2/Kimi-k2-communication-computation.png"
width="1362"
height="318"
loading="lazy"
alt="Kimi-K2 parallelism"
class="gallery-image"
data-flex-grow="428"
data-flex-basis="1027px"
>&lt;/p>
&lt;p>作者发现通过增加 warm-up mircro-batches, 我们可以有效重叠 EP 的 all-to-all communication 以及 computation. 但是，对于 [[DualPipe]], 其要求双倍的存储来保存参数和梯度，并且提高 PP 粒度会引入更多的 bubble, 因此 Kimi-K2 没有使用 DualPipe.&lt;/p>
&lt;p>为了减少 [[1F1B]] 的 PP 通信开销，作者在反向传播的过程中同时进行 PP 通信，来进一步重合通信与计算。&lt;/p>
&lt;p>作者还发现，使用更小的 EP group 可以提高整体的训练速度，因此作者将 EP 设置为 16.&lt;/p>
&lt;p>作者发现，剩余的 GPU 内存不足以存放 MoE 的 activation, 因此作者采取了三个办法解决这个问题：&lt;/p>
&lt;ol>
&lt;li>Selective recomputation: 作者对 LayerNorm, SwiGLU, MLA 的 up-projection 和 MoE 的 down-projections 等进行重新计算。&lt;/li>
&lt;li>FP8 storage for insentive activations: 作者使用了 FP8 精度来存储 MoE up-projections 以及 SwiGLU. 作者发现使用 FP8 进行计算可能会导致性能下降，因此作者并没有使用 FP8 进行计算。&lt;/li>
&lt;li>Activation CPU offload: 作者将其余的 activation 放在 CPU RAM 上，在 1F1B 的过程中，作者再进行加载&lt;/li>
&lt;/ol>
&lt;h3 id="training-recipe">&lt;a href="#training-recipe" class="header-anchor">&lt;/a>Training Recipe
&lt;/h3>&lt;p>模型上下文长度为 4096, 使用 MuonClip 进行优化，使用 WSD lr Scheduler.weight decay 为 0.1, global batch size 为 67M tokens.&lt;/p>
&lt;p>预训练结束之后，作者加入了一个 long-context activation stage. 这个阶段，作者使用了 400B 的 4K 上下文的 token 和 60B 的 32K 上下文的 60B token. 最后作者使用 &lt;a class="link" href="https://maosong.website/p/notes-on-yarn/" target="_blank" rel="noopener"
>YARN&lt;/a> 将模型上下文长度扩展到 128K.&lt;/p>
&lt;h2 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h2>&lt;h3 id="sft">&lt;a href="#sft" class="header-anchor">&lt;/a>SFT
&lt;/h3>&lt;p>数据的构建主要是基于：&lt;/p>
&lt;ol>
&lt;li>prompt 的多样性&lt;/li>
&lt;li>Response 的质量&lt;/li>
&lt;/ol>
&lt;p>作者构建了一系列的数据生成 pipeline, 然后使用 Kimi-k1.5 来生成多样化的回答，最后再进行质量评估和过滤。这里，作者主要介绍了一下 tool-use 数据的构建&lt;/p>
&lt;p>受 ACEBench 启发，作者构建了一个模仿真实世界 tool-use 的数据合成 pipeline. pipeline 如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2/Kimi-k2-tool-use-synthetic-pipeline.png"
width="1366"
height="439"
loading="lazy"
alt="Data synthesis pipeline for tool use"
class="gallery-image"
data-flex-grow="311"
data-flex-basis="746px"
>&lt;/p>
&lt;p>pipeline 主要包含三个阶段：&lt;/p>
&lt;ul>
&lt;li>tool spec generation: 作者基于 MCP tools 和 self-evolved 的数据合成策略来构建 tool repository. MCP tools 包括 3000+ 的 tools,合成了 20，000 个 tools&lt;/li>
&lt;li>Agent and task generation: 作者还用不同的 system prompt 以及不同的 tools combination 来构建对应的 agent. 然后对不同的 agent, 作者构建了对应的成功标准均，工具调用模式以及评估方式&lt;/li>
&lt;li>Trajectory generation: 作者首先构建不同风格的 LLM, 然后作者使用一个 simulator 来执行 tool call 并给予反馈。&lt;/li>
&lt;/ul>
&lt;p>最后，作者对数据进行了过滤。&lt;/p>
&lt;p>作者还加入了 coding 以及软件工程等任务相关数据来进一步提高模型的 agent 能力&lt;/p>
&lt;h3 id="rl">&lt;a href="#rl" class="header-anchor">&lt;/a>RL
&lt;/h3>&lt;p>RL 与 Kimi-K1.5 差不多。作者进一步提高了 RL 阶段的算力并做出了亮点改进：&lt;/p>
&lt;ol>
&lt;li>作者构建了类似 Gym 的框架，用于扩展 RL 的能力&lt;/li>
&lt;li>作者加入了更多 RLVR 的任务&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Data&lt;/strong>
数据主要包括以下几类：&lt;/p>
&lt;ol>
&lt;li>Math, STEM and logical tasks: 数据构建的原则为多样化和中等难度&lt;/li>
&lt;li>Instruction following: 作者基于 hybrid rule verification 和 multi-source instruction generation 来合成复杂的指令跟随数据&lt;/li>
&lt;li>Faithfulness: 作者训练了一个 judge model 来提供 reward&lt;/li>
&lt;li>Coding &amp;amp; Software Engineering: 作者从开源数据收集并合成了代码相关数据&lt;/li>
&lt;li>Safety. 提高模型的安全性，防止 jailbreak&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Reward&lt;/strong>
作者使用了 self-critique rubric reward 的奖励机制。&lt;/p>
&lt;p>首先，对于 Kimi-k2 的回答，作者会使用另一个 kimi-k2 作为 critic，基于三个方面进行排序：&lt;/p>
&lt;ul>
&lt;li>core rubric: AI 的核心价值观&lt;/li>
&lt;li>prescriptive rubric: 避免 reward hacking&lt;/li>
&lt;li>human-annotated rubric: 特定的上下文&lt;/li>
&lt;/ul>
&lt;p>在训练的过程中，critic 也会基于 verifiable signals 进行 refine&lt;/p>
&lt;p>&lt;strong>RL training&lt;/strong>
RL 的训练目标与 Kimi-k1.5 相同&lt;/p>
$$
\mathcal{L}(\pi_\theta) = \mathbb{E}_{x\sim \mathcal{D}}\left[\frac1K\sum_{i=1}^K \left(r(x,y_i)-\bar{r}(x) -\tau\log\frac{\pi_{\mathrm{\theta}}(y_i\mid x)}{\pi_{\mathrm{old}}(y_i\mid x)}\right)^2\right]
$$&lt;p>其中 $\bar{r}(x)=1/K\sum_{i=1}^Kr(x,y_i)$ 是 sample response 的平均奖励。&lt;/p>
&lt;p>作者做了以下几点改进来提高模型在不同 domain 上的表现：&lt;/p>
&lt;ol>
&lt;li>Budget control: 作者针对不同任务分别设置了最大 token 限制，模型超过这个限制会受到惩罚。猜测应该是 length penalty 或者类似 Qwen3 一样，直接中断思考过程输出最终答案&lt;/li>
&lt;li>PTX loss: 作者使用了 PTX loss 来提高模型对于高质量数据的利用率以及降低模型的过拟合&lt;/li>
&lt;li>Temperature Decay: 作者发现，训练后期保持较高的采样温度会影响模型的表现，因此作者设置了一个 schedule, 来逐步降低采样温度。&lt;/li>
&lt;/ol>
&lt;h3 id="rl-infra">&lt;a href="#rl-infra" class="header-anchor">&lt;/a>RL Infra
&lt;/h3>&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2/Kimi-k2-RL-infra.png"
width="734"
height="477"
loading="lazy"
alt="Kimi-K2 RL infra"
class="gallery-image"
data-flex-grow="153"
data-flex-basis="369px"
>&lt;/p>
&lt;p>RL infra 与 Kimi-k1.5 类似。主要包含三个模块。其中 train engine 和 inference engine 两者互相切换，一个 engine 进行 training 的时候，另一个 engine 就进行 inference, 为了提高 engine 切换的效率，作者使用了 checkpoint engine 来传输更新后的参数。&lt;/p>
&lt;h2 id="evaluation">&lt;a href="#evaluation" class="header-anchor">&lt;/a>Evaluation
&lt;/h2>&lt;p>模型评估结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2/Kimi-k2-performance.png"
width="696"
height="979"
loading="lazy"
alt="Performance of Kimi-k2"
class="gallery-image"
data-flex-grow="71"
data-flex-basis="170px"
>&lt;/p>
&lt;p>评估结果显示，模型在 coding 和通用任务上的表现仅次于 Claude 4, 大部分 benchmark 上都是第二名甚至是 sota.&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 Kimi-K2, 一个总参数为 1B 的 MoE 大语言模型。作者在架构，数据和优化器上进行了创新。并且通过 post-training 显著提高了模型的 agent 以及 tool-use 能力&lt;/p>
&lt;p>作者发现模型主要存在的问题有：&lt;/p>
&lt;ol>
&lt;li>reasoning 任务过难或者 tool 的定义不清晰的时候，模型会使用很多 token&lt;/li>
&lt;li>有时候工具调用可能会降低模型的表现&lt;/li>
&lt;li>模型在 agentic coding 任务上的能力需要进一步提升&lt;/li>
&lt;/ol>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://github.com/MoonshotAI/Kimi-K2" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Keye-VL</title><link>https://maosong.website/p/notes-on-keye-vl/</link><pubDate>Wed, 23 Jul 2025 11:11:43 +0800</pubDate><guid>https://maosong.website/p/notes-on-keye-vl/</guid><description>&lt;p>Keye-VL 是快手在 25 年 7 月份提出的一个 8B 的多模态大模型，其亮点为短视频理解能力。预训练包括 4 个 stage，使用了 600B token，后训练包括 2 个 stage，用于提升模型的 reasoning 和 non-reasoning 能力。&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者首先回顾了已有的 MLLM 工作，然后强调理解短视频仍然是一个很难的任务，特别是要求模型基于 video 和 audio 来理解视频。因此，在本文中，作者提出了 Kwai Keye-VL，一个 8B 的多模态大模型，主要用于短视频理解任务。&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;p>Keye-VL 是一个标准的 ViT-MLP-LLM 的架构，其中，ViT 是 SigLIP-400M-384-14, MLP 是一个基于 SwiGLU 的 2 层 MLP，使用了和 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5-vl/" target="_blank" rel="noopener"
>Qwen2.5-VL&lt;/a> 一样的 patch merge 方法，LLM 使用的是 Qwen3-8B，模型架构示意图如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl/Keye-VL-architecture.png"
width="1371"
height="1000"
loading="lazy"
alt="Keye-VL model architecture"
class="gallery-image"
data-flex-grow="137"
data-flex-basis="329px"
>&lt;/p>
&lt;p>作者针对 ViT 和 visual encoding 分别做了如下改进&lt;/p>
&lt;h4 id="navit">&lt;a href="#navit" class="header-anchor">&lt;/a>NaViT
&lt;/h4>&lt;p>作者实现了 native resolution ViT，来处理不同分辨率的图片。&lt;/p>
&lt;p>具体做法为，作者基于 SigLIP-400M-384-14 来初始化 ViT。&lt;/p>
&lt;p>然后，作者首先采用了 interpolation 来将 ViT 的 position encoding 扩展到不同的图片精度下面去。&lt;/p>
&lt;p>接下来，作者提出了 2D RoPE 来进一步提升 position encoding 的表现。&lt;/p>
&lt;p>最后，作者加入了 NaViT 的 packing 技巧来继续预训练 ViT.&lt;/p>
&lt;p>在 ViT 预训练的过程中，作者使用了 &lt;strong>500B&lt;/strong> 的 token&lt;/p>
&lt;h4 id="visual-encoding">&lt;a href="#visual-encoding" class="header-anchor">&lt;/a>Visual Encoding
&lt;/h4>&lt;p>为了提升模型理解图片和视频的能力，作者针对图片和视频进行了进一步的处理。&lt;/p>
&lt;p>对于不同精度的图片，作者将最大 token 个数设置为 16384。&lt;/p>
&lt;p>对于视频，作者将每帧的 token 数限制在 $[128,768]$, 每个视频的最大 token 个数设置为 24576&lt;/p>
&lt;p>对于提取的 frames，作者重新计算了 FPS, 然后在 3D RoPE 中让时间维度与真实时间严格对齐。&lt;/p>
&lt;h3 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h3>&lt;h4 id="data">&lt;a href="#data" class="header-anchor">&lt;/a>Data
&lt;/h4>&lt;p>预训练数据一共包括 600B token，覆盖了 6 个类别：&lt;/p>
&lt;ul>
&lt;li>Image caption: 包括中英文数据，来源为 LAION, DataComp 以及 Coyo. 作者基于 CLIP 来计算相速度，然后过滤掉相似度比较低的数据，作者还对数据进行了 re-caption，实验发现 re-caption 可以提高模型的细粒度图片理解能力&lt;/li>
&lt;li>OCR &amp;amp; VQA: 数据包括开源数据和合成数据。合成数据包括 Synthesis 和 Rendering 两个方法，第一是基于 text-dense image 构建 OCR 数据以及基于 image-caption pair 数据使用 MLLM 构建 VQA 数据。第二是使用字体渲染工具合成高质量的 OCR 数据&lt;/li>
&lt;li>Grounding &amp;amp; Counting: Grounding 数据主要包括 RefCoCo, VisualGenome, TolokaVQA, Counting 数据包括 PixMo. 作者仍然使用 CLIP 对数据进行过滤&lt;/li>
&lt;li>Interleaved text-image data: 作者发现图文交错数据可以提供通用知识，并且还可以提高模型的视觉语言对齐能力，第三就是提升模型的泛化能力。作者主要从 academic PDF 以及结构化知识中提取对应的数据。作者基于 Garbled character recognition, low-resolution/broken image filtering 以及 text-image similarity validation 来保证数据的质量&lt;/li>
&lt;li>Video understanding: 作者使用 Qwen2.5-omni 从 interleaved video-asr 来将视频数据转化图文交错数据， 然后基于 ASR 的结果进行 recaption，最后对每一帧进行 OCR. 作者还构建了 Frame-level re-ordering 以及 multiple video matching 两个任务来提高模型的上下文理解能力&lt;/li>
&lt;li>Pure Text: 未提及&lt;/li>
&lt;/ul>
&lt;p>对于源数据，作者进行了数据清洗：&lt;/p>
&lt;ol>
&lt;li>使用 CLIP 对数据进行打分，然后过滤掉低质量的数据&lt;/li>
&lt;li>使用开源的 MLLM 作为 discriminator 来选择高质量的数据&lt;/li>
&lt;li>去重&lt;/li>
&lt;/ol>
&lt;h4 id="training-recipe">&lt;a href="#training-recipe" class="header-anchor">&lt;/a>Training Recipe
&lt;/h4>&lt;p>预训练包括 4 个 stage：&lt;/p>
&lt;ul>
&lt;li>Stage 0: 使用 SigLIP 损失函数来继续训练 ViT&lt;/li>
&lt;li>Stage 1: cross-modal Alignment，仅训练 MLP&lt;/li>
&lt;li>Stage 2: multi-task pre-training, 解冻所有参数，使用 Multi-task 数据来训练模型&lt;/li>
&lt;li>Stage 3: annealing, 在高质量数据集上进行 fine-tune，进一步提升模型的能力&lt;/li>
&lt;/ul>
&lt;p>作者发现，预训练后的模型在下游任务上的表现对训练数据配比非常敏感。为了解决这个问题，在最后一个训练阶段，作者使用了一个 merging 的技巧，来保持模型的能力。&lt;/p>
&lt;h3 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h3>&lt;p>post-training 阶段一共包含了 2 个 step, 5 个 stage, 第一个 step 包含 2 个 stage，用于提升模型的 non-reasoning 能力。第二个 step 包含 3 个 stage, 用于提升模型的 reasoning 能力&lt;/p>
&lt;h4 id="no-reasoning-training">&lt;a href="#no-reasoning-training" class="header-anchor">&lt;/a>No-reasoning Training
&lt;/h4>&lt;p>第一个 step 是 non-reasoning training, 包含了 SFT 和 MPO 两个 stage, 训练 pipeline 如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl/Keye-VL-non-reasoning-training.png"
width="1340"
height="472"
loading="lazy"
alt="Non reasoning training pipeline"
class="gallery-image"
data-flex-grow="283"
data-flex-basis="681px"
>&lt;/p>
&lt;p>&lt;strong>SFT&lt;/strong>
SFT 阶段一共使用了 &lt;strong>5M&lt;/strong> 的多模态 QA 样本，为了提升数据的多样性，作者使用 TaskGalaxy 来将数据分类为 70,000 种任务类型，然后作者对每条数据，使用 MLLM 评估问题的难度，来过滤掉过于简单的问题。最后，作者请人类来进行标注，保证数据的可靠性。&lt;/p>
&lt;p>&lt;strong>MPO&lt;/strong>
训练方面，作者使用了 MPO 进行训练。
数据方面，作者使用了：&lt;/p>
&lt;ol>
&lt;li>400,000 开源的数据，作者主要进行了去重以及过滤低质量的数据&lt;/li>
&lt;li>50,000 偏好数据，基于 MM-RLHF 和 MMPR 等数据构建，然后构建高质量的 negative examples&lt;/li>
&lt;li>10,000 条 self-improvement 样本：基于 benchmark 和人类反馈，使用 SFT model 的回答作为 chosen samples, 然后基于 reward model 或者 rule-based rewards 评估模型输出，选择分数最低的座位 rejected samples&lt;/li>
&lt;li>90,000 纯文本样本： in-house data&lt;/li>
&lt;li>30,000 人类标注样本：使用开源和闭源模型进行回答，然后请人类进行排序&lt;/li>
&lt;/ol>
&lt;h4 id="reasoning-training">&lt;a href="#reasoning-training" class="header-anchor">&lt;/a>Reasoning Training
&lt;/h4>&lt;p>第二个 step 是 reasoning training, 包含了 CoT Cold-Start, Mix-Mode RL 和 Iterative Alignment 三个 stage, 训练 pipeline 如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl/Keye-VL-reasoning-training.png"
width="1358"
height="596"
loading="lazy"
alt="Non reasoning training pipeline"
class="gallery-image"
data-flex-grow="227"
data-flex-basis="546px"
>&lt;/p>
&lt;p>&lt;strong>CoT cold-start&lt;/strong>
作者收集了如下数据：&lt;/p>
&lt;ul>
&lt;li>330, 000 条 non-reasoning 样本：和第一个 step 的数据分布类似，但是不重合&lt;/li>
&lt;li>230,000 reasoning 样本：作者构建了一个 data construction pipeline,用于保证 long-CoT 的正确性&lt;/li>
&lt;li>20,000 automatic reasoning 样本：作者基于 MMPR, MM-Eureka 以及 OpenR1-math 来收集数据，基于这些模型来训练模型自动化决定是否要进行 reasoning&lt;/li>
&lt;li>100,000 agentic reasoning 样本：训练模型的 &amp;ldquo;think with image&amp;rdquo; 能力。基于 3M QA pairs，作者使用 Qwen2.5-72B 来识别需要进行 manipulating 的问题，然后生成对应的代码。接下来作者构建了一部分 OCR 数据，让模型学会 constrast enhancement 或者 rotation 操作。最后对于数学问题，作者让 Gemini-2.5-Pro 生成对应的思考过程，再让 GPT-4o 将对应的计算转换为可执行的代码。&lt;/li>
&lt;li>32, 000 Video data: 包含了 24,000 条 thinking samples 和 80,000 条 non-thinking samples&lt;/li>
&lt;/ul>
&lt;p>训练时，所有样本混在一起进行训练。作者认为，将日常使用的 SFT 数据和 reasoning 数据放在一起训练，可以保持模型在通用场景下的能力。&lt;/p>
&lt;p>&lt;strong>Mix-Mode RL&lt;/strong>
训练数据主要包括 4 个任务：&lt;/p>
&lt;ol>
&lt;li>Multimodal perception: 复杂文本识别和 counting 任务&lt;/li>
&lt;li>Multimodal reasoning: MMPR 和 MM-Eureka&lt;/li>
&lt;li>Text-based mathematical reasoning: 数学推理问题&lt;/li>
&lt;li>Agentic reasoning: 从 DeepEyes 中获取的 47,000 条样本&lt;/li>
&lt;/ol>
&lt;p>作者使用了 GRPO 来训练，reward 基于 MLLM 进行，包括最终结果和思考过程。&lt;/p>
&lt;p>作者还使用 RL 来提高模型的短视频理解能力。作者发现 RL 训练之后，模型的短视频理解能力有了大幅度的提升。&lt;/p>
&lt;p>&lt;strong>Iterative Alignment&lt;/strong>
这一步主要解决模型的重复性输出，或者 reasoning logic 不对的问题。作者使用了 rejection-sampling 数据，包括 instruction following, OCR, mathematics, charts, counting 等。&lt;/p>
&lt;p>作者基于 Rule-based score 和 model-based score 来进行打分，最后使用 MPO 算法进行训练。通过这个过程，模型的输出格式和动态思考能力都有了提升。&lt;/p>
&lt;h2 id="evaluation">&lt;a href="#evaluation" class="header-anchor">&lt;/a>Evaluation
&lt;/h2>&lt;p>作者首先评估了 ViT 的表现，主要有两点：&lt;/p>
&lt;ol>
&lt;li>在 SigLIP 的基础上加入 1D interpolation 之后，模型的表现所有下降，作者认为这是由于 1D 的 position encoding 无法识别 2D 的 patch 排列导致的&lt;/li>
&lt;li>加入 2D RoPE 之后，ViT 与 SigLIP 的表现持平&lt;/li>
&lt;/ol>
&lt;p>接下来是 Keye-VL 在公开 benchmark 上的表现，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl/Keye-VL-performance.png"
width="1068"
height="1161"
loading="lazy"
alt="Performance of Keye-VL"
class="gallery-image"
data-flex-grow="91"
data-flex-basis="220px"
>&lt;/p>
&lt;p>作者还构建了一个内部的 benchmark, 用于进一步评估模型的能力。&lt;/p>
&lt;p>已有 benchmark 的问题：&lt;/p>
&lt;ol>
&lt;li>contamination&lt;/li>
&lt;li>多语种覆盖不足：大部分 benchmark 都是英文的&lt;/li>
&lt;li>任务和 domain 覆盖不足：大部分 benchmark 只考虑基本的 perception 和 reasoning 能力&lt;/li>
&lt;li>任务难度和评估格式单调&lt;/li>
&lt;/ol>
&lt;p>构建 benchmark 的原则：&lt;/p>
&lt;ol>
&lt;li>在中文场景下的真实用户需求，open ended QA, 包括短视频理解能力&lt;/li>
&lt;li>细粒度的评估&lt;/li>
&lt;li>多样性高&lt;/li>
&lt;li>没有 contamination&lt;/li>
&lt;li>多角度评估策略: 正确性，相关性，理解性，流畅性和创造性&lt;/li>
&lt;/ol>
&lt;p>结果如下：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl/Keye-VL-performance-internal.png"
width="1339"
height="559"
loading="lazy"
alt="Performance of Keye-VL on the internal benchmark"
class="gallery-image"
data-flex-grow="239"
data-flex-basis="574px"
>&lt;/p>
&lt;p>分析：&lt;/p>
&lt;ol>
&lt;li>Keye-VL 在 OCR 等任务上的表现有所不足，其细粒度的识别能力也有所不足，会认错人。有时候还会忽略掉一些信息&lt;/li>
&lt;li>描述 temporal action 时会出现不稳定性，模型对镜头的感知能力不足。需要准确定位时间等&lt;/li>
&lt;li>在需要逻辑链条和数学计算时，模型能力不足，对于特定 domain 上的任务会出现事实性错误。写作时，模型倾向于输出通用的回答，而不是定制化的回答。&lt;/li>
&lt;/ol>
&lt;h2 id="discussion">&lt;a href="#discussion" class="header-anchor">&lt;/a>Discussion
&lt;/h2>&lt;p>作者讨论了两点关键发现：&lt;/p>
&lt;ol>
&lt;li>reasoning 和 non-reasoning 的数据可以互相促进彼此的表现，这与 ERNIE 4.5 的发现一致。&lt;/li>
&lt;li>作者认为通过 mix-mode 的训练，模型在简单和复杂任务上的表现都可以提升，因此作者使用了混合数据来进行训练，结果发现效果很好。&lt;/li>
&lt;/ol>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>本文中，作者提出了 Keye-VL 8B，一个短视频理解能力出色的多模态大模型，作者详细介绍了 pre-training 和 post-training. 其中，mix-mode training 可以有效提高模型的表现。&lt;/p>
&lt;p>作者认为 Keye-VL 有如下改进的地方：&lt;/p>
&lt;ol>
&lt;li>并没有优化 video encoder 或者是改进 video encoding 的策略&lt;/li>
&lt;li>Keye-VL 的视觉感知能力有进一步的提升空间，其 &amp;ldquo;reasoning with image&amp;rdquo; 能力依然落后于领先的 reasoning model&lt;/li>
&lt;li>使用一个额外的 MLLM 作为 reward model 会极大消耗算力，如何构建一个更可靠更高效的 reward model 需要进一步探索。&lt;/li>
&lt;/ol>
&lt;h2 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2507.01949" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/Kwai-Keye/Keye/tree/main" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>LLM Parameter Computation</title><link>https://maosong.website/p/llm-parameter-computation/</link><pubDate>Tue, 22 Jul 2025 10:50:47 +0800</pubDate><guid>https://maosong.website/p/llm-parameter-computation/</guid><description>&lt;p>本文中，我们介绍一下如何计算 LLM 的参数量。我们将基于 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a> 模型架构出发，对模型架构进行拆解，然后给出 LLM 参数量计算公式。&lt;/p>
&lt;h2 id="dense-model">&lt;a href="#dense-model" class="header-anchor">&lt;/a>Dense Model
&lt;/h2>&lt;p>我们首先来看一下 Qwen3 的架构，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/llm-parameter-computation/transformer_architecture.png"
width="1210"
height="1364"
loading="lazy"
alt="Architecture of Qwen3"
class="gallery-image"
data-flex-grow="88"
data-flex-basis="212px"
>&lt;/p>
&lt;p>这里，&lt;code>Qwen3ForCausalLM&lt;/code> 就是我们的的 LLM, 其关键代码如下&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Qwen3ForCausalLM&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">model&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Qwen3Model&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">lm_head&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">vocab_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>我们假设 &lt;code>vocab_size&lt;/code>, 也就是词表大小为 $|V|$ (我们用 $V$ 表示词表), &lt;code>hidden_size&lt;/code> 为 $d$, 则总参数量为&lt;/p>
$$
\mathrm{parameter}(\texttt{Qwen3ForCausalLM}) =d|V| + \mathrm{parameter}(\texttt{Qwen3Model})
$$&lt;p>&lt;code>Qwen3Model&lt;/code> 包含三个（含参数的）模块，分别是 &lt;code>nn.Embedding&lt;/code>, &lt;code>Qwen3DecodeLayer&lt;/code> 以及 &lt;code>Qwen3RMSNorm&lt;/code>, 分别代表了输入 token 的 embedding layer, Transformer block 和对输出的 normalization. 其关键代码如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Qwen3Model&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Qwen3Config&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">embed_tokens&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Embedding&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">vocab_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">padding_idx&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">layers&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ModuleList&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">[&lt;/span>&lt;span class="n">Qwen3DecoderLayer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">layer_idx&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">layer_idx&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_hidden_layers&lt;/span>&lt;span class="p">)]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">norm&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Qwen3RMSNorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">eps&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">rms_norm_eps&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>其中，&lt;code>nn.Embedding&lt;/code> 参数量与 &lt;code>lm_head&lt;/code> 一样，都是 $d|V|$.&lt;/p>
&lt;p>对于 normalization, 现在大部分 LLM 用的都是 &lt;code>RMSNorm&lt;/code>, 其定义如下：&lt;/p>
$$
\mathrm{RMSNorm}(x) = \frac{x}{\sqrt{\|x\|_2^2+\epsilon}}\odot \gamma
$$&lt;p>其参数量为：$d$.&lt;/p>
&lt;p>如果说我们使用的是 &lt;code>LayerNorm&lt;/code>, 则其定义如下：&lt;/p>
$$
\mathrm{LayerNorm}(x) = \frac{x-\mathbb{E}[x]}{\sqrt{\mathrm{var}[x]+\epsilon}}\odot \beta + \gamma
$$&lt;p>其参数量为： $2d$.&lt;/p>
&lt;p>因此，&lt;code>Qwen3Model&lt;/code> 的参数量为&lt;/p>
$$
\mathrm{parameter}(\texttt{Qwen3Model})=d|V| + N*\mathrm{parameter}(\texttt{Qwen3DecoderLayer}) + d
$$&lt;p>这里第一项为 &lt;code>nn.Embedding&lt;/code>, 第三项为 &lt;code>Qwen3RMSNorm&lt;/code>， 第二项里，$N$ 代表 decode layer 的个数，也就是 &lt;code>config.num_hidden_layers&lt;/code>.&lt;/p>
&lt;p>&lt;code>Qwen3DecoderLayer&lt;/code> 包含了四个模块，其关键代码如下&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Qwen3DecoderLayer&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Qwen3Config&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">self_attn&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Qwen3Attention&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">layer_idx&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">layer_idx&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mlp&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Qwen3MLP&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">input_layernorm&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Qwen3RMSNorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">eps&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">rms_norm_eps&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">post_attention_layernorm&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Qwen3RMSNorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">eps&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">rms_norm_eps&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>因此，&lt;code>Qwen3DecoderLayer&lt;/code> 的参数量为&lt;/p>
$$
\mathrm{parameter}(\texttt{Qwen3DecoderLayer}) = 2d + \mathrm{parameter}(\texttt{Qwen3MLP}) + \mathrm{parameter}(\texttt{Qwen3Attention})
$$&lt;p>其中，第一项是两个 &lt;code>Qwen3RMSNorm&lt;/code> 的参数。&lt;/p>
&lt;p>对于 &lt;code>Qwen3MLP&lt;/code>, 其定义如下：&lt;/p>
$$
y = W_2(W_3x\odot \mathrm{SwiGLU}(W_1x))
$$&lt;p>这里 $W_3,W_1\in\mathbb{R}^{d_{ff}\times d}$, $W_2\in\mathbb{R}^{d\times d_{ff}}$, $d_{ff}$ 是 MLP 的 hidden size, 代码中用 &lt;code>intermediate_size&lt;/code> 来表示，因此&lt;/p>
$$
\mathrm{parameter}(\texttt{Qwen3MLP}) = dd_{ff} + dd_{ff} + d_{ff}d=3dd_{ff}
$$&lt;p>这里三项分别代表 $W_1,W_3,W_2$ 的参数量。&lt;/p>
&lt;p>如果说，我们使用原始 transformer 的 MLP, 也就是&lt;/p>
$$
y = W_2\max(0,W_1x+b_1)+b_2
$$&lt;p>其中 $W_1\in\mathbb{R}^{d_{ff}\times d}$, $W_2\in\mathbb{R}^{d\times d_{ff}}$, $b_1\in\mathbb{R}^{d_{ff}}$, $b_2\in\mathbb{R}^d$, 则总参数为&lt;/p>
$$
\mathrm{parameter}(\texttt{TransformerMLP}) = d_{ff}d + dd_{ff} + d_{ff} +d = 2d_{ff}d + d_{ff} + d
$$&lt;p>这里的四项分别代表了 $W_1,W_2,b_1,b_2$.&lt;/p>
&lt;h3 id="qwen3attention">&lt;a href="#qwen3attention" class="header-anchor">&lt;/a>Qwen3Attention
&lt;/h3>&lt;p>接下来，就是 Attention 部分的参数，&lt;code>Qwen3Attention&lt;/code> 的关键代码如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Qwen3Attention&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Qwen3Config&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">layer_idx&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_attention_heads&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">attention_bias&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">k_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_key_value_heads&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">attention_bias&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">v_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_key_value_heads&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">attention_bias&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">o_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_attention_heads&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">attention_bias&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_norm&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Qwen3RMSNorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">eps&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">rms_norm_eps&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">k_norm&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Qwen3RMSNorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">eps&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">rms_norm_eps&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>这里，我们先定义几个量：&lt;/p>
&lt;ul>
&lt;li>我们将 head 的个数记为 $h$, 即 &lt;code>num_attention_heads&lt;/code>&lt;/li>
&lt;li>我们将每个 head 的 hidden size 记为 $h_d$, 即 &lt;code>head_dim&lt;/code>&lt;/li>
&lt;li>我们将 key 和 value head 的个数记为 $h_{kv}$ , 即 &lt;code>num_key_value_heads&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>&lt;code>Qwen3Attention&lt;/code> 的参数由以下几个部分组成：&lt;/p>
&lt;ul>
&lt;li>Query projection: $W_{Q}\in\mathbb{R}^{hh_d\times d}$, $b_Q\in\mathbb{R}^{hh_d}$&lt;/li>
&lt;li>Key projection: $W_K\in\mathbb{R}^{h_{kv}h_d\times d}$, $b_K\in\mathbb{R}^{h_{kv}h_d}$&lt;/li>
&lt;li>Value projection: $W_V\in\mathbb{R}^{h_{kv}h_d\times d}$, $b_V\in\mathbb{R}^{h_{kv}h_d}$&lt;/li>
&lt;li>output projection: $W_O\in\mathbb{R}^{d\times hh_d}$&lt;/li>
&lt;li>RMSNorm：前文已经提到过，两个 normalization (query norm 以及 key norm) 的总参数量为 $2h_d$.&lt;/li>
&lt;/ul>
&lt;p>因此， &lt;code>Qwen3Attention&lt;/code> 部分的总参数量为&lt;/p>
$$
\mathrm{parameter}(\texttt{Qwen3Attention}) = hh_{d}d + 2h_{kv}h_dd +dhh_d + 2h_d = 2hh_dd + 2h_{kv}h_dd + 2h_d
$$&lt;p>分别代表 $W_Q, W_O, W_K,W_V$ 和 两个 normalization layer 的参数量。&lt;/p>
&lt;p>注意，这里我们没有加入 bias, 这是因为 QKV bias 在 Qwen3 中被取消，取而代之的是两个 normalization.&lt;/p>
&lt;p>如果我们查看 &lt;code>Qwen2Attention&lt;/code> 的代码，我们可以得到 &lt;code>Qwen2Attention&lt;/code> 的总参数量为&lt;/p>
$$
\mathrm{parameter}(\texttt{Qwen2Attention}) = 2hh_{d}d + 2h_{kv}h_dd +h_{kv}h_d+hh_d+h_{kv}h_d
$$&lt;p>分别代表 $W_Q, W_K, W_V,W_O$ 和 $b_Q,b_K, b_V$ 的参数量。&lt;/p>
&lt;p>我们将计算结果汇总在一起就得到：&lt;/p>
$$
\begin{aligned}
\mathrm{parameter}(\texttt{Qwen3ForCausalLM}) &amp;=d|V| + \mathrm{parameter}(\texttt{Qwen3Model})\\
&amp;=2d|V| + N*\mathrm{parameter}(\texttt{Qwen3DecoderLayer}) + d\\
&amp;= N*(2d + \mathrm{parameter}(\texttt{Qwen3MLP}) + \mathrm{parameter}(\texttt{Qwen3Attention})) + d(2|V|+1)\\
&amp;= N*(2d+3dd_{ff}+2hh_{d}d + 2h_{kv}h_dd + 2h_d) + d(2|V|+1)\\
\end{aligned}
$$&lt;p>这是针对 &lt;code>Qwen3ForCausalLM&lt;/code> 的参数量计算。这里的变量定义如下：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Math Variable&lt;/th>
&lt;th>Code Variable&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$N$&lt;/td>
&lt;td>&lt;code>num_hidden_layers&lt;/code>&lt;/td>
&lt;td>Transformer block 个数&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\vert V\vert$&lt;/td>
&lt;td>&lt;code>vocab_size&lt;/code>&lt;/td>
&lt;td>词表大小&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$d$&lt;/td>
&lt;td>&lt;code>hidden_size&lt;/code>&lt;/td>
&lt;td>token embedding 的维度&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$d_{ff}$&lt;/td>
&lt;td>&lt;code>intermediate_size&lt;/code>&lt;/td>
&lt;td>MLP 的中间层的维度&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$h_d$&lt;/td>
&lt;td>&lt;code>head_dim&lt;/code>&lt;/td>
&lt;td>每个 head 的维度&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$h$&lt;/td>
&lt;td>&lt;code>num_attention_heads&lt;/code>&lt;/td>
&lt;td>query head 的个数&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$h_{kv}$&lt;/td>
&lt;td>&lt;code>num_key_value_heads&lt;/code>&lt;/td>
&lt;td>key 和 value head 的个数&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="verification">&lt;a href="#verification" class="header-anchor">&lt;/a>Verification
&lt;/h3>&lt;p>接下来，我们就可以基于 Qwen3 的模型来验证了，比如，&lt;code>Qwen3-32B&lt;/code> 的配置如下:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Value&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$N$&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\vert V\vert$&lt;/td>
&lt;td>151936&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$d$&lt;/td>
&lt;td>5120&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$d_{ff}$&lt;/td>
&lt;td>25600&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$h_d$&lt;/td>
&lt;td>128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$h$&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$h_{kv}$&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>根据上式，最终的参数量为&lt;/p>
$$
\mathrm{parameter}(\texttt{Qwen3-32B}) = 32.762123264*10^9\approx 32.8B
$$&lt;p>我们使用 &lt;code>Qwen3-32B&lt;/code> 的 &lt;code>index.json&lt;/code> 可以得到其真实的参数量为&lt;/p>
$$
\texttt{total\_size}/\texttt{precision} = 65524246528/2 = 32762123264
$$&lt;p>与我们计算的结果一致（这里除以 2 的原因是其表示模型权重文件的总大小，以 bytes 为单位，一般模型都是 &lt;code>bfloat16&lt;/code>, 大小为 2 个 bytes, 因此总参数量为总大小除以权重的精度）&lt;/p>
&lt;h2 id="moe-model">&lt;a href="#moe-model" class="header-anchor">&lt;/a>MoE Model
&lt;/h2>&lt;p>MoE model 与 Dense model 不同的地方在于每一层的 FFN, 因此，其总参数计算方式为：&lt;/p>
$$
\mathrm{parameter}(\texttt{Qwen3MoeForCausalLM})=N*(2d+\mathrm{parameter}(\texttt{Qwen3MoE})+hh_{d}d + 2h_{kv}h_dd +dhh_d + 2d) + d(2|V|+1)
$$&lt;p>对于 MoE layer, 其关键代码为：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Qwen3MoeSparseMoeBlock&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_experts&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_experts&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">top_k&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_experts_per_tok&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">gate&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_experts&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">experts&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ModuleList&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">Qwen3MoeMLP&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">intermediate_size&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">moe_intermediate_size&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_experts&lt;/span>&lt;span class="p">)])&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>我们记 $n$ 为总专家个数，即 &lt;code>num_experts&lt;/code>， 记 $k$ 为激活专家个数，即 &lt;code>num_experts_per_tok&lt;/code> 或者 &lt;code>top_k&lt;/code>,&lt;/p>
&lt;p>首先 &lt;code>gate&lt;/code> 的参数量为 $dn$, 接下来每个 expert 都是一个 &lt;code>Qwen3MLP&lt;/code>, 因此 &lt;code>experts&lt;/code> 总参数量为 $n * 3dd_{ff}$. 这样 MoE layer 的总参数量为&lt;/p>
$$
\mathrm{parameter}(\texttt{Qwen3MoE}) = nd + 3ndd_{ff}
$$&lt;p>在推理时，只有一部分专家，也就是 $k$ 个专家会参与计算，此时激活参数量为&lt;/p>
$$
\mathrm{parameter\_activated}(\texttt{Qwen3MoE}) = nd + 3kdd_{ff}
$$&lt;p>我们带入到 &lt;code>Qwen3MoeForCausalLM&lt;/code> 中就得到：&lt;/p>
&lt;p>模型总参数量为：&lt;/p>
$$
\mathrm{parameter}(\texttt{Qwen3MoeForCausalLM})=N*(2d+nd + 3ndd_{ff}+hh_{d}d + 2h_{kv}h_dd +dhh_d + 2h_d) + d(2|V|+1)
$$&lt;p>模型激活参数量为：&lt;/p>
$$
\mathrm{parameter\_activated}(\texttt{Qwen3MoeForCausalLM})=N*(2d+nd + 3kdd_{ff} + 2h_{kv}h_dd +dhh_d + 2h_d) + d(2|V|+1)
$$&lt;p>这里的变量定义如下：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Math Variable&lt;/th>
&lt;th>Code Variable&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$N$&lt;/td>
&lt;td>&lt;code>num_hidden_layers&lt;/code>&lt;/td>
&lt;td>Transformer block 个数&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\vert V\vert$&lt;/td>
&lt;td>&lt;code>vocab_size&lt;/code>&lt;/td>
&lt;td>词表大小&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$d$&lt;/td>
&lt;td>&lt;code>hidden_size&lt;/code>&lt;/td>
&lt;td>token embedding 的维度&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$d_{ff}$&lt;/td>
&lt;td>&lt;code>intermediate_size&lt;/code>&lt;/td>
&lt;td>MLP 的中间层的维度&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$h_d$&lt;/td>
&lt;td>&lt;code>head_dim&lt;/code>&lt;/td>
&lt;td>每个 head 的维度&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$h$&lt;/td>
&lt;td>&lt;code>num_attention_heads&lt;/code>&lt;/td>
&lt;td>query head 的个数&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$h_{kv}$&lt;/td>
&lt;td>&lt;code>num_key_value_heads&lt;/code>&lt;/td>
&lt;td>key 和 value head 的个数&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$n$&lt;/td>
&lt;td>&lt;code>num_experts&lt;/code>&lt;/td>
&lt;td>总专家个数&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$k$&lt;/td>
&lt;td>&lt;code>top_k&lt;/code> (&lt;code>num_experts_per_tok&lt;/code>)&lt;/td>
&lt;td>激活专家个数&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="moe-verification">&lt;a href="#moe-verification" class="header-anchor">&lt;/a>MoE Verification
&lt;/h3>&lt;p>我们用 &lt;code>Qwen3-235B-A22B&lt;/code> 来验证，其配置如下：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Value&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$N$&lt;/td>
&lt;td>94&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\vert V\vert$&lt;/td>
&lt;td>151936&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$d$&lt;/td>
&lt;td>4096&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$d_{ff}$&lt;/td>
&lt;td>1536&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$h_d$&lt;/td>
&lt;td>128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$h$&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$h_{kv}$&lt;/td>
&lt;td>4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$n$&lt;/td>
&lt;td>128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$k$&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>计算得到总参数量为&lt;/p>
$$
\mathrm{parameter}(\texttt{Qwen3-235B-A22B}) = 235.09363456*10^9\approx 235B
$$&lt;p>计算得到激活参数量为&lt;/p>
$$
\mathrm{parameter\_activated}(\texttt{Qwen3-235B-A22B}) = 22.14456064 * 10^9
$$&lt;p>实际总参数量为&lt;/p>
$$
470187269120/2 = 235093634560.0\approx 235B
$$&lt;p>可以看到，计算结果与实际相符。&lt;/p>
&lt;h2 id="extension">&lt;a href="#extension" class="header-anchor">&lt;/a>Extension
&lt;/h2>&lt;p>通过以上计算过程，我们可以很轻松将上述公式扩展到混合架构或者是 MLA 上，对于混合架构，我们分别计算不同 attention 的 layer 个数，然后分别计算。对于 MLA, 我们可以替换 Attention 的计算逻辑。&lt;/p>
&lt;p>对于小语言模型系列，比如 0.6B 和 1.7B, 模型的大部分参数集中在 embedding 上，因此 Qwen3 采取了 tie embedding 的方式来减少参数量，具体做法就是 &lt;code>nn.Embedding&lt;/code> 和 &lt;code>lm_head&lt;/code> 共享参数。&lt;/p>
&lt;h2 id="visualization">&lt;a href="#visualization" class="header-anchor">&lt;/a>Visualization
&lt;/h2>&lt;p>接下来，我们来可视化一下不同大小模型不同模块的参数量占比。计算的代码如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">compute_param_distribution&lt;/span>&lt;span class="p">(&lt;/span> &lt;span class="n">N&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">V&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d_ff&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">h_d&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">h&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">h_kv&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">tie_word_embeddings&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">bool&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">dict&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> Compute parameter distribution across different components of the model.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> Args:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> N: Number of layers
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> V: Vocabulary size
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> d: Hidden dimension
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> d_ff: Feed-forward dimension
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> h_d: Head dimension
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> h: Number of attention heads
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> h_kv: Number of key/value heads
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> tie_word_embeddings: Whether input and output embeddings are tied
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> Returns:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> dict: Parameter counts for each component
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Embedding parameters&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">embedding&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">V&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="n">tie_word_embeddings&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">embedding&lt;/span> &lt;span class="o">*=&lt;/span> &lt;span class="mi">2&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Attention parameters per layer&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attention_per_layer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">h&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">h_d&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">h_kv&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">h_d&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">h&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">h_d&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">h_d&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attention_total&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">attention_per_layer&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">N&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># FFN parameters per layer&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ffn_per_layer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">3&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d_ff&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ffn_total&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ffn_per_layer&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">N&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># RMSNorm (2d), QK-Norm (2d), output normalization (d)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">others&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">N&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">4&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">d&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;Embedding&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">embedding&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;Attention&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">attention_total&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;FFN&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">ffn_total&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;Others&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">others&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>对于 dense 模型，每部分的参数量可视化如下：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/llm-parameter-computation/Qwen3_param_distribution.png"
width="1200"
height="600"
loading="lazy"
alt="Parameter distribution across model components"
class="gallery-image"
data-flex-grow="200"
data-flex-basis="480px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/llm-parameter-computation/Qwen3_param_percentage.png"
width="1200"
height="600"
loading="lazy"
alt="Parameter percentage across model components"
class="gallery-image"
data-flex-grow="200"
data-flex-basis="480px"
>&lt;/p>
&lt;p>可以看到，随着模型 size 增加，模型大部分参数量都集中在 FFN 上。&lt;/p>
&lt;p>接下来，我们可视化一下 MoE 模型的参数分布，核心计算代码如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">compute_moe_param_distribution&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">N&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">V&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d_ff&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">h_d&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">h&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">h_kv&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">activate&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">dict&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> Compute parameter distribution across different components of the MoE model.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> Args:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> N: Number of layers
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> V: Vocabulary size
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> d: Hidden dimension
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> d_ff: Feed-forward dimension
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> h_d: Head dimension
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> h: Number of attention heads
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> h_kv: Number of key/value heads
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> n: Number of experts
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> k: Number of experts per token
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> activate: Whether to compute activated parameters
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> Returns:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> dict: Parameter counts for each component
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Embedding parameters&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">embedding&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">V&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Attention parameters per layer&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attention_per_layer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">h&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">h_d&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">h_kv&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">h_d&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">h&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">h_d&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">h_d&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attention_total&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">attention_per_layer&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">N&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># FFN parameters per layer&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">activate&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">moe_per_layer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">3&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">k&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d_ff&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">moe_per_layer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">3&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d_ff&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ffn_total&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">moe_per_layer&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">N&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># RMSNorm (2d), QK-Norm (2d), output normalization (d)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">others&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">N&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">h_d&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">d&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;Embedding&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">embedding&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;Attention&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">attention_total&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;MoE&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">ffn_total&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;Others&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">others&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/llm-parameter-computation/Qwen3_moe_param_distribution.png"
width="1189"
height="589"
loading="lazy"
alt="Parameter distribution across MoE model components"
class="gallery-image"
data-flex-grow="201"
data-flex-basis="484px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/llm-parameter-computation/Qwen3_moe_param_percentage.png"
width="1188"
height="589"
loading="lazy"
alt="Parameter percentage across MoE model components"
class="gallery-image"
data-flex-grow="201"
data-flex-basis="484px"
>&lt;/p>
&lt;p>可以看到，MoE 模型的大部分参数还是集中在 MoE 模块上，但是由于其稀疏机制，在激活的参数里，MoE 占比从 95% 以上降低到了 60% 左右。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>在本文中，我们基于 Qwen3 大语言模型系列，介绍了如何计算 dense 模型和 MoE 模型的参数量。模型的参数量计算为后面的显存占用以及优化提供了基础。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f" target="_blank" rel="noopener"
>Qwen3 Collection&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Seed1.6</title><link>https://maosong.website/p/notes-on-seed1.6/</link><pubDate>Fri, 18 Jul 2025 14:59:35 +0800</pubDate><guid>https://maosong.website/p/notes-on-seed1.6/</guid><description>&lt;p>Seed 在 2025 年 6 月 25 号发布了 Seed 1.6，支持 adaptive deep thinking, multimodal understanding, GUI-based interaction, and deep reasoning with a 256K context window&lt;/p>
&lt;p>Seed 1.6 基于 Seed 1.5 开发，是一个 MoE 模型, 总参数为 230B, 激活参数为 23B.&lt;/p>
&lt;h2 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h2>&lt;p>pre-training 分为 3 个 stage:&lt;/p>
&lt;ol>
&lt;li>Text-only pre-training: 使用 web pages, books, papers, code 以及其他数据来进行训练，作者使用了 rule-based 以及 model-based 策略来提高数据的质量。&lt;/li>
&lt;li>Multimodal mixed continue training: 提高学科，代码以及 reasoning 相关的数据，来进一步提高知识密度以及 reasoning 的复杂度。同时，加入高质量的视觉数据。&lt;/li>
&lt;li>Long-context continual training: 将模型的上下文长度从 32K 扩展到 256K&lt;/li>
&lt;/ol>
&lt;p>Seed 1.6 Base 的表现&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Seed1.6 Base&lt;/th>
&lt;th>LLaMA-4 -Maverick Base&lt;/th>
&lt;th>DeepSeek-V3Base&lt;/th>
&lt;th>Qwen3-235B-A22B Base&lt;/th>
&lt;th>Seed1.5 Base&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>MMLU&lt;/td>
&lt;td>&lt;strong>88.83&lt;/strong>&lt;/td>
&lt;td>85.16&lt;/td>
&lt;td>87.19&lt;/td>
&lt;td>87.81&lt;/td>
&lt;td>88.35&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MMLU-Pro&lt;/td>
&lt;td>&lt;strong>69.98&lt;/strong>&lt;/td>
&lt;td>63.91&lt;/td>
&lt;td>59.84&lt;/td>
&lt;td>68.18&lt;/td>
&lt;td>66.47&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SuperGPQA&lt;/td>
&lt;td>&lt;strong>45.08&lt;/strong>&lt;/td>
&lt;td>40.85&lt;/td>
&lt;td>41.53&lt;/td>
&lt;td>44.06&lt;/td>
&lt;td>36.81&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>BBH&lt;/td>
&lt;td>&lt;strong>92.08&lt;/strong>&lt;/td>
&lt;td>83.62&lt;/td>
&lt;td>86.22&lt;/td>
&lt;td>88.87&lt;/td>
&lt;td>88.36&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GPQA-Diamond&lt;/td>
&lt;td>43.43&lt;/td>
&lt;td>43.94&lt;/td>
&lt;td>41.92&lt;/td>
&lt;td>&lt;strong>47.47&lt;/strong>&lt;/td>
&lt;td>45.25&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GSM8k&lt;/td>
&lt;td>93.10&lt;/td>
&lt;td>87.72&lt;/td>
&lt;td>87.57&lt;/td>
&lt;td>&lt;strong>94.39&lt;/strong>&lt;/td>
&lt;td>89.99&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MATH&lt;/td>
&lt;td>&lt;strong>72.86&lt;/strong>&lt;/td>
&lt;td>63.32&lt;/td>
&lt;td>62.62&lt;/td>
&lt;td>71.84&lt;/td>
&lt;td>66.18&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MBPP&lt;/td>
&lt;td>&lt;strong>83.60&lt;/strong>&lt;/td>
&lt;td>75.40&lt;/td>
&lt;td>74.20&lt;/td>
&lt;td>81.40&lt;/td>
&lt;td>81.40&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h2>&lt;p>作者在 post-training 构建了两个变体：&lt;/p>
&lt;ol>
&lt;li>Seed1.6-Thinking: 先进的多模态 reasoning 能力&lt;/li>
&lt;li>Seed1.5(Adaptive CoT): 使用 AdaCoT 来压缩 CoT 的长度，提高效率&lt;/li>
&lt;/ol>
&lt;h3 id="seed16-thinking">&lt;a href="#seed16-thinking" class="header-anchor">&lt;/a>Seed1.6-Thinking
&lt;/h3>&lt;p>Seed1.6-Thinking 与 Seed-Thinking-v1.5 的训练方式差不多，作者使用了 multi-stage [[rejection fine-tuning]] (RFT) 和 RL 来提高模型的表现。在每一轮中，先进行 RL 的训练，然后进行 RFT 的训练。相比于 Seed-Thinking-v1.5, Seed1.6-Thinking 使用了更多的算力和更高质量的数据，包括数学问题，代码，谜题以及 non-reasoning 数据。&lt;/p>
&lt;p>表现如下：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-seed1.6/Seed1_6_thinking_LLM_performance.png"
width="5436"
height="2498"
loading="lazy"
alt="Seed1.6-Thinking performance (LLM)"
class="gallery-image"
data-flex-grow="217"
data-flex-basis="522px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-seed1.6/Seed1_6_thinking_VLM_performance.png"
width="5255"
height="2514"
loading="lazy"
alt="Seed1.6-Thinking performance (VLM)"
class="gallery-image"
data-flex-grow="209"
data-flex-basis="501px"
>&lt;/p>
&lt;p>为了进一步提高模型的表现，作者在推理时使用了 parallel decoding, 来让模型在回答之前使用更多的 token, 作者发现 parallel decoding 可以显著提高模型在困难任务上的表现。&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-seed1.6/Seed1_6_thinking_parallel_decoding_performance.png"
width="4468"
height="1468"
loading="lazy"
alt="Performance of Seed1.6 Thinking parallel decoding"
class="gallery-image"
data-flex-grow="304"
data-flex-basis="730px"
>&lt;/p>
&lt;h3 id="seed15adaptive-cot">&lt;a href="#seed15adaptive-cot" class="header-anchor">&lt;/a>Seed1.5(Adaptive CoT)
&lt;/h3>&lt;p>reasoning model 的问题是会出现 overthinking,导致输出的过程中花费很多不必要的 token.&lt;/p>
&lt;p>为了解决这个问题，Seed1.6 提出了 AdaCoT, 来训练得到三个模型：FullCoT, NoCoT 和 AdaCoT&lt;/p>
&lt;p>通过 AdaCoT, 我们可以显著降低 CoT 的长度。为了完成这个目标，作者在 RL 训练阶段构建了一个 reward, 来惩罚模型的 overthinking. 用户可以基于 prompt 来选择不同的模型：&lt;/p>
&lt;ul>
&lt;li>FullCoT: 对每个 prompt，都进行 reasoning, 模型表现与 Seed1.6-Thinking 差不多，但是 CoT length 更小&lt;/li>
&lt;li>NoCoT: 直接回答，不进行 reasoning&lt;/li>
&lt;li>AdaCoT: 由模型自主决定是否需要进行 reasoning, 模型是 FullCoT 和 NoCoT 的一个 fusion 版本&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-seed1.6/Seed1_6_AdaCoT_performance.png"
width="6798"
height="3416"
loading="lazy"
alt="Seed1.6-AdaCoT performance"
class="gallery-image"
data-flex-grow="199"
data-flex-basis="477px"
>&lt;/p>
&lt;p>结果发现，对于不同难度的任务，AdaCoT 的出发概率是不同的，说明模型会基于任务难度调整思考方式。&lt;/p>
&lt;p>作者还评测了以下模型在高考中的表现，结果如下图：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-seed1.6/Seed1_6_gaokao_performance.png"
width="5195"
height="4645"
loading="lazy"
alt="Seed1.6 performance on gaokao"
class="gallery-image"
data-flex-grow="111"
data-flex-basis="268px"
>&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者提出了 Seed1.6 系列多模态大模型，模型在多个 benchmark 上表现优异。作者还提出了 AdaCoT, 让模型基于问题难度动态选择 CoT length&lt;/p>
&lt;p>作者认为下一步工作是：&lt;/p>
&lt;ol>
&lt;li>构建更加高效的架构，来进一步提高 reasoning 的有效性&lt;/li>
&lt;li>扩展模型的多模态能力&lt;/li>
&lt;li>提升模型的 agent 能力，特鄙视 end-to-end task execution&lt;/li>
&lt;/ol>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://seed.bytedance.com/en/blog/introduction-to-techniques-used-in-seed1-6" target="_blank" rel="noopener"
>blog&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on V-Triune</title><link>https://maosong.website/p/notes-on-v-triune/</link><pubDate>Thu, 17 Jul 2025 09:37:36 +0800</pubDate><guid>https://maosong.website/p/notes-on-v-triune/</guid><description>&lt;p>V-Triune 探究了如何使用一套统一的 RL 训练框架，来同时提高模型的 perception 和 reasoning 能力。&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>已有的 RL 训练方法主要提升模型在特定 domain 上的能力，比如 MM-Eureka, Vision-R1 等专注于提升 MLLM 的 reasoning 能力，而 R1-V, DeepPerception 等专注于提升模型的 perception 能力。&lt;/p>
&lt;p>因此，本文的 motivation 就是，如何用一套统一的 RL 训练框架，来同时提高模型的 perception 和 reasoning 能力。&lt;/p>
&lt;p>V-Triune 包含 3 个关键模块：&lt;/p>
&lt;ul>
&lt;li>Sample-Level data formatting: 即在 sample 的层面定义 reward 等信息&lt;/li>
&lt;li>Verifier-Level Computation: 即分离 verifier 和 generator, 提高整体效率&lt;/li>
&lt;li>Source-Level Metric Monitoring: 基于 data source 来监控模型的表现&lt;/li>
&lt;/ul>
&lt;p>作者还提出了 Dynamic IoU reward, 来提高训练的稳定性。&lt;/p>
&lt;p>基于 V-Triune 和 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5-vl/" target="_blank" rel="noopener"
>Qwen2.5-VL&lt;/a>, 作者提出了 Orsta,一个在 reasoning 和 perception 任务上均有提升的多模态大模型。&lt;/p>
&lt;p>论文的主要贡献为：&lt;/p>
&lt;ol>
&lt;li>提出了 V-Triune, 一个统一的 RL 训练框架，来同时提高模型的 reasoning 和 perception 能力&lt;/li>
&lt;li>在 infra 上进行了改进，提高整体的训练效率和稳定性&lt;/li>
&lt;li>基于 V-Triune, 构建了 Orsta, 相比于 Baseline, 模型的 perception 和 reasoning 能力均有了提升。&lt;/li>
&lt;/ol>
&lt;h2 id="preliminary">&lt;a href="#preliminary" class="header-anchor">&lt;/a>Preliminary
&lt;/h2>&lt;p>作者首先回顾了一下 GRPO 和 &lt;a class="link" href="https://maosong.website/p/notes-on-dapo/" target="_blank" rel="noopener"
>DAPO&lt;/a> , 我们这里就不再赘述。&lt;/p>
&lt;p>然后，作者介绍了一下 reward function, reward function 由两部分组成， 第一部分是 format reward, 第二部分是 accuracy reward,.&lt;/p>
&lt;p>对于 format reward, 作者采取了和 OpenR1 相同的做法，也就是要求输出仅包含 1 个给定的 token, 这里特定的 token 为 $s_1=\texttt{&lt;think>}$, $s_2=\texttt{&lt;/think>}$, $s_3=\texttt{&lt;answer>}$, $s_4=\texttt{&lt;/answer>}$, reward 的定义如下：&lt;/p>
$$
R_{\mathrm{format}}(o_q) = 0.25\sum_{i=1}^4 \mathbb{1}(\mathrm{count}(o_q,s_i)=1)
$$&lt;p>这里 $\mathbb{1}(\cdot)$ 是示性函数。&lt;/p>
&lt;p>对于 accuracy reward, 作者根据不同的任务分别进行处理。&lt;/p>
&lt;p>对于 reasoning 任务，reward function 的定义如下&lt;/p>
$$
R_{\mathrm{acc}}(\hat{a},a) = \begin{cases}
1, &amp;\text{ if }\texttt{verify(parse($\hat{a}$), parse($a$))}\text{ is True}\\
0, &amp;\text{ else}
\end{cases}
$$&lt;p>这里 $\hat{a}$, $a$ 分别是 prediction 和 ground truth.&lt;/p>
&lt;p>对于 perception 任务，reward function 与我们要处理的子任务相关。如果是 OCR 和 counting 任务，则我们采取和 reasoning 一样的 reward function. 如果我们要处理的是 grounding 和 detection 任务，则我们可以使用 IoU 以及 mAP 两种 reward 形式。&lt;/p>
&lt;p>IoU reward 的定义如下：&lt;/p>
$$
R_{\mathrm{acc}}(\hat{a},a) = \begin{cases}
\mathrm{IoU}(\hat{a}, a), &amp;\text{ if }\mathrm{IoU}(\hat{a},a)\geq \epsilon\\
0, &amp;\text{ else}
\end{cases}
$$&lt;p>这里 $\epsilon>0$ 为超参数， IoU 定义如下&lt;/p>
$$
\mathrm{IoU}(\hat{a},a) = \frac{\mathrm{Area}(\hat{a}\cap a)}{\mathrm{Area}(\hat{a}\cup a)}
$$&lt;p>mAP 的定义如下&lt;/p>
$$
\mathrm{AP}_c = \int_0^1 \max_{\tilde{r}\geq r}P_c(\tilde{r}) dr,\quad mAP=\frac1C\sum_{c=1}^C \mathrm{AP}_c
$$&lt;p>最终的 reward 是 format reward 和 accuracy reward 的加权求和：&lt;/p>
$$
R = \alpha_{\mathrm{acc}}R_{\mathrm{acc}} + \alpha_{\mathrm{format}}R_{\mathrm{format}}
$$&lt;h2 id="v-triune">&lt;a href="#v-triune" class="header-anchor">&lt;/a>V-Triune
&lt;/h2>&lt;p>V-Triune 框架如下图所示，其包含三个模块&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-v-triune/V-Triune_framework.png"
width="1351"
height="562"
loading="lazy"
alt="V-Triune System"
class="gallery-image"
data-flex-grow="240"
data-flex-basis="576px"
>&lt;/p>
&lt;h3 id="sample-level-data-formatting">&lt;a href="#sample-level-data-formatting" class="header-anchor">&lt;/a>Sample-Level Data Formatting
&lt;/h3>&lt;p>核心思想就是不同的任务的 reward function 可能是不一样的，如果在 task 层面设计 reward function 的话，RL 训练就没有了 flexibility, 因此，在本文中，作者在 sample 层面设计 reward function. 具体做法就是，每个样本除了 QA 相关信息，还有 reward model 的相关参数，这样就可以更加灵活地调整不同 sample 的损失了。&lt;/p>
&lt;h3 id="verifier-level-reward-computation">&lt;a href="#verifier-level-reward-computation" class="header-anchor">&lt;/a>Verifier-Level Reward Computation
&lt;/h3>&lt;p>核心思想就是构建多个不同的 Verifier, 来分别处理不同的 sample, 因为我们 reward model 也是在 sample 层面构建的，因此我们可以基于 sample 的 metadata 来动态分配对应的 verifier, 这样就提升了 veriier 的可扩展性。本文作者使用了 MathVerifier 和 DetectionVerifier 两种。&lt;/p>
&lt;h3 id="source-level-metric-monitoring">&lt;a href="#source-level-metric-monitoring" class="header-anchor">&lt;/a>Source-Level Metric Monitoring
&lt;/h3>&lt;p>核心思想就是根据 sample 的 metadata 信息来监控不同的 metric, 这样可以防止训练不稳定。&lt;/p>
&lt;h3 id="dynamic-iou-reward">&lt;a href="#dynamic-iou-reward" class="header-anchor">&lt;/a>Dynamic IoU Reward
&lt;/h3>&lt;p>核心思想就是根据训练进程，动态调整 IoU 的阈值。&lt;/p>
&lt;p>作者发现，如果将 IoU 阈值设置的比较低的话，模型很容易拿到比较高的 reward; 如果将 IoU 阈值设置的比较高的话，模型又很难拿到奖励。&lt;/p>
&lt;p>因此，作者的解决方案就是，使用课程学习，随着训练的进行，逐步提高 IoU 的阈值。&lt;/p>
&lt;h2 id="data">&lt;a href="#data" class="header-anchor">&lt;/a>Data
&lt;/h2>&lt;p>数据集列举如下：&lt;/p>
&lt;ul>
&lt;li>Math: mm_math, geometry3K, mmk12&lt;/li>
&lt;li>Puzzle: PuzzleVQA, AlgoPuzzleQA, VisualPuzzles&lt;/li>
&lt;li>Science: ScienceQA, SciVQA, ViRL39K (Broader STEM topics, (GradeSchool) Science)&lt;/li>
&lt;li>Chart: ChartQAPro, ChartX, Table-VQA, ViRL39K (Tables/Diagrams/Charts)&lt;/li>
&lt;li>Detection: V3Det, Object365&lt;/li>
&lt;li>Grounding: D3&lt;/li>
&lt;li>Counting: CLEVR&lt;/li>
&lt;li>OCR: LLaVA-OV (OCR questions), EST-VQA&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Rule-based filtering&lt;/strong>
对于 visual reasoning 数据，作者的过滤如下：&lt;/p>
&lt;ul>
&lt;li>多项选择题以及判断题，防止 reward hacking&lt;/li>
&lt;li>答案包含特殊符号如 &amp;ldquo;=&amp;rdquo;, &amp;ldquo;[&amp;rdquo; 等的数据&lt;/li>
&lt;li>答案字符数超过 20 个，防止答案太复杂&lt;/li>
&lt;/ul>
&lt;p>对于 visual perception 数据，作者过滤流程如下：&lt;/p>
&lt;ul>
&lt;li>Detection: 超过 10 个 bounding box 的，或者 box 占 50% 以上面积的数据，保持 single BB 和 multi BB 的比例为 1:2&lt;/li>
&lt;li>Grounding: box 占 50% 以上面积的数据, label 过于复杂的数据&lt;/li>
&lt;li>Counting: 保持类别平衡，仅使用英文数据&lt;/li>
&lt;li>OCR: 保留英文数据，对 labels 进行 verify&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Difficulty-based filtering&lt;/strong>
主要是移除比较简单的问题。&lt;/p>
&lt;p>经过这两个阶段的过滤之后，得到了 &lt;strong>20.6K&lt;/strong> perception samples 以及 &lt;strong>27.1K&lt;/strong> reasoning samples. 数据保存在 Parquet 格式&lt;/p>
&lt;h2 id="training-recipe">&lt;a href="#training-recipe" class="header-anchor">&lt;/a>Training Recipe
&lt;/h2>&lt;h3 id="disable-vit-training">&lt;a href="#disable-vit-training" class="header-anchor">&lt;/a>Disable ViT Training
&lt;/h3>&lt;p>作者首先发现，全量微调会导致训练崩溃。作者分析原因发现，主要是 ViT 在反向传播过程中，存在 gradient amplify 的问题，第一层与最后一层的梯度的 norm 相差 5-10 倍。作者认为有两点原因：&lt;/p>
&lt;ol>
&lt;li>RL 会强制要求模型的不同模态之间进行对齐&lt;/li>
&lt;li>对比学习训练得到的 VIT 可能使得其不适合 RL 的训练。&lt;/li>
&lt;/ol>
&lt;p>基于这两点考虑，作者再进行训练的时候，冻结了 ViT 模块，结果如下图&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-v-triune/V-Triune-frozen-ViT.png"
width="1383"
height="705"
loading="lazy"
alt="Analysis of ViT training instability"
class="gallery-image"
data-flex-grow="196"
data-flex-basis="470px"
>&lt;/p>
&lt;h3 id="spurious-image-special-tokens">&lt;a href="#spurious-image-special-tokens" class="header-anchor">&lt;/a>Spurious Image Special Tokens
&lt;/h3>&lt;p>作者发现，模型的输出可能会包含一些 &lt;code>&amp;lt;image_pad&amp;gt;&lt;/code> 等 token, 因此，作者对输出进行了过滤，然后再进行计算。&lt;/p>
&lt;h3 id="cot-prompt-pool">&lt;a href="#cot-prompt-pool" class="header-anchor">&lt;/a>CoT Prompt Pool
&lt;/h3>&lt;p>作者还发现，多样化的 CoT prompt 会损害模型的表现。因此，作者构建了 20 个 prompt 模版，每次随机挑选其中一个加入到 instruction 中。&lt;/p>
&lt;h3 id="training-configuration">&lt;a href="#training-configuration" class="header-anchor">&lt;/a>Training Configuration
&lt;/h3>&lt;p>模型基于 Qwen2.5-7B-instruct 和 Qwen2.5VL-32B 来进行开发&lt;/p>
&lt;p>作者探究了 on-policy 和 off-policy 两种配置。roll-out batch size 设置为 1024, 使用 GRPO 进行训练，GRPO 的 group size 设置为 8&lt;/p>
&lt;p>作者还设置 $\epsilon_{high}=0.28$ 以及 $\epsilon_{low}=0.2$ 来提高 token 的采样率。作者去除了 KL divergence loss, 并且在 token 层面上进行平均&lt;/p>
&lt;h2 id="evaluation">&lt;a href="#evaluation" class="header-anchor">&lt;/a>Evaluation
&lt;/h2>&lt;h3 id="performance">&lt;a href="#performance" class="header-anchor">&lt;/a>Performance
&lt;/h3>&lt;p>&lt;img src="https://maosong.website/p/notes-on-v-triune/V-Triune-performance.png"
width="1159"
height="1051"
loading="lazy"
alt="Performance of Orsta on MEGA-Bench core"
class="gallery-image"
data-flex-grow="110"
data-flex-basis="264px"
>&lt;/p>
&lt;h3 id="analysis">&lt;a href="#analysis" class="header-anchor">&lt;/a>Analysis
&lt;/h3>&lt;p>&lt;strong>on-policy v.s. off-policy&lt;/strong>
作者首先对比了以下 on-policy RL 和 off-policy RL 两种训练方式的表现， 结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-v-triune/V-triune-on-policy.png"
width="1137"
height="526"
loading="lazy"
alt="Ablation on on-policy v.s. off-policy"
class="gallery-image"
data-flex-grow="216"
data-flex-basis="518px"
>&lt;/p>
&lt;p>结果有两点发现：&lt;/p>
&lt;ol>
&lt;li>on-policy 的表现基本上都是比 off-policy 要好的。&lt;/li>
&lt;li>小模型通过 RL 带来的表现提升更明显，大模型的表现则更慢&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>generalization&lt;/strong>
作者还评估了以下模型的泛化性，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-v-triune/V-Triune-generalization.png"
width="422"
height="673"
loading="lazy"
alt="Generalization of V-Triune"
class="gallery-image"
data-flex-grow="62"
data-flex-basis="150px"
>&lt;/p>
&lt;p>结果发现，RL 的训练更像是对齐，而不是泛化。也就是说，模型在 in-domain 的任务上表现较好，在 OOD 的任务上表现就一般。这与已有的结论是不一样的&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Recall
&lt;a class="link" href="https://maosong.website/p/notes-on-magistral/" target="_blank" rel="noopener"
>Magistral&lt;/a> 发现模型在 math domain 上进行训练，在 code domain 上的表现也会提升&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Training Dynamics&lt;/strong>
作者还分析了不同任务不同指标随训练进行的变化情况，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-v-triune/V-Triune-training-dynamics.png"
width="1150"
height="900"
loading="lazy"
alt="V-Triune training dynamics"
class="gallery-image"
data-flex-grow="127"
data-flex-basis="306px"
>&lt;/p>
&lt;p>结果显示，reasoning 和 OCR 任务岁训练进行其输出长度和正确率都有所提升。但是 detection 相关任务则变化不是很明显。&lt;/p>
&lt;h3 id="ablation-study">&lt;a href="#ablation-study" class="header-anchor">&lt;/a>Ablation Study
&lt;/h3>&lt;p>作者进行了三个消融实验：&lt;/p>
&lt;ol>
&lt;li>训练策略：冻结 ViT, 冻结 LLM, 两者都不冻结&lt;/li>
&lt;li>任务分解策略：仅使用 reasoning 数据进行训练，仅使用 perception 数据进行训练，同时使用两种数据进行训练。&lt;/li>
&lt;li>learning rate: 不同的学习率&lt;/li>
&lt;/ol>
&lt;p>实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-v-triune/V-Triune-ablation-study.png"
width="1146"
height="585"
loading="lazy"
alt="Ablation study"
class="gallery-image"
data-flex-grow="195"
data-flex-basis="470px"
>&lt;/p>
&lt;p>结果发现，&lt;/p>
&lt;ol>
&lt;li>仅训练 ViT 对表现没有任何帮助，只有训练 LLM 才会带来性能的提升。&lt;/li>
&lt;li>同时使用两种数据进行训练的效果是最好的，其次是仅使用 reasoning 数据进行训练，最后是仅使用 perception 数据进行训练&lt;/li>
&lt;li>较大的学习率会损害模型的表现。&lt;/li>
&lt;/ol>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>本文中，作者提出了 V-Triune, 一个使用 RL 来同时提高 MLLM 的 reasoning 和 perception 能力的 RL 训练框架。&lt;/p>
&lt;p>作者发现 RL 对于 VLM 来说更像是一种 Alignment 策略，而不是一种 Generalization 策略，也就是模型并没有引入新的能力，而是提升模型本身的 utility 以及 robustness.&lt;/p>
&lt;p>作者认为本文有以下 Limitation:&lt;/p>
&lt;ol>
&lt;li>在 perception 任务上没有看到明显的 test-time scaling 现象。作者认为，探究使用 o3 类似的方法或许能提高模型的表现&lt;/li>
&lt;li>探究 RL-zero 在 VLM 中的应用，也就是直接在 base model 上进行 RL 的训练。&lt;/li>
&lt;/ol>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2505.18129" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Magistral</title><link>https://maosong.website/p/notes-on-magistral/</link><pubDate>Wed, 16 Jul 2025 11:04:04 +0800</pubDate><guid>https://maosong.website/p/notes-on-magistral/</guid><description>&lt;p>Magistral 是 Mistral 提出的一个 reasoning model 系列，主要针对 math 和 code 两个 domain&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>Mistral 在 2025 年 6 月 12 日发布了 Magistral ，一个 reasoning model, 包含两个模型，一个是由纯 RL 训练得到的 &lt;em>Magistral Medium&lt;/em>, 另一个是由 SFT 和蒸馏 Magistral Medium 得到的 &lt;strong>Magistral Small&lt;/strong>&lt;/p>
&lt;p>作者首先介绍了一下本文的贡献：&lt;/p>
&lt;ol>
&lt;li>介绍了如何仅使用 RL (而不是用蒸馏) 来训练 Magistral Medium&lt;/li>
&lt;li>infra 上的改进，主要使用最新的权重来更新 generator&lt;/li>
&lt;li>多语种能力，支持多种语言&lt;/li>
&lt;li>系统性探究了 RLVR 的能力边界&lt;/li>
&lt;li>开源了 Magistral small (24B)&lt;/li>
&lt;/ol>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="rl">&lt;a href="#rl" class="header-anchor">&lt;/a>RL
&lt;/h3>&lt;p>RL 算法基于 GRPO 改进，主要有以下几点：&lt;/p>
&lt;ol>
&lt;li>去掉了 KL Divergence loss, 这一点跟 &lt;a class="link" href="https://maosong.website/p/notes-on-dapo/" target="_blank" rel="noopener"
>DAPO&lt;/a> 是一致的，提升模型的探索能力&lt;/li>
&lt;li>Loss Normalization，在 sample 层面做平均，还是跟 DAPO 一致，减少不同长度输出对训练造成的影响&lt;/li>
&lt;li>Advantage Normalization, 作者首先将每个 token 的 Advantage 定义为 $\hat{A}_i=R_i-\mu$, 其中 $\mu$ 是每个 group 的 reward 均值， 然后在每个 mini-batch 里对 advantage 进行 Normalization (这里 $\hat{A}_{mean}$ 和 $\hat{A}_{std}$ 分别为 mini-batch advantage 的均值和方差)：&lt;/li>
&lt;/ol>
$$
\hat{A}_{i,t}^{norm}=\frac{\hat{A}_i-\hat{A}_{mean}}{\hat{A}_{std}}
$$&lt;ol start="4">
&lt;li>CLIP-Higher, 跟 DAPO 一致，提高稀有 token 的被采样概率&lt;/li>
&lt;li>Eliminating non-divsere groups, 跟 DAPO 一致，去掉过于简单和过于难的题目&lt;/li>
&lt;/ol>
&lt;p>最终 RL 阶段训练的损失函数为：&lt;/p>
$$
\mathcal{L}(\theta) = \mathbb{E}_{q\sim P(Q),\{o_i\}_{i=1}^G\sim \pi_{old}(\cdot\mid q)}\frac{1}{\sum_{i=1}^G|o_i|}\sum_{i=1}^G\sum_{t=1}^{|o_i|}\min\left[r_{i,t}(\theta)\hat{A}_{i,t}^{norm}, \mathrm{clip}(r_{i,t}(\theta), 1-\epsilon_{low}, 1+\epsilon_{high})\hat{A}_{i,t}^{norm}\right], \mathrm{s.t.}, \exists 1\leq m &lt; n \leq G, r_m\neq r_n.
$$&lt;p>其中&lt;/p>
$$
r_{i,t}(\theta) = \frac{\pi_{\theta}(o_{i,t}\mid q, o_{i,&lt;t})}{\pi_{old}(o_{i,t}\mid q, o_{i,&lt;t})}
$$&lt;h3 id="reward-shaping">&lt;a href="#reward-shaping" class="header-anchor">&lt;/a>Reward Shaping
&lt;/h3>&lt;p>作者还基于四个维度来构建 reward: formatting, correctness, length, 以及 language consistency&lt;/p>
&lt;p>&lt;strong>Formatting&lt;/strong>
针对数学和代码问题，作者要求模型输出符合特定的格式&lt;/p>
&lt;ul>
&lt;li>Tag requirements: 思考过程用 &lt;code>&amp;lt;think&amp;gt;&amp;lt;/think&amp;gt;&lt;/code> 包含，且只能包含一个 tag&lt;/li>
&lt;li>mathematical responses: 对于数学问题，结果用 &lt;code>\boxed{}&lt;/code> 包含&lt;/li>
&lt;li>code response：包含一个 markdown block&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>[!tip] Future
最新的 &lt;a class="link" href="https://maosong.website/p/notes-on-glm-4.1v-thinking/" target="_blank" rel="noopener"
>GLM-4.1V-Thinking&lt;/a> 认为，不应该在 RL 训练阶段加入 format reward&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>correctness&lt;/strong>
基于答案的正确性分配奖励&lt;/p>
&lt;ul>
&lt;li>math correctness：使用 rule-based verifier 进行打分，使用 parser 和 Sympy 来比较模型输出以及 ground truth&lt;/li>
&lt;li>code correctness: 构建单元测试，评估输出代码是否能通过所有的单元测试&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Length penalty&lt;/strong>
与 DAPO 一致，使用 Length penalty 来惩罚过长的回答。&lt;/p>
&lt;p>&lt;strong>Language consistency&lt;/strong>
减少模型混合语言输出的问题。作者的做法是将 10% 的问题从英语转化为其他语种，然后使用 fastText 进行分类，确保内容都是一个语言。作者发现，通过这个简单的修改，就提高了模型的语言跟随能力。&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Recall
MiMo 中也遇到了混合语言输出的问题，但是其并没有给出解决办法。&lt;/p>
&lt;/blockquote>
&lt;p>作者还在 system prompt 中规定了输出的格式以及语言。&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Observation
作者发现，RL 的训练对于 system prompt 非常的敏感，因为 system prompt 会提高 model 的 entropy, 然后提高模型的探索能力。&lt;/p>
&lt;/blockquote>
&lt;h2 id="data">&lt;a href="#data" class="header-anchor">&lt;/a>Data
&lt;/h2>&lt;p>数据包括 math problems 以及 code problems&lt;/p>
&lt;h3 id="math">&lt;a href="#math" class="header-anchor">&lt;/a>Math
&lt;/h3>&lt;p>作者首先收集了 700K 样本，然后作者通过预处理以及过滤来保证数据的质量：&lt;/p>
&lt;p>&lt;strong>format filtering&lt;/strong>
要求问题完整，答案准确且可验证；去掉证明题和 multi-part 的问题；改写多项选择题为解答题，提高难度防止 reward hacking&lt;/p>
&lt;p>&lt;strong>difficulty filtering&lt;/strong>
使用两阶段的过滤策略。&lt;/p>
&lt;ol>
&lt;li>第一阶段使用 LLM 进行多次采样，然后去除掉比较简单或者比较复杂的&lt;/li>
&lt;li>第二阶段使用 RL checkpoint 进行多次采样，去除掉标准答案可能会存在问题的题目&lt;/li>
&lt;/ol>
&lt;p>最终一共得到 &lt;strong>38K&lt;/strong> 的样本&lt;/p>
&lt;h3 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h3>&lt;p>首先去掉没有 solution 和 test 的问题；然后去除掉标准答案不能通过所有 test 的问题。最终一共得到 &lt;strong>35K&lt;/strong> 的样本，包含 Python 和 C++ 两种语言&lt;/p>
&lt;h2 id="training">&lt;a href="#training" class="header-anchor">&lt;/a>Training
&lt;/h2>&lt;p>训练框架如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-magistral/Magistral_data_training.png"
width="1515"
height="523"
loading="lazy"
alt="Magistral_data_training"
class="gallery-image"
data-flex-grow="289"
data-flex-basis="695px"
>&lt;/p>
&lt;p>对于两个模型，作者采用了不同的方式进行训练&lt;/p>
&lt;ul>
&lt;li>Magistral Medium: 使用 pure RL 进行训练&lt;/li>
&lt;li>Magistral small: 使用 SFT + RL 进行训练&lt;/li>
&lt;/ul>
&lt;p>Magistral Medium 训练时满足的要求：&lt;/p>
&lt;ol>
&lt;li>dataset is not too easy: 太简单的题目对模型提升没有帮助&lt;/li>
&lt;li>Generation length does not stop growing: 逐步提升模型的最大输出长度&lt;/li>
&lt;li>KV-cache memory burden is not too large: 降低 batch size 来减少 KV-cache 的内存占用&lt;/li>
&lt;/ol>
&lt;p>Magistral small 训练&lt;/p>
&lt;p>&lt;strong>SFT&lt;/strong>
数据集包括两部分，一部分是 Magistral Medium 回答正确的这部分数据，第二部分是公开数据集，包括 [[OpenThoughts]] 和 [[OpenR1]] 两个数据集，作者使用 Magistral Medium 来生成回答。作者还加入了 10% 的 instruction tuning 数据来保持模型的通用能力。&lt;/p>
&lt;p>&lt;strong>RL&lt;/strong>
RL 的训练与 Magistral Medium 一致。&lt;/p>
&lt;h2 id="infra">&lt;a href="#infra" class="header-anchor">&lt;/a>Infra
&lt;/h2>&lt;p>作者首先介绍了一下 RL 训练的 infra, infra 主要包括三个模块：&lt;/p>
&lt;ul>
&lt;li>Trainers: 用于更新模型的权重&lt;/li>
&lt;li>Generators: 用于采样，生成 roll-out&lt;/li>
&lt;li>Verifiers: 对模型输出的结果进行打分&lt;/li>
&lt;/ul>
&lt;p>分布式 RL 训练的主要问题在于，不同长度的 roll-out 花费的时间不一致，作者发现，最长的 roll-out 和最短的 roll-out 的时间相差超过 5 倍以上。&lt;/p>
&lt;p>因此，作者就提出了异步生成这个方法。具体的做法就是&lt;/p>
&lt;ol>
&lt;li>首先由 Generator 生成多条 roll-out&lt;/li>
&lt;li>当 roll-out 完成之后，立马用 Verifiers 对轨迹进行打分&lt;/li>
&lt;li>收集 roll-out 以及对应的 reward, 直到达到给定的 batch 大小&lt;/li>
&lt;li>使用 Trainer 更新 Generator 的权重, 将更新后的权重同步给 Generator，这样其他 generator 在生成新的 token 时用的就是新的权重&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-magistral/Magistral_infra.png"
width="1169"
height="968"
loading="lazy"
alt="Magistral_infra"
class="gallery-image"
data-flex-grow="120"
data-flex-basis="289px"
>&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Recall
MiMo 里的做法是，当我们收集到给定数量的 roll-out 之后，我们就基于这些 roll-out 更新权重，然后进行下一次采样&lt;/p>
&lt;/blockquote>
&lt;p>训练时，对于每个 rank, 只要其收集到足够的 roll-out, 就会进行一次梯度更新。&lt;/p>
&lt;h2 id="evaluation">&lt;a href="#evaluation" class="header-anchor">&lt;/a>Evaluation
&lt;/h2>&lt;p>&lt;img src="https://maosong.website/p/notes-on-magistral/Magistral_performance_AIME.png"
width="1810"
height="1002"
loading="lazy"
alt="Performance of Magistral Medium"
class="gallery-image"
data-flex-grow="180"
data-flex-basis="433px"
>&lt;/p>
&lt;h2 id="ablation">&lt;a href="#ablation" class="header-anchor">&lt;/a>Ablation
&lt;/h2>&lt;p>&lt;strong>RL 的泛化性&lt;/strong>
作者探究了 RL 的 cross-domain generalization 能力，实验结果如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>AIME’24&lt;/th>
&lt;th>LiveCodeBench v5&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Starting Checkpoint&lt;/td>
&lt;td>32.2&lt;/td>
&lt;td>22.7&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RL (Math only)&lt;/td>
&lt;td>62.5&lt;/td>
&lt;td>38.3 (+15.6)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RL (Code only)&lt;/td>
&lt;td>49.7 (+17.5)&lt;/td>
&lt;td>42.7&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，不管是使用 math 还是 code 数据单独进行训练，模型在另一个 domain 上的表现都有所提升。&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Future
这个结论与最新的 GLM-4.1V-Thinking 结论一致&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Distillation v.s. RL for small models&lt;/strong>
作者探究了对于小语言模型，使用 RL 进行训练的效果更好，还是使用蒸馏的效果更好。&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-magistral/Magistral_distillation_vs_rl_performance.png"
width="1822"
height="934"
loading="lazy"
alt="Performance of RL v.s. distillation"
class="gallery-image"
data-flex-grow="195"
data-flex-basis="468px"
>&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Observation
实验结果发现，仅使用 RL 的效果与蒸馏差不多，甚至更好&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Batch size&lt;/strong>
作者还探究了 batch size $n_{batch}$ 以及 mini-batch size $n_{mini}$ 的影响。这里 batch size 指的是用于更新梯度的 roll-out 数量，mini-batch size 指的是计算梯度的 roll-out 数量，作者还定义了并行生成 roll-out 的数量 $n_{async}$. 当 $n_{async}>> n_{batch}$ 时，生成的 sequence 就很可能是 off-policy 的。作者固定 $n_{async}=4096$, 然后对比了不同的 $n_{batch}$ 和 $n_{mini}$ 对模型表现的影响，结果如下：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-magistral/Magistral_batch_ablation.png"
width="1602"
height="698"
loading="lazy"
alt="Ablation study on batch size"
class="gallery-image"
data-flex-grow="229"
data-flex-basis="550px"
>&lt;/p>
&lt;blockquote>
&lt;p>![tip] Observation
当 $n_{batch}=n_{mini}$ 时，模型表现差不太多（左图）；当 $n_{batch}$ 为常数，而 $n_{mini}$ 逐渐减小时，模型表现会逐渐变差。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Advantage normalization&lt;/strong>
作者对比了三种针对 advantage 的 normalization 方式：&lt;/p>
&lt;ul>
&lt;li>mini-batch: 在每个 mini-batch 里对模型进行 normalization&lt;/li>
&lt;li>Group normalization: 在每个 group 里进行 normalization&lt;/li>
&lt;li>no normalization: 没有 normalization&lt;/li>
&lt;/ul>
&lt;p>实验结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-magistral/Magistral_normalization.png"
width="1820"
height="500"
loading="lazy"
alt="Ablation study on normalization"
class="gallery-image"
data-flex-grow="364"
data-flex-basis="873px"
>&lt;/p>
&lt;p>作者发现，三种方式的区别并不是很大。&lt;/p>
&lt;h2 id="analysis">&lt;a href="#analysis" class="header-anchor">&lt;/a>Analysis
&lt;/h2>&lt;p>&lt;strong>length dimension&lt;/strong>
作者首先保存模型的 weight, 然后使用 PCA 进行降维，并在 2 维上进行可视化，作者对权重进行扰动，然后记录模型的 reward 以及输出长度。结果发现，模型存在一个 length dimension. 可视化结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-magistral/Magistral_length_dimension.png"
width="1546"
height="602"
loading="lazy"
alt="Magistral length dimension visualization"
class="gallery-image"
data-flex-grow="256"
data-flex-basis="616px"
>&lt;/p>
&lt;p>&lt;strong>multimodal extension&lt;/strong>
由于 Magistral medium 和 Magistral small 都是基于 MLLM 中的 LLM 开发得到的，作者还探究了更换 LLM 的 checkpoint 之后，原始 MLLM 的表现，结果发现，模型在多模态 reasoning 相关任务上的表现也得到了提升，结果如下图所示：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-magistral/Magistral_multimodal_extention.png"
width="1782"
height="822"
loading="lazy"
alt="Magistral multimodal performance"
class="gallery-image"
data-flex-grow="216"
data-flex-basis="520px"
>&lt;/p>
&lt;p>&lt;strong>Impact on other capabilities&lt;/strong>
作者还探究了 RL 训练对模型其他表现的影响，结果发现，RL 训练可以提高模型的 tool calling 和指令跟随能力，实验结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Category&lt;/th>
&lt;th>Benchmark&lt;/th>
&lt;th>Mistral Medium 3&lt;/th>
&lt;th>Magistral Medium&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Function calling&lt;/td>
&lt;td>Internal bench&lt;/td>
&lt;td>87.2&lt;/td>
&lt;td>87.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Instruction following&lt;/td>
&lt;td>IFEval&lt;/td>
&lt;td>86.8&lt;/td>
&lt;td>87.4&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>failed approaches&lt;/strong>
作者还介绍了一些尝试失败的做法：&lt;/p>
&lt;ol>
&lt;li>partial Reward: 对于 coding 任务，作者使用 test 的通过率作为奖励，结果发现效果并不好，这是因为一些错误的解法的 test 通过率也很高&lt;/li>
&lt;li>entropy bonus loss: 作者发现在损失函数中加入 entropy bonus loss 之后，模型的训练变得不稳定，而且效果不如使用更高的 $\epsilon_{high}$&lt;/li>
&lt;li>作者还进一步验证在 PPO loss 中加入 KL divergence loss, 结果发现效果并不好，这与 DAPO 的结论一致&lt;/li>
&lt;li>作者还尝试先 SFT Magistral Medium, 再进行 RL, 结果发现 RL 可以大幅度提高 SFT checkpoint 的表现。&lt;/li>
&lt;/ol>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>本文中，作者提出了 Magistral, 一个针对 math 和 code 的 reasoning model, 作者介绍了训练细节。但是，从方法层面来看，和 DAPO 区别不是很大。关键点应该是作者详细介绍了各种消融实验，为后来相关探索提供了经验。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2506.10910v1" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on SmolLM3</title><link>https://maosong.website/p/notes-on-smollm3/</link><pubDate>Tue, 15 Jul 2025 11:01:13 +0800</pubDate><guid>https://maosong.website/p/notes-on-smollm3/</guid><description>&lt;p>Hugging Face 在 2025 年 7 月 8 号发布了 SmolLM3, 一个 3B 的，128K 上下文，支持 6 种语言，支持 dual mode reasoning 的小语言模型。&lt;/p>
&lt;h2 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h2>&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;p>SmolLM3 是一个基于 transformer 的小语言模型，其模型架构与 SmolLM2 相似，SmolLM3 做了以下改进：&lt;/p>
&lt;ol>
&lt;li>使用 GQA 代替 multi-head attention. 作者通过消融实验发现 &lt;a class="link" href="https://maosong.website/p/notes-on-gqa/" target="_blank" rel="noopener"
>GQA&lt;/a> 和 MHA 的效果差不多，并且还可以减少 KV cache 的 size&lt;/li>
&lt;li>&lt;a class="link" href="https://maosong.website/p/notes-on-nope/" target="_blank" rel="noopener"
>NoPE&lt;/a>: 作者使用了NoPE, 来选择性（每 4 个 layer）移除 position embedding. 这个方法可以在不损害模型短文本能力的同时，提高模型长上下文的表现&lt;/li>
&lt;li>Intra-Document Masking: 不同的文档之间使用 attention masking 隔开&lt;/li>
&lt;li>Training stability: 与 olmo 2 一样，作者移除了 embedding layer 的 weight decay, 来提升训练的稳定性。&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-smollm3/SmolLM3_model_atanomy.png"
width="2554"
height="1470"
loading="lazy"
alt="SmolLM3 model atanomy"
class="gallery-image"
data-flex-grow="173"
data-flex-basis="416px"
>&lt;/p>
&lt;h3 id="data">&lt;a href="#data" class="header-anchor">&lt;/a>Data
&lt;/h3>&lt;p>&lt;img src="https://maosong.website/p/notes-on-smollm3/SmolLM3_pre-training_recipe.png"
width="1932"
height="1196"
loading="lazy"
alt="SmolLM3 pre-training recipe"
class="gallery-image"
data-flex-grow="161"
data-flex-basis="387px"
>&lt;/p>
&lt;p>与 SmolLM2 一样，作者使用了&lt;strong>11.2T&lt;/strong> token 进行训练，训练包括 3 个 stage。作者针对数据混合策略进行了消融实验，实验配置如下：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Stage&lt;/th>
&lt;th>Stable phase&lt;/th>
&lt;th>Stable phase&lt;/th>
&lt;th>Decay phase&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Description&lt;/td>
&lt;td>Base training&lt;/td>
&lt;td>High quality injection&lt;/td>
&lt;td>LR Decay&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Tokens&lt;/td>
&lt;td>8T&lt;/td>
&lt;td>2T&lt;/td>
&lt;td>1.1T&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Web&lt;/td>
&lt;td>85%&lt;/td>
&lt;td>75%&lt;/td>
&lt;td>63%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Code&lt;/td>
&lt;td>12%&lt;/td>
&lt;td>15%&lt;/td>
&lt;td>24%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Math&lt;/td>
&lt;td>3%&lt;/td>
&lt;td>10%&lt;/td>
&lt;td>13%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>datasets&lt;/td>
&lt;td>&lt;strong>web&lt;/strong>: FineWeb-Edu, DCLM, FineWeb2, Fineweb2-HQ&lt;br>&lt;strong>code&lt;/strong>: The Stack v2, StarCoder2 PRS, Jupyter and Kaggle notebooks, Github issues, StackExchange&lt;br>&lt;strong>math&lt;/strong>: FineMath3, InfiWebMath3+&lt;/td>
&lt;td>Adding Stack-Edu, FineMath4+, InfiWebMath4+, MegaMath&lt;/td>
&lt;td>upsampling of high-quality code data&lt;br>upsampling math data and introducing instruction and reasoning datasets such as OpenMathReasoning&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>其中，Web data 包含 12% 的多语种数据。&lt;/p>
&lt;h3 id="training-recipe">&lt;a href="#training-recipe" class="header-anchor">&lt;/a>Training Recipe
&lt;/h3>&lt;p>训练过程中，作者使用了 2.36M tokens 的 batch size, 上下文长度为 4096. 优化器为 AdamW&lt;/p>
&lt;p>作者使用了 nanotron 进行训练， datatrove 来处理数据， lighteval 来评估模型的表现。&lt;/p>
&lt;p>模型在 384 张 H100 上训练了 24 天。分布式训练的配置如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-smollm3/SmolLM3_distributed_training.png"
width="1672"
height="1024"
loading="lazy"
alt="SmolLM3 distributed training"
class="gallery-image"
data-flex-grow="163"
data-flex-basis="391px"
>&lt;/p>
&lt;h2 id="mid-training">&lt;a href="#mid-training" class="header-anchor">&lt;/a>Mid-training
&lt;/h2>&lt;p>Mid-training 的主要目标为扩展模型的上下文以及提升模型的 reasoning 能力。&lt;/p>
&lt;h3 id="long-context-extension">&lt;a href="#long-context-extension" class="header-anchor">&lt;/a>Long Context Extension
&lt;/h3>&lt;p>&lt;img src="https://maosong.website/p/notes-on-smollm3/SmolLM3_long_context_training.png"
width="1942"
height="1128"
loading="lazy"
alt="SmolLM3 long context training"
class="gallery-image"
data-flex-grow="172"
data-flex-basis="413px"
>&lt;/p>
&lt;p>在预训练阶段结束之后，作者使用了额外的 &lt;strong>100B&lt;/strong> tokens 来扩展模型的上下文。作者将扩展过程分为两个 stage:&lt;/p>
&lt;ol>
&lt;li>Stage 1: 将模型的上下文从 4K 提升到 32K. 具体做法是将 RoPE 的 base frequency 提升到 1.5M&lt;/li>
&lt;li>Stage 2: 将，模型的上下文从 32K 提升到 64K. 具体做法是将 RoPE 的 base frequency 提升到 5M&lt;/li>
&lt;/ol>
&lt;p>训练过程中，作者对 math, code 和 reasoning data 进行了上采样。作者发现，对长文本数据进行上采样并不会提高模型在 RULER 和 HELMET 上的表现。&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Recall
&lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5-1m/" target="_blank" rel="noopener"
>Qwen2.5-1M&lt;/a> 里也分析了长文本数据的问题，也就是大部分长文本数据依然是局部相关性强，而全局相关性弱&lt;/p>
&lt;/blockquote>
&lt;p>与 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5/" target="_blank" rel="noopener"
>Qwen2.5&lt;/a> 一样，作者还是用了 &lt;a class="link" href="https://maosong.website/p/notes-on-yarn/" target="_blank" rel="noopener"
>YARN&lt;/a> 来在推理阶段进一步提高模型的上下文长度，作者发现模型可以处理 128K 上下文长度的文本。&lt;/p>
&lt;h3 id="reasoning-mid-training">&lt;a href="#reasoning-mid-training" class="header-anchor">&lt;/a>Reasoning Mid-training
&lt;/h3>&lt;p>扩展模型上下文长度之后，作者还额外增加了一个 mid-training stage 来提高模型的 reasoning 能力。这个阶段的目标在于提升模型的通用能力。作者希望模型不针对特定的 domain, 如 math 或者 code 等。&lt;/p>
&lt;p>训练过程中，作者使用了 &lt;strong>35B&lt;/strong> 的 token. 数据来源包括 Open-Thoughts3-1.2M 以及 NVIDIA 的 Llama-Nemetron-Post-Training-Dataset-v1.1. Reasoning trace 由 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-r1/" target="_blank" rel="noopener"
>DeepSeek-R1&lt;/a> 生成。作者使用了 ChatML 的格式，还使用了 Packing 来提升训练效率。训练持续了 4 个 epoch.&lt;/p>
&lt;h2 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h2>&lt;p>如何构建 dual instruction model 来同时支持 reasoning 和 non-reasoning 任务并没有一个共识，大部分模型的数据都是非公开的。因此，作者就构建了一个 training pipeline, 用于提升模型在两种模式下的能力。&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-smollm3/SmolLM3_post-training_recipe.png"
width="1796"
height="1208"
loading="lazy"
alt="SmolLM3 post-training recipe"
class="gallery-image"
data-flex-grow="148"
data-flex-basis="356px"
>&lt;/p>
&lt;h3 id="chat-template">&lt;a href="#chat-template" class="header-anchor">&lt;/a>Chat Template
&lt;/h3>&lt;p>作者首先构建了一个 chat template, 用于支持 reasoning 和 non-reasoning 两种模式。该 chat template 支持用户使用 &lt;code>/think&lt;/code> 和 &lt;code>/no_think&lt;/code> flag 来控制模型的思考模式。在 non-reasoning 模式下，作者还在模型输出中 prefill 了 &lt;code>&amp;lt;think&amp;gt;\n\n&amp;lt;/think&amp;gt;&lt;/code>, 这一点与 Qwen3 一致。&lt;/p>
&lt;p>SmolLM3 还支持工具调用，其在 chat template 中加入了两种描述方式：XLM tools 和 Python tools.&lt;/p>
&lt;p>SmolLM3 还在 system prompt 中加入了 metadata, 如知识的截止时间，当前的 reasoning mode 等。&lt;/p>
&lt;h3 id="sft">&lt;a href="#sft" class="header-anchor">&lt;/a>SFT
&lt;/h3>&lt;p>&lt;img src="https://maosong.website/p/notes-on-smollm3/SmolLM3_synthetic_SFT_data.png"
width="1562"
height="940"
loading="lazy"
alt="SmolLM3 Synthetic SFT data"
class="gallery-image"
data-flex-grow="166"
data-flex-basis="398px"
>&lt;/p>
&lt;p>作者针对 math, code, general reasoning, instruction following, 以及 multilinguality 这几个领域来提升模型的表现。&lt;/p>
&lt;p>作者首先使用 reasoning mode 的 Qwen3-32B 来合成数据，合成数据的过程如上图所示。&lt;/p>
&lt;p>最终，SFT 阶段的数据包括 &lt;strong>1.8B&lt;/strong> token, 其中 &lt;strong>1B&lt;/strong> 为 non-reasoning mode 的 token, 覆盖 12 个数据集， &lt;strong>0.8B&lt;/strong> 为 reasoning token, 覆盖 10 个数据集。作者训练了 4 个 epoch, 使用了 Packing 的技巧。&lt;/p>
&lt;h3 id="apo">&lt;a href="#apo" class="header-anchor">&lt;/a>APO
&lt;/h3>&lt;p>&lt;img src="https://maosong.website/p/notes-on-smollm3/SmolLM3_synthetic_preference_data.png"
width="1542"
height="760"
loading="lazy"
alt="SmolLM3 synthetic preference data"
class="gallery-image"
data-flex-grow="202"
data-flex-basis="486px"
>&lt;/p>
&lt;p>SFT 阶段之后，作者使用了 Tulu3 的 preference dataset 用于 non-reasoning mode 的训练，然后合成了一批数据用于 reasoning mode 的训练，这批合成数据使用 Qwen3 32B 和 Qwen3 0.6B 生成得到，具体做法就是 Qwen3 32B 输出的结果定义为正样本，Qwen3 0.6B 输出的结果定义为负样本。&lt;/p>
&lt;p>作者使用了 APO 算法来进行训练，APO 是 &lt;a class="link" href="https://maosong.website/p/notes-on-dpo/" target="_blank" rel="noopener"
>DPO&lt;/a> 的一个变体， DPO 的目标函数为&lt;/p>
$$
\mathcal{L}_{DPO}(x,y_w,y_{l}; \theta) = -\log \sigma(r_\theta(x,y_w) - r_\theta(x, y_l))
$$&lt;p>其中&lt;/p>
$$
r_\theta(x,y) = \beta\log \frac{\pi_{\theta}(y\mid x)}{\pi_{ref}(y\mid x)}
$$&lt;p>$\beta>0$ 是一个超参数。APO 的目标函数如下&lt;/p>
$$
\mathcal{L}_{APO}(x,y_w,y_l;\theta) = -\sigma(r_\theta(x,y_w)) + \sigma(r_\theta(x,y_l))
$$&lt;p>作者发现，模型的 reasoning 能力提升之后，其长上下文能力反而下降了。作者认为这是因为在 reasoning mid-training stage 的训练中，提升 reasoning 能力损害了模型的 long context 能力。并且，APO 的训练数据上下文长度大多都在 24K 左右。为了解决这个问题，作者提出了 Model merging 的方法&lt;/p>
&lt;h3 id="model-merging">&lt;a href="#model-merging" class="header-anchor">&lt;/a>Model Merging
&lt;/h3>&lt;p>作者使用 MergeKit 来完成 model merging 的任务。merge 的过程包括两步：&lt;/p>
&lt;ol>
&lt;li>构造一个 model soup, 包括 APO 的每个 checkpoint&lt;/li>
&lt;li>将 model soup 与 mid-training 的一个拥有强上下文能力的 checkpoint 结合起来，作者使用了 linear merge, APO model soup 和 mid-training checkpoint 的权重分别为 0.9 和 0.1.&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>[!tip] Recall
&lt;a class="link" href="https://maosong.website/p/notes-on-kimi-k1.5/" target="_blank" rel="noopener"
>Kimi-k1.5&lt;/a> 也使用了 model merging 的方法，来将 long CoT 模型的能力迁移到 short-CoT 模型上去&lt;/p>
&lt;/blockquote>
&lt;h2 id="evaluation">&lt;a href="#evaluation" class="header-anchor">&lt;/a>Evaluation
&lt;/h2>&lt;p>作者首先评估了一下 base model 的表现，评估使用了 12 个 benchmark, 对比的模型包括 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a>, Gemma3, LLaMA 3.2. 结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-smollm3/SmolLM3_base_model_performance.png"
width="2992"
height="2059"
loading="lazy"
alt="SmolLM3 base model performance"
class="gallery-image"
data-flex-grow="145"
data-flex-basis="348px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-smollm3/SmolLM3_base_model_extact_performance.png"
width="2194"
height="954"
loading="lazy"
alt="SmolLM3 base model performance"
class="gallery-image"
data-flex-grow="229"
data-flex-basis="551px"
>&lt;/p>
&lt;p>模型的多语种表现如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-smollm3/SmolLM3_multilingual_performance.png"
width="1600"
height="1062"
loading="lazy"
alt="SmolLM3 multilingual performance"
class="gallery-image"
data-flex-grow="150"
data-flex-basis="361px"
>&lt;/p>
&lt;p>Instruction (non-reasoning) 版本的评估结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-smollm3/SmolLM3_instruction_performance.png"
width="2992"
height="2064"
loading="lazy"
alt="SmolLM3 Instruct models performance (w/o reasoning)"
class="gallery-image"
data-flex-grow="144"
data-flex-basis="347px"
>&lt;/p>
&lt;p>Reasoning 版本的评估结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-smollm3/SmolLM3_reasoning_performance.png"
width="2158"
height="1208"
loading="lazy"
alt="SmolLM3 reasoning performance"
class="gallery-image"
data-flex-grow="178"
data-flex-basis="428px"
>&lt;/p>
&lt;p>可以看到，Qwen3-4B 的表现是最好的。而 SmolLM3 的表现在 3B 左右也是非常强劲的&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者提出了 SmolLM3, 一个拥有长上下，文多语种以及 dual mode reasoning 能力的大语言模型，作者详细介绍了数据，训练以及 model merging 的技巧，来提高模型的表现。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://huggingface.co/blog/smollm3" target="_blank" rel="noopener"
>blog&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on GLM-4.1V-Thinking</title><link>https://maosong.website/p/notes-on-glm-4.1v-thinking/</link><pubDate>Mon, 14 Jul 2025 10:32:04 +0800</pubDate><guid>https://maosong.website/p/notes-on-glm-4.1v-thinking/</guid><description>&lt;p>智谱 AI 在 25 年 7 月份发布了 GLM-4.1V-Thinking, 一个 9B 的多模态大语言模型，其在多个 benchmark 上达到了相同大小 MLLM 的 SOTA&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>已有工作如 &lt;a class="link" href="https://maosong.website/p/notes-on-mimo-vl/" target="_blank" rel="noopener"
>MiMo-VL&lt;/a> 和 V-Triune 主要集中在特定的领域，目前还没有一个在所有领域都能超过 non-reasoning model 表现的多模态 reasoning model.&lt;/p>
&lt;p>基于这个目标，作者提出了 GLM-4.1V-Thinking, 一个 9B 的，用于通用领域多模态 reasoning 的 MLLM. 在预训练阶段，作者通过构建多样化的数据来提升模型的基础能力。在 post-training 阶段，作者构建了 domain-specific 的数据来让模型学会 reasoning. 最后，作者提出了 Reinforcement Learning with Curriculum Sampling (RLCS) 来扩展模型的 reasoning 能力。&lt;/p>
&lt;p>作者主要发现如下：&lt;/p>
&lt;ol>
&lt;li>multi-domain RL 的泛化性非常强，在某一个 domain 上进行 RL 训练也可以提高模型在其他 domain 上的表现&lt;/li>
&lt;li>动态选择 RL 训练的数据可以有效提高模型的表现和训练效率&lt;/li>
&lt;li>一个 robust 以及 precise 的 reward model 对于 multi-domain RL 是至关重要的&lt;/li>
&lt;/ol>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;p>GLM-4.1V-Thinking 是一个标准的 ViT-MLP-LLM 架构，其中：&lt;/p>
&lt;ul>
&lt;li>ViT: ViT 使用的是 AIMv2-Huge&lt;/li>
&lt;li>MLP: 是一个 3 层的 MLP，架构为 &lt;code>linear-LayerNorm-GELU-SwiGLU&lt;/code>, 并且在进入 MLP 之前，GLM-4.1V-Thinking 还会在 spatial 上进行降采样，降采样率为 2&lt;/li>
&lt;li>LLM: LLM 使用的是 GLM&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_Thinking_architecture.png"
width="1804"
height="988"
loading="lazy"
alt="Architecture of GLM4.1V-Thinking"
class="gallery-image"
data-flex-grow="182"
data-flex-basis="438px"
>&lt;/p>
&lt;p>与 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5-vl/" target="_blank" rel="noopener"
>Qwen2.5-VL&lt;/a> 一样，作者将 encoder 的 2D convolution 替换为了 3D convolution，用于处理视频输入，然后对于 single-image inputs，GLM-4.1V-Thinking 会将输入复制，变成一个含有两帧相同图片的 video 输入&lt;/p>
&lt;p>为了支持不同分辨率图片输入，作者进行了两点改进：&lt;/p>
&lt;p>第一是使用了 2D RoPE 来处理不同分辨率的图片输入&lt;/p>
&lt;p>第二是使用 bicubic interpolation 来将不同分辨率的图片适应到 encoder 的 position embedding 上。具体来讲，对一个拥有 $H_p\times W_p$ patches 的图片，每个 patch 的坐标 $(w,h)$ 首先会被 normalized 到 $[-1,1]$ 中：&lt;/p>
$$
g_{norm} = (w_{norm}, h_{norm}) = 2* \left(\frac{w+0.5}{W_p}, \frac{h+0.5}{H_p}\right) - 1
$$&lt;p>然后，我们再使用 bicubic interpolation $\mathcal{I}_{bicubic}$ 将坐标映射到原始的 position embedding 上：&lt;/p>
$$
P_{adapted} = \mathcal{I}_{bicubic}(P_{orig}, g_{norm})
$$&lt;p>对于视频输入，作者在每一帧之后插入了一个 time index token, 用每一帧的 timestamp string 来表示。这个做法与 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5-vl/" target="_blank" rel="noopener"
>Qwen2.5-VL&lt;/a> 一样。&lt;/p>
&lt;h3 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h3>&lt;h4 id="pre-training-data">&lt;a href="#pre-training-data" class="header-anchor">&lt;/a>Pre-training Data
&lt;/h4>&lt;p>Pre-training 数据包括 image-caption data, interleaved image-text data, OCR data, grounding data, video data 和 instruction tuning data&lt;/p>
&lt;p>&lt;strong>Image caption data&lt;/strong>
Image caption data 用于提升模型的世界知识。作者从 LAION, DataComp, DNF 以及 Wukong 等数据集收集了 10B 的 image-text pairs, 然后进行数据清洗：&lt;/p>
&lt;ol>
&lt;li>Heuristic-based filtering: 基于规则，如 resolution, caption length 等过滤掉低质量的图片&lt;/li>
&lt;li>Relevance filtering: 计算 CLIP score, 过滤掉低质量的数据&lt;/li>
&lt;li>Concept-balanced resampling: 基于 MetaCLIP , 来提升数据的覆盖程度，解决长尾分布的问题。这里 &lt;a class="link" href="https://maosong.website/p/notes-on-seed1.5-vl/" target="_blank" rel="noopener"
>Seed1.5-VL&lt;/a> 有一个类似的消融实验。&lt;/li>
&lt;li>Factual-centered re-captioning: 通过 rewrite caption 来提升 caption 的质量和信息密度。Idefics2 通过实验发现，rewriting caption 可以提高模型的表现。作者在最终的数据集里，混合了一部分 re-caption 的数据。&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Interleaved image-text data&lt;/strong>
相比于 image-caption data, interleaved image-text data 体量更大，且逻辑关系更强。但是噪声也更多，因此作者针对不同来源的数据构建了不同的数据清洗策略：&lt;/p>
&lt;ol>
&lt;li>Web data processing pipeline: 数据来源为 MINT, MMC4, OmniCorpus 等。作者首先基于 CLIP-score，去掉与上下文不相关的图片；然后，作者去除掉广告，二维码等低质量的图片；最后，作者将图多文少的样本也给去除掉，比如相册图片。作者还训练了一个 high-knowledge-density image classifier, 来识别信息密度高的样本。&lt;/li>
&lt;li>Academic book processing pipeline: 作者从电子书中收集了 100M 的样本，这些电子书主要是 STEM domain，然后作者构建了一个 PDF parsing tool 来提取文档内容。&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>OCR data&lt;/strong>
OCR 数据包含&lt;strong>220M images&lt;/strong>, 数据集由三部分组成：&lt;/p>
&lt;ol>
&lt;li>Synthetic document images: 基于预训练语料，使用不同的格式来渲染文字生成不同的图片，然后基于 LAION 的图片作为背景&lt;/li>
&lt;li>Natural scene text images: 使用 Paddle-OCR 来从图片中提取文字以及对应的 bounding box&lt;/li>
&lt;li>Academic documents: 使用类似 Nougat 的方法来构建 PDF page-Markdown 的数据。&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Grounding data&lt;/strong>
主要包含自然场景和 GUI 两个 domain 的 grounding 数据：&lt;/p>
&lt;ol>
&lt;li>Natural image grounding: 基于 LAION-115M 得到。作者使用 GLIPv2, 来自动化解析 caption 中的实体，然后预测对应的 bounding box, 作者将 bounding box 较少的样本去除掉。最终得到&lt;strong>40M&lt;/strong>高质量的自然场景数据&lt;/li>
&lt;li>GUI grounding: 基于 CC 提取对应的 url, 然后使用 Playwright 来与网页进行交互，进而获取到所有的 DOM elements 与其对应的 bounding box. 最终收集到 &lt;strong>140M&lt;/strong>高质量的 QA 样本&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Video data&lt;/strong>
Video data 从多个来源得到，作者使用了 fine-grained human annotation 来保证数据质量。作者还标注了 camera motion 等信息&lt;/p>
&lt;p>为了保证数据的质量，作者进行了去重，来减少数据语义的相似性。&lt;/p>
&lt;p>&lt;strong>Instruction tuning data&lt;/strong>
instruction tuning data 的策略如下：&lt;/p>
&lt;ol>
&lt;li>Task coverage and taxonomy: 优化数据分布&lt;/li>
&lt;li>Complex scenario augmentation: 构建复杂的指令跟随数据&lt;/li>
&lt;li>Data contamination check: 数据污染检查&lt;/li>
&lt;/ol>
&lt;p>最终一共收集到 &lt;strong>50M&lt;/strong>样本，覆盖 visual perception, multimodal reasoning, GUI agent 等任务。&lt;/p>
&lt;h4 id="pre-training-training">&lt;a href="#pre-training-training" class="header-anchor">&lt;/a>Pre-training Training
&lt;/h4>&lt;p>Pre-training 由两个 stage 组成：&lt;/p>
&lt;ol>
&lt;li>Multimodal pre-training: 提高模型的通用多模态能力，上下文长度为 8192, global batch size 为 1536. 使用了 data packing 策略，使用了 2-way tensor parallelism&lt;/li>
&lt;li>Long-context continue training: 提升模型在高精度图片，长视频，长上下文场景下的表现，上下文长度为 32768, 使用了 2-way tensor parallelism 和 4-way context parallelism.&lt;/li>
&lt;/ol>
&lt;h3 id="sft">&lt;a href="#sft" class="header-anchor">&lt;/a>SFT
&lt;/h3>&lt;h4 id="sft-data">&lt;a href="#sft-data" class="header-anchor">&lt;/a>SFT Data
&lt;/h4>&lt;p>数据包括 long CoT reasoning 数据，来让模型以标准格式输出 multi-step solution.&lt;/p>
&lt;p>数据包括 verifiable tasks 以及 non-verifiable tasks, 覆盖中英两种语言。作者过滤了非常简单和非常困难的样本。&lt;/p>
&lt;p>数据格式如下&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;lt;think&amp;gt; {think_content} &amp;lt;/think&amp;gt; &amp;lt;answer&amp;gt; {answer_content} &amp;lt;/answer&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>对于 verifiable tasks, 输出格式为&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;lt;answer&amp;gt; &amp;lt;|begin_of_box|&amp;gt; {answer_content} &amp;lt;|end_of_box|&amp;gt; &amp;lt;/answer&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>数据清洗策略包括：&lt;/p>
&lt;ol>
&lt;li>严格遵守格式要求&lt;/li>
&lt;li>reasoning style 不一致或者有噪声&lt;/li>
&lt;li>混合语言输出。&lt;/li>
&lt;/ol>
&lt;p>作者还使用 RL checkpoints 来生成一部分高质量数据，加入到 SFT 阶段的数据集里面。&lt;/p>
&lt;h4 id="sft-training">&lt;a href="#sft-training" class="header-anchor">&lt;/a>SFT Training
&lt;/h4>&lt;p>全参数微调。上下文长度为 32768， batch size 为 32.数据包括 long-form reasoning data 以及纯文本数据，包括 math, multi-turn conversation, agent planning 以及 instruction following 等.&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Observation
作者发现，在 SFT 阶段即使是使用带噪声的 reasoning data，模型在 RL 阶段也能继续训练。也就是说，不完美的 reasoning trace 也能提供 guidance. 但是，使用更高质量的数据可以让模型 RL 阶段的训练更稳定且表现更好。&lt;/p>
&lt;/blockquote>
&lt;h3 id="rl">&lt;a href="#rl" class="header-anchor">&lt;/a>RL
&lt;/h3>&lt;p>与 &lt;a class="link" href="https://maosong.website/p/notes-on-seed1.5-vl/" target="_blank" rel="noopener"
>Seed1.5-VL&lt;/a> 一样，作者在 RL 阶段结合了 RLVR 和 RLHF 两个训练目标。任务包括 STEM problem solving (such as mathematics, physics, chemistry), grounding, optical character recognition (OCR), video understanding, GUI agents, chart and document understanding, logical reasoning, and instruction following.&lt;/p>
&lt;p>&lt;strong>Data&lt;/strong>
作者首先构建了一个任务集合，然后基于这些任务，来过滤或者生成对应的 QA pair, 接下来作者基于难度和质量来过滤，最后作者在每个 domain 上进行 RL 的训练来保证数据的有效性。&lt;/p>
&lt;p>&lt;strong>Reward modelling&lt;/strong>
作者构建了一个 reward system, 用于给不同 domain 的任务进行奖励。为了探究不同 domain 的 reward 对模型整体表现的影响，作者设计了一个 low-quality reward system，实验结果如下，可以看到模型在训练后期出现了 collapse 或者 reward hacking 的情况。&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_reward_ablation.png"
width="1778"
height="876"
loading="lazy"
alt="GLM-4.1V-Thinking reward ablation"
class="gallery-image"
data-flex-grow="202"
data-flex-basis="487px"
>&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Observation
作者发现，不同任务的不同信号奖励会对模型最终表现产生巨大影响。&lt;/p>
&lt;/blockquote>
&lt;p>在使用 LLM 提取模型回答的 answer 的时候，作者要求模型的最终答案为 &lt;code>&amp;lt;|begin_of_box|&amp;gt; {answer_content} &amp;lt;|end_of_box|&amp;gt;&lt;/code> 这种形式，避免大语言模型提取出错。&lt;/p>
&lt;p>作者构建的 reward system 包含如下模块：&lt;/p>
&lt;ol>
&lt;li>shared verification functions: 包括格式检查等&lt;/li>
&lt;li>domain specific modules: 与 domain 相关的模块&lt;/li>
&lt;li>unit testing: 观测 domain 输出的分布，然后基于输出的分布来调整 reward logic&lt;/li>
&lt;/ol>
&lt;p>最终 reward 的 design 如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_reward_design.png"
width="1812"
height="712"
loading="lazy"
alt="GLM-4.1V-Thinking reward design"
class="gallery-image"
data-flex-grow="254"
data-flex-basis="610px"
>&lt;/p>
&lt;p>&lt;strong>RLCS&lt;/strong>
作者使用了 GRPO 来进行优化训练。作者发现，仅需 200 training steps 模型在一半以上问题的准确率就达到了 $90\%$.&lt;/p>
&lt;p>为了提升模型的训练效率，作者提出了 Reinforcement Learning with Curriculum Sampling (RLCS), 也就是将课程学习与 RL 结合起来。作者基于模型训练情况动态调整训练样本的难度，来保证每个样本对模型训练的提升都是最大的。&lt;/p>
&lt;p>在 RLCS 训练之前，作者先使用模型评测得到每个样本的 pass@k, 然后根据结果将样本进行分类。在训练过程中，作者记录每个样本的 pass@k， 然后动态更新其分类结果。在采样的时候，作者基于难度来对样本进行加权，来降低太简单或者太难样本的采样概率。&lt;/p>
&lt;p>作者还提出了一些提高 RL 表现的方法：&lt;/p>
&lt;ol>
&lt;li>Larger batch size: 使用更大的 batch size 效果更好&lt;/li>
&lt;li>Dynamic sampling expansion via ratio EMA: 作者定义了 naive 样本和高质量样本的比例，然后根据这个比例进行采样，来提升整体的采样效率。&lt;/li>
&lt;li>Force answering: 与 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a> 一样，对于比较难的问题，当输出的 token 数过长时，作者对模型输出进行截断，然后让模型基于已有思考过程直接输出结果。&lt;/li>
&lt;li>Discard KL loss: 与 DAPO 一样，作者也移除了 KL divergence loss&lt;/li>
&lt;li>Clip-higher: 与 DAPO 一样，作者通过修改超参数来提高模型的探索能力&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>[!tip] Observation&lt;/p>
&lt;ol>
&lt;li>RL 阶段的表现与 cold-start SFT 的表现并不完全相关。作者发现，cold-start SFT 取得更好的表现并不一定保证 RL 的表现就更好。&lt;/li>
&lt;li>RL 阶段各个 domain 数据的影响是正交的，这与 cold-start SFT 阶段的结果不同。&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>作者还提出了一些提高训练稳定性的方法：&lt;/p>
&lt;ol>
&lt;li>提高 cold-start SFT 阶段数据的质量&lt;/li>
&lt;li>移除 KL divergence loss&lt;/li>
&lt;li>使用 top-p 为 1，而不是 0.9 (如 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen-llm/" target="_blank" rel="noopener"
>Qwen-LLM&lt;/a> 中的设置)，top-p 为 1 可以 cover 整个 vocabulary，避免某些 token 没有参与训练&lt;/li>
&lt;li>per-sample loss 和 per-token loss 没有显著区别，但是 per-sample loss 稳定性更高&lt;/li>
&lt;li>在 cold-start SFT 阶段让模型学会格式要求，而不是在 RL 阶段加入 format reward&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Infra&lt;/strong>
在 infra 方面，作者做了如下改进：&lt;/p>
&lt;ol>
&lt;li>Load balancing of sequence lengths across DP ranks. 平衡每个 rank 的 sequensequence length 以及 compute load&lt;/li>
&lt;li>Intra-rank training with sequence packing and gradient accumulation. 将 sequence packing 和 gradient accumulation 结合起来&lt;/li>
&lt;li>Sample packing and reorganization within DP ranks. data packing&lt;/li>
&lt;li>Dynamic sampling expansion via ratio EMA. 平衡 naive 样本和高质量样本之间的比例&lt;/li>
&lt;/ol>
&lt;h2 id="evaluation">&lt;a href="#evaluation" class="header-anchor">&lt;/a>Evaluation
&lt;/h2>&lt;h3 id="performance">&lt;a href="#performance" class="header-anchor">&lt;/a>Performance
&lt;/h3>&lt;p>&lt;img src="https://maosong.website/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_performance.png"
width="1066"
height="1446"
loading="lazy"
alt="Performance of GLM4.1V Thinking"
class="gallery-image"
data-flex-grow="73"
data-flex-basis="176px"
>&lt;/p>
&lt;p>可以看到，GLM-4.1V-Thinking 在多个 benchmark 上都达到了 SOTA.&lt;/p>
&lt;h3 id="ablation-study">&lt;a href="#ablation-study" class="header-anchor">&lt;/a>Ablation Study
&lt;/h3>&lt;p>作者评估了一下不同 domain 的 RL 训练对模型整体表现的影响。作者使用不同的数据来进行训练，然后分析结果的相关性，结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_cross_domain_generalization.png"
width="1226"
height="962"
loading="lazy"
alt="Cross-domain generalization in reinforcement learning"
class="gallery-image"
data-flex-grow="127"
data-flex-basis="305px"
>&lt;/p>
&lt;p>实验结果发现：&lt;/p>
&lt;ol>
&lt;li>在某个 domain 上的训练会提高模型在其他 domain 上的表现&lt;/li>
&lt;li>联合多个 domain 进行训练最终的表现会更好&lt;/li>
&lt;/ol>
&lt;p>作者还发现，不同的任务之间存在关联，如 GUI-agent 和 grounding 这两个任务是高度相关的。OCR &amp;amp; Chart 以及 GUI-agent 也是高度相关的。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>在本文中，作者提出了 GLM-4.1V-Thinking, 一个 9B 的通用多模态 reasoning model, 作者通过构建高质量的数据，多阶段的训练，让模型在 reasoning 和 non-reasoning 任务上均超过了已有的 SOTA 模型的表现&lt;/p>
&lt;p>作者认为，模型有以下局限：&lt;/p>
&lt;ol>
&lt;li>模型并没有提升 reasoning 的质量，在某些情况下，模型结果正确，但是中间过程错误。作者分析这是因为目前只有针对结果的奖励机制。因此，未来需要能够评估中间过程的 reward model&lt;/li>
&lt;li>RL 的训练具有不稳定性，作者发现模型对设置比较敏感，这对 RL 训练的 scaling 提出了挑战。&lt;/li>
&lt;li>模型在复杂场景下表现依旧不太好，比如图片中有物体遮挡的情况，这些感知误差也会影响最终的 reasoning 表现。作者认为如何同时提高 perception 和 reasoning 的表现是一个需要研究的课题。&lt;/li>
&lt;/ol>
&lt;p>未来的工作有：&lt;/p>
&lt;ol>
&lt;li>如何构建更好的 reward 机制，来同时评估结果和中间过程。从而防止模型 reward hacking&lt;/li>
&lt;li>如何基于 multimodal training 来提升模型在纯文本任务上的表现。探究 visual reasoning 和 text reasoning 如何互相促进也是一个课题&lt;/li>
&lt;li>如何更好评估模型的表现，即如何更好评估模型的 failure modes, 比如幻觉，short reasoning 等&lt;/li>
&lt;/ol>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2507.01006" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://huggingface.co/THUDM/GLM-4.1V-9B-Thinking" target="_blank" rel="noopener"
>Huggingface&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="appendix">&lt;a href="#appendix" class="header-anchor">&lt;/a>Appendix
&lt;/h2>&lt;p>GLM-4.1V-Thinking 的 patch merger 代码&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Glm4vVisionPatchMerger&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">context_dim&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_act&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">bool&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="fm">__init__&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">bias&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">post_projection_norm&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">LayerNorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">gate_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">context_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">bias&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">up_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">context_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">bias&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">down_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">context_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">bias&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">act1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">GELU&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">act_fn&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ACT2FN&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">hidden_act&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_state&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_state&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_state&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_state&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">act1&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">post_projection_norm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_state&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">down_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">act_fn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">gate_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_state&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">up_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_state&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Glm4vVisionModel&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">downsample&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Conv2d&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">in_channels&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">out_channels&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">out_hidden_size&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">kernel_size&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">spatial_merge_size&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">stride&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">spatial_merge_size&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">merger&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Glm4vVisionPatchMerger&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">out_hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">context_dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">intermediate_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_act&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_act&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">...&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">spatial_merge_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">spatial_merge_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">permute&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">downsample&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">out_hidden_size&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">merger&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>Notes on Qwen2.5-1M</title><link>https://maosong.website/p/notes-on-qwen2.5-1m/</link><pubDate>Sat, 12 Jul 2025 11:00:47 +0800</pubDate><guid>https://maosong.website/p/notes-on-qwen2.5-1m/</guid><description>&lt;p>Qwen 在 2025 年 1 月提出了 Qwen2.5-1M，一个拥有 1M 上下文长度的大语言模型系列。包含 7B，14B 两个开源模型以及 API 模型 Qwen2.5-Turbo. 主要改进方法包括长上下文数据合成，渐进式预训练以及多阶段 post-training 等。作者还对 inference 进行了优化，提高了 inference 的效率。&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;p>架构上，Qwen2.5-1M 与 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5/" target="_blank" rel="noopener"
>Qwen2.5&lt;/a> 的架构一致，Qwen2.5-1M 包括 7B，14B 两个 size，还包括一个基于 MoE 的 API 模型 Qwen2.5-Turbo，不同的点在于，Qwen2.5-1M 的上下文长度为 1M，最大生成长度为 8K&lt;/p>
&lt;h3 id="pretraining">&lt;a href="#pretraining" class="header-anchor">&lt;/a>Pretraining
&lt;/h3>&lt;p>&lt;strong>Data&lt;/strong>
作者首先从 CC, arxiv, book, code repositories 等 domain 收集了原始数据。但是，作者发现，原始数据的局部相关性强，但是全局相关性弱。因此，作者基于原始数据进行了增广，来提高数据的长上下文依赖关系。具体有三个任务：&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Fill in the middle&lt;/strong>: FIM 是 openAI 提出来的一个做法，核心思想就是将填空类问题转化为 next-token-prediction 的问题。通过这种方式，作者希望提升模型理解长上下文依赖的能力&lt;/li>
&lt;li>&lt;strong>Keyword-based and Position-based retrieval&lt;/strong>: 基于 keywords 或者 position 来找到对应的 paragraph，这个任务的目的是提高模型识别并连接相关信息的能力&lt;/li>
&lt;li>&lt;strong>Paragraph Reordering&lt;/strong>: 对输入的 paragraphs 进行随机打乱，然后要求模型重新组织段落的关系&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Training&lt;/strong>
作者将训练拆分为了 5 个 stage：&lt;/p>
&lt;ol>
&lt;li>stage 1 和 stage 2 与 Qwen2.5 的训练过程一致，stage 1 的上下文长度为 4096，stage 2 的上下文长度为 32768, 训练时，作者使用了 ABF 技巧来将 RoPE 的 base frequency 从 10,000 调整到了 1,000,000.&lt;/li>
&lt;li>stage 3, stage 4 和 stage 5 分别将模型的上下文长度扩展到了 65,536 tokens, 131,072 tokens 以及 262,144 tokens, 对应的 RoPE base frequency 分别为 1M, 5M 和 10M. 训练时，作者使用了 75% 的长文本和 25% 的短文本，这样可以保证模型在短文本任务上的表现&lt;/li>
&lt;/ol>
&lt;p>最后，作者在评估了一下每个 stage 的表现，结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Training Length&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>RULER&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Avg.&lt;/td>
&lt;td>4K&lt;/td>
&lt;td>8K&lt;/td>
&lt;td>16K&lt;/td>
&lt;td>32K&lt;/td>
&lt;td>64K&lt;/td>
&lt;td>128K&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>32,768 Tokens&lt;/td>
&lt;td>82.3&lt;/td>
&lt;td>96.8&lt;/td>
&lt;td>94.7&lt;/td>
&lt;td>95.9&lt;/td>
&lt;td>92.2&lt;/td>
&lt;td>76.4&lt;/td>
&lt;td>37.6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>65,536 Tokens&lt;/td>
&lt;td>86.8&lt;/td>
&lt;td>96.5&lt;/td>
&lt;td>95.5&lt;/td>
&lt;td>93.6&lt;/td>
&lt;td>92.5&lt;/td>
&lt;td>86.7&lt;/td>
&lt;td>56.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>131,072 Tokens&lt;/td>
&lt;td>92.5&lt;/td>
&lt;td>96.5&lt;/td>
&lt;td>95.9&lt;/td>
&lt;td>93.0&lt;/td>
&lt;td>92.6&lt;/td>
&lt;td>93.0&lt;/td>
&lt;td>83.8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>262,144 Tokens&lt;/td>
&lt;td>92.7&lt;/td>
&lt;td>95.6&lt;/td>
&lt;td>93.8&lt;/td>
&lt;td>93.1&lt;/td>
&lt;td>94.1&lt;/td>
&lt;td>91.9&lt;/td>
&lt;td>87.6&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，随着训练的上下文长度的提升，模型在更长上下文下的能力也有提升，说明模型具有一定的泛化性。&lt;/p>
&lt;h3 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h3>&lt;p>Post-training 阶段与 Qwen2.5 一样，也分为了 SFT 和 RL 两个阶段。&lt;/p>
&lt;p>在 SFT 阶段，作者从预训练预料中选择了一部分长文档的片段，然后让 Qwen2.5 来生成对应的 query，query 类型包括 summarization, information retrieval, multi-hop QA 等任务。接下来，作者使用 Qwen-Agent 框架基于全文来回答这些问题。最后，作者基于生成的 query，全文，以及模型产生的回答作为训练数据。&lt;/p>
&lt;p>SFT 训练时，作者拆分为了两个 stage。 stage 1 作者在 32768 的上下文上进行训练，来提高模型短文本回答能力。第二个阶段，作者混合了 262,144 和 32768 上下文长度的训练数据。&lt;/p>
&lt;p>RL 训练时，与 Qwen2.5 不一样的是，作者进使用了 offline RL，也就是 DPO。作者仅在 8192 的上下文长度上面进行训练。作者认为，长上下文的 RL 训练是非常耗时的，并且作者发现，短文本上进行 RL 的训练之后，模型在长文本上的表现也能得到提升。结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Before RL&lt;/th>
&lt;th>After RL&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Qwen2.5-7B-Instruct-1M&lt;/td>
&lt;td>7.32&lt;/td>
&lt;td>8.08 (+0.75)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen2.5-14B-Instruct-1M&lt;/td>
&lt;td>8.56&lt;/td>
&lt;td>8.76 (+0.20)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen2.5-Turbo&lt;/td>
&lt;td>7.60&lt;/td>
&lt;td>8.34 (+0.74)&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="inference">&lt;a href="#inference" class="header-anchor">&lt;/a>Inference
&lt;/h2>&lt;p>前面是训练部分的优化，主要是提升模型的上下文能力。接下来，作者详细介绍了如何在 Inference 阶段提升整体的推理效率和减少内存占用。&lt;/p>
&lt;h3 id="length-extrapolation">&lt;a href="#length-extrapolation" class="header-anchor">&lt;/a>Length Extrapolation
&lt;/h3>&lt;p>与 Qwen2.5 一样，Qwen2.5-1M 也是用了 &lt;a class="link" href="https://maosong.website/p/dual-chunk-attention/" target="_blank" rel="noopener"
>Dual Chunk Attention&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/notes-on-yarn/" target="_blank" rel="noopener"
>YARN&lt;/a> 来在推理阶段扩展模型的上下文长度，作者做了如下实验，来对比 Qwen2.5, Qwen2.5-1M 加上 DCA 之后的影响&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen2.5-1m/Qwen2_5_1M_DCA.png"
width="1351"
height="764"
loading="lazy"
alt="Qwen2.5 performance of DCA"
class="gallery-image"
data-flex-grow="176"
data-flex-basis="424px"
>&lt;/p>
&lt;p>结果显示，Qwen2.5-1M 的表现比 Qwen2.5 更好，并且加上 DCA 之后，两者的表现都有进一步的提升。&lt;/p>
&lt;h3 id="sparse-attention">&lt;a href="#sparse-attention" class="header-anchor">&lt;/a>Sparse Attention
&lt;/h3>&lt;p>为了进一步提高计算效率，作者基于 MInference 来加速 perfilling phase. 并结合了 前面的技巧来防止模型性能下降。&lt;/p>
&lt;p>&lt;strong>MInference&lt;/strong>
MInference 的主要思想就是在长上下文中，有一些 critical token 对最终结果的影响是更大的。因此我们可以识别出这些 critical token 并只计算这些 token 对应的 attention score. 这些 critical token 对应的 pattern 被称为 Vertical-Slash pattern.&lt;/p>
&lt;p>为了识别出这个 pattern，作者首先进行离线搜索，来决定最优的 configuration。这个 configuration 决定了 attention 应该如何计算。在 Inference 阶段，MInference 首先计算最后一个 query 和前面所有 key 的 attention，然后基于 configuration 来动态选择 pattern。通过 MInference，我们可以降低 10 倍以上的内存和算力消耗。&lt;/p>
&lt;p>&lt;strong>Integrating with Chunked prefill&lt;/strong>
但是 MInference 的问题在于，整个 sequence 是并行处理的，这会导致内存占用持续上升。为了解决这个问题，作者提出了 chunked prefilling 的技巧，来降低 VRAM 的消耗。具体做法就是，将整个 sequence 分为若干个 chunk，然后每个 chunk 里，选取最后 64 个 token 作为 query，在每个 chunk 中分别识别出 critical token，这样就降低了 MInference 的内存占用&lt;/p>
&lt;p>接下来，作者在集成 DCA 的时候，发现性能有所下降。作者认为，这是由于 DCA 的 position id 信息不连续所导致的，为了解决这个问题，作者在选择 critical token 的时候，使用了连续版的 position id 信息。在最终推理的时候，还是使用 DCA 本身的位置信息。&lt;/p>
&lt;p>&lt;strong>Sparsity refinement&lt;/strong>
前面提到，MInference 需要先进行离线搜索决定最优的 configuration，但是对于 1M token 的上下文，这个过程还是非常耗时的。因此，作者构建了一个加速离线搜索的方法，具体做法就是定义两个 attention score，一个是 full attention, 另一个是 sparse attention， 然后计算两者的差值，如果说相差比较小，则说明 critical token 抓住了全局信息，这个配置是有效的。其公式定义如下：&lt;/p>
$$
\mathrm{Attention\_Recall} = \exp\left(\log\sum_{0\leq j\leq i}\exp \left(\frac{q^Tk_j}{\sqrt {d}}\right) - \log\sum_{j\in\mathcal{critical}}\exp \left(\frac{q^Tk_j}{\sqrt {d}}\right)\right)
$$&lt;p>Attention Recall 越高，说明选取的 critical token 越好，其 configuration 也就越好。&lt;/p>
&lt;p>作者进一步分析了 sparse attention 对 accuracy 的影响，结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen2.5-1m/Qwen2_5_1M_sparsity_config_performance.png"
width="1151"
height="836"
loading="lazy"
alt="Qwen2.5 performance on sparsity refinement"
class="gallery-image"
data-flex-grow="137"
data-flex-basis="330px"
>&lt;/p>
&lt;p>可以看到，仅使用 MInference 会导致模型性能 下降，但是加入 refinement 之后，模型的表现基本上和 full attention 差不太多。&lt;/p>
&lt;h3 id="inference-engine">&lt;a href="#inference-engine" class="header-anchor">&lt;/a>Inference Engine
&lt;/h3>&lt;p>&lt;strong>Kernel Optimization&lt;/strong>
作者还对 inference engine 进行了优化，作者使用 BladeLLM 作为 Qwen2.5-1M 的推理引擎。&lt;/p>
&lt;p>作者主要做了两点优化，第一是对 sparse attention kernel 进行了优化，提高了 sparse attention 的计算效率，结果发现，在 1M 的上下文下，BladeLLM 比 &lt;a class="link" href="https://maosong.website/p/notes-on-flashattention/" target="_blank" rel="noopener"
>flash attention&lt;/a> 要快 27.8 倍。&lt;/p>
&lt;p>第二是针对 MoE kernel 的优化。作者发现，decoding 的表现是与 memory access speed 相关的。具体来讲，当 batch size 超过 32 之后，获取模型参数成了效率的瓶颈。因此，作者使用了一系列技巧来提高 memory access 的效率&lt;/p>
&lt;p>&lt;strong>Pipeline parallelism&lt;/strong>
作者还对 Chunked pipeline parallelism 进行了优化，Chunked pipeline parallelism 的问题在于，在长上下文的场景下，不同长度的 chunk 会对 attention 的计算时间产生很大影响。不同的计算时间会产生 pipeline bubbles.&lt;/p>
&lt;p>BladeLLm 使用了 Dynamic Chunked pipeline parallelism 来解决这个问题，该方法通过计算复杂度来调整每个 chunk 的大小，进而使得最终的处理时间尽可能一致&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen2.5-1m/Qwen2_5_1M_PP.png"
width="1366"
height="224"
loading="lazy"
alt="Qwen2.5-1M DCPP"
class="gallery-image"
data-flex-grow="609"
data-flex-basis="1463px"
>&lt;/p>
&lt;p>&lt;strong>Scheduling&lt;/strong>
作者还在 Scheduling 上进行了优化，已有的推理引擎主要分为四个模块：API server, scheduler, model runner 以及 decoder&lt;/p>
&lt;p>已有方法的问题在于，non-GPU 的操作会占用大量时间，导致 GPU 利用率非常低。因此，作者在 BladeLLM 中进行了改进，使用了 Totally Asynchronous Generator (TAG) 的架构，主要有：&lt;/p>
&lt;ol>
&lt;li>Scheduler：动态分配 KV cache，类似于 speculative sampling, 而不必等前面的结果完成&lt;/li>
&lt;li>Runner: 基于 Scheduler 分配的任务直接进行处理，处理完之后直接处理下一个任务&lt;/li>
&lt;li>Decoder：基于 token id，进行解码，然后发送给前端的 API server&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen2.5-1m/Qwen2_5_1M_PP.png"
width="1366"
height="224"
loading="lazy"
alt="Qwen2.5-1M scheduling"
class="gallery-image"
data-flex-grow="609"
data-flex-basis="1463px"
>&lt;/p>
&lt;h2 id="evaluation">&lt;a href="#evaluation" class="header-anchor">&lt;/a>Evaluation
&lt;/h2>&lt;p>作者主要在三个 benchmark 上进行了评测：&lt;/p>
&lt;ol>
&lt;li>RULER: RULER 是 Needle-in-ahaystack 任务的一个扩展笨笨，其要求模型从不相关的上下文中找到多个 &amp;ldquo;needles&amp;rdquo; 或者回答多个问题，数据最长为 128K tokens.&lt;/li>
&lt;li>LV-Eval: LV-Eval 要求模型从上文本中同时理解多个 evidence fragments，数据最长为 256K tokens&lt;/li>
&lt;li>Longbench-Chat: 评估模型在长上下文下与人类偏好对齐的程度，数据最长为 100K tokens&lt;/li>
&lt;/ol>
&lt;p>Qwen2.5-1M 与 Qwen2.5 的对比表现如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen2.5-1m/Qwen2_5_1M_RULER_performance.png"
width="1340"
height="652"
loading="lazy"
alt="Qwen2.5-1M perofermence on RULER"
class="gallery-image"
data-flex-grow="205"
data-flex-basis="493px"
>&lt;/p>
&lt;p>可以看到，相比于 Qwen2.5，Qwen2.5 模型的表现有了大幅度的提升。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>在本文中，作者提出了 Qwen2.5-1M 系列大语言模型，包括 7B，14B 两个 size，以及一个 MoE 架构的 API 模型 Qwen2.5-Turbo。作者在训练和推理两方面进行了改进，最终将模型的上下文长度扩展到了 1M。从现在的角度来看，不管是 Reasoning model 还是 agent 的训练都依赖 long Context 作为基础能力。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2501.15383" target="_blank" rel="noopener"
>Qwen2.5-1M Technical Report&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Qwen2.5</title><link>https://maosong.website/p/notes-on-qwen2.5/</link><pubDate>Sat, 12 Jul 2025 10:51:42 +0800</pubDate><guid>https://maosong.website/p/notes-on-qwen2.5/</guid><description>&lt;p>2024 年 12 月 Qwen 发布了 Qwen 2.5 系列大语言模型，包括 7 个 dense 模型以及两个 MoE 模型，Qwen2.5 在 pre-training 阶段使用了 18T token 进行。在 post-training 阶段使用了 1M 的样本，还使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-dpo/" target="_blank" rel="noopener"
>DPO&lt;/a> 以及 GRPO 来进行 RL 的训练&lt;/p>
&lt;p>Qwen2.5 主要在以下方面进行了改进&lt;/p>
&lt;ol>
&lt;li>模型方面，提供了更多的 size，&lt;a class="link" href="https://maosong.website/p/notes-on-qwen2/" target="_blank" rel="noopener"
>Qwen2&lt;/a> 中只有 0.5B, 1.5B, 7B, 72B 四个 size, 在 Qwen2.5 中，加入了 3B, 14B 和 32B 三个 size 的模型&lt;/li>
&lt;li>数据方面，pre-training 阶段使用了 18T 的 token， post-training 阶段使用了 1M 的样本&lt;/li>
&lt;li>功能方面，Qwen2.5 支持更长的上下文长度（8K），支持结构化输入和输出，拥有更强的工具调用能力。&lt;/li>
&lt;/ol>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;p>模型架构这方面，Qwen2.5 和 Qwen2 的模型架构是一致的，tokenizer 页没有太大变化。为了支持工具调用，作者额外增加了 18 个 control token&lt;/p>
&lt;h3 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h3>&lt;p>&lt;strong>data&lt;/strong>
Qwen2.5 从以下方面提高了预训练数据的质量&lt;/p>
&lt;ol>
&lt;li>Better data filtering: 使用 Qwen2-Instruct 来过滤掉质量的数据，然后从多维度对训练数据进行打分，从而提高数据的质量&lt;/li>
&lt;li>Better math and code data: 加入了 Qwen2.5 Math 以及 Qwen2.5 Coder 的训练数据来提高模型的数学和代码能力&lt;/li>
&lt;li>Better synthetic data: 作者使用 Qwen2-72B-Instruct 以及 Qwen2-Math-72B-Instruct 来合成 math, code, knowledge domain 的数据，然后通过过滤以及 Qwen2-Math-RM-72B 来提高数据的质量&lt;/li>
&lt;li>Better data mixture: 作者使用 Qwen2-Instruct 来分类，然后平衡不同 domain 的数据分布。作者发现 e-commerce, social media 以及 entertainment 的数据重复性高，且大多都是机器生成的。而 technology, science 以及 academic research 等 domain 的数据质量更高。作者对不同 domain 的数据进行了上采样或者下采样。&lt;/li>
&lt;/ol>
&lt;p>基于这个过程，作者一共收集了&lt;strong>18T&lt;/strong> tokens&lt;/p>
&lt;p>&lt;strong>Hyper-parameters&lt;/strong>
作者构建了针对超参数的 scaling law，即决定最优的训练超参数如 batch size, learning rate 等&lt;/p>
&lt;p>作者通过实验得到了 model size $N$ 以及 pre-training data size $D$ 与 learning rate $\mu_{opt}$ 和 batch size $B_{opt}$ 之间的关系。&lt;/p>
&lt;p>&lt;strong>Long context pre-training&lt;/strong>
为了提升模型的上下文长度，作者将 pre-training 拆分为两个 stage，第一个 stage 的上下文长度为 4096， 第二个 stage，作者将上下文长度从 4096 扩展到 32768.&lt;/p>
&lt;p>在提升模型上下文过程中，作者使用 ABF 技巧将 &lt;a class="link" href="https://maosong.website/p/notes-on-position-encoding/" target="_blank" rel="noopener"
>Position Encoding&lt;/a> 的 base frequency 从 10,000 提升到了 1,000,000.&lt;/p>
&lt;p>对于 Qwen2-5-Turbo，作者实现了渐进式上下文长度扩展策略，模型上下文长度扩展经历四个阶段：32768, 65536, 131072 到最终的 262,144. 此时，RoPE 的 base frequency 为 10,000,000. 在训练的每个阶段，作者都使用了 40% 的长文本以及 60% 的短文本，以保证在扩展模型上下文长度的同时，还能保持模型在不同上下文长度下的表现。&lt;/p>
&lt;p>为了提高模型在 inference 时的长上下文表现，作者使用了 &lt;a class="link" href="https://maosong.website/p/dual-chunk-attention/" target="_blank" rel="noopener"
>Dual Chunk Attention&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/notes-on-yarn/" target="_blank" rel="noopener"
>YARN&lt;/a> 两个技巧。通过这两个技巧，作者将 Qwen2.5-Turbo 的上锈阿文扩展到了 1M，将其他模型的上下文长度扩展到了 131072.&lt;/p>
&lt;h3 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h3>&lt;p>Qwen2.5 的 post-training 分为两个大的 stage: SFT 和 RL，其中 RL 又分为两个小的 stage，分别是 offline RL 和 online RL&lt;/p>
&lt;p>在 SFT 阶段，作者主要做了以下改进：&lt;/p>
&lt;ol>
&lt;li>Long-sequence generation: 作者将 Qwen2.5 的输出长度提升到了 8192, 为了扩展模型输出的长度，作者构建了 Long-response 数据集，然后基于 back-translation 来生成对应的 query，最后使用 Qwen2 来过滤低质量的数据&lt;/li>
&lt;li>Math: 作者在 SFT 阶段加入了 Qwen2.5-Math 的 CoT 数据，包括公开数据集，K12 问题集一集合成数据等。作者通过 rejection sampling 以及 annotated answers 来生成 CoT 过程&lt;/li>
&lt;li>Code: 作者加入了 Qwen2.5-Coder 的 SFT 数据，作者基于多个 agent 来生成多样化高质量的 Instruction, 然后还从 code-related QA website 以及 Github 上获取数据来扩展数据集。对于最终的数据，作者使用了 sandbox 来保证代码的质量&lt;/li>
&lt;li>Instruction following: 作者构建了一个基于 code 的验证框架，让 LLM 同时生成 Instruction 和对应的验证代码，验证的单元测试。最后，通过 rejection sampling 来得到最终的数据集&lt;/li>
&lt;li>Structured Data Understanding: 作者还构建了针对 tabular QA, fact verification, error correction 以及 structured understanding 等数据集。作者在回答中加入 CoT，作者提高了模型对 structured data 的理解能力&lt;/li>
&lt;li>Logical Reasoning: 作者构建了 70,000 个不同 domain 的 query，有多种格式，覆盖了 analogical reasoning, causal reasoning 等 domain&lt;/li>
&lt;li>Cross-Lingual Transfer: 作者使用了一个翻译模型，来将 Instruction 转换到 low-resource language 上，进而提高模型在对应语种上的表现&lt;/li>
&lt;li>Robust System Instruction: 作者构建了不同的 system prompt 用于提升 system prompt 的多样性。作者发现，使用不同的 system prompt 可以减少模型的 variance, 提高模型的 robustness.&lt;/li>
&lt;li>Response Filtering: 作者使用了多种自动化标注方法来保证最终 response 的质量&lt;/li>
&lt;/ol>
&lt;p>最终，作者一共收集到 &lt;strong>1M&lt;/strong> 的 SFT 样本，模型训练了两个 epoch&lt;/p>
&lt;p>在 RL 阶段，作者首先基于 SFT model 来进行采样，然后将高质量的回答作为正样本，低质量的回答作为负样本，通过这个过程，一共采集到了&lt;strong>150K&lt;/strong>的样本。最后，作者使用 DPO 来进行训练。&lt;/p>
&lt;p>然后，作者进行了 online stage 的 RL 训练，这一阶段主要是对齐模型与人类的价值观。这一阶段的数据包括公开数据集，私有数据集。作者使用不同的 checkpoint 来进行采样，然后作者使用 GRPO 来进行训练.&lt;/p>
&lt;h3 id="long-cotnext-fine-tuning">&lt;a href="#long-cotnext-fine-tuning" class="header-anchor">&lt;/a>Long Cotnext Fine-tuning
&lt;/h3>&lt;p>作者还针对 Qwen2.5-Turbo 做了额外的 post-training, 来进一步提高其在长上下文下的表现。&lt;/p>
&lt;p>在 SFT 阶段，作者使用了一个两阶段方法，第一阶段仅在短文本上进行训练（上下文长度为 32768），这一阶段的训练数据与其他 Qwen2.5 的模型训练数据相同。第二个阶段，作者混合了短文本和长文本（262144）来进行训练，来提高模型在长上下文情景下的指令跟随能力&lt;/p>
&lt;p>在 RL 阶段，作者使用了和其他 Qwen2.5 模型相同的训练策略。作者认为：&lt;/p>
&lt;ol>
&lt;li>长上下文下训练 RL 代价很大&lt;/li>
&lt;li>reward model 更偏向于长文本&lt;/li>
&lt;li>RL 尽管只在短文本上进行训练，其还是可以提高模型在长上下文下的表现。&lt;/li>
&lt;/ol>
&lt;h2 id="evaluation">&lt;a href="#evaluation" class="header-anchor">&lt;/a>Evaluation
&lt;/h2>&lt;p>我们仅关注 instruction 版本的 72B,32B 和 7B 模型&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen2.5/Qwen2_5_72b_instruct_performance.png"
width="1112"
height="757"
loading="lazy"
alt="Performance of Qwen2.5 72B"
class="gallery-image"
data-flex-grow="146"
data-flex-basis="352px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen2.5/Qwen2_5_32b_instruct_performance.png"
width="1339"
height="744"
loading="lazy"
alt="Performance of Qwen2.5 32B"
class="gallery-image"
data-flex-grow="179"
data-flex-basis="431px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen2.5/Qwen2_5_7b_instruct_performance.png"
width="845"
height="746"
loading="lazy"
alt="Performance of Qwen2.5 7B"
class="gallery-image"
data-flex-grow="113"
data-flex-basis="271px"
>&lt;/p>
&lt;p>可以看到，Qwen2.5 72B 模型表现和 LLaMA3.1 405B 表现差不多，其他两个 size 的模型基本上达到了 SOTA&lt;/p>
&lt;p>最后，作者评估了一下 DCA+YaRN v.s. Full attention 的表现，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen2.5/Qwen2_5_long_context_TTFT.png"
width="1078"
height="958"
loading="lazy"
alt="TTFT of Qwen2.5 on long context"
class="gallery-image"
data-flex-grow="112"
data-flex-basis="270px"
>&lt;/p>
&lt;p>可以看到，使用 DCA+YaRN 之后，模型的推理效率比 full attention 要快 3-4 倍。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>在本文中，作者提出了 Qwen2.5 系列大语言模型，包括 7 个 dense 模型以及两个 MoE 模型，作者详细介绍了模型的 pre-training 和 post-training. 评测结果发现 Qwen2.5 模型基本上达到了 SOTA.&lt;/p>
&lt;p>作者认为，未来工作有：&lt;/p>
&lt;ol>
&lt;li>使用更多更多样化的 pre-training 和 post-training 数据&lt;/li>
&lt;li>多模态大模型的构建，特别是 omni-modal&lt;/li>
&lt;li>提高模型的 Reasoning 能力&lt;/li>
&lt;/ol>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2412.15115" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Dual Chunk Attention</title><link>https://maosong.website/p/dual-chunk-attention/</link><pubDate>Sat, 12 Jul 2025 10:41:12 +0800</pubDate><guid>https://maosong.website/p/dual-chunk-attention/</guid><description>&lt;p>Dual Chunk Attention (DCA) 由阿里巴巴在 2024 年 9 月份提出，DCA 是一个无需训练的，扩展 LLM 上下文长度的方法，后续，DCA 被应用于 Qwen2, Qwen2.5, Qwen2.5-1M 以及 Qwen3 中，与 &lt;a class="link" href="https://maosong.website/p/notes-on-yarn/" target="_blank" rel="noopener"
>YARN&lt;/a> 一起作为扩展模型上下文的有效手段&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>提升 LLM 上下文长度的方法可以分为两类：一类是 training-free 的，包括 LM-infinite 和 StreamingLLM 等，这些方法以损失 long range dependency 为代价来保持较低的 perplexity。另一类为了保留全局信息，则是通过外插来扩展模型的上下文，主要工作我们在 &lt;a class="link" href="https://maosong.website/p/notes-on-yarn/" target="_blank" rel="noopener"
>YARN&lt;/a> 中已经回顾了。&lt;/p>
&lt;p>第二类方法的问题在于，其依赖训练，在 training-free 的 setting 下，这些方法也会导致 perplexity 的上升&lt;/p>
&lt;p>因此，在本文中，作者就提出了 Dual Chunk Attention (DCA) ，一个无需训练的，扩展 LLM 上下文长度的方法。DCA 的主要做法是将 attention 的计算进行分块，这样就可以提高计算效率。&lt;/p>
&lt;p>通过实验，作者给出了三点关键发现：&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Extrapolation&lt;/strong>： DCA 可以在无需训练的情况下，将 LLM 的上下文提升到 32K，而不导致 Perplexity 大幅度增加&lt;/li>
&lt;li>&lt;strong>Orthogonality&lt;/strong>： DCA 可以和其他方法一起使用，如 &lt;a class="link" href="https://maosong.website/p/notes-on-yarn/" target="_blank" rel="noopener"
>YARN&lt;/a>, 这一点已经在 Qwen2.5-1M 以及 Qwen3 中得到了应用&lt;/li>
&lt;li>&lt;strong>Long Context Understanding&lt;/strong>: DCA 可以在无需训练的情况下，在长上下文设置下，达到已有 SOTA 模型的表现&lt;/li>
&lt;/ol>
&lt;h2 id="preliminary">&lt;a href="#preliminary" class="header-anchor">&lt;/a>Preliminary
&lt;/h2>&lt;p>对于一个长度为 $L$ 的 token 序列，我们首先定义对应的 position id 如下&lt;/p>
$$
P_{\mathbf{q}} = [0,1,\dots,L-1],\quad P_{\mathbf{k}} = [0,1,\dots,L-1]
$$&lt;p>然后，对于第 $i$ 个位置和第 $j$ 个位置的 token，其 attention score 定义为：&lt;/p>
$$
\langle f(\mathbf{q}, i), f(\mathbf{k}, j)\rangle =\langle R_{\theta,i}\mathbf{q}, R_{\theta,j}\mathbf{k}\rangle =\mathbf{q}^TR_{\theta, i-j}\mathbf{k}
$$&lt;p>具体细节参考 &lt;a class="link" href="https://maosong.website/p/notes-on-position-encoding/" target="_blank" rel="noopener"
>Position Encoding&lt;/a> 中的 RoPE 部分介绍。这里面的关键在于，最后的结果只与相对位置 $i-j$ 相关，而与绝对位置 $i$ 和 $j$ 无关。因此，我们可以用一个相对位置矩阵 $M\in\mathbb{R}^{L\times L}$ 来表示这个信息，其中 $M_{ij}=P_{\mathbf{q},i}- P_{\mathbf{k},j}$ 代表了第 $i$ 个位置的 query $\mathbf{q}$ 和第 $j$ 个位置的 key $\mathbf{k}$ 的相对位置信息，其示意图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/dual-chunk-attention/DCA_relative_position_visualization.png"
width="379"
height="384"
loading="lazy"
alt="Relative Position Visualization"
class="gallery-image"
data-flex-grow="98"
data-flex-basis="236px"
>&lt;/p>
&lt;p>原始版本的 RoPE 的问题在于，在训练时，模型没有见过更长的上下文，因此其泛化性也最差，这一点在 &lt;a class="link" href="https://maosong.website/p/notes-on-yarn/" target="_blank" rel="noopener"
>YARN&lt;/a> 已经得到了验证&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;p>DCA 的关键在于将 sequence 分割为若干个 Chunk，然后将 attention 的计算拆分为三个部分：&lt;/p>
&lt;ol>
&lt;li>intra-chunk：负责计算每个 chunk 内部的 attention&lt;/li>
&lt;li>inter-chunk ：负责计算 chunk 之间的 attention&lt;/li>
&lt;li>successive-chunk：负责计算相邻两个 chunk 之间的 attention&lt;/li>
&lt;/ol>
&lt;p>为了更好理解 DCA，我们接下来假设 $L=12$, 这时我们有&lt;/p>
$$
P_{\mathbf{q}} = [0,1,\dots,11],\quad P_{\mathbf{k}} = [0,1,\dots,11]
$$&lt;h3 id="intra-chunk-attention">&lt;a href="#intra-chunk-attention" class="header-anchor">&lt;/a>Intra-Chunk Attention
&lt;/h3>&lt;p>我们首先定义个超参数 chunk size $s>0$, 然后我们将我们的 sequence 分割成 $L/s$ 个 chunk，然后每个 chunk 重新进行编号，就得到了如下的 position id&lt;/p>
$$
P_{\mathbf{q}}^{Intra} = [0,1,\dots,L-1]\mod s,\quad P_{\mathbf{k}}^{Intra} = [0,1,\dots,L-1]\mod s
$$&lt;p>接下来，我们定义 intra-chunk 的相对位置矩阵 $M$， 此时，我们仅在每个 chunk 内部进行计算 attention，即&lt;/p>
$$
M[i][j] = \begin{cases} P_{\mathbf{q},i}^{Intra} - P_{\mathbf{k},j}^{Intra},&amp;\text{if}\lfloor P_{\mathbf{q},i}/s \rfloor = \lfloor P_{\mathbf{k},j} /s\rfloor ,\\
0,&amp;\text{otherwise}
\end{cases}
$$&lt;p>在上面的例子中，假设 $s=6$, 那么我们新的 position id 就变成了&lt;/p>
$$
\begin{aligned}
P_{\mathbf{q}}^{Intra} = [0,1,2,3,4,5,0,1,2,3,4,5]\\
P_{\mathbf{k}}^{Intra} = [\underbrace{0,1,2,3,4,5}_{\text{Chunk 0}},\underbrace{0,1,2,3,4,5}_{\text{Chunk 1}}]
\end{aligned}
$$&lt;p>对其进行可视化，我们就得到&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/dual-chunk-attention/DCA_intra_chunk_attention_visualization.png"
width="362"
height="386"
loading="lazy"
alt="Intra Chunk Attention Visualization"
class="gallery-image"
data-flex-grow="93"
data-flex-basis="225px"
>&lt;/p>
&lt;h3 id="inter-chunk-attention">&lt;a href="#inter-chunk-attention" class="header-anchor">&lt;/a>Inter-Chunk Attention
&lt;/h3>&lt;p>接下来，我们来看一下不同 chunk 之间如何计算彼此的 attention。在 Intra-chunk attention 计算中，我们忽略了跨 chunk 的信息，而且，由于现在的 position id 不再是单调递增的了，我们直接使用 $P_{\mathbf{q}}^{Intra}$ 和 $P_{\mathbf{k}}^{Intra}$ 给出的位置信息不对，这也是为什么我们在 Intra-chunk attention 中要求 query 和 key 在同一个 chunk 中才能计算的原因。&lt;/p>
&lt;p>为了解决这个问题，作者构建了一个新的 position id。首先我们引入一个新的超参数 $c>\max_i P_{\mathbf{q},i}$, $c$ 代表了模型预训练时的上下文长度，如 4096。&lt;/p>
&lt;p>接下来，基于 $c$, 我们定义新的 position id 如下：&lt;/p>
$$
P_{\mathbf{q}}^{Inter} = [c-1,c-1,\dots,c-1]\in\mathbb{R}^s,\quad P_{\mathbf{k}}^{Inter} = P_{\mathbf{k}}^{Intra}
$$&lt;blockquote>
&lt;p>注：这里的 $P_{\mathbf{q}}^{Inter}$ 指的是某一个 chunk 中的 position id，每个 chunk 中的 position id 都相同，请参考例子理解，后面不再赘述。&lt;/p>
&lt;/blockquote>
&lt;p>也就是说，在计算跨 chunk 的 attention 的时候，我们直接把 query 的 position id 设置为最大值，然后 key 的 position id 依然使用 intra-chunk 的位置信息。由于 $\max_i P_{\mathbf{k},i}=s-1$, 因此我们有&lt;/p>
$$
M[i][j] = P_{\mathbf{q},i}^{Inter} - P_{\mathbf{k},j}^{Inter} = c - 1 - P_{\mathbf{k},j}^{Inter}\geq c - 1 - (s- 1) \geq c-s.
$$&lt;p>最后，我们对于 inter chunk 的位置矩阵 $M$ 定义如下：&lt;/p>
$$
M[i][j] = \begin{cases} P_{\mathbf{q},i}^{Inter} - P_{\mathbf{k},j}^{Inter},&amp;\text{if}\lfloor P_{\mathbf{q},i}/s \rfloor \neq \lfloor P_{\mathbf{k},j} /s\rfloor ,\\
0,&amp;\text{otherwise}
\end{cases}
$$&lt;p>在上面的例子中，当 $c=10$ 时，$c-1=9$, 我们有&lt;/p>
$$
P_{\mathbf{q}}^{Inter}=[\underbrace{9,9,9,9,9,9}_{\text{Chunk 0}},\underbrace{9,9,9,9,9,9}_{\text{Chunk 1}}]
$$&lt;p>对其进行可视化，得到&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/dual-chunk-attention/DCA_inter_chunk_attention_visualization.png"
width="351"
height="388"
loading="lazy"
alt="Inter Chunk Attention Visualization"
class="gallery-image"
data-flex-grow="90"
data-flex-basis="217px"
>&lt;/p>
&lt;h3 id="successive-chunk-attention">&lt;a href="#successive-chunk-attention" class="header-anchor">&lt;/a>Successive-Chunk Attention
&lt;/h3>&lt;p>现在我们既可以计算 intra-chunk，也可以计算 inter-block 的 attention，但是问题是对于相邻的 chunk，其位置信息不对了，从上面的可视化中，我们可以看到，当 $P_{\mathbf{q},i}=6$ , $P_{\mathbf{k},j}=5$ 时，我们有&lt;/p>
$$
P_{\mathbf{q},i}^{Inter} - P_{\mathbf{k},j}^{Inter}=9-5=4\neq 1 = P_{\mathbf{q},i}-P_{\mathbf{k},j}
$$&lt;p>也就是说，inter-block 的 attention 会破坏原有的相对位置信息，因此我们就通过 successive chunk attention 来解决这个问题，使得 $P_{\mathbf{q},i}^{Inter} - P_{\mathbf{k},j}^{Inter}\approx P_{\mathbf{q},i}-P_{\mathbf{k},j}$.&lt;/p>
&lt;p>作者发现，这个问题不是所有的 chunk 都有，而是只存在于相邻的 chunk 中，因此，作者又加入了一个超参数 $w>0$ 代表了 local window size，我们可以直接将其设置为 $c-s$, 通过这个 local window，我们调整对应的 position id 如下：&lt;/p>
$$
P_{\mathbf{q}}^{Succ} = [\overbrace{s,s+1,\dots,s+w-1}^{w \text{ elements}},c-1, \dots,c-1]\in\mathbb{R}^s,\quad P_{\mathbf{k}}^{Succ} = P_{\mathbf{k}}^{Inter}
$$&lt;p>对于 successive chunk 的位置矩阵 $M$ 定义如下：&lt;/p>
$$
M[i][j] = \begin{cases} P_{\mathbf{q},i}^{Inter} - P_{\mathbf{k},j}^{Inter},&amp;\text{if}\lfloor P_{\mathbf{q},i}/s \rfloor - \lfloor P_{\mathbf{k},j} /s\rfloor=1 ,\\
0,&amp;\text{otherwise}
\end{cases}
$$&lt;p>在上面的例子中，我们设置 $w=4$, 就得到&lt;/p>
$$
P_{\mathbf{q}}^{Succ}=[\underbrace{6,7,8,9,9,9}_{\text{Chunk 0}},\underbrace{6,7,8,9,9,9}_{\text{Chunk 1}}]
$$&lt;p>对其进行可视化，得到&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/dual-chunk-attention/DCA_successive_chunk_attention_visualization.png"
width="388"
height="388"
loading="lazy"
alt="Successive Chunk Attention Visualization"
class="gallery-image"
data-flex-grow="100"
data-flex-basis="240px"
>&lt;/p>
&lt;h3 id="computation">&lt;a href="#computation" class="header-anchor">&lt;/a>Computation
&lt;/h3>&lt;p>接下来，我们把所有的改进放在一起，就得到&lt;/p>
$$
M[i][j] = \begin{cases}
P_{\mathbf{q},i}^{Intra} - P_{\mathbf{k},j},&amp;\text{if}\lfloor P_{\mathbf{q},i}/s \rfloor - \lfloor P_{\mathbf{k},j} /s\rfloor=0 ,\\
P_{\mathbf{q},i}^{Succ} - P_{\mathbf{k},j},&amp;\text{if}\lfloor P_{\mathbf{q},i}/s \rfloor - \lfloor P_{\mathbf{k},j} /s\rfloor=1 ,\\
P_{\mathbf{q},i}^{Inter} - P_{\mathbf{k},j},&amp;\text{if}\lfloor P_{\mathbf{q},i}/s \rfloor - \lfloor P_{\mathbf{k},j} /s\rfloor>1.
\end{cases}
$$&lt;p>基于上面的位置矩阵 $M$， 我们再依次计算对应的 attention score&lt;/p>
$$
\langle f(\mathbf{q}, i), f(\mathbf{k}, j)\rangle = \begin{cases}
\langle f(\mathbf{q}, P_{\mathbf{q},i}^{Intra})
, f(\mathbf{k}, P_{\mathbf{k},j})\rangle,&amp;\text{if}\lfloor P_{\mathbf{q},i}/s \rfloor - \lfloor P_{\mathbf{k},j} /s\rfloor=0 ,\\
\langle f(\mathbf{q}, P_{\mathbf{q},i}^{Succ})
, f(\mathbf{k}, P_{\mathbf{k},j})\rangle,&amp;\text{if}\lfloor P_{\mathbf{q},i}/s \rfloor - \lfloor P_{\mathbf{k},j} /s\rfloor=1 ,\\
\langle f(\mathbf{q}, P_{\mathbf{q},i}^{Inter})
, f(\mathbf{k}, P_{\mathbf{k},j})\rangle,&amp;\text{if}\lfloor P_{\mathbf{q},i}/s \rfloor - \lfloor P_{\mathbf{k},j} /s\rfloor>1.
\end{cases}
$$&lt;h2 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h2>&lt;p>首先是 &lt;code>RotaryEmbedding&lt;/code> 部分的修改&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">DCARotaryEmbedding&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">max_len&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">chunk_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">local_window&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">max_len&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">max_len&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">chunk_size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">chunk_size&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">local_window&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">local_window&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">inv_freq&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="o">...&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q_t&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">chunk_size&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">qc_t&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">q_t&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">chunk_size&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">clamp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">max&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">chunk_size&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">k_t&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">seq_len&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">chunk_size&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q_freqs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">outer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">q_t&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">inv_freq&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># seq_len x dim/2&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">qc_freqs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">outer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">qc_t&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">inv_freq&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">k_freqs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">outer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">k_t&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">inv_freq&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># seq_len x dim/2&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q_emb&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cat&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">q_freqs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">q_freqs&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># seq_len x dim&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">qc_emb&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cat&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">qc_freqs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">qc_freqs&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">k_emb&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cat&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">k_freqs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k_freqs&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># seq_len x dim&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># compute related sin, cos&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">q_sin&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">q_cos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">qc_sin&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">qc_cos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k_sin&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k_cos&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>attention 计算时的逻辑&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Attention&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">...&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">key_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">apply_rotary_pos_emb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">key_states&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k_cos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k_sin&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">position_ids&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q_states_intra&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">apply_rotary_pos_emb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">query_states&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="p">:&lt;/span>&lt;span class="n">chunk_len&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:],&lt;/span> &lt;span class="n">q_cos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">q_sin&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">position_ids&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="p">:&lt;/span>&lt;span class="n">chunk_len&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">k_states_prev&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">key_states&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="p">:&lt;/span>&lt;span class="n">chunk_len&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">v_states_prev&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">value_states&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="p">:&lt;/span>&lt;span class="n">chunk_len&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># first chunk&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">flash_result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">do_flash_attn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">q_states_intra&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k_states_prev&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">v_states_prev&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">flash_results&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">flash_result&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">remain_len&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">kv_seq_len&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">chunk_len&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="n">remain_len&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">flash_per_chunk&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">begin&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">kv_seq_len&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">remain_len&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">curr_chunk_len&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">chunk_len&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">remain_len&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">end&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">begin&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">curr_chunk_len&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># current chunk, intra-chunk attention&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q_states_intra&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">apply_rotary_pos_emb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">query_states&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="n">begin&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:],&lt;/span> &lt;span class="n">q_cos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">q_sin&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">position_ids&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="n">begin&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">k_states_intra&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">key_states&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="n">begin&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">v_states_intra&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">value_states&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="n">begin&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">flash_result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">do_flash_attn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">q_states_intra&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k_states_intra&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">v_states_intra&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">flash_per_chunk&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">flash_result&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># successive chunk attention&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q_states_succ&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">apply_rotary_pos_emb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">query_states&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="n">begin&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:],&lt;/span> &lt;span class="n">qc_cos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">qc_sin&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">position_ids&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="n">begin&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">flash_result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">do_flash_attn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">q_states_succ&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k_states_prev&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">v_states_prev&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">flash_per_chunk&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">flash_result&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># inter chunk attention&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">begin&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">k_states_prev&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">prev_len&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">k_states_prev&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q_states_inter&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">apply_rotary_pos_emb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">query_states&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="n">begin&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:],&lt;/span> &lt;span class="n">qc_cos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">qc_sin&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">position_ids&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="n">chunk_len&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">][:,&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">repeat&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">curr_chunk_len&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">k_states_inter&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">key_states&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="p">:&lt;/span>&lt;span class="n">begin&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">prev_len&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">v_states_inter&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">value_states&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="p">:&lt;/span>&lt;span class="n">begin&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">prev_len&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">flash_result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">do_flash_attn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">q_states_inter&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k_states_inter&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">v_states_inter&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">flash_per_chunk&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">flash_result&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">flash_results&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">flash_per_chunk&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">k_states_prev&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">k_states_intra&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">v_states_prev&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">v_states_intra&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">remain_len&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">remain_len&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">chunk_len&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># merge the final results&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attn_output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">merge_attn_outputs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">flash_results&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="evaluation">&lt;a href="#evaluation" class="header-anchor">&lt;/a>Evaluation
&lt;/h2>&lt;p>&lt;img src="https://maosong.website/p/dual-chunk-attention/DCA_perplexity_evaluation_PG19.png"
width="1202"
height="418"
loading="lazy"
alt="Perplexity evaluation on PG19"
class="gallery-image"
data-flex-grow="287"
data-flex-basis="690px"
>&lt;/p>
&lt;p>作者还分析了一下 DCA 的效率，结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/dual-chunk-attention/DCA_efficiency_analysis.png"
width="1080"
height="618"
loading="lazy"
alt="Efficiency of DCA"
class="gallery-image"
data-flex-grow="174"
data-flex-basis="419px"
>&lt;/p>
&lt;p>可以看到，在 &lt;a class="link" href="https://maosong.website/p/notes-on-flashattention/" target="_blank" rel="noopener"
>flash attention&lt;/a> 的基础上加上 DCA 之后，内存占用和推理时间并没有发生太大变化&lt;/p>
&lt;p>作者还分析了三种 attention 对结果的贡献，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/dual-chunk-attention/DCA_ablation_study.png"
width="1088"
height="404"
loading="lazy"
alt="Ablation study on three modules"
class="gallery-image"
data-flex-grow="269"
data-flex-basis="646px"
>&lt;/p>
&lt;p>结果显示，intra block 的 perplexity 是最低的，但是其在下游任务上表现是最差的。当三者结合在一起之后，perplexity 和下游任务上的表现都是最好的。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>本文中，我们回顾了 Qwen 系列扩展大模型上下文的方法 Dual Chunk Attention （DCA） 通过将 attention 切分成更小的 chunk，然后将 attention 的计算分为 intra-chunk，inter-chunk 和 successive-chunk，分别处理 chunk 内部，chunk 之间以及相邻 chunk 的 attention，通过这种方式，在无需训练的情况下，我们可以有效将模型上下文长度扩展 4 倍以上。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2402.17463" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/HKUNLP/ChunkLlama/tree/main" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Qwen2</title><link>https://maosong.website/p/notes-on-qwen2/</link><pubDate>Sat, 12 Jul 2025 10:36:43 +0800</pubDate><guid>https://maosong.website/p/notes-on-qwen2/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>2024 年 9 月 Qwen 发布了 Qwen2 系列技术报告，Qwen2 系列包括 4 个 dense 模型（0.5B, 1.5B, 7B, 72B）和一个 MoE 模型（总参数 57B，激活参数 14B），作者主要在架构，数据和长上下文上进行了改进。&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="model">&lt;a href="#model" class="header-anchor">&lt;/a>Model
&lt;/h3>&lt;p>对于 dense 模型，Qwen2 在 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen-llm/" target="_blank" rel="noopener"
>Qwen-LLM&lt;/a> 的基础上做了如下改动：&lt;/p>
&lt;ol>
&lt;li>使用 &lt;a class="link" href="https://maosong.website/p/notes-on-gqa/" target="_blank" rel="noopener"
>Group Query Attention (GQA)&lt;/a> 替换 MHA，来优化 KV cache，提高 throughput&lt;/li>
&lt;li>使用 &lt;a class="link" href="https://maosong.website/p/dual-chunk-attention/" target="_blank" rel="noopener"
>Dual Chunk Attention&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/notes-on-yarn/" target="_blank" rel="noopener"
>YARN&lt;/a> 来提高模型上下文长度和训练效率&lt;/li>
&lt;/ol>
&lt;p>其余与 Qwen 一致，包括 SwiGLU，RoPE，RMSNorm 和 pre-normalization&lt;/p>
&lt;p>对于 MoE 模型，Qwen2-MoE 基于 &lt;a class="link" href="https://maosong.website/p/dual-chunk-attention/" target="_blank" rel="noopener"
>Qwen1.5&lt;/a> 进行了改进，主要是 3 点：&lt;/p>
&lt;ol>
&lt;li>作者使用了更细粒度的专家个数，作者认为细粒度的专家可以提供更丰富的 combination，这一点与 &lt;a class="link" href="https://maosong.website/p/notes-on-olmoe/" target="_blank" rel="noopener"
>olmoe&lt;/a> 的结论相同&lt;/li>
&lt;li>与 DeepSeek-MoE 一样，作者使用了共享专家和路由专家&lt;/li>
&lt;li>作者使用了类似 upcycling 的方法来初始化模型。假设一共有 $n$ 个专家，每个专家的维度为 $h_E$, 原始 dense 模型的维度为 $h_{FFN}$, 那么我们会把 dense 模型的参数复制 $[nh_E/h_{FFN}]$ 次，这样就可以扩展到任意个数的 MoE 模型上。作者还对参数进行 shuffle，来提高 diversity。最后，作者还对 50% 的参数进行随机初始化，来提高模型的 capacity。&lt;/li>
&lt;/ol>
&lt;p>模型配置如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Configuration&lt;/th>
&lt;th>0.5B&lt;/th>
&lt;th>1.5B&lt;/th>
&lt;th>7B&lt;/th>
&lt;th>72B&lt;/th>
&lt;th>57B-A14B&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Hidden Size&lt;/td>
&lt;td>896&lt;/td>
&lt;td>1,536&lt;/td>
&lt;td>3,584&lt;/td>
&lt;td>8,192&lt;/td>
&lt;td>3,584&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Layers&lt;/td>
&lt;td>24&lt;/td>
&lt;td>28&lt;/td>
&lt;td>28&lt;/td>
&lt;td>80&lt;/td>
&lt;td>28&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Query Heads&lt;/td>
&lt;td>14&lt;/td>
&lt;td>12&lt;/td>
&lt;td>28&lt;/td>
&lt;td>64&lt;/td>
&lt;td>28&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># KV Heads&lt;/td>
&lt;td>2&lt;/td>
&lt;td>2&lt;/td>
&lt;td>4&lt;/td>
&lt;td>8&lt;/td>
&lt;td>4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Head Size&lt;/td>
&lt;td>64&lt;/td>
&lt;td>128&lt;/td>
&lt;td>128&lt;/td>
&lt;td>128&lt;/td>
&lt;td>128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Intermediate Size&lt;/td>
&lt;td>4,864&lt;/td>
&lt;td>8,960&lt;/td>
&lt;td>18,944&lt;/td>
&lt;td>29,568&lt;/td>
&lt;td>2,560&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Routed Experts&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Activated Experts&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Shared Experts&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Embedding Tying&lt;/td>
&lt;td>True&lt;/td>
&lt;td>True&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Vocabulary Size&lt;/td>
&lt;td>151,646&lt;/td>
&lt;td>151,646&lt;/td>
&lt;td>151,646&lt;/td>
&lt;td>151,646&lt;/td>
&lt;td>151,646&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Trained Tokens&lt;/td>
&lt;td>12T&lt;/td>
&lt;td>7T&lt;/td>
&lt;td>7T&lt;/td>
&lt;td>7T&lt;/td>
&lt;td>4.5T&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h3>&lt;p>预训练阶段的数据基于 Qwen 和 Qwen1.5，数据处理策略如下：&lt;/p>
&lt;ol>
&lt;li>使用基于 heuristic 和 model-based 方法来过滤掉低质量的数据&lt;/li>
&lt;li>加入了 code， math 和 multilingual 的数据&lt;/li>
&lt;li>平衡了各个类别的数据分布&lt;/li>
&lt;/ol>
&lt;p>初始数据包括 12T token，经过过滤得到 7T token。作者发现，使用 12T token 进行训练，模型的表现不如使用 7B token 训练得到的模型效果好。因此除了 0.5B 的模型，其他模型使用的都是 7T 的 token&lt;/p>
&lt;p>对于 MoE 模型，作者使用了额外的 4.5T token 来进行预训练。&lt;/p>
&lt;p>在训练过程中，作者还加入了 multi-task instruction 数据，来提高模型的上下文学习能力和指令跟随能力。&lt;/p>
&lt;p>作者还将 Qwen2 模型系列的上下文长度从 4096 扩展到 32768，扩展过程中作了三个改动：&lt;/p>
&lt;ol>
&lt;li>加入了更多高质量的长上下文数据&lt;/li>
&lt;li>将 RoPE 的 frequency 从 10,000 提升到了 1,000,000&lt;/li>
&lt;li>使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-yarn/" target="_blank" rel="noopener"
>YARN&lt;/a> 来扩展上下文长度&lt;/li>
&lt;li>使用了 &lt;a class="link" href="https://maosong.website/p/dual-chunk-attention/" target="_blank" rel="noopener"
>Dual Chunk Attention&lt;/a> 来优化 attention 的计算&lt;/li>
&lt;/ol>
&lt;h3 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h3>&lt;p>post-training 包括 SFT 和 RLHF 两个阶段&lt;/p>
&lt;p>数据包括 SFT 数据和 RLHF 使用的偏好数据&lt;/p>
&lt;p>数据标注过程有：&lt;/p>
&lt;ol>
&lt;li>使用 InsTag 对数据进行打标&lt;/li>
&lt;li>选取高质量的 instruction&lt;/li>
&lt;li>构建了一个 self-evolution 策略，来扩展 instruction 数据&lt;/li>
&lt;li>请人类来标注数据&lt;/li>
&lt;/ol>
&lt;p>作者还合成了一些数据，合成数据的过程如下：&lt;/p>
&lt;ol>
&lt;li>rejection sampling：对 LLM 进行多次采样，然后保留结论正确的数据作为 SFT 数据，以正确和错误的数据对作为偏好数据&lt;/li>
&lt;li>Execution feedback：对于代码任务，使用 Python 来验证答案的正确性&lt;/li>
&lt;li>Data Repurposing：对于写作类任务，以文档为输入，让 LLM 生成对应的 instruction&lt;/li>
&lt;li>Constitutional Feeback：基于预设的 principle 来生成回答&lt;/li>
&lt;/ol>
&lt;p>最终，SFT 数据包括 500, 000 条样本&lt;/p>
&lt;p>RLHF 的训练包括 offline stage 和 online stage，offline stage 就是用收集到的偏好数据。在 online stage，作者使用 reward model 来给输出的回答进行打分，然后再使用 DPO 进行训练。&lt;/p>
&lt;p>与 Qwen 不同，Qwen2 中作者使用了 Online Merging Optimizer 来解决因为 alignment 导致的性能降低&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>本文提出了 Qwen2 系列，在 Qwen2 中，首次使用了 GQA 代替 MHA，Qwen2 在上下文上做出了初步探索&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2407.10671" target="_blank" rel="noopener"
>Qwen2 tech report&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Qwen1.5</title><link>https://maosong.website/p/notes-on-qwen1.5/</link><pubDate>Thu, 03 Jul 2025 17:37:39 +0800</pubDate><guid>https://maosong.website/p/notes-on-qwen1.5/</guid><description>&lt;p>Qwen 在 24 年 1 月份发布了 Qwen1.5，包含 0.5B, 1.8B, 4B, 7B, 14B, 32B, 72B, 以及 110B 6 个 size，还有一个 MoE 模型。&lt;/p>
&lt;h2 id="介绍">&lt;a href="#%e4%bb%8b%e7%bb%8d" class="header-anchor">&lt;/a>介绍
&lt;/h2>&lt;p>Qwen1.5 的主要特点：&lt;/p>
&lt;ol>
&lt;li>支持 12 中语言&lt;/li>
&lt;li>统一支持 32768 tokens 上下文长度 。&lt;/li>
&lt;li>提供 量化版本 （Int4、Int8、AWQ、GGUF）以适应低资源环境或部署需求。&lt;/li>
&lt;/ol>
&lt;p>训练过程使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-dpo/" target="_blank" rel="noopener"
>DPO&lt;/a> 以及 PPO 来进行对齐&lt;/p>
&lt;h2 id="qwen15-moe">&lt;a href="#qwen15-moe" class="header-anchor">&lt;/a>Qwen1.5-MoE
&lt;/h2>&lt;p>Qwen1.5-MoE 的激活参数为 2.7B，一共包含 64 个专家，其中激活 4 个专家，共享 4 个专家&lt;/p>
&lt;p>相比于 Qwen1.5-7B，去训练的 FLOPS 降低了 75%，inference 的速度提高了 174%&lt;/p>
&lt;p>Qwen1.5-MoE 采用了改进的 MoE 架构，主要优化包括：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>细粒度专家（Fine-grained experts）&lt;/strong> ：通过将 FFN 层划分为多个片段，构建更多专家而不增加参数总量。&lt;/li>
&lt;li>&lt;strong>初始化策略（Upcycling）&lt;/strong> ：基于 Qwen-1.8B 初始化模型，并引入随机性以加速收敛。&lt;/li>
&lt;li>&lt;strong>路由机制（Routing Mechanism）&lt;/strong> ：在每个 MoE 层中使用 64 个专家，其中 4 个共享专家始终激活，60 个路由专家中有 4 个被激活，提高了灵活性和效率。&lt;/li>
&lt;/ul>
&lt;h2 id="效率对比">&lt;a href="#%e6%95%88%e7%8e%87%e5%af%b9%e6%af%94" class="header-anchor">&lt;/a>效率对比
&lt;/h2>&lt;p>作者对比了 throughput (requests processed per second) 以及 tokens per second (TPS):&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Throughput&lt;/th>
&lt;th>TPS&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Qwen1.5-7B-Chat&lt;/td>
&lt;td>1.15&lt;/td>
&lt;td>2298.89&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen1.5-MoE-A2.7B-Chat&lt;/td>
&lt;td>2.01&lt;/td>
&lt;td>4010.27&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://qwenlm.github.io/blog/qwen-moe/" target="_blank" rel="noopener"
>Qwen1.5 MoE&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://qwenlm.github.io/blog/qwen1.5/" target="_blank" rel="noopener"
>Qwen 1.5&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on YaRN</title><link>https://maosong.website/p/notes-on-yarn/</link><pubDate>Thu, 03 Jul 2025 14:40:49 +0800</pubDate><guid>https://maosong.website/p/notes-on-yarn/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>YaRN (Yet Another RoPE extentionN method) 时23年9月EleutherAI等提出来的一个扩展LLM上下文长度的方法，后来被Qwen系列模型所应用。&lt;/p>
&lt;h2 id="preliminary">&lt;a href="#preliminary" class="header-anchor">&lt;/a>Preliminary
&lt;/h2>&lt;p>作者首先回顾了一下RoPE, 具体内容请参见上一篇&lt;a class="link" href="https://maosong.website/p/notes-on-position-encoding/" target="_blank" rel="noopener"
>blog&lt;/a>。并使用了 $f_{W}(x_m, m, \theta_d)$ 来表示RoPE：&lt;/p>
$$
f_{W}(x_m, m, \theta_{d}) = \Theta_{\theta, m}^d W x_m
$$&lt;p>其中 $\Theta_{\theta, m}^d\in\mathbb{R}^{d\times d}$ 是多维旋转矩阵, $\theta_d=[\theta_{0,d},\dots,\theta_{(d-2)/2,d}]\in\mathbb{R}^{d/2}$, $\theta_{i,d}=\theta^{2i/d}$, $\theta>0$ 是一个超参数，RoPE中设置为 $\theta=10000$, $W\in\{W_q,W_k\}\subset\mathbb{R}^{d\times d}$ 是对应的query/key projection layer的权重矩阵， $x\in\mathbb{R}^{d}$ 是输入的hidden states.&lt;/p>
&lt;p>接下来，作者定义了两个新的变量：&lt;/p>
&lt;p>&lt;strong>scaling factor&lt;/strong>
假设预训练的上下文长度为 $L$, 扩展的上下文长度为 $L'>L$, 则我们定义scaling factor 为&lt;/p>
$$
s = \frac{L'}{L}
$$&lt;p>易知 $s>1$.&lt;/p>
&lt;p>&lt;strong>wavelength&lt;/strong>
我们将 $\lambda_d$ 定义为 $i$-th 维的RoPE embedding对应的 wavelength：&lt;/p>
$$
\lambda_{i,d} = \frac{2\pi}{\theta_{i,d}} = 2\pi\theta^{2i/d}, \ i=0,\dots,(d-2)/2
$$&lt;p>wavelength描述了对于第$i$ 个维度，RoPE旋转一周 ($2\pi$) 所需要的上下文长度。&lt;/p>
&lt;h2 id="unified-perspective-on-related-work">&lt;a href="#unified-perspective-on-related-work" class="header-anchor">&lt;/a>Unified Perspective on Related Work
&lt;/h2>&lt;p>基于 $f_{W}(x_m, m, \theta_d)$, 作者统一了已有的扩展上下文长度的方法，作者将不同的扩展方法使用一个通用函数 $f_W(x_m,g(m), h(\theta_d))$ 来表示，这里 $g(m)$ 和 $h(\theta_d)$ 分别代表了不同的长度外推方法所使用的变换。&lt;/p>
&lt;h3 id="position-interpolation">&lt;a href="#position-interpolation" class="header-anchor">&lt;/a>Position Interpolation
&lt;/h3>&lt;p>Position Interpolation (PI) 的核心思想在于，我们可以通过Interpolation，将超过预训练长度的文本给压缩到当前最大长度，以此来避免RoPE外推产生的问题，其对应的公式为&lt;/p>
$$
f'_W(x_m,m,\theta_d) = f_W(x_m, \frac{mL}{L'}, \theta_d)
$$&lt;p>其中 $L'>L$ 为我们扩展之后的上下文长度， $L$ 为我们预训练的上下文长度。使用通用函数表示的话，我们有&lt;/p>
$$
g(m) = \frac{m}{s},\quad h(\theta_d) = \theta_d
$$&lt;h3 id="ntk-aware-interpolation">&lt;a href="#ntk-aware-interpolation" class="header-anchor">&lt;/a>NTK-aware Interpolation
&lt;/h3>&lt;p>PI的问题是，并没有考虑不同维度的wavelength。
基于NTK理论，DNN当输入维度比较低，且embedding缺少高频内容时，模型就会很难学习到高频信息。对应到RoPE里面，输入的token position id是低位信息（1维），而输出的RoPE是一个 $d$ 维的复杂向量。因此，当输入token非常相似却距离非常近时，RoPE就会丢失高频的细节信息&lt;/p>
&lt;p>因此，为了解决这个问题，作者对不同的维度使用了不同的缩放策略：&lt;strong>维度比较小时，其缩放的更多，维度比较大是，其缩放的更少。&lt;/strong>&lt;/p>
&lt;p>基于这个策略，作者提出了NTK-aware interpolation，其定义如下：&lt;/p>
$$
g(m) = m, \quad h(\theta_{i,d}) = \theta'^{-2i/d}, i=0,\dots,(d-2)/2
$$&lt;p>其中&lt;/p>
$$
\theta' = \theta\cdot s^{\frac{d}{d-2}}
$$&lt;p>实际中，这种方法会产生out-of-bound的值，因此最终结果会比PI要差一点，为了解决这个问题，一般会使用比 $s$ 更大的scaling factor.&lt;/p>
&lt;blockquote>
&lt;p>上式的推导基于一个简单的假设：我们希望最后一个维度的wavelength在scaling之后，是线性变化的，即 $\theta'_{(d-2)/2,d}=s\theta_{(d-2)/2,d}$, 求解之后，我们就得到了上面的定义&lt;/p>
&lt;/blockquote>
&lt;p>PI 和NTK-aware interpolation的问题在于，我们对不同的维度的处理都是一样的。这类不在乎wavelength的方法被称为&lt;strong>blind interpolation methods&lt;/strong>, 接下来要介绍的就是基于wavelength的方法，即&lt;strong>target interpolation methods&lt;/strong>.&lt;/p>
&lt;h3 id="ntk-by-parts-interpolation">&lt;a href="#ntk-by-parts-interpolation" class="header-anchor">&lt;/a>NTK-by-parts Interpolation
&lt;/h3>&lt;p>与NTK-aware interpolation， NTK-by-parts interpolation基于wavelength来考虑不同维度上所做的变换。&lt;/p>
&lt;p>对于低维度，其 $\theta_{i,d}$ 非常大，因此旋转一周所需要的上下文长度也非常大。实际上就导致某些维度的embedding并不是均匀分布的，（比如说只有 $0\sim\pi$ 这个区间的embedding），这个时候，模型就只能访问到绝对位置信息，而访问不到相对位置信息。
另外，当我们对所有的维度都进行scale的时候，所有的token都会与彼此更加靠近，这损害了模型对于局部信息的获取能力。
基于这些认知，作者基于wavelength，对不同的维度分别进行处理：&lt;/p>
&lt;ol>
&lt;li>如果wavelength远小于上下文长度 $L$， 则我们不做任何处理&lt;/li>
&lt;li>如果wavelength等于或者大于上下文长度 $L$， 则我们使用NTK-aware interpolation 进行处理&lt;/li>
&lt;li>对于中间的其他维度，我们进行了一个trade off&lt;/li>
&lt;/ol>
&lt;p>作者定义了一个ratio $r$ 来描述原始上下文长度 $L$ 和 wavelength 之间的关系&lt;/p>
$$
r(i,d) = \frac{L}{\lambda_{i,d}} = \frac{L}{2\pi\theta'^{2i/d}}
$$&lt;p>基于这个ratio，我们可以定义上面的三种处理方式对应的权重&lt;/p>
$$
\gamma(r) = \begin{cases}
0, &amp;\text{if } r &lt; \alpha\\
1, &amp;\text{if } r > \beta\\
\frac{r-\alpha}{\beta-\alpha}, &amp;\text{otherwise}
\end{cases}
$$&lt;p>其中， $\beta>\alpha>0$ 是超参数， $r&lt;\alpha$, $r&lt;\beta$ 分别代表了上面的第1种，第2种情况。&lt;/p>
&lt;p>最后，NTK-by-parts interpolation的定义如下&lt;/p>
$$
g(m) = m, \quad h(\theta_i) = \left(1-\gamma(r(i,d)\right)\frac{\theta_i}{s} + \gamma(r(i,d))\theta_i
$$&lt;p>作者通过实验发现，在LLaMA上，$\alpha=1$ 和 $\beta=32$ 是一个比较好的选择&lt;/p>
&lt;h3 id="dynamic-ntk-interpolation">&lt;a href="#dynamic-ntk-interpolation" class="header-anchor">&lt;/a>Dynamic NTK Interpolation
&lt;/h3>&lt;p>在实际中，一个经常遇到的场景就是sequence length会从1逐步上升到最大上下文长度，比如说inference的时候。对于这种情况，我们有两种解决方法：&lt;/p>
&lt;ol>
&lt;li>在整个inference周期中，RoPE的scaling factor都设置为 $s=K'/L$, 其中$L'$ 是扩展后的上下文长度&lt;/li>
&lt;li>在每次foward的过程汇总，都更新sclaing factor $s=\max(1, \ell'/L)$, 这里 $\ell'$ 是当前sequence的长度&lt;/li>
&lt;/ol>
&lt;p>作者发现，方案1在sequence长度小于 $L$的时候性能会下降，并且当上下文长度超过 $L'$ 时，性能下降的更快。
但是，方案2可以让模型的性能下降曲线更平缓。因此，作者将这种inference-time方法称为 &lt;strong>Dynamic Scaling method&lt;/strong>, 当其与NTK-aware方法结合时，就得到了 &lt;strong>Dynamic NTK interpolation&lt;/strong>&lt;/p>
&lt;p>作者通过实验发现，Dynamic NTK interpolation在$L'=L$ 时，效果非常好&lt;/p>
&lt;h2 id="yarn">&lt;a href="#yarn" class="header-anchor">&lt;/a>YaRN
&lt;/h2>&lt;p>在YaRN中，作者针对Dynamic NTK interpolation做了进一步改进，也就是在计算attention softmax时，加入了一个温度参数 $t>0$, 这样attention的计算就变成了&lt;/p>
$$
\mathrm{Attn}(Q, K, V) = \mathrm{softmax}\left(\frac{QK^T}{t\sqrt{d}}\right)V
$$&lt;p>作者发现，通过这种scaling的方式，YaRN可以在不改变代码的前提下，更改attention的机制。并且，其不增加训练和推理的cost&lt;/p>
&lt;p>作者将YaRN定义为&lt;strong>结合了NTK-by-parts interpolation和上述scaling技巧的方法&lt;/strong>&lt;/p>
&lt;p>对于LLaMA，作者推荐使用如下参数：&lt;/p>
$$
\sqrt{\frac{1}{t}} = 0.1\ln(s) + 1.
$$&lt;p>实验结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-yarn/YaRN_temperature_ablation.png"
width="1049"
height="528"
loading="lazy"
alt="Ablation study on temperature"
class="gallery-image"
data-flex-grow="198"
data-flex-basis="476px"
>&lt;/p>
&lt;p>作者发现：&lt;/p>
&lt;ol>
&lt;li>对于合适的 $t$, 扩展上下文之后，perplexity会变的更好&lt;/li>
&lt;li>最好的$t$ 对于不同的位置和样本提升都是一样的&lt;/li>
&lt;/ol>
&lt;h2 id="evaluation">&lt;a href="#evaluation" class="header-anchor">&lt;/a>Evaluation
&lt;/h2>&lt;p>实验结果如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Extension Method&lt;/th>
&lt;th>Trained Tokens&lt;/th>
&lt;th>Context Window&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>Evaluation Context Window Size&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>2048&lt;/td>
&lt;td>4096&lt;/td>
&lt;td>6144&lt;/td>
&lt;td>8192&lt;/td>
&lt;td>10240&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>PI (s = 2)&lt;/td>
&lt;td>1B&lt;/td>
&lt;td>8k&lt;/td>
&lt;td>&lt;/td>
&lt;td>3.92&lt;/td>
&lt;td>3.51&lt;/td>
&lt;td>3.51&lt;/td>
&lt;td>3.34&lt;/td>
&lt;td>8.07&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>NTK ($\theta$ = 20k)&lt;/td>
&lt;td>1B&lt;/td>
&lt;td>8k&lt;/td>
&lt;td>&lt;/td>
&lt;td>4.20&lt;/td>
&lt;td>3.75&lt;/td>
&lt;td>3.74&lt;/td>
&lt;td>3.59&lt;/td>
&lt;td>6.24&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>YaRN (s = 2)&lt;/td>
&lt;td>400M&lt;/td>
&lt;td>8k&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;strong>3.91&lt;/strong>&lt;/td>
&lt;td>&lt;strong>3.50&lt;/strong>&lt;/td>
&lt;td>&lt;strong>3.51&lt;/strong>&lt;/td>
&lt;td>3.35&lt;/td>
&lt;td>&lt;strong>6.04&lt;/strong>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，YaRN使用的数据更少，并且当模型扩展到10240的时候，其表现下降的最慢，这说明了YaRN在扩展上下文长度时的有效性&lt;/p>
&lt;p>原始RoPE，dynamic-PI和dynamic-YaRN的对比&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-yarn/YaRN_comparison_RoPE_PI.png"
width="1154"
height="573"
loading="lazy"
alt="Comparison with RoPE and PI"
class="gallery-image"
data-flex-grow="201"
data-flex-basis="483px"
>&lt;/p>
&lt;p>可以看到，RoPE的上下文扩展能力很差，Dynamic-YaRN的表现最好。&lt;/p>
&lt;h2 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h2>&lt;p>YaRN的实现在&lt;a class="link" href="https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_rope_utils.py" target="_blank" rel="noopener"
>HuggingFace/src/transformers/modeling_rope_utils.py&lt;/a> 里的 &lt;code>_compute_yarn_parameters&lt;/code> 函数里，其返回 &lt;code>inv_freq&lt;/code> 以及 &lt;code>attention_factor&lt;/code> 两个量，前者代表了 $\theta_d$, 后者代表 $t\sqrt{d}$.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">get_mscale&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">scale&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">scale&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="mf">1.0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="mf">0.1&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">math&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">log&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">scale&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mf">1.0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">find_correction_dim&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num_rotations&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">base&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">max_position_embeddings&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Inverse dimension formula to find the dimension based on the number of rotations&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">math&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">log&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">max_position_embeddings&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">num_rotations&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">math&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pi&lt;/span>&lt;span class="p">)))&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">math&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">log&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">base&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">find_correction_range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">low_rot&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">high_rot&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">base&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">max_position_embeddings&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Find dimension range bounds based on rotations&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">low&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">math&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">floor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">find_correction_dim&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">low_rot&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">base&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">max_position_embeddings&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">high&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">math&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ceil&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">find_correction_dim&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">high_rot&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">base&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">max_position_embeddings&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="nb">max&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">low&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="nb">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">high&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">linear_ramp_factor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">min&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">max&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="nb">min&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="nb">max&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">max&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="mf">0.001&lt;/span> &lt;span class="c1"># Prevent singularity&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">linear_func&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="nb">min&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="nb">max&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="nb">min&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ramp_func&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">clamp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">linear_func&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">ramp_func&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">YaRNRotaryEmbedding&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">beta_fast&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;beta_fast&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="ow">or&lt;/span> &lt;span class="mi">32&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">beta_slow&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;beta_slow&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="ow">or&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dim&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;head_dim&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">factor&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">max_position_embeddings&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">original_max_position_embeddings&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pos_freqs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">base&lt;/span> &lt;span class="o">**&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">inv_freq_extrapolation&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">1.0&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">pos_freqs&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">inv_freq_interpolation&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">1.0&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">factor&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">pos_freqs&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">low&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">high&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">find_correction_range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">beta_fast&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">beta_slow&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">base&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">original_max_position_embeddings&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Get n-dimensional rotational scaling corrected for extrapolation&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">inv_freq_extrapolation_factor&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">linear_ramp_factor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">low&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">high&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">inv_freq&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">inv_freq_interpolation&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">inv_freq_extrapolation_factor&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">inv_freq_extrapolation&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">inv_freq_extrapolation_factor&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attention_factor&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">get_mscale&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">factor&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">position_ids&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">...&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">cos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sin&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>实际的transformers代码中，Qwen使用的还是默认的RoPE，在inference时如果我们需要扩展上下文，可以通过修改config的形式：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">transformers&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">pipeline&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">model_name_or_path&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;Qwen/Qwen3-8B&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">generator&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">pipeline&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;text-generation&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">model_name_or_path&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">torch_dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;auto&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">device_map&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;auto&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">model_kwargs&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;max_position_embeddings&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="mi">131072&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;rope_scaling&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;rope_type&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;yarn&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;factor&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="mf">4.0&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;original_max_position_embeddings&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="mi">32768&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>在本文中，作者首先构建了一个统一的表征不同上下文长度扩展的形式，接下来作者分析了不同上下文长度扩展的不足，并提出了YaRN这种上下文长度扩展方式，结果发现，YaRN不仅在短上下文长度下面表现很好，当上下文长度扩展之后，其表现依然非常优秀。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2309.00071" target="_blank" rel="noopener"
>arxiv YaRN&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2306.15595" target="_blank" rel="noopener"
>arxiv PI&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://qwen.readthedocs.io/en/latest/inference/transformers.html#enabling-long-context" target="_blank" rel="noopener"
>Qwen documentation&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Qwen-LLM</title><link>https://maosong.website/p/notes-on-qwen-llm/</link><pubDate>Thu, 03 Jul 2025 10:47:27 +0800</pubDate><guid>https://maosong.website/p/notes-on-qwen-llm/</guid><description>&lt;p>Qwen 在 23 年 9 月份发布了 Qwen 系列大语言模型，包括 1.8B， 7B，14B 三个 size，训练过程使用了 3T token. 作者还基于 Qwen，构建了 Code-Qwen-Chat，Math-Qwen-Chat 等系列领域大语言模型。&lt;/p>
&lt;h2 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h2>&lt;h3 id="data">&lt;a href="#data" class="header-anchor">&lt;/a>Data
&lt;/h3>&lt;p>数据一共使用了 &lt;strong>3T token&lt;/strong>，主要是 public web documents, encyclopedia, books, codes, etc，覆盖了中文和英文两种语言&lt;/p>
&lt;p>数据处理：&lt;/p>
&lt;ol>
&lt;li>语言识别&lt;/li>
&lt;li>去重，包括 MinHash 和 LSH 算法&lt;/li>
&lt;li>质量过滤，包括基于规则和和基于 ML 的方法&lt;/li>
&lt;li>上采样，特定数据会进行上采样&lt;/li>
&lt;li>加入指令数据，提高模型的 zero-shot 和 few-shot 表现&lt;/li>
&lt;/ol>
&lt;h3 id="tokenization">&lt;a href="#tokenization" class="header-anchor">&lt;/a>Tokenization
&lt;/h3>&lt;p>BPE tokenizer，最终的 tokenizer 大小为 152K&lt;/p>
&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;p>模型架构基于 LLaMA， 改动：&lt;/p>
&lt;ol>
&lt;li>tie embdding: input embdding 和 output embdding 使用的权重相同&lt;/li>
&lt;li>position encoding:RoPE, inverse frequency 的精度为 FP32&lt;/li>
&lt;li>bias: 取消了大部分的 bias，增加了 QKV bias，来提高模型的外推能力&lt;/li>
&lt;li>Pre-Norm &amp;amp; RMSNorm&lt;/li>
&lt;li>Activation function: SwiGLU&lt;/li>
&lt;/ol>
&lt;h3 id="training">&lt;a href="#training" class="header-anchor">&lt;/a>Training
&lt;/h3>&lt;ul>
&lt;li>上下文长度：2048&lt;/li>
&lt;li>attention：&lt;a class="link" href="https://maosong.website/p/notes-on-flashattention/" target="_blank" rel="noopener"
>flash attention&lt;/a>&lt;/li>
&lt;li>optimizer：AdamW， $\beta_1=0.9$, $\beta_2=0.95$, $\epsilon=10^{-8}$.&lt;/li>
&lt;li>data type: BF16&lt;/li>
&lt;/ul>
&lt;h3 id="context-extention">&lt;a href="#context-extention" class="header-anchor">&lt;/a>Context Extention
&lt;/h3>&lt;p>使用了三个技巧：&lt;/p>
&lt;ol>
&lt;li>NTK-aware position interpolation&lt;/li>
&lt;li>log-N scaling&lt;/li>
&lt;li>window attention&lt;/li>
&lt;/ol>
&lt;p>后续前两个统一成了 &lt;a class="link" href="https://maosong.website/p/notes-on-yarn/" target="_blank" rel="noopener"
>YARN&lt;/a>.&lt;/p>
&lt;p>observation: lower layer 对上下文长度扩展更敏感, 因此作者动态调整了 window size&lt;/p>
&lt;h2 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h2>&lt;p>包括 SFT 和 RLHF 两个阶段&lt;/p>
&lt;h3 id="sft">&lt;a href="#sft" class="header-anchor">&lt;/a>SFT
&lt;/h3>&lt;p>data： 使用了 ChatML 格式&lt;/p>
&lt;h3 id="rlhf">&lt;a href="#rlhf" class="header-anchor">&lt;/a>RLHF
&lt;/h3>&lt;p>PPO 算法&lt;/p>
&lt;p>reward model 构建：基于 Qwen-base model&lt;/p>
&lt;p>RL 训练：先更新 value model 50 steps&lt;/p>
&lt;p>发现：top-p 设置为 0.9 比设置为 1.0 更好&lt;/p>
&lt;h3 id="tool-use-and-agent">&lt;a href="#tool-use-and-agent" class="header-anchor">&lt;/a>Tool-use and Agent
&lt;/h3>&lt;p>作者使用了 self-instruct 来进行 SFT，基于 ReAct 构建数据，数据包括 2000 条高质量数据&lt;/p>
&lt;h2 id="specialization">&lt;a href="#specialization" class="header-anchor">&lt;/a>Specialization
&lt;/h2>&lt;h3 id="code-qwen">&lt;a href="#code-qwen" class="header-anchor">&lt;/a>Code-Qwen
&lt;/h3>&lt;p>code-qwen 基于 qwen continue Pretraining 得到，然后基于 code-qwen 进行 sft 得到 code-qwen-chat，包括 7B 和 14B 两个 size&lt;/p>
&lt;h3 id="math-qwen">&lt;a href="#math-qwen" class="header-anchor">&lt;/a>Math-Qwen
&lt;/h3>&lt;p>基于 qwen 直接 SFT 得到，包括 7B 和 14B 两个 size&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中介绍了 Qwen 系列大语言模型，模型使用了 3T token，作者介绍了训练的细节以及如何扩展到领域大语言模型 Code-Qwen 和 Math-Qwen&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://spaces.ac.cn/archives/9444" target="_blank" rel="noopener"
>Length exploration&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2309.16609" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Hands on LLM(2) Transformer</title><link>https://maosong.website/p/hands-on-llm2-transformer/</link><pubDate>Sun, 29 Jun 2025 11:40:39 +0800</pubDate><guid>https://maosong.website/p/hands-on-llm2-transformer/</guid><description>&lt;p>Transformer 实现&lt;/p>
&lt;p>我们采用 top-down 的形式构建 transformer 的代码&lt;/p>
&lt;h2 id="架构">&lt;a href="#%e6%9e%b6%e6%9e%84" class="header-anchor">&lt;/a>架构
&lt;/h2>&lt;p>&lt;img src="https://maosong.website/p/hands-on-llm2-transformer/transformer_architecture.png"
width="1210"
height="1364"
loading="lazy"
alt="bg right"
class="gallery-image"
data-flex-grow="88"
data-flex-basis="212px"
>&lt;/p>
&lt;p>我们以 Qwen3 的代码为例子讲解 Assignment1 的代码实现&lt;/p>
&lt;p>我们通过在 transformer 架构上加上一个 linear layer 就可以完成不同的下游任务，比如：&lt;/p>
&lt;ul>
&lt;li>&lt;code>Qwen3ForQuestionAnswering&lt;/code>&lt;/li>
&lt;li>&lt;code>Qwen3ForCausalLM&lt;/code>&lt;/li>
&lt;li>&lt;code>Qwen3ForSequenceClassification&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>因此，大语言模型是 transformer 的一个附加产物&lt;/p>
&lt;h2 id="causallm">&lt;a href="#causallm" class="header-anchor">&lt;/a>CausalLM
&lt;/h2>&lt;p>编写大语言模型的第一步为定义 &lt;code>Qwen3ForCausalLM&lt;/code>&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">CausalLM&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">model&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Transformer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">lm_head&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">vocab_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">...&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">outputs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">model&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">***&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">logits&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">lm_head&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">outputs&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">logits&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>这里 &lt;code>lm_head&lt;/code> 的作用就是构建 embedding space 到 vocabulary 的映射，即 $\mathbb{R}^d\to\mathbb{R}^{|V|}$&lt;/p>
&lt;h2 id="transformer">&lt;a href="#transformer" class="header-anchor">&lt;/a>Transformer
&lt;/h2>&lt;p>transformer 部分包括四个部分：&lt;/p>
&lt;ol>
&lt;li>Embedding Layer：将 token 映射到 embedding space&lt;/li>
&lt;li>layers：Transformer 的主体部分，由 $n$ 个 &lt;code>DecodeLayer&lt;/code> 组成&lt;/li>
&lt;li>Norm：在输出之前，进行一次 Normalization&lt;/li>
&lt;li>Position Embedding：由于输入的 sequence 长度是固定的，因此我们提前计算好每一层的 position embedding&lt;/li>
&lt;/ol>
&lt;p>&lt;code>Transformer&lt;/code> 部分的代码&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Transformer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">embedding&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Embedding&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">vocab_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pad_token_id&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">layers&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ModuleList&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">[&lt;/span>&lt;span class="n">DecodeLayer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">layer_idx&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">layer_idx&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_hidden_layers&lt;/span>&lt;span class="p">)]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">norm&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">RMSNorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">eps&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">rms_norm_eps&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">rotary_emb&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">RotaryEmbedding&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">input_ids&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="o">...&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">input_embeds&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">embedding&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">input_ids&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">input_embeds&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">position_embeddings&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">rotary_emb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">position_ids&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">decode_layer&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">layers&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">layer_outputs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">decode_layer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">position_ids&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">position_embeddings&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">layer_outputs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">norm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">logits&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="decodelayer">&lt;a href="#decodelayer" class="header-anchor">&lt;/a>DecodeLayer
&lt;/h3>&lt;p>&lt;code>DecodeLayer&lt;/code> 就是 transformer 的核心部分，里面包含四个模块：&lt;/p>
&lt;ol>
&lt;li>Pre-Normalization：一般是 RMSNorm 或者 LayerNorm&lt;/li>
&lt;li>Attention：self-attention&lt;/li>
&lt;li>Post-Normalization：与 Pre-Normalization 一致&lt;/li>
&lt;li>MLP：FFN，SwiGLU 或者 MoE&lt;/li>
&lt;/ol>
&lt;p>&lt;code>DecodeLayer&lt;/code> 还会使用 residual connection 来防止梯度消失&lt;/p>
&lt;p>&lt;code>DecodeLayer&lt;/code> 部分的代码&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">DecodeLayer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">layer_idx&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">attn&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Attention&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">layer_idx&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mlp&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">MLP&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pre_norm&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">RMSNorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">eps&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">rms_norm_eps&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">post_norm&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">RMSNorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">eps&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">rms_norm_eps&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">position_ids&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">position_embeddings&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">residual&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">hidden_states&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pre_norm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">attn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">position_ids&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">position_embeddings&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># residual&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">residual&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">residual&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">hidden_states&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">post_norm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mlp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># residual&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">residual&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">hidden_states&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>我们接下来按照&lt;/p>
&lt;ol>
&lt;li>Normalization&lt;/li>
&lt;li>MLP&lt;/li>
&lt;li>Attention&lt;/li>
&lt;li>Position embedding&lt;/li>
&lt;/ol>
&lt;p>的顺序来介绍&lt;/p>
&lt;h2 id="rmsnorm">&lt;a href="#rmsnorm" class="header-anchor">&lt;/a>RMSNorm
&lt;/h2>&lt;p>RMSNorm 的作用和 LayerNorm 是一样的，但是实现上更简单&lt;/p>
$$
\mathrm{LayerNorm}(x) = \frac{x-\mathbb{E}[x]}{\sqrt{\mathrm{var}[x]+\epsilon}}\odot \beta + \gamma
$$&lt;p>其中 $\beta,\gamma\in\mathbb{R}^d$ 是可学习的参数&lt;/p>
$$
\mathrm{RMSNorm}(x) = \frac{x}{\sqrt{\|x\|_2^2+\epsilon}}\odot \gamma
$$&lt;p>其中 $\gamma\in\mathbb{R}^d$ 是可学习的参数&lt;/p>
&lt;p>RMSNorm 代码实现&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">RMSNorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">eps&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">weight&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Parameter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ones&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">d&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">eps&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">eps&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">input_dtype&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dtype&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">variance&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pow&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mean&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">keepdim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">rsqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">variance&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">eps&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">weight&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">input_dtype&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="mlp">&lt;a href="#mlp" class="header-anchor">&lt;/a>MLP
&lt;/h2>&lt;p>现在大语言模型的 MLP 使用的激活函数一般都是 SwiGLU, 其定义为&lt;/p>
$$
\mathrm{SwiGLU}(x) = x\odot \sigma(x)
$$&lt;p>其中 $\sigma(\cdot)$ 是 sigmoid 函数&lt;/p>
&lt;p>MLP 的定义为&lt;/p>
$$
y = W_2(W_3x\odot \mathrm{SwiGLU}(W_1x))
$$&lt;p>其中 $W_3,W_1\in\mathbb{R}^{d_{ff}\times d}$, $W_2\in\mathbb{R}^{d\times d_{ff}}$&lt;/p>
&lt;p>一般地，由于 FFN 只有两个权重矩阵，且 $d_{ff}=4d$, 在 SwiGLU 中，为了保证参数量一致，其隐藏层大小设置为 $d_{ff}'=\frac23d_{ff}=\frac83 d$.&lt;/p>
&lt;p>MLP 的代码如下所示&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">SwiGLU&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sigmoid&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">MLP&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d_ff&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">gate_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">d&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d_ff&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">up_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">d&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d_ff&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">down_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">d_ff&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">down_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">SwiGLU&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">gate_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">up_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="attention">&lt;a href="#attention" class="header-anchor">&lt;/a>Attention
&lt;/h2>&lt;p>我们先不考虑 position embedding，直接看 attention，attention 定义为&lt;/p>
$$
\mathrm{Attention}(X) = \mathrm{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V\in\mathbb{R}^{m\times d}
$$&lt;p>其中 $X\in\mathbb{R}^{m\times d}$,&lt;/p>
$$
Q = W_QX\in\mathbb{R}^{m\times d},\quad
K =W_KX\in\mathbb{R}^{n\times d},\quad
V = W_VX\in\mathbb{R}^{n\times d}
$$&lt;p>在自回归模型里，我们还会加上 mask, 让每个 token 只能看见前面的 token 的信息&lt;/p>
$$
\mathrm{Attention}(X) = \mathrm{softmax}\left(\frac{QK^T}{\sqrt{d}}\odot M\right)V
$$&lt;p>其中&lt;/p>
$$
M = [M_{ij}] = \begin{cases}
1, &amp;\text{ if } i &lt; j\\
0, &amp;\text{ otherwise}
\end{cases}
$$&lt;p>self-attention 的代码如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">scaled_dot_product_attention&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Q&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">K&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">V&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">mask&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">d_k&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Q&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="c1"># d_k&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">scaled_factor&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">d_k&lt;/span>&lt;span class="o">**&lt;/span>&lt;span class="mf">0.5&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">scores&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">einsum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;... s_q d_k, ... s_k d_k -&amp;gt; ... s_q s_k&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Q&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">K&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">scores&lt;/span> &lt;span class="o">*=&lt;/span> &lt;span class="n">scaled_factor&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">mask&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">scores&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">scores&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">masked_fill&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">mask&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">float&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;-inf&amp;#34;&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">scores&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">scores&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">softmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">einsum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;... s_q s_k, ... s_k d_v -&amp;gt; ... s_q d_v&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">scores&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">V&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="multi-head-attention">&lt;a href="#multi-head-attention" class="header-anchor">&lt;/a>Multi-Head Attention
&lt;/h2>&lt;p>Multi-Head Attention 定义如下&lt;/p>
$$
\mathrm{MultiHeadAttention}(X) = [\mathrm{Attention}_1(X),\dots,\mathrm{Attention}_h(X)]W_o\in\mathbb{R}^{m\times d}
$$&lt;p>其中 $W_o\in\mathbb{R}^{d\times d}$, 且每一个 Attention heads 的维度会从 $d\to d/h$.&lt;/p>
&lt;p>Multi-Head Attention 的主要作用为：&lt;/p>
&lt;ol>
&lt;li>让不同的 head 关注不同的信息&lt;/li>
&lt;li>并行计算，提高计算效率&lt;/li>
&lt;/ol>
&lt;p>MHA 代码&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">MultiHeadAttention&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d_model&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_heads&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">d_model&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d_model&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">k_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">d_model&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d_model&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">v_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">d_model&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d_model&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">output_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">d_model&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d_model&lt;/span>&lt;span class="p">,)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">position_embeddings&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">mask&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">Q&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">rearrange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="s2">&amp;#34;... seq_len (num_heads head_dim) -&amp;gt; ... num_heads seq_len head_dim&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">num_heads&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_heads&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">head_dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">K&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">rearrange&lt;/span>&lt;span class="p">(&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">k_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="s2">&amp;#34;... seq_len (num_heads head_dim) -&amp;gt; ... num_heads seq_len head_dim&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">num_heads&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_heads&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">head_dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">mask&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">mask&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ones&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Q&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">K&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">mask&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tril&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">mask&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">position_embeddings&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sin&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">cos&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">position_embeddings&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">Q&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">apply_rotary_pos_emb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Q&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sin&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">cos&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">K&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">apply_rotary_pos_emb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">K&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sin&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">cos&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">V&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">rearrange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">v_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="s2">&amp;#34;... seq_len (num_heads head_dim) -&amp;gt; ... num_heads seq_len head_dim&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">num_heads&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_heads&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">head_dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">scaled_dot_product_attention&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Q&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">K&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">V&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">mask&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">mask&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">rearrange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">output&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;... num_heads seq_len head_dim -&amp;gt; ... seq_len (num_heads head_dim)&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">output_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">output&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="position-encoding">&lt;a href="#position-encoding" class="header-anchor">&lt;/a>Position Encoding
&lt;/h2>&lt;p>Attention 对于输入的顺序是不敏感的，也就是&lt;/p>
$$
\mathrm{Attention}(Q, \Pi K, \Pi V) = \mathrm{Attention}(Q, K, V)
$$&lt;p>这里 $\Pi\in \{0,1\}^{d\times d}$ 是一个置换矩阵 (permutation matrix)&lt;/p>
&lt;p>Transformer 的解决方法是在 query 和 key 上加上位置信息：&lt;/p>
$$
Q' = Q + PE(Q),\ K'=K + PE(K)
$$&lt;p>这样&lt;/p>
$$
\mathrm{Attention}(Q, K, V) = \mathrm{softmax}\left(\frac{(Q + PE(Q))(K + PE(K))^T}{\sqrt{d}}\right)V\in\mathbb{R}^{m\times d}
$$&lt;p>就包含了位置信息&lt;/p>
&lt;h3 id="绝对位置编码">&lt;a href="#%e7%bb%9d%e5%af%b9%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81" class="header-anchor">&lt;/a>绝对位置编码
&lt;/h3>&lt;p>Transformer 的使用的位置编码如下所示&lt;/p>
$$
PE(pos, 2i) = \sin\left(\frac{pos}{10000^{2i/d}}\right)
$$$$
PE(pos, 2i+1) = \cos\left(\frac{pos}{10000^{2i/d}}\right)
$$&lt;h3 id="rope">&lt;a href="#rope" class="header-anchor">&lt;/a>RoPE
&lt;/h3>&lt;p>苏剑林老师提出了 &lt;a class="link" href="https://maosong.website/p/notes-on-position-encoding/" target="_blank" rel="noopener"
>Position Encoding&lt;/a>，现在已经被广泛使用&lt;/p>
$$
q' = R_{\theta,m}^dq, k' = R_{\theta,n}^d k
$$&lt;p>这样 $\langle q, k\rangle$ 就&lt;strong>仅&lt;/strong>包含两者的相对位置信息&lt;/p>
$$
\langle q_m, k_n\rangle = x^TW_qR_{\theta, n-m}^d W_kx_n
$$&lt;p>RoPE 的矩阵定义如下&lt;/p>
$$
R_{\theta,m}^d = \mathrm{diag}(M_1,\dots,M_{d/2})
$$&lt;p>其中&lt;/p>
$$
M_i = \begin{bmatrix}
\cos m\theta_i &amp; -\sin m\theta_i\\
\sin m\theta_i &amp; \cos m\theta_i
\end{bmatrix}
$$&lt;p>这里&lt;/p>
$$
\theta_i = \frac{1}{10000^{2(i-1)/d}}, i\in\{1,2,\dots,d/2\}
$$&lt;p>简化后得到&lt;/p>
$$
R_{\theta,m}^dq = \begin{bmatrix}
\cos m\theta_0\\
\cos m\theta_0\\
\vdots\\
\cos m\theta_{d/2}\\
\cos m\theta_{d/2}\\
\end{bmatrix}\odot \begin{bmatrix}
x1\\
x2\\
\vdots\\
x_{d-1}\\
x_d\\
\end{bmatrix} + \begin{bmatrix}
\sin m\theta_0\\
\sin m\theta_0\\
\vdots\\
\sin m\theta_{d/2}\\
\sin m\theta_{d/2}\\
\end{bmatrix}\odot
\begin{bmatrix}
-\ x_2\\
x_1\\
\vdots\\
-x_d\\
x_{d-1}\\
\end{bmatrix}
$$&lt;h3 id="rope-代码-naive-实现">&lt;a href="#rope-%e4%bb%a3%e7%a0%81-naive-%e5%ae%9e%e7%8e%b0" class="header-anchor">&lt;/a>RoPE 代码 Naive 实现
&lt;/h3>&lt;p>&lt;code>RotaryEmbedding&lt;/code> 代码&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">RotaryEmbedding&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">inv_freq&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">1.0&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">theta&lt;/span> &lt;span class="o">**&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)[:&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)]&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">position_ids&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">freqs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">einsum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">max_seq_len&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">inv_freq&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;seq_len, d_k_half -&amp;gt; seq_len d_k_half&amp;#34;&lt;/span>&lt;span class="p">)[&lt;/span>&lt;span class="n">token_positions&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sin&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sin&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">freqs&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">cos&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cos&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">freqs&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">sin&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">cos&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>计算部分代码&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">apply_rotary_pos_emb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sin&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">cos&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x_even&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">...&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">::&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="c1"># (seq_len, d_k_half)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x_odd&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">...&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">::&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="c1"># (seq_len, d_k_half)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">odds&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">cos&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">x_even&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">sin&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">x_odd&lt;/span> &lt;span class="c1"># (...,seq_len, d_k_half)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">evens&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">sin&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">x_even&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">cos&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">x_odd&lt;/span> &lt;span class="c1"># (...,seq_len, d_k_half)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">stacked&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">stack&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">odds&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">evens&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># (...,seq_len, 2, d_k_half)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">stacked_trans&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">rearrange&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">stacked&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;... seq_len double d_k_half -&amp;gt; ... seq_len d_k_half double&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span> &lt;span class="c1"># (...,seq_len, d_k_half, 2)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">out&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">rearrange&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">stacked_trans&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;... seq_len d_k_half double -&amp;gt; ... seq_len (d_k_half double)&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span> &lt;span class="c1"># (..., seq_len, d_k)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">out&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="rope-标准实现">&lt;a href="#rope-%e6%a0%87%e5%87%86%e5%ae%9e%e7%8e%b0" class="header-anchor">&lt;/a>RoPE 标准实现
&lt;/h3>&lt;p>&lt;code>RotaryEmbedding&lt;/code> 代码 (LLaMA)&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">LlamaRotaryEmbedding&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">LlamaConfig&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">device&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">inv_freq&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">inv_freq&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">1.0&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">base&lt;/span> &lt;span class="o">**&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">int64&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="c1"># d_k_half&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">position_ids&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># (bsz, d_k_half, 1)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">inv_freq_expanded&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">inv_freq&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">expand&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">position_ids&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># (bsz, 1, seq_len)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">position_ids_expanded&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">position_ids&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># (bsz, seq_len, d_k_half)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">freqs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">inv_freq_expanded&lt;/span> &lt;span class="o">@&lt;/span> &lt;span class="n">position_ids_expanded&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transpose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">emb&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cat&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">freqs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">freqs&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># (..., seq_len, d_k)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">cos&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">emb&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cos&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="c1"># (..., seq_len, d_k)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sin&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">emb&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sin&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="c1"># (..., seq_len, d_k)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">cos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sin&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>计算部分代码&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">rotate_half&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">...&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">...&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="p">:]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cat&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">x2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x1&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">apply_rotary_pos_emb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">q&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">cos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sin&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">position_ids&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">unsqueeze_dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">cos&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">cos&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">unsqueeze&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">unsqueeze_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sin&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">sin&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">unsqueeze&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">unsqueeze_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q_embed&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">q&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">cos&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">rotate_half&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">q&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">sin&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">k_embed&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">k&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">cos&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">rotate_half&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">sin&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">q_embed&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k_embed&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ol>
&lt;li>&lt;a class="link" href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen3/modeling_qwen3.py" target="_blank" rel="noopener"
>Qwen3 transformer source code&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://huggingface.co/blog/designing-positional-encoding" target="_blank" rel="noopener"
>position encoding blog&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>Unified perspective on dLLM and LLM</title><link>https://maosong.website/p/unified-perspective-on-dllm-and-llm/</link><pubDate>Sat, 28 Jun 2025 15:02:09 +0800</pubDate><guid>https://maosong.website/p/unified-perspective-on-dllm-and-llm/</guid><description>&lt;h1 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h1>&lt;p>在上一篇blog里，我们介绍了MLE和KL minimization的等价性。在这篇blog里，我们将要基于这个等价性，推导masked diffusion LLM, autoregressive LLM, any-order diffusion LLM之间的等价性。最终，我们发现，这几种建模方式本质上都是一致的。&lt;/p>
&lt;h1 id="preliminary">&lt;a href="#preliminary" class="header-anchor">&lt;/a>Preliminary
&lt;/h1>&lt;p>对于 $\bm{x}=(x_1,\dots,x_L)\in\mathbb{R}^L$, 基于概率的链式法则，我们有&lt;/p>
$$
p(\bm{x}) = \prod_{i=1}^Lp(x_i\mid x_{&lt;i})
$$&lt;p>1D的类别分布，对于随机变量 $X\in\{1,\dots, K\}$以及概率分布 $\bm{p}=(p_1,\dots,p_K)$ 我们定义类别分布为 $\mathrm{Cat}(\bm{x};\bm{p})$, 其中 $\bm{x}\in\mathbb{R}^K$是一个向量，其PMF定义为：&lt;/p>
$$
f(\bm{x}; \bm{p}) = \prod_{i=1}^Kp_i^{[x=i]}
$$&lt;p>这里 $[x=i]$ 当 $x=i$时为$1$，否则为 $0$.&lt;/p>
&lt;p>一个比较好理解的例子是骰子，骰子有$K=6$种可能性，每一面出现的可能性都是 $p_i=1/6$. 当 $\bm{x}$是one-hot向量是，其代表了出现某一面的概率。&lt;/p>
&lt;p>Discrete-Time Discrete Markov Chain. 接下来我们考虑离散时间离散Markov链，我们现在的随机变量不仅与状态 $\{1,\dots,K\}$ 还与时间有关，我们记 $X_n$ 为 $n$ 时刻的状态分布。这样，我们可以规定不同时刻之间的状态转换矩阵：&lt;/p>
$$
Q_{ij} = \mathrm{Pr}(x_{n+1}=j\mid x_n=i)
$$&lt;h2 id="categorical-distribution">&lt;a href="#categorical-distribution" class="header-anchor">&lt;/a>Categorical distribution
&lt;/h2>&lt;h1 id="autoregressive-llm">&lt;a href="#autoregressive-llm" class="header-anchor">&lt;/a>autoregressive LLM
&lt;/h1>&lt;p>首先是我们最常见的自回归大语言模型，给定文本语料 $\bm{x}=(x_1,\dots,x_L)\sim p_{data}$， 大语言模型的目标在于求解最大似然估计&lt;/p>
$$
\begin{aligned}
\theta^* &amp;= \arg\max_{\theta}\mathbb{E}_{\bm{x}\sim p_{data}}[\log p(\bm{x}\mid \theta))]\\
&amp;= \arg\max_{\theta}\mathbb{E}_{\bm{x}\sim p_{data}}\left[\log \left(\prod_{i=1}^Lp(x_i\mid x_{&lt;i},\theta )\right) \right]\\
&amp;= \arg\max_{\theta}\mathbb{E}_{\bm{x}\sim p_{data}}\left[\sum_{i=1}^L\log p(x_i\mid x_{&lt;i},\theta ) \right]
\end{aligned}
$$&lt;p>这里我们使用了链式法则.&lt;/p>
&lt;h1 id="any-order-autoregressive-llm">&lt;a href="#any-order-autoregressive-llm" class="header-anchor">&lt;/a>Any order autoregressive LLM
&lt;/h1>&lt;p>接下来，我们证明Any order autoregressive LLM的目标函数等价于求解MLE。
令 $S_D$ 为在集合 $\{1,\dots, D\}$ 上所有可能的permutation，令 $\sigma\in S_D$ 为一个其中的一个permutation，则我们有&lt;/p>
$$
p(\sigma) = \frac{1}{D!}
$$&lt;p>因此&lt;/p>
$$
p(x) = \sum_{\sigma\in S_d}p(x,\sigma) = \sum_{\sigma\in S_D}p(x\mid \sigma)p(\sigma) = \frac{1}{D!}\sum_{\sigma\in S_D}p(x\mid \sigma)=\mathbb{E}_{\sigma\sim U(S_D)}[p(x\mid \sigma)]
$$&lt;p>这里 $U(\cdot)$ 代表均匀分布，从而我们有&lt;/p>
$$
\log p(x) = \log \mathbb{E}_{\sigma\sim U(S_D)}[p(x\mid \sigma)] \geq \mathbb{E}_{\sigma\sim U(S_D)}[\log p(x\mid \sigma)]
$$&lt;p>
这里我们使用了Jensen不等式&lt;/p>
&lt;p>那么对于any-order autoregressive model, 我们有：&lt;/p>
$$
\log p(x) \geq \mathbb{E}_{\sigma\sim U(S_D)}\sum_{t=1}^D \log p(x_{\sigma(t)}\mid x_{\sigma(&lt;t)})
$$&lt;p>也就是说，对于any-order autoregressive model，我们的目标函数是MLE的一个下界。&lt;/p>
&lt;h1 id="masked-language-modeling">&lt;a href="#masked-language-modeling" class="header-anchor">&lt;/a>Masked Language modeling
&lt;/h1>&lt;h1 id="discrete-diffusion-model">&lt;a href="#discrete-diffusion-model" class="header-anchor">&lt;/a>Discrete Diffusion Model
&lt;/h1>&lt;h1 id="any-order-diffusion-llm">&lt;a href="#any-order-diffusion-llm" class="header-anchor">&lt;/a>Any-order diffusion LLM
&lt;/h1>&lt;h1 id="masked-diffusion-llm">&lt;a href="#masked-diffusion-llm" class="header-anchor">&lt;/a>Masked diffusion LLM
&lt;/h1>&lt;h1 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2107.03006" target="_blank" rel="noopener"
>Structured Denoising Diffusion Models in Discrete State-Spaces&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Relationship between MLE and KL divergence</title><link>https://maosong.website/p/relationship-between-mle-and-kl-divergence/</link><pubDate>Fri, 27 Jun 2025 11:35:33 +0800</pubDate><guid>https://maosong.website/p/relationship-between-mle-and-kl-divergence/</guid><description>&lt;h1 id="mle">&lt;a href="#mle" class="header-anchor">&lt;/a>MLE
&lt;/h1>&lt;p>最大似然估计，即MLE (maximum likelihood estimation), 是一个估计参数分布的方法，其核心思想是：模型的参数，应该让观察样本出现的概率最大。&lt;/p>
&lt;p>假设我们有一个参数分布 $p(x\mid \theta)$, 其中 $\theta$ 是参数，如正态分布中的均值和方差。我们从$p(x\mid \theta)$进行采样得到 $i.i.d.$ 的数据 $X=\{x_1,\dots,x_n\}$.&lt;/p>
&lt;p>似然函数 (likelihood function) 定义为给定数据 $X$ 的联合分布，即：&lt;/p>
$$
\mathcal{L}(\theta\mid X) = P(X\mid \theta)
$$&lt;p>由于 $X=\{x_1,\dots,x_n\}$ 是 $i.i.d.$, 因此，我们可以将上式改写为：&lt;/p>
$$
\mathcal{L}(\theta\mid X) = \prod_{i=1}^n p(x_i\mid \theta)
$$&lt;p>这样我们的优化目标就是&lt;/p>
$$
\begin{aligned}
\theta_{MLE}^* &amp;= \arg\max_{\theta} \mathcal{L}(\theta\mid X)\\
&amp;= \arg\max_{\theta} \prod_{i=1}^n p(x_i\mid \theta)\\
&amp;= \arg\max_{\theta} \log\prod_{i=1}^n p(x_i\mid \theta)\\
&amp;=\arg\max_{\theta} \sum_{i=1}^n \log p(x_i\mid \theta)\\
\end{aligned}
$$&lt;p>即&lt;/p>
$$
\theta_{MLE}^* = \arg\max_{\theta} \sum_{i=1}^n \log p(x_i\mid \theta)
$$&lt;h1 id="kl-divergence">&lt;a href="#kl-divergence" class="header-anchor">&lt;/a>KL divergence
&lt;/h1>&lt;p>KL divergence 用于衡量概率分布 $Q(x)$ 到概率分布 $P(x)$ 的不同程度，我们可以将其理解为：如果我们用 $Q(x)$来替换 $P(x)$, 会有多大的信息损失？&lt;/p>
&lt;p>连续概率分布的KL divergence的定义如下&lt;/p>
$$
D_{KL}(P\mid\mid Q) =\int P(x)\log\left(\frac{P(x)}{Q(x)}\right)dx
$$&lt;p>
离散概率分布的KL divergence定义如下&lt;/p>
$$
D_{KL}(P\mid\mid Q) = \sum_{x} P(x)\log\left(\frac{P(x)}{Q(x)}\right)
$$&lt;p>KL divergence有两个关键性质：&lt;/p>
&lt;ol>
&lt;li>非负性：$D_{KL}(P\mid\mid Q)\geq0$, 且 $D_{KL}(P\mid\mid Q)=0$ 当且仅当 $P(x)=Q(x)$ 对任意 $x$成立&lt;/li>
&lt;li>非对称性： 一般情况下，$D_{KL}(P\mid\mid Q)\neq D_{KL}(Q\mid\mid P)$.&lt;/li>
&lt;/ol>
&lt;h1 id="mle和kl-divergence的等价性">&lt;a href="#mle%e5%92%8ckl-divergence%e7%9a%84%e7%ad%89%e4%bb%b7%e6%80%a7" class="header-anchor">&lt;/a>MLE和KL Divergence的等价性
&lt;/h1>&lt;p>我们假设 $p_{data}(x)$ 是数据$X$的真实分布， 我们现在需要找到合适的参数 $\theta$ 以及其对应的分布 $p(x\mid \theta)$ 来近似 $p_{data}(x)$, 此时我们可以用KL Divergence作为我们的目标函数，即&lt;/p>
$$
\theta_{KL} = \arg\min_{\theta}D_{KL}(p_{data}(x)\mid\mid p(x\mid \theta))
$$&lt;p>我们将上面的式子进行展开得到&lt;/p>
$$
\begin{aligned}
\theta_{KL}^* &amp;= \arg\min_{\theta}D_{KL}(p_{data}(x)\mid\mid p(x\mid \theta))\\
&amp;= \arg\min_{\theta} \int p_{data}(x)\frac{p_{data}(x)}{p(x\mid \theta)} dx\\
&amp;= \arg\min_{\theta}\int p_{data}(x)\log p_{data}(x) dx - \int p_{data}(x)\log p(x\mid \theta)dx \\
&amp;= \arg\min_{\theta} - \int p_{data}(x)\log p(x\mid \theta)dx \\
&amp;= \arg\max_{\theta} \int p_{data}(x)\log p(x\mid \theta)dx
\end{aligned}
$$&lt;p>实际上，真实的数据分布 $p_{data}(x)$ 是未知的，我们只有从 $p_{data}(x)$ 采样得到的一批数据 $X=\{x_1,\dots,x_n\}\sim p_{data}(x)$.&lt;/p>
&lt;p>基于大数定律，我们有&lt;/p>
$$
\frac{1}{n}\sum_{i=1}^n\log p(\theta_i\mid \theta)=\mathbb{E}_{x\sim p_{data}}[\log p(x\mid \theta)] = \int p_{data}(x)\log p(x\mid \theta)dx, n\to \infty
$$&lt;p>这样，最大似然估计就与最小化KL divergence构建起了联系：&lt;/p>
$$
\begin{aligned}
\theta_{MLE}^*&amp;=\arg\max_{\theta} \sum_{i=1}^n \log p(x_i\mid \theta)\\
&amp;= \arg\max_{\theta} \int p_{data}(x)\log p(x\mid \theta)dx\\
&amp;= \theta_{KL}^*, n\to\infty.
\end{aligned}
$$&lt;p>也就是说，当采样样本足够多的时候，最大似然估计和最小KL divergence是等价的。&lt;/p></description></item><item><title>Notes on MiMo-VL</title><link>https://maosong.website/p/notes-on-mimo-vl/</link><pubDate>Thu, 05 Jun 2025 10:51:43 +0800</pubDate><guid>https://maosong.website/p/notes-on-mimo-vl/</guid><description>&lt;h1 id="介绍">&lt;a href="#%e4%bb%8b%e7%bb%8d" class="header-anchor">&lt;/a>介绍
&lt;/h1>&lt;p>小米在5月30号发布了MiMo-VL 7B多模态推理大模型，模型包括SFT和RL两个版本，作者在技术报告里讲解了MiMo-VL的架构，预训练和后训练过程，最后对MiMo-VL的性能进行了评测&lt;/p>
&lt;h1 id="架构">&lt;a href="#%e6%9e%b6%e6%9e%84" class="header-anchor">&lt;/a>架构
&lt;/h1>&lt;p>MiMo-VL 7B的架构是一个标准的 &amp;ldquo;ViT-MLP-LLM&amp;quot;的架构，其中：&lt;/p>
&lt;ul>
&lt;li>ViT使用了Qwen2.5-VL的Qwen2.5-ViT&lt;/li>
&lt;li>MLP是一个两层的SwiGLU MLP Layer&lt;/li>
&lt;li>LLM是之前发布的MiMo-7B&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-mimo-vl/architecture.png"
width="1237"
height="798"
loading="lazy"
alt="Architecture of MiMo-VL 7B"
class="gallery-image"
data-flex-grow="155"
data-flex-basis="372px"
>&lt;/p>
&lt;h1 id="预训练">&lt;a href="#%e9%a2%84%e8%ae%ad%e7%bb%83" class="header-anchor">&lt;/a>预训练
&lt;/h1>&lt;h2 id="数据">&lt;a href="#%e6%95%b0%e6%8d%ae" class="header-anchor">&lt;/a>数据
&lt;/h2>&lt;p>预训练阶段一共使用了2.4T的token&lt;/p>
&lt;ul>
&lt;li>Image caption data: 使用了rewrite caption等方法提升和过滤数据，作者还基于MetaCLIP来降低数据的不平衡性&lt;/li>
&lt;li>Interleaved data: 主要从webpage, books, papers中提取，最后基于relevance, complementarity, balance of information density来保证数据的质量&lt;/li>
&lt;li>OCR and grounding data: OCR数据包括文档，表格，数学公式等。grounding data包括单物体和多物体&lt;/li>
&lt;li>video data: rewrite caption, caption有对应的时间戳，加入了video analysis数据&lt;/li>
&lt;li>GUI data: 覆盖mobile, web, desktop三种场景, 加入了GUI grounding和GUI action数据&lt;/li>
&lt;li>Synthetic reasoning data:使用一个reasoning model来生成带有思考过程的reasoning数据&lt;/li>
&lt;/ul>
&lt;h2 id="训练">&lt;a href="#%e8%ae%ad%e7%bb%83" class="header-anchor">&lt;/a>训练
&lt;/h2>&lt;p>训练有四个阶段，基本上和之前的模型一致，先训练MLP，然后解冻ViT，再训练全部参数，最后扩展模型的上下文。训练过程如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-mimo-vl/pretraining.png"
width="1339"
height="570"
loading="lazy"
alt="Pretraining of MiMo-VL 7B"
class="gallery-image"
data-flex-grow="234"
data-flex-basis="563px"
>&lt;/p>
&lt;h1 id="后训练">&lt;a href="#%e5%90%8e%e8%ae%ad%e7%bb%83" class="header-anchor">&lt;/a>后训练
&lt;/h1>&lt;p>作者提出了Mixed On-policy Reinforcement Learning (MORL)框架来把RLVR和RLHF结合起来&lt;/p>
&lt;h2 id="数据-1">&lt;a href="#%e6%95%b0%e6%8d%ae-1" class="header-anchor">&lt;/a>数据
&lt;/h2>&lt;p>RLVR的数据包括:&lt;/p>
&lt;ol>
&lt;li>visual reasoning, 80K数据，使用rule-based math-verify library来评估&lt;/li>
&lt;li>text reasoning, 使用了MiMo的数学推理数据，评估方式与visual reasoning一致&lt;/li>
&lt;li>Image grounding, 包括general and GUI grounding任务，使用GIoU来计算奖励&lt;/li>
&lt;li>Visual Counting, 使用准确率来打分&lt;/li>
&lt;li>Temporal Video Grounding, temporal video grounding任务，使用IoU来计算奖励&lt;/li>
&lt;/ol>
&lt;p>RLHF的数据包括中英文的query，作者使用MiMo-VL-7B和其他模型来采样，然后使用一个advanced VLM来排序.
RLHF的reward model使用Breadley-Terry model作为训练目标，text-only reward由MiMo-7B初始化得到，多模态reward model由MiMo-VL-7B初始化得到&lt;/p>
&lt;h2 id="训练-1">&lt;a href="#%e8%ae%ad%e7%bb%83-1" class="header-anchor">&lt;/a>训练
&lt;/h2>&lt;p>训练过程就是将RLVR和RLHF放在一起进行训练，作者使用了MiMo的seamless rollout engine.&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-mimo-vl/RL_training.png"
width="1328"
height="466"
loading="lazy"
alt="RL Training"
class="gallery-image"
data-flex-grow="284"
data-flex-basis="683px"
>&lt;/p>
&lt;p>训练目标与GRPO一致，但是本文中作者使用了on-policy的训练方式。&lt;/p>
&lt;h1 id="评估">&lt;a href="#%e8%af%84%e4%bc%b0" class="header-anchor">&lt;/a>评估
&lt;/h1>&lt;p>作者评估了MiMo-VL-7B的通用能力，reasoning能力，GUI交互理解能力，最后还给出了MiMo-VL-7B在ChatbotArena上的排名&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-mimo-vl/Performance.png"
width="1367"
height="688"
loading="lazy"
alt="Performance of MiMo-VL-7B"
class="gallery-image"
data-flex-grow="198"
data-flex-basis="476px"
>&lt;/p>
&lt;h2 id="ablation-study">&lt;a href="#ablation-study" class="header-anchor">&lt;/a>Ablation study
&lt;/h2>&lt;p>作者首先探究了在预训练阶段加入Long CoT数据对模型表现的影响，结果发现模型正在MMMU, OlympiadBench等benchmark上都有了提升&lt;/p>
&lt;p>作者还比较了原始的GRPO和本文中使用的on-policy版本的GRPO，结果发现on-policy版本的GRPO效果更好&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-mimo-vl/on-policy-GRPO-ablation.png"
width="1368"
height="843"
loading="lazy"
alt="Ablation of on-policy GRPO"
class="gallery-image"
data-flex-grow="162"
data-flex-basis="389px"
>&lt;/p>
&lt;p>最后，作者探讨了一下将不同任务放在一起训练导致的互相干扰问题，这个在MiMo里也提到过，核心问题就是有的任务是short CoT的，有的任务是Long CoT的，如何让模型根据任务来选择不同的reasoning length是一个需要探讨的问题，这个问题在&lt;a class="link" href="https://maosong.website/p/notes-on-vc-ppo/" target="_blank" rel="noopener"
>VC-PPO&lt;/a>里进行了初步的探讨。&lt;/p>
&lt;h1 id="结论">&lt;a href="#%e7%bb%93%e8%ae%ba" class="header-anchor">&lt;/a>结论
&lt;/h1>&lt;p>与MiMo一样，MiMo-VL-7B通过在预训练阶段加入reasoning data，来提高模型的reasoning能力，并指出模型可以通过这个方式来媲美其他模型的表现&lt;/p>
&lt;h1 id="参考文献">&lt;a href="#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae" class="header-anchor">&lt;/a>参考文献
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://github.com/XiaomiMiMo/MiMo-VL" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Hands on LLM(1) Tokenizer</title><link>https://maosong.website/p/hands-on-llm1-tokenizer/</link><pubDate>Sat, 24 May 2025 19:56:34 +0800</pubDate><guid>https://maosong.website/p/hands-on-llm1-tokenizer/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>在自然语言处理中，tokenizer的作用是将一个文本序列通过一个字典转化为一个token id的序列。我们回顾图片分类任务，模型在预测的时候，实际上预测的是类别对应的id，而不是类别本身。tokenizer做的事情就是提供一个类似于从类别到对应id的字典。&lt;/p>
&lt;p>一般来说，一个tokenizer处理文本序列的过程有两步：&lt;/p>
&lt;ol>
&lt;li>pre-tokenize，也就是预处理，我们需要将文本序列分割成合适大小的chunks (words)&lt;/li>
&lt;li>tokenize，构建chunks (words)到token id的映射&lt;/li>
&lt;/ol>
&lt;p>实际上, huggingface的tokenizer包括&lt;a class="link" href="https://huggingface.co/docs/tokenizers/pipeline" target="_blank" rel="noopener"
>四个步骤&lt;/a>, 其中第二第三个步骤与上述一致. 在pre-tokenize之前, 我们有一个normalization过程, 该过程会对文本序列进行处理, 如将文本序列变为小写, 删掉声调符号等, 如下面例子所示：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">tokenizers&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">normalizers&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">tokenizers.normalizers&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">NFD&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">StripAccents&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">normalizer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">normalizers&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Sequence&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">NFD&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">StripAccents&lt;/span>&lt;span class="p">()])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">normalizer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">normalize_str&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Héllò hôw are ü?&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># &amp;#34;Hello how are u?&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>在tokenize之后, 我们会有一个post-processing过程, 比如BERT会在生成的token系列前后加入 &lt;code>[CLS]&lt;/code> token 和 &lt;code>[SEP]&lt;/code> token, 例子如下:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">transformers&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">AutoTokenizer&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">tokenizer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">AutoTokenizer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">from_pretrained&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;bert-base-cased&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">token_ids&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tokenizer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">encode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;I love NLP.&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">token_ids&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># [101, 146, 1567, 21239, 2101, 119, 102]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># represents [[CLS], &amp;#34;I&amp;#34;, &amp;#34;love&amp;#34;, &amp;#34;NL&amp;#34;, &amp;#34;##P&amp;#34;, &amp;#34;.&amp;#34;, [SEP]]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>其完整流程如下图所示 (图源: &lt;a class="link" href="https://huggingface.co/learn/llm-course/chapter6/4?fw=pt" target="_blank" rel="noopener"
>huggingface llm-course&lt;/a>)&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/hands-on-llm1-tokenizer/tokenization_pipeline.png"
width="1702"
height="1234"
loading="lazy"
alt="tokenization pipeline"
class="gallery-image"
data-flex-grow="137"
data-flex-basis="331px"
>&lt;/p>
&lt;p>构建好tokenizer之后, 我们还要保证tokenizer提供两个接口：&lt;/p>
&lt;ol>
&lt;li>encoding, 给定文本序列, 将其映射到字典中去得到token id序列&lt;/li>
&lt;li>decoding, 给定token id序列, 将其解码成文本序列&lt;/li>
&lt;/ol>
&lt;p>接下来, 我们将简单介绍一下word tokenizer, character tokenizer以及byte tokenizer, 并分析它们各自的不足。
然后, 我们介绍现代大语言模型中使用最多的BPE tokenizer。最后, 我们介绍一些sub-word tokenizer。&lt;/p>
&lt;h2 id="training-free-tokenizer">&lt;a href="#training-free-tokenizer" class="header-anchor">&lt;/a>Training-free tokenizer
&lt;/h2>&lt;p>本节我们将要介绍word tokenizer, character tokenizer以及byte tokenizer, 它们的特点就是简单易懂, 不需要额外的规则和学习.&lt;/p>
&lt;h3 id="word-tokenizer">&lt;a href="#word-tokenizer" class="header-anchor">&lt;/a>Word tokenizer
&lt;/h3>&lt;p>给定一个文本序列, 我们现在需要将其转化为一个token序列。一个比较自然的想法是, 我们按照空格将序列拆分成若干个单词, 这样每个单词的语义都能比较好地保留。下面是一个例子：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">tiktoken&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">tokenizer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tiktoken&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_encoding&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;gpt2&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">indices&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tokenizer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">encode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;hello world&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># indices = [31373, 995]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># decode = [&amp;#34;hello&amp;#34;, &amp;#34; world&amp;#34;]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>接下来我们基于一个预定义好的词典, 将其转化为一个token id的序列。&lt;/p>
&lt;p>word tokenizer的优点是能够保留语义信息，且压缩率比较高（每个token包含的bytes数），其问题是不能处理预定义好的词典之外的词 (out of vocabulary, OOV)。现有的处理方法是使用 &lt;code>&amp;lt;UNK&amp;gt;&lt;/code> token来表示这些OOV的词。
但这样显然会丢失语义信息, 因为我们编码成 &lt;code>&amp;lt;UNK&amp;gt;&lt;/code> token之后, 就没办法再解码回原有的语义信息了。&lt;/p>
&lt;p>word tokenizer的缺点为：&lt;/p>
&lt;ol>
&lt;li>单词数量很大, 很多罕见单词的出现频率很低, 降低了tokenizer的利用率&lt;/li>
&lt;li>对于不在词典内的单词只能用&lt;code>&amp;lt;UNK&amp;gt;&lt;/code> token表示, 损害了语义信息&lt;/li>
&lt;/ol>
&lt;p>既然基于word的tokenizer有OOV的问题，我们能否想办法解决这个问题呢？答案是可以的, 我们可以使用 character tokenizer。&lt;/p>
&lt;h3 id="character-tokenizer">&lt;a href="#character-tokenizer" class="header-anchor">&lt;/a>Character tokenizer
&lt;/h3>&lt;p>Character tokenizer的基本思想是使用字符而不是单词来编码文本序列。其实现方式如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">CharacterTokenizer&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">encode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">list&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="nb">list&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">ord&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">c&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">c&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">decode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">token_ids&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">list&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">join&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">chr&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">token_id&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">token_id&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">token_ids&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>character tokenizer的词表大小取决于我们的编码方式，UTF-8的编码大概有&lt;a class="link" href="https://en.wikipedia.org/wiki/UTF-8" target="_blank" rel="noopener"
>110K code points&lt;/a>。character tokenizer的缺点总结如下：&lt;/p>
&lt;ol>
&lt;li>character tokenizer会导致我们的词表非常大&lt;/li>
&lt;li>和word tokenizer一样, 很多character非常罕见, 会降低词表的利用率&lt;/li>
&lt;li>token序列的上下文语义信息较差&lt;/li>
&lt;/ol>
&lt;h3 id="byte-tokenizer">&lt;a href="#byte-tokenizer" class="header-anchor">&lt;/a>Byte tokenizer
&lt;/h3>&lt;p>我们发现, character tokenizer和word tokenizer的词表都很大, 我们能否想办法降低词表大小, 提升每个token的利用率呢？答案是使用Byte tokenizer.&lt;/p>
&lt;p>Byte tokenizer的基本思想是, 所有的字符(character)都是由byte组成的, 比如对于UTF-8编码来说, 每个字符由1-4个byte组成。
因此, 所有满足UTF-8编码的文本, 我们都可以将它们转换为基于byte的token序列。
由于现在的大部分文本都是基于UTF-8的, 因此, 我们只讨论UTF-8编码的文本。&lt;/p>
&lt;p>Byte tokenizer的实现如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">ByteTokenizer&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">encode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">list&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="nb">list&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">encode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;utf-8&amp;#34;&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">decode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">token_ids&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">list&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="nb">bytes&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">token_ids&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">decode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;utf-8&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>byte tokenizer的词表很小, 其词表大小为 &lt;code>256&lt;/code>, 这是因为一个byte可以有256中可能的值.&lt;/p>
&lt;p>尽管byte tokenizer实现简单，并且词表也很小，可以说byte tokenizer解决了character tokenizer和word tokenizer的问题。
但是，byte tokenizer的问题在于，其encode的到的token序列可能会非常长！我们知道，transformer计算量与token序列的长度是平方级关系的，也就是说token序列长度增加10倍，整体的计算量就会增加100倍，因此我们势必需要考虑token序列的长度。&lt;/p>
&lt;p>总之，byte tokenizer的问题为：&lt;/p>
&lt;ol>
&lt;li>产生的token序列过长, 增加了transformer的计算量&lt;/li>
&lt;li>没有上下文语义信息&lt;/li>
&lt;/ol>
&lt;h3 id="总结">&lt;a href="#%e6%80%bb%e7%bb%93" class="header-anchor">&lt;/a>总结
&lt;/h3>&lt;p>我们总结一下word tokenizer, character tokenizer以及byte tokenizer三者各自的特点:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Feature&lt;/th>
&lt;th>Word Tokenizer&lt;/th>
&lt;th>Character Tokenizer&lt;/th>
&lt;th>Byte Tokenizer&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Granularity&lt;/td>
&lt;td>Coarse&lt;/td>
&lt;td>Medium&lt;/td>
&lt;td>Fine&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Vocabulary&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>No&lt;/td>
&lt;td>No&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Support OOV&lt;/td>
&lt;td>Bad&lt;/td>
&lt;td>Good&lt;/td>
&lt;td>Best&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>#Tokens&lt;/td>
&lt;td>Small&lt;/td>
&lt;td>Large&lt;/td>
&lt;td>Very Large&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Chinese&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Support Spell Error&lt;/td>
&lt;td>Bad&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Context&lt;/td>
&lt;td>Good&lt;/td>
&lt;td>Bad&lt;/td>
&lt;td>Worst&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>因此, 这三种tokenizer尽管实现起来很简单, 但是其都有各自的问题. 为了解决这些问题, 我们的做法就是折衷, 使用sub-word tokenizer, 也就是介于word tokenizer和byte tokenizer之间的方法.&lt;/p>
&lt;h2 id="bpe">&lt;a href="#bpe" class="header-anchor">&lt;/a>BPE
&lt;/h2>&lt;h3 id="基本原理与实现">&lt;a href="#%e5%9f%ba%e6%9c%ac%e5%8e%9f%e7%90%86%e4%b8%8e%e5%ae%9e%e7%8e%b0" class="header-anchor">&lt;/a>基本原理与实现
&lt;/h3>&lt;p>实际生活中，对于出现频率比较高的词，我们会有一个简写的方式，也就是我们使用一个新的单词来表示这个词。比如在英语中，我们会使用&lt;code>plz&lt;/code> 来代替 &lt;code>please&lt;/code> 以及使用&lt;code>how r u&lt;/code> 来代替&lt;code>how are you&lt;/code>。
BPE，即byte pair tokenizer的原理也是类似的，对于出现频率比较高的byte pair或者character pair, 我们会使用一个新的token来表示这个pair，这样就压缩了sequence的长度。&lt;/p>
&lt;p>BPE算法包括以下几个步骤:&lt;/p>
&lt;ol>
&lt;li>对文本序列进行pre-tokenize, 分割成不同的单词&lt;/li>
&lt;li>当&lt;code>len(vocab)&amp;lt;vocab_size&lt;/code>时, 重复以下步骤:
&lt;ol>
&lt;li>对所有单词, 统计其相邻character或者byte pair的频率&lt;/li>
&lt;li>计算出现频率最高的pair, 使用一个新的token来表示这个pair&lt;/li>
&lt;li>将新的token和其对应的&lt;code>token_id&lt;/code>加入到&lt;code>vocab&lt;/code>中, 并更新单词的分割表示&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;p>算法如下图所示 (参考文献2)&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/hands-on-llm1-tokenizer/bpe_algorithm.png"
width="724"
height="828"
loading="lazy"
alt="BPE algorithm"
class="gallery-image"
data-flex-grow="87"
data-flex-basis="209px"
>&lt;/p>
&lt;blockquote>
&lt;p>注意：实际上，我们实现的是BBPE (byte BPE算法)，BBPE与BPE的区别在于我们的最小单元是character还是bytes。本质上原理是一致的&lt;/p>
&lt;/blockquote>
&lt;p>实现代码见 &lt;a class="link" href="https://github.com/MaoSong2022/assignment1-basics/blob/main/cs336_basics/naive_bpe.py" target="_blank" rel="noopener"
>Github naive BPE&lt;/a>&lt;/p>
&lt;h3 id="高效实现">&lt;a href="#%e9%ab%98%e6%95%88%e5%ae%9e%e7%8e%b0" class="header-anchor">&lt;/a>高效实现
&lt;/h3>&lt;p>BPE的原理很简单, 我们也实现了其naive版本, 但是naive版本的问题是太慢了。因此我们将要优化naive版本的效率。&lt;/p>
&lt;p>首先我们发现, 我们不需要遍历所有的word, 只有含有&lt;code>best_pair&lt;/code>的word我们才会进行处理, 因此, 我们的第一个改进就是使用 &lt;code>pair_to_word&lt;/code> 来记录每个pair的来源, 比如：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">pair_to_word&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">(&lt;/span>&lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39; &amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;t&amp;#39;&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39; the&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39; it&amp;#39;&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">(&lt;/span>&lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;t&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;h&amp;#39;&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;the&amp;#39;&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>这样, 我们在merge的时候, 直接使用 &lt;code>pair_to_word[best_pair]&lt;/code> 来获取需要被更新的token序列就可以了。&lt;/p>
&lt;p>其次, 注意到每次merge之后, 我们都需要重新计算一次 &lt;code>pair_freq&lt;/code>, 而实际上, 只有被merge的token序列才需要被重新计数, 其他大部分token序列都是不需要重新计数的。
因此, 一个改进点就是我们在merge的过程中就更新 &lt;code>pair_freq&lt;/code>, 而不是重新计算。为了达到这个目标, 我们其实只需要两个操作。
我们用&lt;code>(b'x', b'a', b'b', b'y')&lt;/code> 和 &lt;code>best_pair=(b'a', b'b')&lt;/code>来说明, merge之前, 这个序列贡献的&lt;code>pair_freq&lt;/code>为：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">(&lt;/span>&lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;x&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;a&amp;#39;&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">(&lt;/span>&lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;a&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;b&amp;#39;&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">(&lt;/span>&lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;b&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;y&amp;#39;&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>merge之后, token序列变成了&lt;code>(b'x', b'z', b'y')&lt;/code> (假设&lt;code>best_pair&lt;/code>对应的新的token为&lt;code>b'z'&lt;/code>), 这时候的计数为:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">(&lt;/span>&lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;x&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;a&amp;#39;&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">(&lt;/span>&lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;a&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;b&amp;#39;&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">(&lt;/span>&lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;b&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;y&amp;#39;&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">(&lt;/span>&lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;x&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;z&amp;#39;&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">(&lt;/span>&lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;z&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="sa">b&lt;/span>&lt;span class="s1">&amp;#39;y&amp;#39;&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>也就是说, merge之后, 三个pair的计数减少了1, 分别是&lt;code>(token_seq[i-1], merge_pair[0])&lt;/code>,&lt;code>merge_pair&lt;/code> 和 &lt;code>(merge_pair[1], token_seq[i+2])&lt;/code>。两个pair的个数增加了1, 分别是 &lt;code>(token_seq[i-1], new_token)&lt;/code>和&lt;code>(new_token, token_seq[i+2])&lt;/code> (这里我们假设&lt;code>merge_pair=(token_seq[i], token_seq[i+1])&lt;/code>)。&lt;/p>
&lt;p>基于这个结论，我们就可以优化BPE算法了，具体逻辑就是：&lt;/p>
&lt;ol>
&lt;li>pretokenize, 将 text 切分为若干个 word&lt;/li>
&lt;li>计算&lt;code>word_count&lt;/code>, &lt;code>pair_freq&lt;/code>, &lt;code>pair_to_word&lt;/code>, 使用&lt;code>splits&lt;/code>记录每个word对应的token分布&lt;/li>
&lt;li>重复以下过程：
&lt;ol>
&lt;li>挑选频率最高的pair将其merge为一个新的token, 基于&lt;code>pair_to_words&lt;/code>更新对应的&lt;code>pair_freq&lt;/code>&lt;/li>
&lt;li>对每个&lt;code>split&lt;/code>, 按照上述方式更新&lt;code>pair_freq&lt;/code>和&lt;code>split&lt;/code>&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;p>其具体实现见 &lt;a class="link" href="https://github.com/MaoSong2022/assignment1-basics/blob/main/cs336_basics/efficient_bpe.py" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/p>
&lt;h2 id="other-subword-tokenizers">&lt;a href="#other-subword-tokenizers" class="header-anchor">&lt;/a>Other subword tokenizers
&lt;/h2>&lt;h3 id="wordpiece">&lt;a href="#wordpiece" class="header-anchor">&lt;/a>WordPiece
&lt;/h3>&lt;p>WordPiece是Google在预训练BERT时采用的tokenizer，WordPiece的基本思想和BPE差不多，都是从一个较小的vocab开始的。&lt;/p>
&lt;p>首先，WordPiece会通过加上prefix &lt;code>##&lt;/code>来把单词进行切分，比如 &lt;code>word&lt;/code> 会被拆分为：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;w&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;##o&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;##r&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;##d&amp;#39;&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>接下来，对于pair $(a, b)$, WordPiece定义了merge pair的规则如下：&lt;/p>
$$
\mathrm{score}((a, b)) = \frac{\#(a, b)}{\#a \times \#b}
$$&lt;p>其中 $\#(a, b)$, $\#a$, $\#b$ 分别代表 pair $(a, b)$, 元素 $a$ 和元素 $b$ 的频率。
通过这个方式，我们会给予包含元素出现频率较低的pair更高的优先级。通过这个方式，我们选取score最高的pair，然后将其用一个新的token表示，然后和BPE算法一样，继续这一过程直到我们的vocab size达到指定大小。&lt;/p>
&lt;p>在tokenize的时候，WordPiece会找出现在vocab中的最长的subword, 比如对于&lt;code>'hugs'&lt;/code>, 假设从左向右在词典中的最长subword是&lt;code>'hug'&lt;/code>, 那么&lt;code>'hugs'&lt;/code> 就会被拆分为 &lt;code>['hug', '##s']&lt;/code>。如果我们在词表中找不到对应的subword，这个时候我们就会使用&lt;code>'[UNK]'&lt;/code>来表示。&lt;/p>
&lt;p>具体实现见 &lt;a class="link" href="https://github.com/MaoSong2022/assignment1-basics/blob/main/cs336_basics/word_piece.py" target="_blank" rel="noopener"
>Github wordpiece&lt;/a> (基于&lt;a class="link" href="https://huggingface.co/learn/llm-course/chapter6/4?fw=pt" target="_blank" rel="noopener"
>huggingface llm course&lt;/a>)。 代码实现除了选择最优pair的方式不同之外，和BPE基本一致。&lt;/p>
&lt;h3 id="unigram">&lt;a href="#unigram" class="header-anchor">&lt;/a>Unigram
&lt;/h3>&lt;p>Unigram也是由Google提出来的tokenizer，与BPE和wordpiece不同，unigram从一个非常大的vocab开始，然后merge token来降低vocab的size，直到达到指定大小。初始的vocab可以基于BPE算法或者使用prefix subword来构建。并且，初始vocab还包含所有的base characters来保证所有的word都可以被tokenize。&lt;/p>
&lt;p>算法的描述如下:&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/hands-on-llm1-tokenizer/unigram.png"
width="619"
height="669"
loading="lazy"
alt="unigram"
class="gallery-image"
data-flex-grow="92"
data-flex-basis="222px"
>&lt;/p>
&lt;p>我们来看一下算法的细节, 首先对于一个word, 我们有多种切割方式, 比如&lt;code>'bug'&lt;/code>可以被切分为如下三种形式:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="p">[[&lt;/span>&lt;span class="s1">&amp;#39;b&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;u&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;g&amp;#39;&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;b&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;ug&amp;#39;&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;bu&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;g&amp;#39;&lt;/span>&lt;span class="p">]]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>unigram 假设每个 word 出现的概率是其 subword 出现概率的乘积, 即对于包含 $n$个subword的单词 $\bm{x}=(x_1,\dots,x_n)$, 我们有:&lt;/p>
$$
p(\bm{x}) = \prod_{i=1}^n p(x_i)
$$&lt;p>其中，对于给定的vocab $\mathcal{V}$, 我们有：&lt;/p>
$$\sum_{v\in\mathcal{V}} p(x)=1$$&lt;p>unigram的目的就是选择合适的切分 $\bm{x}\in S(\bf{x})$ (这里我们用 $\bf{x}$ 表示单词本身, 用 $\bm{x}$ 表示 $\bf{x}$ 的一个切分), 使得 $p(\bm{x})$的概率最大. 这样我们就可以写出unigram的损失函数了:&lt;/p>
$$
\mathcal{L} = \sum_{i=1}^{N} \log\left(\sum_{\bm{x}\in S(\bf{x})}p(\bm{x})\right)
$$&lt;p>其本质就是: 我们希望对每个单词找到一种合适的切分, 切分得到的subword的概率分布满足其求和为1, 并且使得每个单词的概率最大.&lt;/p>
&lt;p>但是直接对上面概率最大化的问题就是我们每个subword的概率是未知的, unigram的做法是使用EM算法求解这个问题.&lt;/p>
&lt;p>当我们求解完成之后, 对每个subword, 我们都尝试将其从 $\mathcal{V}$中移除, 然后计算移除后的损失 $loss_i$, 我们依照$loss_i$对subword进行排序, 然后我们去掉 $\eta \%$ 比例的subword.&lt;/p>
&lt;p>unigram的伪代码逻辑如下:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;span class="lnt">9
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">while&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">model&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">vocab_size&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">scores&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">compute_scores&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">model&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sorted_scores&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">sorted&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">scores&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">items&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="k">lambda&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Remove percent_to_remove tokens with the lowest scores.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">model&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">percent_to_remove&lt;/span>&lt;span class="p">)):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">_&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">token_freqs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pop&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">sorted_scores&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">total_sum&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">sum&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">freq&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">token&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">freq&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">token_freqs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">items&lt;/span>&lt;span class="p">()])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">model&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="n">token&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="n">log&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">freq&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">total_sum&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">token&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">freq&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">token_freqs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">items&lt;/span>&lt;span class="p">()}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>其中 &lt;code>compute_scores&lt;/code> 用于计算最优分割以及从&lt;code>model&lt;/code>中去掉每个token之后的loss.&lt;/p>
&lt;p>具体实现见 &lt;a class="link" href="https://github.com/MaoSong2022/assignment1-basics/blob/main/cs336_basics/unigram.py" target="_blank" rel="noopener"
>Github wordpiece&lt;/a> (基于&lt;a class="link" href="https://huggingface.co/learn/llm-course/chapter6/4?fw=pt" target="_blank" rel="noopener"
>huggingface llm course&lt;/a>)。代码实现的关键在于为每个word选取最优分割，huggingface是采取了动态规划的方法，也就是我们使用 &lt;code>dp[i]&lt;/code> 来表示 &lt;code>word[:i]&lt;/code> 的最优score，这样我们有：&lt;/p>
$$
dp[i] = \max_{0 \leq j &lt; i} dp[j]* p(word[j:i]),\quad \mathrm{s.t.}\ word[j:i]\in \mathcal{V}
$$&lt;p>这里的乘法代表 $p(\bm{x}) = \prod_{i=1}^n p(x_i)$, 在实现的时候我们会取log变成加法，然后概率会由频率来代替。&lt;/p>
&lt;h3 id="subword-tokenizer总结">&lt;a href="#subword-tokenizer%e6%80%bb%e7%bb%93" class="header-anchor">&lt;/a>Subword tokenizer总结
&lt;/h3>&lt;p>sub-word tokenizer的对比 (来自&lt;a class="link" href="https://huggingface.co/learn/llm-course/chapter6/4?fw=pt" target="_blank" rel="noopener"
>huggingface llm course&lt;/a>)：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Method&lt;/th>
&lt;th>BPE&lt;/th>
&lt;th>WordPiece&lt;/th>
&lt;th>Unigram&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Start Vocabulary&lt;/td>
&lt;td>Small&lt;/td>
&lt;td>Small&lt;/td>
&lt;td>Large&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Train&lt;/td>
&lt;td>Merge tokens&lt;/td>
&lt;td>Merge tokens&lt;/td>
&lt;td>Remove tokens&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Training Step&lt;/td>
&lt;td>Merge with most frequent pair&lt;/td>
&lt;td>Merge with best score&lt;/td>
&lt;td>Remove all tokens minimized the loss&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Learns&lt;/td>
&lt;td>Merge rules and a vocab&lt;/td>
&lt;td>A vocab&lt;/td>
&lt;td>A vocab with a score for each token&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Encoding&lt;/td>
&lt;td>Splits into words and applies merge rules&lt;/td>
&lt;td>Find the longest subword from the beginning that is in the vocab&lt;/td>
&lt;td>Finds the most likely split into tokens with learned scores&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Model&lt;/td>
&lt;td>GPT&lt;/td>
&lt;td>BERT&lt;/td>
&lt;td>T5&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="实践">&lt;a href="#%e5%ae%9e%e8%b7%b5" class="header-anchor">&lt;/a>实践
&lt;/h2>&lt;h3 id="tiktoken">&lt;a href="#tiktoken" class="header-anchor">&lt;/a>tiktoken
&lt;/h3>&lt;p>&lt;a class="link" href="https://github.com/openai/tiktoken" target="_blank" rel="noopener"
>tiktoken&lt;/a>是openAI提出来的一个BPE tokenizer, openAI的模型都基于这个tokenizer, 其主要用于调用GPT系列模型是对token进行计数, 我们可以在&lt;a class="link" href="https://platform.openai.com/tokenizer" target="_blank" rel="noopener"
>tokenizer&lt;/a> 这个网站查看其分词情况.&lt;/p>
&lt;h3 id="sentencepiece">&lt;a href="#sentencepiece" class="header-anchor">&lt;/a>SentencePiece
&lt;/h3>&lt;p>&lt;a class="link" href="https://github.com/google/sentencepiece" target="_blank" rel="noopener"
>SentencePiece&lt;/a>是google开源的一个无监督的text tokenizer，其实现了BPE和unigram两种算法，SentencePiece还是一个语言无关的tokenizer，使其更适合多语种大语言模型的开发。&lt;/p>
&lt;h3 id="tokenizer">&lt;a href="#tokenizer" class="header-anchor">&lt;/a>Tokenizer
&lt;/h3>&lt;p>&lt;a class="link" href="https://github.com/huggingface/tokenizers" target="_blank" rel="noopener"
>tokenizer&lt;/a> 是huggingface推出的为基于transformer服务的tokenizer库, 其支持BPE, wordpiece和unigram等分词算法, 使用简便. 并且, huggingface的tokenizer包括两种:&lt;/p>
&lt;ol>
&lt;li>fast tokenizer, 即&lt;a class="link" href="https://github.com/huggingface/tokenizers" target="_blank" rel="noopener"
>Tokenizer库&lt;/a>, 这个库是基于Rust开发的&lt;/li>
&lt;li>slow tokenizer, 这个是transformer库里模型自带的, 比如ChatGLM就有自己开发的tokenizer&lt;/li>
&lt;/ol>
&lt;p>huggingface比较了并行处理时两者的区别:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Setting&lt;/th>
&lt;th>Fast Tokenizer&lt;/th>
&lt;th>Slow Tokenizer&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>batched=True&lt;/code>&lt;/td>
&lt;td>10.8s&lt;/td>
&lt;td>4min41s&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>batched=False&lt;/code>&lt;/td>
&lt;td>59.2s&lt;/td>
&lt;td>5min3s&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>huggingface提供的tokenizer库已经非常齐全了, 如果我们要训练新的基于transformer的模型的话，建议直接使用Huggingface的&lt;code>AutoTokenizer&lt;/code>。&lt;/p>
&lt;h3 id="总结-1">&lt;a href="#%e6%80%bb%e7%bb%93-1" class="header-anchor">&lt;/a>总结
&lt;/h3>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>特性&lt;/th>
&lt;th>SentencePiece&lt;/th>
&lt;th>Tokenizer&lt;/th>
&lt;th>tiktoken&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>是否适合中文&lt;/td>
&lt;td>√&lt;/td>
&lt;td>√&lt;/td>
&lt;td>×&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>是否适合英文&lt;/td>
&lt;td>√&lt;/td>
&lt;td>√&lt;/td>
&lt;td>√&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>是否适合训练&lt;/td>
&lt;td>√&lt;/td>
&lt;td>√&lt;/td>
&lt;td>×&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>是否快速&lt;/td>
&lt;td>√&lt;/td>
&lt;td>√&lt;/td>
&lt;td>fast&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>是否用于 GPT 系列&lt;/td>
&lt;td>×&lt;/td>
&lt;td>×&lt;/td>
&lt;td>√&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>是否可解码&lt;/td>
&lt;td>√&lt;/td>
&lt;td>√&lt;/td>
&lt;td>√&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>是否支持多语言&lt;/td>
&lt;td>√&lt;/td>
&lt;td>√&lt;/td>
&lt;td>×&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="结论">&lt;a href="#%e7%bb%93%e8%ae%ba" class="header-anchor">&lt;/a>结论
&lt;/h2>&lt;p>本文中, 我们介绍了大语言模型中的tokenizer, 我们从byte level, word level到sub-word level, 再到现代大语言模型最常使用的BPE tokenizer, 并给出了其（高效版本）实现。最后, 我们介绍了一下tokenizer-free的大语言模型和huggingface的tokenizer库。在未来, 我们将继续深入了解大语言模型的基本原理和实现细节。&lt;/p>
&lt;h2 id="参考文献">&lt;a href="#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae" class="header-anchor">&lt;/a>参考文献
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://stanford-cs336.github.io/spring2025/" target="_blank" rel="noopener"
>cs336 Lecture1&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/1508.07909" target="_blank" rel="noopener"
>Neural Machine Translation of Rare Words with Subword Units&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/1808.06226" target="_blank" rel="noopener"
>SentencePiece&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/1804.10959" target="_blank" rel="noopener"
>Unigram&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf" target="_blank" rel="noopener"
>WordPiece&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://huggingface.co/learn/llm-course/chapter6/1" target="_blank" rel="noopener"
>Huggingface LLM Course&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on attention bias</title><link>https://maosong.website/p/notes-on-attention-bias/</link><pubDate>Thu, 22 May 2025 15:25:07 +0800</pubDate><guid>https://maosong.website/p/notes-on-attention-bias/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>我们知道，transformer使用position encoding的一个原因就是，attention layer具有置换不变性，也就是说，我们随机打乱输入token的顺序，并不影响其最终结果 (我们后面会证明，实际上只对key和value具有置换不变性，对query具有置换等变性，也就是改变query的顺序之后，结果的顺序也相应改变)。因此为了让模型学习到正确的上下文知识，我们需要加上position encoding。&lt;/p>
&lt;p>已有的工作大部分都在讨论如何构建更好的position encoding，但是鲜有工作探究为什么attention layer具有置换不变性. 因此，本文将从这一点出发，抽丝剥茧探究其内在原因，最后通过数学公式证明原始transformer是如何具有置换不变性的。&lt;/p>
&lt;h2 id="attention-layer介绍">&lt;a href="#attention-layer%e4%bb%8b%e7%bb%8d" class="header-anchor">&lt;/a>attention layer介绍
&lt;/h2>&lt;p>原始transformer layer的架构比较简单，其结构具有&lt;code>attention-LayerNorm-FFN-LayerNorm&lt;/code>的形式。给定输入 $X\in\mathbb{R}^{d\times m}$ 和上下文 $Y\in\mathbb{R}^{d\times n}$. 其中，attention的定义为&lt;/p>
$$
\mathrm{Attn}(X, Y, Y) = V\mathrm{softmax}\left(\frac{K^TQ}{\sqrt{d}}\right)\in\mathbb{R}^{d\times m}
$$&lt;p>
其中 $d$是模型的&lt;code>hidden_size&lt;/code>, $Q=W_QX\in\mathbb{R}^{d\times m}$, $K=W_KY\in\mathbb{R}^{d\times n}$, $V=W_VY\in\mathbb{R}^{d\times n}$, $W_Q, W_K, W_V\in\mathbb{R}^{d\times d}$ 分别是QKV projection layer的参数.&lt;/p>
&lt;p>LayerNorm的定义为：&lt;/p>
$$
\mathrm{LayerNorm}(x) = \frac{x-\mathbb{E}[x]}{\sqrt{\mathrm{var}[x]+\epsilon}}\odot \gamma + \beta
$$&lt;p>
其中 $\epsilon>0$是一个超参数， $\gamma, \beta\in\mathbb{R}^d$ 是可学习的参数.&lt;/p>
&lt;p>FFN的定义为：&lt;/p>
$$
\mathrm{FFN}(x) = W_2\max(0, W_1x+b_1)+b_2
$$&lt;p>
其中 $W_1\in\mathbb{R}^{d_{ff}\times d}$, $W_2\in\mathbb{R}^{d\times d_{ff}}$, $b_1\in\mathbb{R}^{d_{ff}}$, $b_2\in\mathbb{R}^d$ 是可学习的参数。&lt;/p>
&lt;p>最后，一个attention layer的的结构可以表达为：&lt;/p>
$$
X = X + \mathrm{LayerNorm}(\mathrm{Attn}(X, Y, Y))\\
X = X + \mathrm{LayerNorm}(\mathrm{FFN}(X))\\
$$&lt;h3 id="置换不变性的定义">&lt;a href="#%e7%bd%ae%e6%8d%a2%e4%b8%8d%e5%8f%98%e6%80%a7%e7%9a%84%e5%ae%9a%e4%b9%89" class="header-anchor">&lt;/a>置换不变性的定义
&lt;/h3>&lt;p>置换不变性(permutation invariant)的定义：假设 $f:\mathbb{R}^n\to\mathbb{R}^n$，如果&lt;/p>
$$
f(\sigma(\bm{x})) = (f(\bm{x}))
$$&lt;p>则我们说 $f$是置换不变的. 这里 $\sigma:\mathbb{R}^n\to\mathbb{R}^n$ 是一个置换函数 (permutation function). 当输入的是一个矩阵时，我们默认置换其列，即对 $X=[X_1,\dots,X_n]\in\mathbb{R}^{d\times n}$, 我们有 $\sigma(X)=[X_{\sigma_1},\dots, X_{\sigma_n}]=Y\Pi $, 其中 $\Pi\in\mathbb{R}^{n\times n}\in \{0,1\}^{n\times n}$ 是一个置换矩阵 (permutation matrix)。&lt;/p>
&lt;p>置换等变性 (permutation equivariant)的定义：假设 $f:\mathbb{R}^n\to\mathbb{R}^n$，如果&lt;/p>
$$
f(\sigma(\bm{x})) = \sigma(f(\bm{x}))
$$&lt;p>
则我们说 $f$是置换等变的.&lt;/p>
&lt;h2 id="attention的置换不变性与置换等变性">&lt;a href="#attention%e7%9a%84%e7%bd%ae%e6%8d%a2%e4%b8%8d%e5%8f%98%e6%80%a7%e4%b8%8e%e7%bd%ae%e6%8d%a2%e7%ad%89%e5%8f%98%e6%80%a7" class="header-anchor">&lt;/a>attention的置换不变性与置换等变性
&lt;/h2>&lt;p>我们首先证明attention 对于key和value是置换不变的，即&lt;/p>
$$
\boxed{\mathrm{Attn}(X, \sigma(Y),\sigma(Y)) = \mathrm{Attn}(X, Y, Y)}
$$&lt;p>&lt;strong>证明&lt;/strong>: 我们直接计算即可得到：&lt;/p>
$$
\begin{aligned}
\mathrm{Attn}(X, \sigma(Y),\sigma(Y)) &amp;= V\Pi\mathrm{softmax}\left(\frac{(K\Pi)^TQ}{\sqrt{d}}\right)\\
&amp;=V\Pi\mathrm{softmax}\left(\frac{\Pi^TK^TQ}{\sqrt{d}}\right)\\
\end{aligned}
$$&lt;p>
由于softmax是按列计算的，置换只是改变了元素的顺序，因此我们自然有&lt;/p>
$$
\mathrm{Attn}(X, \sigma(Y),\sigma(Y)) = V\Pi\Pi^T\mathrm{softmax}\left(\frac{K^TQ}{\sqrt{d}}\right)=V\mathrm{softmax}\left(\frac{K^TQ}{\sqrt{d}}\right)=\mathrm{Attn}(X, Y, Y)
$$&lt;p>
这里我们使用了性质 $\Pi\Pi^T=\mathbf{I}$.&lt;/p>
&lt;p>接下来我们证明，attention对于query是置换等变的，即&lt;/p>
$$
\boxed{\mathrm{Attn}(\sigma(X), Y, Y) = \sigma(\mathrm{Attn}(X,Y,Y))}
$$$$
\begin{aligned}
\mathrm{Attn}(\sigma(X), Y, Y) &amp;= V\mathrm{softmax}\left(\frac{K^TQ\Pi}{\sqrt{d}}\right)\\
&amp;= V\mathrm{softmax}\left(\frac{K^TQ\Pi}{\sqrt{d}}\right)\\
&amp;= V\mathrm{softmax}\left(\frac{K^TQ}{\sqrt{d}}\right)\Pi\\
&amp;= \mathrm{Attn}(X,Y,Y)\Pi\\
&amp;= \sigma(\mathrm{Attn}(X,Y,Y))
\end{aligned}
$$&lt;p>从以上的证明可以看到，attention layer对于key和value具有置换不变性，也就是说，我们改变文字顺序不影响最终的输出结果。
但是，我们发现，尽管我们证明了attention具有置换不变性，我们却忽略了一件事：那就是我们计算query, key和value的时候，没有加上bias! 为什么bias如此重要呢？这是因为，$W\sigma(x) = \sigma(Wx)$, 但是 $W\sigma(X)\neq \sigma(Wx+b)$.
因此，我们就会思考，难道是transformer实际上可以通过增加bias的方式来让模型学习到上下文知识？事实上并非如此，我们将要通过分析表明，我们计算query, key和value时，增加的query bias和key bias会被softmax操作给消除掉，而key bias则会被LayerNorm消除掉。因此，我们加与加bias，对attention的置换不变性没有任何影响。&lt;/p>
&lt;h2 id="bias对attention-layer的影响">&lt;a href="#bias%e5%af%b9attention-layer%e7%9a%84%e5%bd%b1%e5%93%8d" class="header-anchor">&lt;/a>Bias对attention layer的影响
&lt;/h2>&lt;p>接下来，我们考虑在计算query, key和value时加入bias。为了简化，我们只考虑query为一个向量的情况，即 $X=\bm{x}\in\mathbb{R}^d$, 我们计算query, key和value如下：&lt;/p>
$$
\bm{q} = W_Q\bm{x}+\bm{b}_Q\in\mathbb{R}^{d}\\
K = W_KY + \bm{b}_K\mathbf{1}^T\in\mathbb{R}^{d\times n}\\
V = W_VY + \bm{b}_V\mathbf{1}^T\in\mathbb{R}^{d\times n}
$$&lt;p>这里 $\mathbf{1}^T\in\mathbb{R}^{n}$. 我们这里简化了scaling的操作，因为其不对结果产生影响。&lt;/p>
&lt;blockquote>
&lt;p>注：以下证明参考了【参考文献2】&lt;/p>
&lt;/blockquote>
&lt;p>我们首先展开attention中的 $V$:&lt;/p>
$$
\begin{aligned}
\mathrm{Attn}(\bm{x}, Y, Y) &amp;= V\mathrm{softmax}\left(K^T\bm{q}\right)\\
&amp;= \left(W_VY + \bm{b}_V\mathbb{1}^T\right)\mathrm{softmax}\left(K^T\bm{q}\right)\\
&amp;= W_VY\mathrm{softmax}\left(K^T\bm{q}\right) + \bm{b}_V\mathbb{1}^T \mathrm{softmax}\left(K^T\bm{q}\right)
\end{aligned}
$$&lt;p>
由于 $\mathrm{softmax}\left(K^T\bm{q}\right)\in\mathbb{R}^{n}$的列求和为$1$, 因此，$\mathbb{1}^T\mathrm{softmax}\left(K^T\bm{q}\right)=1$, 我们有&lt;/p>
$$
\mathrm{Attn}(\bm{x}, Y, Y) = W_VY\mathrm{softmax}\left(K^T\bm{q}\right) + \bm{b}_V
$$&lt;p>接下来，我们展开 $K$:&lt;/p>
$$
\begin{aligned}
\mathrm{Attn}(\bm{x}, Y, Y) &amp;= W_VY\mathrm{softmax}\left(K^T\bm{q}\right) + \bm{b}_V\\
&amp;= W_VY\mathrm{softmax}\left((W_KY + \bm{b}_K\mathbf{1}^T)^T\bm{q}\right) + \bm{b}_V\\
&amp;= W_VY\mathrm{softmax}\left(Y^TW_K^Tq + \mathbf{1}\bm{b}_K^T\bm{q}\right) + \bm{b}_V\\
\end{aligned}
$$$$
\mathrm{softmax}(\bm{x}+\delta)_i = \frac{e^{x_i+\delta}}{\sum_{j}e^{x_j+\delta}} = \frac{e^{x_i} * e^{\delta}}{\sum_{j}e^{x_j} * e^{\delta}} = \mathrm{softmax}(\bm{x})_i
$$&lt;p>
而这里 $\bm{b}_K^T\bm{q}\in\mathbb{R}$，因此我们可以将这一项给去掉，我们得到：&lt;/p>
$$
\mathrm{Attn}(\bm{x}, Y, Y) = W_VY\mathrm{softmax}\left(Y^TW_K^T\bm{q}\right) + \bm{b}_V
$$$$
\boxed{
\begin{aligned}
\mathrm{Attn}(\bm{x}, Y, Y) &amp;= W_VY\mathrm{softmax}\left(Y^TW_K^T\bm{q}\right) + \bm{b}_V\\
&amp;= W_VY\mathrm{softmax}\left(Y^TW_K^T(W_Q\bm{x}+\bm{b}_Q)\right) + \bm{b}_V\\
\end{aligned}}
$$&lt;p>因此，我们最终的结论为： &lt;strong>key bias对attention输出没有任何贡献，query bias和key bias会影响结果。&lt;/strong>&lt;/p>
&lt;p>到这里，看了参考文献3，我本以为可以进一步简化。但实际上并不行。参考文献3关于“transformer block is equivariant&amp;quot;的结果是错的，因为在attention layer之后还有一个LayerNorm，而LayerNorm不是置换不变的，这也是LayerNorm和BatchNorm之间的区别。也就是&lt;em>如果我们在&lt;code>nn.Linear&lt;/code>后加一个BatchNorm，那么&lt;code>nn.Linear&lt;/code>的bias是无效的，反之如果是LayerNorm的话，则bias是有效的&lt;/em>.&lt;/p>
&lt;h2 id="为什么没有bias">&lt;a href="#%e4%b8%ba%e4%bb%80%e4%b9%88%e6%b2%a1%e6%9c%89bias" class="header-anchor">&lt;/a>为什么没有bias
&lt;/h2>&lt;p>实际上这个问题并没有定论。特别是加入position encoding之后，就更难探究bias对最终结果的影响了。但是，我认为一个原因就是bias其实就是某种先验知识，假设输入满足高斯分布，那么我们有&lt;/p>
$$
\mathbb{E}[W\bm{x}+b] = b
$$&lt;p>加上先验知识后，当训练数据出现distribution shift之后，模型在训练过程中可能就会不稳定(PaLM). 而后来将LayerNorm替换为RMSNorm，使用RoPE而不是其他的additive position encoding, 我认为也是避免模型学习到先验知识，从而影响其泛化性。在未来，我认为transformer里应该是没有bias的，尽管这样效果可能会差一些，但是其稳定性更好，泛化性应该也会更好。&lt;/p>
&lt;h2 id="结论">&lt;a href="#%e7%bb%93%e8%ae%ba" class="header-anchor">&lt;/a>结论
&lt;/h2>&lt;p>在本文中，我们分析了attention的性质，我们发现，在原始transformer架构中，attention对于key和value有置换不变性，对于query有置换等变性。然后，我们给出了一些猜测，也就是bias会让模型产生先验知识，而这种先验知识很可能会影响训练的稳定性和模型的泛化性。&lt;/p>
&lt;h2 id="参考文献">&lt;a href="#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae" class="header-anchor">&lt;/a>参考文献
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" target="_blank" rel="noopener"
>Attention is All you Need&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2302.08626" target="_blank" rel="noopener"
>Role of Bias Terms in Dot-Product Attention&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://openreview.net/forum?id=ByxRM0Ntvr" target="_blank" rel="noopener"
>Are Transformers universal approximators of sequence-to-sequence functions?&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="附录">&lt;a href="#%e9%99%84%e5%bd%95" class="header-anchor">&lt;/a>附录
&lt;/h2>&lt;p>下面是测试上面结论的python代码&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;span class="lnt">49
&lt;/span>&lt;span class="lnt">50
&lt;/span>&lt;span class="lnt">51
&lt;/span>&lt;span class="lnt">52
&lt;/span>&lt;span class="lnt">53
&lt;/span>&lt;span class="lnt">54
&lt;/span>&lt;span class="lnt">55
&lt;/span>&lt;span class="lnt">56
&lt;/span>&lt;span class="lnt">57
&lt;/span>&lt;span class="lnt">58
&lt;/span>&lt;span class="lnt">59
&lt;/span>&lt;span class="lnt">60
&lt;/span>&lt;span class="lnt">61
&lt;/span>&lt;span class="lnt">62
&lt;/span>&lt;span class="lnt">63
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">torch&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">torch.nn&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">nn&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">torch.nn.functional&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">F&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 设置随机种子，确保可复现性&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">manual_seed&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">42&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 输入参数&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">batch_size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">seq_len&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">16&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">embed_dim&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1024&lt;/span> &lt;span class="c1"># 嵌入维度&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">num_heads&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">32&lt;/span> &lt;span class="c1"># 多头注意力头数&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">head_dim&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">embed_dim&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="n">num_heads&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 输入张量 (batch_size, seq_len, embed_dim)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">batch_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">seq_len&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">embed_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 有 bias 的 QKV 线性层&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Attention&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">embed_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">q_bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k_bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">v_bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="fm">__init__&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embed_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">embed_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">q_bias&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">k&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embed_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">embed_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">k_bias&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">v&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embed_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">embed_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">v_bias&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">B&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">N&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">C&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reshape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">B&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">N&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_heads&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">head_dim&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transpose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">k&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reshape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">B&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">N&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_heads&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">head_dim&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transpose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">v&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">v&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reshape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">B&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">N&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_heads&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">head_dim&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transpose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attn&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">q&lt;/span> &lt;span class="o">@&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transpose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mf">1.0&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">head_dim&lt;/span>&lt;span class="o">**&lt;/span>&lt;span class="mf">0.5&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attn&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">attn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">softmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">attn&lt;/span> &lt;span class="o">@&lt;/span> &lt;span class="n">v&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transpose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reshape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">B&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">N&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">C&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">attn&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 初始化模型&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">model_no_bias&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Attention&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embed_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">q_bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k_bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">v_bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">model_with_bias&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Attention&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embed_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">q_bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k_bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">v_bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">model_with_bias&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">weight&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">data&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">model_no_bias&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">q&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">weight&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">data&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">model_with_bias&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">weight&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">data&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">model_no_bias&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">weight&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">data&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">model_with_bias&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">v&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">weight&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">data&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">model_no_bias&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">v&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">weight&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">data&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 推理&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">out_no_bias&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">attn_no_bias&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">model_no_bias&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">out_with_bias&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">attn_with_bias&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">model_with_bias&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 比较差异&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">diff_output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">abs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">out_no_bias&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">out_with_bias&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mean&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">diff_variance&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">abs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">out_no_bias&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">out_with_bias&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">var&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">diff_attn&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">abs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">attn_no_bias&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">attn_with_bias&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mean&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">Mean difference in output:&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">diff_output&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">item&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Mean difference in variance:&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">diff_variance&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">item&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Mean difference in attention weights:&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">diff_attn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">item&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Mean difference in output: 1.2734082233123445e-08&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Mean difference in variance: 1.7173628739783402e-16&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Mean difference in attention weights: 3.949708116124384e-09&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>Notes on Position encoding</title><link>https://maosong.website/p/notes-on-position-encoding/</link><pubDate>Mon, 19 May 2025 10:46:39 +0800</pubDate><guid>https://maosong.website/p/notes-on-position-encoding/</guid><description>&lt;blockquote>
&lt;p>本文前半部分参考 &lt;a class="link" href="https://huggingface.co/blog/designing-positional-encoding" target="_blank" rel="noopener"
>参考文献1&lt;/a>，推荐大家看博客原文。&lt;/p>
&lt;/blockquote>
&lt;h2 id="position-encoding总结">&lt;a href="#position-encoding%e6%80%bb%e7%bb%93" class="header-anchor">&lt;/a>Position encoding总结
&lt;/h2>&lt;p>在 &lt;a class="link" href="https://maosong.website/p/notes-on-attention-bias/" target="_blank" rel="noopener"
>上一篇blog&lt;/a> 中, 我们介绍了 Attention 的两个性质，也就是在不加 position encoding 的情况下，Attention 对于 query 是 permutation equivariant 的，对于 key 和 value 是 permutation invariant 的。&lt;/p>
&lt;p>但是“我爱你”和“你爱我”这两句话所表示的含义应该是不一样的，我们将这两句话作为 key 和 value 的时候，我们发现模型的输出是一致的，这显然是不能接受的。因此，我们就需要加入 position encoding，让模型学习到语序信息，从而明白不同的语序有不同的含义。&lt;/p>
&lt;p>下面是测试代码 （来自 &lt;a class="link" href="https://huggingface.co/blog/designing-positional-encoding" target="_blank" rel="noopener"
>参考文献1&lt;/a>）&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">torch&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">torch.nn&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">nn&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">transformers&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">AutoTokenizer&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">AutoModel&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">model_id&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;meta-llama/Llama-3.2-1B&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">tok&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">AutoTokenizer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">from_pretrained&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">model_id&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">model&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">AutoModel&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">from_pretrained&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">model_id&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">text&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;The dog chased another dog&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">tokens&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tok&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">text&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">return_tensors&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;pt&amp;#34;&lt;/span>&lt;span class="p">)[&lt;/span>&lt;span class="s2">&amp;#34;input_ids&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">embeddings&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">model&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">embed_tokens&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tokens&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">hdim&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">embeddings&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">W_q&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hdim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hdim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">W_k&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hdim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hdim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">W_v&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hdim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hdim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">mha&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">MultiheadAttention&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embed_dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">hdim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_heads&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">4&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">batch_first&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">with&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">no_grad&lt;/span>&lt;span class="p">():&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">param&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">mha&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">parameters&lt;/span>&lt;span class="p">():&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">init&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">normal_&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">param&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">std&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># Initialize weights to be non-negligible&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">output&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">mha&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">W_q&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embeddings&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">W_k&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embeddings&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">W_v&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embeddings&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">dog1_out&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">output&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">dog2_out&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">output&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;Dog output identical?: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">allclose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dog1_out&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dog2_out&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">atol&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">1e-6&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1">#True&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Position encoding 可以分为绝对位置编码 (absolute position encoding, APE)，相对位置编码 (relative position encoding, RPE) 以及可学习的位置编码。可学习位置编码主要是 BERT 类的模型在使用，其训练成本比较高，本文不做讨论。绝对位置编码是原始 transformer 里提出的编码模式，现在的大多数基于 transformer 模型使用的都是相对位置编码。&lt;/p>
&lt;p>本文中，我们先介绍位置编码应该具有的性质，然后我们分别介绍绝对位置编码和相对位置编码，我们将着重关注苏剑林老师提出来的 RoPE。&lt;/p>
&lt;h2 id="位置编码">&lt;a href="#%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81" class="header-anchor">&lt;/a>位置编码
&lt;/h2>&lt;p>在介绍位置编码之前，我们首先应该关注位置编码的性质，位置编码的目标是为输入的 token embedding 增加位置信息，那么理想的位置编码应该是怎么样的呢？&lt;/p>
&lt;p>我们这里直接引用 &lt;a class="link" href="https://huggingface.co/blog/designing-positional-encoding" target="_blank" rel="noopener"
>参考文献1&lt;/a> 中给定的性质：&lt;/p>
&lt;ol>
&lt;li>&lt;strong>性质 1&lt;/strong>: token sequence 中每个位置的位置编码都是唯一的。这个很好理解，如果不唯一的话，那么根据前面推导的性质，这两个位置的 attention 输出就完全一致了&lt;/li>
&lt;li>&lt;strong>性质 2&lt;/strong>: 线性相关性。也就是说，如果我们知道了位置 $p$ 处的位置编码，那么理想情况下，我们应该能比较简单地得到 $p+k$ 处的位置编码，理想情况下，我们应该有 $PE(p+k)=W_kPE(p)$.&lt;/li>
&lt;li>&lt;strong>性质 3&lt;/strong>: 泛化到长上下文中去。我们希望位置编码不仅在 8K 的上下文起作用，还希望位置编码能够泛化到 32K 的上下文&lt;/li>
&lt;li>&lt;strong>性质 4&lt;/strong>: 生成模式是固定的。固定的模式有助于模型更好地学习位置相关的信息&lt;/li>
&lt;li>&lt;strong>性质 5&lt;/strong>: 可以扩展到多维。我们希望位置编码可以从文本扩展到图片再到视频，也就是从 $1D$ 到 $nD$.&lt;/li>
&lt;/ol>
&lt;h2 id="绝对位置编码">&lt;a href="#%e7%bb%9d%e5%af%b9%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81" class="header-anchor">&lt;/a>绝对位置编码
&lt;/h2>&lt;p>绝对位置编码依照其名称，其思想就是为每个位置的 token 分配一个固定的位置信息，也就是对于输入的 hidden states $\bm{x}=[\bm{x}_1,\dots,\ \bm{x}_m]\in\mathbb{R}^{m\times d}$, 我们有&lt;/p>
$$
\bm{x}_i' = \bm{x}_i + p_i, i=1,\dots, m
$$&lt;p>这里，$p_i\in\mathbb{R}^d$. 我们的 attention 就变成了&lt;/p>
$$
\mathrm{Attn}(X) = \mathrm{softmax}\left(\frac{(Q+P)(K+P)^T}{\sqrt{d}}\right)V\in\mathbb{R}^{m\times d}
$$&lt;p>这里&lt;/p>
$$
P = [p_1,\dots,p_m]\in\mathbb{R}^{m\times d}， Q= W_QX\in\mathbb{R}^{m\times d}, K=W_KX, V=W_VX\in\mathbb{R}^{n\times d}
$$&lt;h3 id="整数位置编码">&lt;a href="#%e6%95%b4%e6%95%b0%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81" class="header-anchor">&lt;/a>整数位置编码
&lt;/h3>&lt;p>一个最简单的想法就是我们使用正整数来标记 token 所在的位置，也就是&lt;/p>
$$
PE(i) = [i, \dots, i]=i\mathbf{1}_{d\times 1}\in\mathbb{R}^d,\ i=1,\dots,m
$$&lt;p>可以看到，这个简单的设计满足性质 1，性质 2，性质 3，性质 4.&lt;/p>
&lt;p>但是，注意到 attention 的输入 $X$ 通常是经过 Layer Normalization 处理过后的，因此其按列符合正态分布，并且均值和方差一般较小。当我们加上整数位置编码之后，其 token 本身的信息就会被污染，也就是信噪比非常低。一个解决方法就是我们对 $PE(i)$ 进行 normalization，即&lt;/p>
$$
PE(i)' = \frac{1}{m}PE(i) = \frac{i}{m}\mathbf{1}_{d\times 1}
$$&lt;p>现在所有的位置编码的值都比较小，但是我们发现新的位置编码不满足性质 2 了，这是因为现在位置编码还和 sequence 长度有关，我们从位置 $p$ 到位置 $p+k$ 不仅取决于 $k$ 还取决于 sequence 长度 $m$&lt;/p>
&lt;h3 id="二进制位置编码">&lt;a href="#%e4%ba%8c%e8%bf%9b%e5%88%b6%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81" class="header-anchor">&lt;/a>二进制位置编码
&lt;/h3>&lt;p>既然整数位置编码的主要问题是对输入影响太大，我们能否找一个不影响输入的整数位置编码方式呢？ &lt;a class="link" href="https://huggingface.co/blog/designing-positional-encoding" target="_blank" rel="noopener"
>参考文献1&lt;/a> 提出了二进制位置编码，因为每个 token 是 $d$ 维的，因此我们可以使用 $d$ 位二进制来表示 $i$. 比如说，当 $d=3$, $m=4$ 时，我们的位置编码分别为&lt;/p>
$$
PE(0) =p_{(000)_2} = [0, 0, 0],\ PE(1) =p_{(001)_2}= [0, 0, 1],\ PE(2) =p_{(010)_2} = [0, 1, 0],\ PE(3) =p_{(011)_2} = [0, 1, 1]
$$&lt;p>现在，我们二进制位置编码满足性质 1，性质 2. 对于性质 3，由于 $d$ 位二进制的表示范围为 $[0, 2^d-1]$，因此其泛化性受到 $d$ 的影响。&lt;/p>
&lt;p>&lt;a class="link" href="https://huggingface.co/blog/designing-positional-encoding" target="_blank" rel="noopener"
>参考文献1&lt;/a> 画出了不同位置的值的变化情况。我们这里也模仿绘制出类似的曲线图&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-position-encoding/RoPE_binary_position_encoding.png"
width="1400"
height="800"
loading="lazy"
alt="Binary Position Encoding"
class="gallery-image"
data-flex-grow="175"
data-flex-basis="420px"
>&lt;/p>
&lt;p>我们发现，二进制位置编码高位，也就是 $PE(i)_{d}$ 的变化很慢，而低位，也就是 $PE(i)_{0}$ 变化很快，&lt;/p>
&lt;p>二进制位置编码解决了整数位置编码的信噪比过低和线性相关性。但是其问题是其对不同位置的 token embedding 产生的影响是不一样的。比如位置 1 和位置 2 的相同的 token embedding 之间的区别是：&lt;/p>
$$
(\bm{x}_2 + PE(2)) - (\bm{x}_1 + PE(1)) = (\bm{x}_2-\bm{x}_1)+ [0, 1, -1]
$$&lt;p>一般来说, $\bm{x}_2-\bm{x}_1$ 比较小，因此使用二进制位置编码的问题是输入位置的微小变化（增加一个 token 或减少一个 token）都会对最终结果产生巨大影响。因此，我们需要想办法解决这个问题。&lt;/p>
&lt;h3 id="sinusoidal">&lt;a href="#sinusoidal" class="header-anchor">&lt;/a>Sinusoidal
&lt;/h3>&lt;p>前面提到二进制位置编码的问题是相邻 token 之间变化太大，不够光滑。因此我们想要增加一个光滑性质，也就是说我们希望：&lt;/p>
&lt;ol>
&lt;li>位置编码值在 $[-1, 1]$ 之间，防止对 token embedding 产生影响&lt;/li>
&lt;li>相邻 token 的位置编码尽可能相近，即 $|PE(k+p)-PE(p)| \leq \delta |k|$, 其中 $\delta>0$ 是一个比较小的数。&lt;/li>
&lt;li>与二进制一样，高位的变化比较慢，低位的变化比较快&lt;/li>
&lt;/ol>
&lt;p>一个想法就是利用三角函数 $\sin$ 或者 $\cos$，三角函数满足前两个性质， 对于第三个性质，我们可以通过控制频率来满足。这样我们得到的位置编码就具有如下形式：&lt;/p>
$$
PE(p, i) = \sin\left(\frac{p}{\theta^{i/d}}\right)
$$&lt;p>其中 $\theta$ 是我们的超参数。&lt;/p>
&lt;p>我们现在来推导一下上面位置编码的线性相关性：&lt;/p>
$$
PE(p+k) = \sin\left(\frac{p+k}{\theta^{i/d}}\right)=PE(p)\cos\left(\frac{k}{\theta^{i/d}}\right) + \cos\left(\frac{p}{\theta^{i/d}}\right)\sin\left(\frac{k}{\theta^{i/d}}\right)
$$&lt;p>我们发现，$\sin$ 位置编码不满足线性相关性。但是出现的 $\cos$ 给了我们启发，也就是我们可以同时使用 $\sin$ 和 $\cos$ 来完成位置编码，这也是原始 transformer 里提出来的 Sinusoidal 位置编码，其形式为：&lt;/p>
$$
\begin{aligned}
PE(p, 2i) &amp;= \sin\left(\frac{p}{\theta^{2i/d}}\right)\\
PE(p, 2i+1) &amp;= \cos\left(\frac{p}{\theta^{2i/d}}\right)
\end{aligned}
$$&lt;p>现在，记 $\omega_i=1/\theta^{2i/d}$, 我们再推导一下线性相关性，就得到：&lt;/p>
$$
\begin{aligned}
\begin{bmatrix}
PE(p+k, 2i)\\
PE(p+k, 2i+1)\\
\end{bmatrix}&amp;=\begin{bmatrix}
\sin \omega_i(p+k)\\
\cos \omega_i(p+k)
\end{bmatrix}\\
&amp;=\begin{bmatrix}
\sin \omega_i(\omega_ip)\cos(\omega_ik)+\cos w_i(\omega_ip)\sin(\omega_ik)\\
\cos \omega_i(\omega_ip)\cos(\omega_ik)-\sin w_i(\omega_ip)\sin(\omega_ik)
\end{bmatrix}\\
&amp;= \begin{bmatrix}
\cos(\omega_ik)&amp; \sin(\omega_ik)\\
-\sin(\omega_ik)&amp; \cos(\omega_ik)
\end{bmatrix}\begin{bmatrix}
PE(p, 2i)\\
PE(p, 2i+1)\\
\end{bmatrix}
\end{aligned}
$$&lt;p>也就是说，Sinusoidal 位置编码满足线性相关性。对于 Sinusoidal 位置编码我们也可以进行可视化：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-position-encoding/RoPE_sinusoidal_position_encoding.png"
width="1400"
height="800"
loading="lazy"
alt="Sinusoidal Position Encoding"
class="gallery-image"
data-flex-grow="175"
data-flex-basis="420px"
>&lt;/p>
&lt;h2 id="相对位置编码">&lt;a href="#%e7%9b%b8%e5%af%b9%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81" class="header-anchor">&lt;/a>相对位置编码
&lt;/h2>&lt;p>前面介绍了绝对位置编码，每个位置的位置编码是固定的。但是绝对位置编码的问题是，模型比较难以学习相对位置关系。&lt;/p>
&lt;p>举个例子，我们提到上下文时，通常会使用“上一节”，“上一章”这些表示相对位置关系的词。&lt;/p>
&lt;p>因此，我们希望让模型学习相对位置关系而不是绝对位置关系，因为相对关系更符合我们的认知。&lt;/p>
&lt;h3 id="rope">&lt;a href="#rope" class="header-anchor">&lt;/a>RoPE
&lt;/h3>&lt;p>RoPE 由苏剑林老师提出，最早应用于 LLaMA 架构（没有确认），后续被大多数模型所采用。&lt;/p>
&lt;p>之前的 PE 大多数关注于加性位置编码，也就是&lt;strong>假设位置编码的形式为 $\bm{x}+\bm{p}$&lt;/strong>, 基于这种假设，已有的工作基本都集中于优化下面的 Q 和 K 的内积&lt;/p>
$$
\langle f_q(\bm{x}_q, m), f_k(\bm{x}_k, n) \rangle
$$&lt;p>这里 $f_q(\bm{x}_q, m)=W_q(\bm{x}_q+\bm{p}_m)$, $f_k(\bm{x}_k, n)=W_k(\bm{x}_k+ \bm{p}_n)$.&lt;/p>
&lt;p>而 RoPE 里面，作者使用了一个不同的假设： &lt;strong>假设内积应该仅包含两者的相对信息&lt;/strong>，也就是&lt;/p>
$$
\langle f_q(\bm{x}_q, m), f_q(\bm{x}_k, n)\rangle := g(\bm{x}_q,\bm{x}_k, m-n)
$$&lt;p>这里的 $f$ 和 $g$ 都是未知函数。我们的目标就是从这个公式中推导出一个合适的位置编码出来。&lt;/p>
&lt;p>不失一般性，我们可以假设&lt;/p>
$$
f_q(\bm{x}_m,0) = \bm{x}_q,\quad f_q(\bm{x}_n, 0) = \bm{x}_k
$$&lt;p>这个假设代表初始条件下，我们不对输入做任何改变，也就是不增加位置信息。&lt;/p>
&lt;h2 id="2d-推导">&lt;a href="#2d-%e6%8e%a8%e5%af%bc" class="header-anchor">&lt;/a>2D 推导
&lt;/h2>&lt;p>与 RoPE 一样，我们直接使用复平面来进行推导。&lt;/p>
&lt;p>我们假设 $d=2$, 注意到二维平面上的每个点都可以表示为如下形式&lt;/p>
$$
\bm{z} = (x,y) = re^{i\theta}
$$&lt;p>其中 ($\mathrm{atan2}$ 定义参考 &lt;a class="link" href="https://en.wikipedia.org/wiki/Polar_coordinate_system" target="_blank" rel="noopener"
>维基百科&lt;/a>)&lt;/p>
$$
r = \|\bm{z}\|_2 = \sqrt{x^2+y^2}\in\mathbb{R},\quad \theta = \mathrm{atan2}(y, x)\in\mathbb{R},
$$&lt;p>现在，对于三个向量 $f_q(\bm{x}_q, m)$, $f_q(\bm{x}_k, n)$, $g(\bm{x}_q,\bm{x}_k, m-n)$ 我们可以写出其极坐标形式：&lt;/p>
$$
\begin{aligned}
f_q(\bm{x}_q,m) &amp;:= r_q(\bm{x}_q,m)e^{i\theta_q(\bm{x}_q,m)}\\
f_k(\bm{x}_k, n) &amp;:= r_k(\bm{x}_k, n)e^{i\theta_k(\bm{x}_k, n)}\\
g(\bm{x}_q,\bm{x}_k, m-n) &amp;:= r_g(\bm{x}_q,\bm{x}_k, m-n)e^{i\theta_g(\bm{x}_q,\bm{x}_k, m-n)}
\end{aligned}
$$&lt;p>我们计算内积并比较同类项得到：&lt;/p>
$$
\begin{aligned}
r_g(\bm{x}_q,\bm{x}_k, m-n) &amp;:= r_q(\bm{x}_q,m)r_k(\bm{x}_k, n)\\
\theta_g(\bm{x}_q,\bm{x}_k, m-n) &amp;:= \theta_q(\bm{x}_q,m)-\theta_k(\bm{x}_k, n)
\end{aligned}\tag{3}
$$&lt;p>我们接下来分别推导 $r_g(\bm{x}_q,\bm{x}_k, m-n)$ 和 $\theta_g(\bm{x}_q,\bm{x}_k, m-n)$ 的形式&lt;/p>
&lt;h3 id="r_gbmx_qbmx_k-m-n">&lt;a href="#r_gbmx_qbmx_k-m-n" class="header-anchor">&lt;/a>$r_g(\bm{x}_q,\bm{x}_k, m-n)$
&lt;/h3>&lt;p>我们令 $m=n=0$ 可以得到初始条件&lt;/p>
$$
r_g(\bm{x}_q,\bm{x}_k, 0) = r_q(\bm{x}_q,0)r_k(\bm{x}_k, 0)=\|\bm{q}\|_2\|\bm{k}\|_2
$$&lt;p>我们再令 $n=0$,得到&lt;/p>
$$
r_g(\bm{x}_q,\bm{x}_k, m) = r_q(\bm{x}_q,m)r_k(\bm{x}_k, 0)=r_q(\bm{x}_q,m)\|\bm{k}\|_2=\frac{r_g(\bm{x}_q,\bm{x}_k, m-n)}{r_k(\bm{x}_k, n)}\|\bm{k}\|_2
$$&lt;p>这里最后一个等式带入了原始等式 (3)，注意到左侧与 $n$ 无关，因此右侧我们选取 $n=1$, 得到&lt;/p>
$$
r_g(\bm{x}_q,\bm{x}_k, m) = \frac{r_g(\bm{x}_q,\bm{x}_k, m-1)}{r_k(\bm{x}_k, 1)}\|\bm{k}\|_2 =\cdots= r_g(\bm{x}_q,\bm{x}_k, 0)\left(\frac{\|\bm{k}\|_2 }{r_k(\bm{x}_k, 1)}\right)^{m+1}
$$&lt;p>令 $m=0$ 我们有&lt;/p>
$$
r_k(\bm{x}_k, 1) = \|\bm{k}\|_2.
$$&lt;p>因此我们最终的表达式为：&lt;/p>
$$
r_g(\bm{x}_q,\bm{x}_k, m) = r_g(\bm{x}_q,\bm{x}_k, 0) = \|\bm{q}\|_2\|\bm{k}\|_2.
$$&lt;p>并且，通过分别设置 $m=0$ 以及 $n=0$ 我们还可以得到&lt;/p>
$$
r_q(\bm{x}_q,m) = \|\bm{q}\|_2,\quad r_k(\bm{x}_k, n) = \|\bm{k}\|_2
$$&lt;h3 id="theta_gbmx_qbmx_k-m-n">&lt;a href="#theta_gbmx_qbmx_k-m-n" class="header-anchor">&lt;/a>$\theta_g(\bm{x}_q,\bm{x}_k, m-n)$
&lt;/h3>&lt;p>令 $m=n=0$, 我们得到初始条件&lt;/p>
$$
\theta_g(\bm{x}_q,\bm{x}_k, 0) = \theta_q(\bm{x}_q,0)-\theta_k(\bm{x}_k, 0)=\theta_q-\theta_k
$$&lt;p>令 $n=1$, 我们有&lt;/p>
$$
\begin{aligned}
\theta_g(\bm{x}_q,\bm{x}_k, m-1) &amp;= \theta_q(\bm{x}_q,m)-\theta_k(\bm{x}_k, 1)\\
&amp;=\theta_g(\bm{x}_q,\bm{x}_k, m-n) + \theta_k(\bm{x}_k, n)-\theta_k(\bm{x}_k, 1)
\end{aligned}
$$&lt;p>这里我们带入了公式 (3)，注意到公式左边与 $n$ 无关，因此在公式右侧我们令 $n=0$, 得到&lt;/p>
$$
\theta_g(\bm{x}_q,\bm{x}_k, m-1) = \theta_g(\bm{x}_q,\bm{x}_k, m)+ \theta_k(\bm{x}_k, 0)-\theta_k(\bm{x}_k, 1)
$$&lt;p>分别令 $m=1,2,\dots$ 并相加这些等式，我们得到&lt;/p>
$$
\theta_g(\bm{x}_q,\bm{x}_k, 0) = \theta_g(\bm{x}_q,\bm{x}_k, m) + m(\theta_k(\bm{x}_k, 0)-\theta_k(\bm{x}_k, 1))
$$&lt;p>即&lt;/p>
$$
\theta_g(\bm{x}_q,\bm{x}_k, m) = m(\theta_k(\bm{x}_k, 1)-\theta_k(\bm{x}_k, 0))+(\theta_q-\theta_k)\tag{4}
$$&lt;p>注意到&lt;/p>
$$
\theta_g(\bm{x}_q,\bm{x}_k, m) = \theta_q(\bm{x}_q,m)-\theta_k(\bm{x}_k, 0)=\theta_q(\bm{x}_q,m)-\theta_k
$$&lt;p>带入上式我们就得到&lt;/p>
$$
\theta_q(\bm{x}_q,m) = m(\theta_k(\bm{x}_k, 1)-\theta_k(\bm{x}_k, 0))+\theta_q
$$&lt;p>在 (4) 式中再令 $m=m-n$，并带入 $\theta_q(\bm{x}_q,m)$ 就有&lt;/p>
$$
\theta_k(\bm{x}_k,n) = n(\theta_k(\bm{x}_k, 1)-\theta_k(\bm{x}_k, 0))+\theta_k
$$&lt;h3 id="汇总">&lt;a href="#%e6%b1%87%e6%80%bb" class="header-anchor">&lt;/a>汇总
&lt;/h3>&lt;p>最后，我们将以上结果放在一起，就得到&lt;/p>
$$
f_q(\bm{x}_q,m) = \bm{q}e^{im\theta}, f_v(\bm{x}_k,m) = \bm{k}e^{in\theta}
$$&lt;p>这里 $\theta=\theta_k(\bm{x}_k, 1)-\theta_k(\bm{x}_k, 0)$ 是一个超参数，用于控制频率。&lt;/p>
&lt;p>我们记&lt;/p>
$$
R_{\theta,m} = \begin{bmatrix}
\cos m\theta &amp; -\sin m\theta\\
\sin m\theta &amp; \cos m\theta
\end{bmatrix}
$$&lt;p>则我们有：&lt;/p>
$$
f_q(\bm{x}_q,m) = R_{\theta,m}\bm{q}, f_v(\bm{x}_k,m) = R_{\theta,n}\bm{k}.
$$&lt;p>并且&lt;/p>
$$
\langle f_q(\bm{x}_q,m), f_v(\bm{x}_k,m)\rangle = \bm{q}^TR_{\theta,m-n}\bm{k} \tag{5}
$$&lt;h3 id="多维扩展">&lt;a href="#%e5%a4%9a%e7%bb%b4%e6%89%a9%e5%b1%95" class="header-anchor">&lt;/a>多维扩展
&lt;/h3>&lt;p>上面是 2D 的情况，对于多维情况，苏剑林老师通过将两个元素组对，然后分别进行处理，得到了多维的情形：&lt;/p>
$$
R_{\theta,m}^d = \begin{bmatrix}
R_{\theta_1,m} &amp; &amp; &amp;&amp; &amp; \\
&amp; &amp; R_{\theta_2,m} &amp; &amp; &amp; \\
&amp;&amp;&amp;&amp; \ddots &amp; \\
&amp;&amp;&amp;&amp; &amp; R_{\theta_{d/2},m}
\end{bmatrix}\in\mathbb{R}^{d\times d}
$$&lt;p>我们可以验证公式 (5) 仍然是成立的。&lt;/p>
&lt;h2 id="rope-的远程衰减性质">&lt;a href="#rope-%e7%9a%84%e8%bf%9c%e7%a8%8b%e8%a1%b0%e5%87%8f%e6%80%a7%e8%b4%a8" class="header-anchor">&lt;/a>RoPE 的远程衰减性质
&lt;/h2>&lt;p>我们接下来看一下结果与相对距离 $m-n$ 之间的关系， 注意到&lt;/p>
$$
\langle f_q(\bm{x}_q,m), f_v(\bm{x}_k,m)\rangle = \bm{q}^TR_{\theta,m-n}\bm{k} = \sum_{i=1}^{d/2} \bm{q}_i^TR_{\theta, m-n}\bm{k}_i
$$&lt;p>这里 $\bm{q}_i=[q_{2i},q_{2i+1}]^T$, $\bm{k}_i=[k_{2i},k_{2i+1}]^T$ 分别是对应的 pair，我们考虑其中一个分量，不妨假设 $\|\bm{q}\|_2=\|\bm{k}\|_2=1$, 我们有&lt;/p>
$$
\begin{aligned}
\bm{q}_i^TR_{\theta, m-n}\bm{k}_i
&amp;\leq \bm{q}_i^TR_{\theta, m-n}\bm{q}_i\\
&amp;= \bm{q}_i^T\left(\frac{R_{\theta, m-n}+R_{\theta, m-n}^T}{2}\right)\bm{q}_i\\
&amp;\leq \lambda_{\max}\left(\frac{R_{\theta, m-n}+R_{\theta, m-n}^T}{2}\right) \\
&amp;= \cos (m-n)\theta_i
\end{aligned}
$$&lt;p>其中，第一个不等式是因为 两个向量相等时其内积最大，第二个不等式是由于二次型最大值为矩阵的特征值。&lt;/p>
&lt;p>这样我们就有&lt;/p>
$$
\langle f_q(\bm{x}_q,m), f_v(\bm{x}_k,m)\rangle \leq \sum_{i=1}^{d/2}\cos (m-n)\theta_i.
$$&lt;p>我们可以简单画出对应的曲线：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-position-encoding/RoPE_long_term_decay.png"
width="1200"
height="600"
loading="lazy"
alt="Long term decay of RoPE"
class="gallery-image"
data-flex-grow="200"
data-flex-basis="480px"
>&lt;/p>
&lt;p>这里针对不同的配置，结果也会略微不同，具体分析可以参加知乎回答 &lt;a class="link" href="https://zhuanlan.zhihu.com/p/705492804" target="_blank" rel="noopener"
>RoPE的远距离衰减&lt;/a>&lt;/p>
&lt;h2 id="rope-代码实现与理解">&lt;a href="#rope-%e4%bb%a3%e7%a0%81%e5%ae%9e%e7%8e%b0%e4%b8%8e%e7%90%86%e8%a7%a3" class="header-anchor">&lt;/a>RoPE 代码实现与理解
&lt;/h2>&lt;h3 id="naive-实现">&lt;a href="#naive-%e5%ae%9e%e7%8e%b0" class="header-anchor">&lt;/a>Naive 实现
&lt;/h3>&lt;p>我们接下来看一下如何实现 RoPE&lt;/p>
$$
\Theta_m\bm{x}=\begin{bmatrix}
\cos m\theta_0 &amp; -\sin m\theta_0 &amp; &amp;&amp;\cdots &amp;\cdots &amp;\cdots \\
\sin m\theta_0 &amp; \cos m\theta_0 &amp; &amp;&amp;\cdots &amp;\cdots &amp;\cdots\\
&amp; &amp; \cos m\theta_1 &amp; -\sin m\theta_1 &amp; \cdots &amp;\cdots &amp;\cdots\\
&amp; &amp; \sin m\theta_1 &amp; \cos m\theta_1 &amp; \cdots &amp;\cdots &amp;\cdots \\
&amp;&amp;&amp;&amp; \ddots &amp;\vdots &amp;\vdots\\
&amp;&amp;&amp;&amp; &amp; \cos m\theta_{d/2} &amp; -\sin m\theta_{d/2}\\
&amp;&amp;&amp;&amp; &amp; \sin m\theta_{d/2} &amp; \cos m\theta_{d/2}
\end{bmatrix}\begin{bmatrix}
x_1\\
x_2\\
\vdots \\
x_d
\end{bmatrix}
$$&lt;p>在实现的时候，我们一般根据 $\sin$ 和 $\cos$ 进行分组，也就是&lt;/p>
$$
\Theta_m\bm{x}=\begin{bmatrix}
\cos m\theta_0\\
\cos m\theta_0\\
\vdots\\
\cos m\theta_{d/2}\\
\cos m\theta_{d/2}\\
\end{bmatrix}\odot \begin{bmatrix}
x1\\
x2\\
\vdots\\
x_{d-1}\\
x_d\\
\end{bmatrix} + \begin{bmatrix}
\sin m\theta_0\\
\sin m\theta_0\\
\vdots\\
\sin m\theta_{d/2}\\
\sin m\theta_{d/2}\\
\end{bmatrix}\odot
\begin{bmatrix}
-\ x_2\\
x_1\\
\vdots\\
-x_d\\
x_{d-1}\\
\end{bmatrix}
$$&lt;p>我们通常按照奇偶 index 来分别计算，然后通过重排序来得到最终的结果，实现代码如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">apply_rotary_pos_emb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sin&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">cos&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x_even&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">...&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">::&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="c1"># (seq_len, d_k_half)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x_odd&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">...&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">::&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="c1"># (seq_len, d_k_half)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">odds&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">cos&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">x_even&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">sin&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">x_odd&lt;/span> &lt;span class="c1"># (...,seq_len, d_k_half)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">evens&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">sin&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">x_even&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">cos&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">x_odd&lt;/span> &lt;span class="c1"># (...,seq_len, d_k_half)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">stacked&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">stack&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">odds&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">evens&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># (...,seq_len, 2, d_k_half)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">stacked_trans&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">rearrange&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">stacked&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;... seq_len double d_k_half -&amp;gt; ... seq_len d_k_half double&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span> &lt;span class="c1"># (...,seq_len, d_k_half, 2)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">out&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">rearrange&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">stacked_trans&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;... seq_len d_k_half double -&amp;gt; ... seq_len (d_k_half double)&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span> &lt;span class="c1"># (..., seq_len, d_k)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">out&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="llama-实现">&lt;a href="#llama-%e5%ae%9e%e7%8e%b0" class="header-anchor">&lt;/a>LLaMA 实现
&lt;/h2>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">precompute_freqs_cis&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">end&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">theta&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">float&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">10000.0&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">freqs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">1.0&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">theta&lt;/span> &lt;span class="o">**&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)[:&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)]&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">t&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">device&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">freqs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">device&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># type: ignore&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">freqs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">outer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">freqs&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="c1"># type: ignore&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">freqs_cis&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">polar&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ones_like&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">freqs&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">freqs&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># complex64&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">freqs_cis&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">reshape_for_broadcast&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">freqs_cis&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ndim&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ndim&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">assert&lt;/span> &lt;span class="mi">0&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">ndim&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">assert&lt;/span> &lt;span class="n">freqs_cis&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">shape&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">d&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="ow">or&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">ndim&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">enumerate&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">)]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">freqs_cis&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">apply_rotary_emb&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">xq&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">xk&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">freqs_cis&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">Tuple&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">xq_&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view_as_complex&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">xq&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reshape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">xq&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[:&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">xk_&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view_as_complex&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">xk&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reshape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">xk&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[:&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">freqs_cis&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">reshape_for_broadcast&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">freqs_cis&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">xq_&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">xq_out&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view_as_real&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">xq_&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">freqs_cis&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">flatten&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">xk_out&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view_as_real&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">xk_&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">freqs_cis&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">flatten&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">xq_out&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">type_as&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">xq&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">xk_out&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">type_as&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">xk&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>在 LLaMA 中，我们首先还是计算 $\theta_i$, 然后在计算的过程中，我们将 $(x_i,x_{i+1})$ 视作一个复数，然后 乘以 $\exp(im\theta)$, 最后再取实部得到最终的结果&lt;/p>
&lt;h2 id="通用实现">&lt;a href="#%e9%80%9a%e7%94%a8%e5%ae%9e%e7%8e%b0" class="header-anchor">&lt;/a>通用实现
&lt;/h2>&lt;p>实际上，naive 版本的实现与现在大语言模型所采用的实现并不一致，我们先看一下现有的大语言模型的 RoPE 实现，这里我们将 &lt;a class="link" href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py" target="_blank" rel="noopener"
>LLaMA的transformer代码&lt;/a> 放在下面，&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">rotate_half&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;Rotates half the hidden dims of the input.&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">...&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">...&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="p">:]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cat&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">x2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x1&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">apply_rotary_pos_emb&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">q&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">cos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sin&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">position_ids&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">unsqueeze_dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">cos&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">cos&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">unsqueeze&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">unsqueeze_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sin&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">sin&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">unsqueeze&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">unsqueeze_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q_embed&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">q&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">cos&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">rotate_half&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">q&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">sin&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">k_embed&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">k&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">cos&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">rotate_half&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">sin&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">q_embed&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k_embed&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">LlamaRotaryEmbedding&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">max_position_embeddings&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">2048&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">base&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">10000&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">device&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="fm">__init__&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dim&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">dim&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">max_position_embeddings&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">max_position_embeddings&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">base&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">base&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">inv_freq&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">1.0&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">base&lt;/span> &lt;span class="o">**&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">device&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">register_buffer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;inv_freq&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">inv_freq&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">max_seq_len_cached&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">seq_len&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">t&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">max_seq_len_cached&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">device&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">device&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">inv_freq&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dtype&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">freqs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">einsum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;i,j-&amp;gt;ij&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">t&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">inv_freq&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Different from paper, but it uses a different permutation in order to obtain the same calculation&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">emb&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cat&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">freqs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">freqs&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">register_buffer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;cos_cached&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">emb&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cos&lt;/span>&lt;span class="p">()[&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="p">:]&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dtype&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">persistent&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">register_buffer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;sin_cached&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">emb&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sin&lt;/span>&lt;span class="p">()[&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="p">:]&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dtype&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">persistent&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">seq_len&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># x: [bs, num_attention_heads, seq_len, head_size]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">seq_len&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">max_seq_len_cached&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_set_cos_sin_cache&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">seq_len&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">seq_len&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">device&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">device&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dtype&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cos_cached&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="p">:&lt;/span>&lt;span class="n">seq_len&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">...&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dtype&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sin_cached&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="p">:,&lt;/span> &lt;span class="p">:&lt;/span>&lt;span class="n">seq_len&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">...&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dtype&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>我们将上述代码翻译成公式，现在我们的 $\Theta$ 变成了 (对应 &lt;code>emb = torch.cat((freqs, freqs), dim=-1)&lt;/code>)&lt;/p>
$$
\Theta = [\theta_0,\dots,\theta_{d/2},\theta_0,\dots,\theta_{d/2}]^T
$$&lt;p>实际上 $\sin$ 部分对应的向量现在变成了&lt;/p>
$$
[-x_{d/2+1},
-x_{d/2+2},
\dots,
-x_{d},
x_1,
\dots,
x_{d/2}]^T
$$&lt;p>我们带回到原始公式，可以得到对应的 RoPE 操作变成了&lt;/p>
$$
R_{\theta,m}^d=\begin{bmatrix}
\cos m\theta_0 &amp; &amp; &amp; -\sin m\theta_0 &amp; \cdots &amp;\cdots &amp;\cdots \\
&amp; &amp; \cos m\theta_1 &amp; &amp;-\sin m\theta_1 &amp; \cdots &amp;\cdots \\
&amp; &amp; &amp; \cos m\theta_2 &amp; &amp;-\sin m\theta_2 &amp; \cdots \\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp; \vdots &amp;\vdots &amp;\vdots\\
&amp; &amp; &amp; &amp;\cos m\theta_{d/2 - 1} &amp; &amp; -\sin m\theta_{d/2 - 1} \\
\sin m\theta_0 &amp;&amp; &amp; \cos m\theta_0 &amp;&amp;\cdots &amp;\cdots \\
&amp; &amp; \sin m\theta_1 &amp; &amp;\cos m\theta_1 &amp; \cdots &amp;\cdots \\
&amp; &amp; &amp; \sin m\theta_2 &amp; &amp;\cos m\theta_2 &amp; \cdots \\
\vdots&amp;\vdots&amp;\vdots&amp;\vdots&amp; \vdots &amp;\vdots &amp;\vdots\\
&amp;&amp;&amp;&amp; \sin m\theta_{d/2 - 1}&amp; &amp; \cos m\theta_{d/2 - 1}
\end{bmatrix}
$$&lt;p>这列每一行的 $\cos$ 和 $\sin$ 都相差了 $d/2$ 列.&lt;/p>
&lt;p>因此，这里的区别在于，原始 RoPE 计算的 pair 为 $(x_{i}, x_{i+1})$, 而 LLaMA 里的 RoPE 计算的 pair 为 $(x_{i}, x_{i+d/2})$. transformers library 使用这种方式，可以减少计算量，提高整体的计算效率。&lt;/p>
&lt;p>为了适应使用 LLaMA 中实现的 RoPE 的，Huggingface 对权重进行了转换，使得基于原始 RoPE 实现的模型也可以获得加速.&lt;/p>
&lt;p>假设 $d=8$，原始 RoPE 的 pair 为 &lt;code>[(q_0, q_1), (q_2, q_3), (q_4, q_5), (q_6, q_7)]&lt;/code>, 新的 pair 为 &lt;code>[(q_0, q_4), (q_1, q_5), (q_2, q_6), (q_3, q_7)]&lt;/code>. 我们希望对 index 进行 remap，我们发现一个满足条件的 permutation 为 &lt;code>[0, 2, 4, 6, 1, 3, 5, 7]&lt;/code>, 也就是 &lt;code>q_0-&amp;gt;q_0&lt;/code>, &lt;code>q_2-&amp;gt;q_1&lt;/code>, &amp;hellip;, &lt;code>q_7-&amp;gt;q_7&lt;/code>.&lt;/p>
&lt;p>但是，如果我们在推理时这样做，就会降低整体速度，因此 Huggingface 的做法是改变 $W_Q$ 和 $W_K$ 的权重，具体来说，就是 $\Pi q=(\Pi W_Q)x$， 左边是在线转换，右侧离线转换。转换好 $W_Q$ 之后，正常计算就可以了。&lt;a class="link" href="https://github.com/huggingface/transformers/blob/e42587f596181396e1c4b63660abf0c736b10dae/src/transformers/models/llama/convert_llama_weights_to_hf.py" target="_blank" rel="noopener"
>具体代码&lt;/a> 为&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># permute for sliced rotary&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">permute&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">w&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n_heads&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">n_heads&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim1&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim2&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">w&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n_heads&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim1&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="n">n_heads&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim2&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transpose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reshape&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">state_dict&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;model.layers.&lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">layer_i&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">.self_attn.q_proj.weight&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">permute&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">loaded&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;layers.&lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">layer_i&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">.attention.wq.weight&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;model.layers.&lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">layer_i&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">.self_attn.k_proj.weight&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">permute&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">loaded&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;layers.&lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">layer_i&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">.attention.wk.weight&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">...&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="结论">&lt;a href="#%e7%bb%93%e8%ae%ba" class="header-anchor">&lt;/a>结论
&lt;/h2>&lt;p>本文中，我们回顾了位置编码，包括绝对位置编码和相对位置编码，我们着重介绍了 RoPE 的原理，推导以及代码实现。&lt;/p>
&lt;h2 id="参考文献">&lt;a href="#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae" class="header-anchor">&lt;/a>参考文献
&lt;/h2>&lt;ol>
&lt;li>&lt;a class="link" href="https://huggingface.co/blog/designing-positional-encoding" target="_blank" rel="noopener"
>You could have designed state of the art positional encoding&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://discuss.huggingface.co/t/is-llama-rotary-embedding-implementation-correct/44509/2" target="_blank" rel="noopener"
>Is LLaMA rotary embedding implementation correct?&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/huggingface/transformers/issues/25199" target="_blank" rel="noopener"
>[LLaMA] Rotary positional embedding differs with official implementation&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://kexue.fm/archives/8130/comment-page-6#comments" target="_blank" rel="noopener"
>RoPE blog&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2104.09864" target="_blank" rel="noopener"
>RoFormer&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://zhuanlan.zhihu.com/p/1894384438206505105" target="_blank" rel="noopener"
>位置编码之路&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://zhuanlan.zhihu.com/p/705492804" target="_blank" rel="noopener"
>RoPE的远距离衰减&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>Notes on Qwen3</title><link>https://maosong.website/p/notes-on-qwen3/</link><pubDate>Thu, 15 May 2025 14:48:11 +0800</pubDate><guid>https://maosong.website/p/notes-on-qwen3/</guid><description>&lt;p>Qwen 在 2025 年 5 月发布了 Qwen3 系列大语言模型，Qwen3 包括 6 个 dense 模型和 2 个 MoE 模型，主要亮点为多语种能力，自适应快慢思考能力以及支持用户设置 thinking budget.&lt;/p>
&lt;p>Qwen3 包括 6 个 dense 模型和 2 个 MoE 模型，其旗舰模型是一个 235B 的 MoE 模型，激活参数为 22B. Qwen3 系列的主要亮点如下:&lt;/p>
&lt;ol>
&lt;li>快慢思考融合，模型原生支持在 reasoning/non-reasoning 模式之间切换&lt;/li>
&lt;li>Reasoning budget, 用户可以指定思考需要的 budget，来平衡 latency 和 performance&lt;/li>
&lt;li>Distillation, 使用蒸馏的方法训练小模型，大幅度提高模型的表现&lt;/li>
&lt;li>多语种支持，相比于 Qwen2.5，Qwen3 支持 119 中语言和方言&lt;/li>
&lt;/ol>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="model">&lt;a href="#model" class="header-anchor">&lt;/a>Model
&lt;/h3>&lt;p>Qwen3 的 dense 模型的架构与 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5/" target="_blank" rel="noopener"
>Qwen2.5&lt;/a> 基本一致，包括使用 &lt;a class="link" href="https://maosong.website/p/notes-on-gqa/" target="_blank" rel="noopener"
>GQA&lt;/a> , SwiGLU, RoPE, RMSNorm 和 pre-normalization. Qwen3 进一步移除了 QKV bias, 然 后加入了 &lt;a class="link" href="https://maosong.website/p/notes-on-qk-norm/" target="_blank" rel="noopener"
>QK-Norm&lt;/a> 来提高训练的稳定性。&lt;/p>
&lt;p>Qwen3 的 MoE 架构使用了 128 个专家，激活专家个数为 8 个。与 Qwen2.5-MoE 不同，Qwen3 里没有使用 shard experts。并且，Qwen3 加入了 global-batch load balancing loss,来提高 expert 的特化程度。&lt;/p>
&lt;p>在 tokenizer 方面，Qwen 系列的 tokenizer 一直都是一样的，这也是 Qwen 系列领先的一点。&lt;/p>
&lt;p>模型的具体参数如下两张表所示。&lt;/p>
&lt;p>&lt;strong>MoE 架构&lt;/strong>：上下文长度为 128K，128 个专家，每个 token 由 8 个专家负责处理&lt;/p>
&lt;ul>
&lt;li>Qwen3-235B-A22B, 总参数 235B，激活参数 22B&lt;/li>
&lt;li>Qwen3-30B-A3B, 总参数 30B，激活参数 3B&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Models&lt;/th>
&lt;th>Layers&lt;/th>
&lt;th>Heads (Q / KV)&lt;/th>
&lt;th># Experts (Total / Activated)&lt;/th>
&lt;th>Context Length&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Qwen3-30B-A3B&lt;/td>
&lt;td>48&lt;/td>
&lt;td>32 / 4&lt;/td>
&lt;td>128 / 8&lt;/td>
&lt;td>128K&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen3-235B-A22B&lt;/td>
&lt;td>94&lt;/td>
&lt;td>64 / 4&lt;/td>
&lt;td>128 / 8&lt;/td>
&lt;td>128K&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>dense 架构&lt;/strong>: Qwen3-32B, Qwen3-14B, Qwen3-8B, Qwen3-4B, Qwen3-1.7B, and Qwen3-0.6B&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Models&lt;/th>
&lt;th>Layers&lt;/th>
&lt;th>Heads (Q / KV)&lt;/th>
&lt;th>Tie Embedding&lt;/th>
&lt;th>Context Length&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Qwen3-0.6B&lt;/td>
&lt;td>28&lt;/td>
&lt;td>16 / 8&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>32K&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen3-1.7B&lt;/td>
&lt;td>28&lt;/td>
&lt;td>16 / 8&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>32K&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen3-4B&lt;/td>
&lt;td>36&lt;/td>
&lt;td>32 / 8&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>32K&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen3-8B&lt;/td>
&lt;td>36&lt;/td>
&lt;td>32 / 8&lt;/td>
&lt;td>No&lt;/td>
&lt;td>128K&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen3-14B&lt;/td>
&lt;td>40&lt;/td>
&lt;td>40 / 8&lt;/td>
&lt;td>No&lt;/td>
&lt;td>128K&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen3-32B&lt;/td>
&lt;td>64&lt;/td>
&lt;td>64 / 8&lt;/td>
&lt;td>No&lt;/td>
&lt;td>128K&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h3>&lt;p>预训练数据一共包括 &lt;strong>36T token&lt;/strong>，覆盖了 119 种语言。数据包括 coding, STEM, reasoning, books, multilingual texts 以及合成数据。&lt;/p>
&lt;p>为了扩展训练数据，作者微调了 Qwen2.5-VL 来从 PDF 文档中提取文字，然后使用 Qwen2.5 来进行修正。最终收集到了几 T 的 token。另外，作者还使用 Qwen2.5, Qwen2.5-Math, Qwen2.5-Coder 来合成不同格式的数据，包括教科书，QA，指令以及代码片段等。最后，作者加入了更多的多语种数据。&lt;/p>
&lt;p>作者从 educational value, fields, domains 以及 safety 对数据进行了标注。在数据混合时，Qwen3 在 instance 层面进行操作。&lt;/p>
&lt;p>预训练阶段包括 3 个 stage：&lt;/p>
&lt;ol>
&lt;li>General Stage (S1): 这一阶段的目的是让模型掌握世界知识，使用了 &lt;strong>30T&lt;/strong> 的 token，模型上下文长度为 4096&lt;/li>
&lt;li>Reasoning Stage (S2): 这一阶段的目的是提高模型的推理能力，使用了 &lt;strong>5T&lt;/strong> 的高质量 token，模型上下文长度为 4096，数据包括 STEM, coding, reasoning 以及合成数据&lt;/li>
&lt;li>Long Context Stage (S3): 这一阶段的目的是提升模型的长上下文能力，使用了&lt;strong>几百 B&lt;/strong>的 token，模型上下文长度为 32768.训练时数据混合 75% 的长文档数据，25% 的短文本数据。作者将 RoPE 的 frequency 从 10000 提升到了 1,000,000. 作者还是用 YARN 以及 Dual Chunk Attention 来提高 inference 效率&lt;/li>
&lt;/ol>
&lt;p>对 pre-training 的 base model 进行评测之后，作者发现：&lt;/p>
&lt;ol>
&lt;li>&lt;code>Qwen3-235B-A22B-Base&lt;/code> 超过了其他 base 模型的表现，包括 &lt;code>DeepSeek-V3 Base&lt;/code>, &lt;code>Llama-4-Maverick Base&lt;/code>, &lt;code>Qwen2.5-72B Base&lt;/code>&lt;/li>
&lt;li>Qwen3-MoE 模型与相同大小的 Qwen3-Dense 模型参数相比，其只需要 1/5 的参数就可以达到相同的表现&lt;/li>
&lt;li>Qwen3-MoE 模型与 2 倍参数量的 Qwen2.5-MoE 模型表现差不多&lt;/li>
&lt;li>Qwen3-Dense 模型与大一个量级的 Qwen2.5-Dense 模型表现差不多&lt;/li>
&lt;/ol>
&lt;h3 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h3>&lt;p>Qwen3 的 post-training 如下图所示：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen3/Qwen3_post-training.png"
width="1355"
height="614"
loading="lazy"
alt="Post-training Pipeline of Qwen3"
class="gallery-image"
data-flex-grow="220"
data-flex-basis="529px"
>&lt;/p>
&lt;p>对于旗舰模型 (&lt;code>Qwen3-235B-A22B&lt;/code>, &lt;code>Qwen3-32B&lt;/code>) 的训练，Qwen3 使用了一个四阶段的训练 pipeline。对于轻量化模型（其他模型）的训练，Qwen3 使用了知识蒸馏。&lt;/p>
&lt;p>旗舰模型的训练包括四个阶段，前两个阶段用于提升模型的 reasoning 能力，后两个阶段用于将 reasoning 和 non-reasoning 能力结合起来。&lt;/p>
&lt;h4 id="flashship-model">&lt;a href="#flashship-model" class="header-anchor">&lt;/a>Flashship Model
&lt;/h4>&lt;p>&lt;strong>Stage 1 (Long CoT Cold Start)&lt;/strong>
这个阶段的目的是让模型掌握 reasoning 的基础。这个阶段使用了数学，代码，逻辑推理和通用的 STEM 相关问题。每个问题都有参考答案或者 test-cases. 作者使用了 Qwen2.5-72B 来过滤数据，包括 non-verifiable prompts 以及太简单的 prompt. 作者认为，这一阶段应该减少训练使用的样本和训练步数。&lt;/p>
&lt;p>&lt;strong>Stage 2 (Reasoning RL)&lt;/strong>
这个阶段的目的是提升模型的 reasoning 能力。该阶段使用了 3,995 条过滤得到的样本，算法为 GRPO. 作者发现提高 batch size 和每个 query 的 rollouts 可以提高模型的表现。作者通过调整模型的 entropy 来控制 exploration 和 exploitation 的平衡&lt;/p>
&lt;p>&lt;strong>Stage 3 (Thinking Mode Fusion)&lt;/strong>
这一阶段的目的是将 non-reasoning 能力加入到之前的 reasoning 模型中。作者在第二阶段的 model 上进行了 continual SFT，然后构建了一个 chat template 用于融合两种模式。&lt;/p>
&lt;p>reasoning 数据来源于 stage1 的 rejection sampling 和 stage 2 的模型. non-reasoning 数据来源于各种任务，如 coding, math, multilingual 等。为了保证模型的多语种能力，作者还加入了一些翻译相关的数据。&lt;/p>
&lt;p>作者还构建了一个 chat template, 用于统一数据格式。chat template 如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen3/Qwen3_chat_template.png"
width="740"
height="320"
loading="lazy"
alt="chat template of Qwen3"
class="gallery-image"
data-flex-grow="231"
data-flex-basis="555px"
>&lt;/p>
&lt;p>作者使用 &lt;code>/think&lt;/code> 和 &lt;code>/no_think&lt;/code> 来标记两种模式，对于 non-reasoning mode, 其 &lt;code>&amp;lt;think&amp;gt;&amp;lt;/think&amp;gt;&lt;/code> 会被置空。模型在默认情况下处于 reasoning mode, 因此作者加入了一些不包含 &lt;code>/think&lt;/code> 的 reasoning 数据。&lt;/p>
&lt;p>作者发现，通过这种 Think mode fusion, 模型可以学会在 reasoning mode 和 non-reasoning mode 下进行回答，因此，模型也可以基于中间结果来给出最终的答案。 当超出 budget 之后，作者使用以下 Instruction&lt;/p>
&lt;p>&lt;code>Considering the limit time by the user. I have to give the solution based on the thinking directly now. \n&amp;lt;/think&amp;gt;.\n\n&lt;/code>&lt;/p>
&lt;p>来让模型直接终止思考二给出最终的答案。&lt;/p>
&lt;p>&lt;strong>Stage 4 (General RL)&lt;/strong>
这个阶段的目的是提升模型在不同场景下的能力。作者构建了一个 reward system 来覆盖 20 多种不同的任务。这些任务包括：instruction following, format following, preference alignment, agent ability 以及 abilities for specialized scenarios.&lt;/p>
&lt;p>作者构建了三种不同的 rewards:&lt;/p>
&lt;ol>
&lt;li>Rule-based rewards: 覆盖的任务包括 instruction following 和 format following&lt;/li>
&lt;li>Model-based rewards: 作者使用 Qwen2.5-72B 来判别答案的正确性&lt;/li>
&lt;li>Model-based Reward without reference answer: 作者训练一个 reward model 来给模型的回答进行打分&lt;/li>
&lt;/ol>
&lt;h4 id="lightweight-model">&lt;a href="#lightweight-model" class="header-anchor">&lt;/a>Lightweight Model
&lt;/h4>&lt;p>对于轻量化的模型，作者发现直接通过蒸馏可以有效提高学生模型的表现，并且训练效率也更高。蒸馏训练包括两个阶段：&lt;/p>
&lt;ol>
&lt;li>Off-policy Distillation: 这个阶段的目的是让模型拥有基本的 reasoning 能力并且可以在不同的模式中进行切换。作者使用了教师模型的 reasoning 输出和 non-reasoning 输出来蒸馏学生模型&lt;/li>
&lt;li>On-policy Distillation: 在这个阶段，学生模型生成回答，然后基于教师模型的输出，使用 KL-divergence 来更新学生模型的参数&lt;/li>
&lt;/ol>
&lt;h2 id="evaluation">&lt;a href="#evaluation" class="header-anchor">&lt;/a>Evaluation
&lt;/h2>&lt;p>&lt;strong>Thinking budget&lt;/strong>. 作者发现当我们提高 Thinking budget 之后，模型的表现是可以持续提升的。结果如下图&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen3/Qwen3_thinking_budget_performance.png"
width="1355"
height="938"
loading="lazy"
alt="Performance according to thinking budget"
class="gallery-image"
data-flex-grow="144"
data-flex-basis="346px"
>&lt;/p>
&lt;p>&lt;strong>Efficiency of distillation&lt;/strong>. 作者发现使用 distillation 可以大幅度提高模型的表现和训练效率。下面是结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen3/Qwen3_distillation_ablation.png"
width="1309"
height="200"
loading="lazy"
alt="Comparison between distillation and RL"
class="gallery-image"
data-flex-grow="654"
data-flex-basis="1570px"
>&lt;/p>
&lt;p>&lt;strong>Effects of Thinking mode fusion and RL&lt;/strong> 作者进一步探究了三个 stage 对模型表现的影响，为此，作者构建了 in-house benchmarks 来评估模型的表现，这些 benchmarks 包括：&lt;/p>
&lt;ol>
&lt;li>CounterFactrQA. 问题是不符合事实的，用于评估模型的幻觉&lt;/li>
&lt;li>LengthCtrl. 有长度要求的写作任务，评估生成内容长度和给定长度之间的差别&lt;/li>
&lt;li>ThinkFollow. 多轮对话，每轮对话随机插入 &lt;code>/think&lt;/code> 和 &lt;code>/no_think&lt;/code> flag，评估模型是否能在两种模式之间切换&lt;/li>
&lt;li>Tooluse. 评估模型的工具调用能力&lt;/li>
&lt;/ol>
&lt;p>结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen3/Qwen3_stage_ablation.png"
width="1358"
height="684"
loading="lazy"
alt="Performance of difference stages"
class="gallery-image"
data-flex-grow="198"
data-flex-basis="476px"
>&lt;/p>
&lt;p>结论如下：&lt;/p>
&lt;ol>
&lt;li>Stage3 可以提高模型在两种 reasoning mode 切换的能力，并且 stage3 还可以提高模型的通用以及 instruction following 能力&lt;/li>
&lt;li>Stage4 进一步提高模型在两种模式下的通用，instruction following 和 agent 能力&lt;/li>
&lt;li>Stage3 和 stage4 并没有显著提高模型在 knowledge, STEM, math 和 coding 相关任务上的表现。甚至在一些竞赛如 AIME24 上模型的表现还有所下降，作者认为这是由于我们提升了模型的通用能力而导致其特化能力下降导致的，作者认为作为一个通用模型，这是可以接受的。&lt;/li>
&lt;/ol>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>在本文中，作者提出了 Qwen3 系列大语言模型，包括 6 个 Dense 模型和 2 个 MoE 模型。Qwen3 模型标志了一个新的 SOTA，其特点主要是快慢思考结合，thinking budget，以及多语种。&lt;/p>
&lt;p>作者认为后续工作有以下几点：&lt;/p>
&lt;ol>
&lt;li>使用更高质量的数据来进行预训练&lt;/li>
&lt;li>优化模型架构和训练方式，提升模型的上下文&lt;/li>
&lt;li>提高针对 RL 的计算资源，来进一步提高模型的 agent 能力&lt;/li>
&lt;/ol>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2505.09388" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/QwenLM/Qwen3" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Seed1.5-VL</title><link>https://maosong.website/p/notes-on-seed1.5-vl/</link><pubDate>Wed, 14 May 2025 09:28:07 +0800</pubDate><guid>https://maosong.website/p/notes-on-seed1.5-vl/</guid><description>&lt;p>字节Seed在5月11号发布了Seed1.5-VL技术报告。技术报告详细介绍了Seed1.5-VL的架构，训练和评估细节&lt;/p>
&lt;h1 id="简介">&lt;a href="#%e7%ae%80%e4%bb%8b" class="header-anchor">&lt;/a>简介
&lt;/h1>&lt;p>Seed1.5-VL是多模态大模型，它是一个标准的 Vision Encoder - MLP - LLM 的架构，Vision Encoder是自研的Seed-ViT，参数量为532M，大语言模型的激活参数为20B（虽然论文未提及总参数量，但是从seed-Thinking v1.5来看，总参数量应该是200B）。Seed1.5-VL强调了agent能力，比如GUI control和gameplay。&lt;/p>
&lt;h1 id="介绍">&lt;a href="#%e4%bb%8b%e7%bb%8d" class="header-anchor">&lt;/a>介绍
&lt;/h1>&lt;p>现有的LVLM还不能与人类的通用能力相比，特别是在3D空间理解，物体技术，交互游戏等方面。并且，LVLM训练预料的多样性也不如LLM。最后，多模态数据的异质性也对模型的训练和推理提出了挑战。&lt;/p>
&lt;p>基于这些问题，作者提出了Seed1.5-VL，一个视觉多模态大模型。通过构建高质量的训练数据，作者大幅度提高了模型的表现。作者还进一步探究了模型的scaling law以及pre-training和post training算法的设计。为了提高训练和推理效率，作者在infra上也进行了改进&lt;/p>
&lt;h1 id="架构">&lt;a href="#%e6%9e%b6%e6%9e%84" class="header-anchor">&lt;/a>架构
&lt;/h1>&lt;p>Seed1.5-VL的架构如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-seed1.5-vl/architecture.png"
width="1387"
height="1062"
loading="lazy"
alt="Architecture of See1.5-VL"
class="gallery-image"
data-flex-grow="130"
data-flex-basis="313px"
>&lt;/p>
&lt;p>Seed1.5-VL的架构包含三个部分：&lt;/p>
&lt;ol>
&lt;li>Vision Encoder: vision encoder是seed自研的Seed-ViT，拥有532M参数&lt;/li>
&lt;li>Adapter：一个两层的MLP&lt;/li>
&lt;li>LLM：Seed1.5-LLM，一个MoE架构的LLM&lt;/li>
&lt;/ol>
&lt;p>Seed1.5-VL支持文本，图片和视频输入，输入为文本。其可以进行理解和推理(reasoning)。&lt;/p>
&lt;h2 id="seed-vit">&lt;a href="#seed-vit" class="header-anchor">&lt;/a>Seed-ViT
&lt;/h2>&lt;p>Seed-ViT主要是在两个方面进行了改进：&lt;/p>
&lt;ol>
&lt;li>在pre-training阶段使用了视频数据，来提高模型的时空间感知能力&lt;/li>
&lt;li>使用了2D-RoPE来处理动态分辨率的图片&lt;/li>
&lt;/ol>
&lt;p>Seed-ViT的超参数如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Patch size&lt;/th>
&lt;th>Pos Embedding&lt;/th>
&lt;th>Head dim&lt;/th>
&lt;th>#heads&lt;/th>
&lt;th>Embedding dim&lt;/th>
&lt;th>MLP ratio&lt;/th>
&lt;th>#layers&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>14&lt;/td>
&lt;td>2D RoPE&lt;/td>
&lt;td>64&lt;/td>
&lt;td>20&lt;/td>
&lt;td>1280&lt;/td>
&lt;td>4.0&lt;/td>
&lt;td>27&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>对于输入的图片，Seed-ViT首先将图片resize到28*28的倍数，接下来图片会被分割为14*14的patch. 与NaViT一样，Seed-ViT也是用了token-packing的操作，来将多个图片packing到一个sequence里进行训练。最后，对于Seed-ViT输出的图片特征，作者还是用了 $2\times 2$的 average pooling来降低token个数。&lt;/p>
&lt;p>Seed-ViT的预训练包括三个阶段，其设置和超参数如下表所示：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Categories&lt;/th>
&lt;th>Unlabeled Image&lt;/th>
&lt;th>Image-text pairs&lt;/th>
&lt;th>Video-audio-text tuples&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Training samples&lt;/td>
&lt;td>2.2B&lt;/td>
&lt;td>4.8B&lt;/td>
&lt;td>65M&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Token percentages&lt;/td>
&lt;td>4.0%&lt;/td>
&lt;td>91.2%&lt;/td>
&lt;td>4.8%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Batch sizes&lt;/td>
&lt;td>55,296&lt;/td>
&lt;td>32,768&lt;/td>
&lt;td>1,024&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LR warm up steps&lt;/td>
&lt;td>1,692&lt;/td>
&lt;td>2,000&lt;/td>
&lt;td>12,800&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Maximum LR&lt;/td>
&lt;td>$7.06\times 10^{-3}$&lt;/td>
&lt;td>$1.0\times 10^{-4}$&lt;/td>
&lt;td>$5.0\times 10^{-5}$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Minimum LR&lt;/td>
&lt;td>$1.05\times 10^{-5}$&lt;/td>
&lt;td>$1.2\times 10^{-6}$&lt;/td>
&lt;td>$2.02\times 10^{-7}$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>在预训练时，作者考虑了三点：&lt;/p>
&lt;ol>
&lt;li>训练效率：尽可能提高模型的训练效率&lt;/li>
&lt;li>原生图片精度输入处理：让模型可以尽早处理不同分辨率图片输入&lt;/li>
&lt;li>数据利用率：可以使用不同类型的数据&lt;/li>
&lt;/ol>
&lt;p>基于这三点考虑，Seed-ViT的训练包括三个阶段：&lt;/p>
&lt;ol>
&lt;li>Masked Image modeling with 2D RoPE：这一阶段的目的是提高模型对与几何和结构的感知能力。作者使用 &lt;code>EVA02-CLIP-E&lt;/code>作为教师模型，然后随机将75%的patches进行mask，最后让Seed-ViT输出的特征尽可能匹配教师模型输出的特征。这里的损失函数是cosine similarity. 作者发现，scale up MIM之后，VLM在OCR和文档理解任务上的表现得到了大幅度的提升。&lt;/li>
&lt;li>Native Resolution Contrastive Learning：这一阶段和CLIP,SigLIP的训练差不多，text encoder与&lt;code>EVA02-CLIP-E&lt;/code>相同。损失函数为SigLIP loss和SuperClass loss&lt;/li>
&lt;li>Omni-modal pret-raining：这一阶段使用了MiCo框架，来将video frames, audio, visual captions以及audio captions进行对齐，text encoder编码caption，ViT编码其他部分。通过这一阶段的训练，模型可以学习omni-model representations,并且还可以提高模型的图片和视频理解人物上的表现。&lt;/li>
&lt;/ol>
&lt;h2 id="视频编码">&lt;a href="#%e8%a7%86%e9%a2%91%e7%bc%96%e7%a0%81" class="header-anchor">&lt;/a>视频编码
&lt;/h2>&lt;p>对于视频输入，Seed1.5-VL采用了 &lt;strong>Dynamic Frame-Resolution Sampling&lt;/strong>技巧，核心思想在于根据budget和处理的任务来动态调整采样率(frame)和每一帧的图片精度(resolution)。&lt;/p>
&lt;p>具体来讲，在时序上，默认的FPS为1，如果要处理有关时序信息任务的话，FPS调整为2，如果要处理motion tracking等任务的话，FPS提高到5.为了进一步提高模型的时间感知能力，Seed 1.5-VL加入了timestamp tokens (e.g., &lt;code>[1.5 second]&lt;/code>)。&lt;/p>
&lt;p>在空间上，每个frame的可选精度为 $\{640, 512, 384, 256, 160, 128\}$，选择的精度根据我们的budget进行决定。如果说我们使用最低配置还是不能处理视频输入的话，我们就会先进行均匀采样，然后再进行处理，这样可以让模型处理所有的视频信息。&lt;/p>
&lt;h1 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h1>&lt;p>pre-training阶段一共包括了3T token. 作者详细介绍了每一个类别。这里我们就简单总结一下。数据一共包括以下7个类别&lt;/p>
&lt;ol>
&lt;li>Generic Image-text pairs &amp;amp; knowledge data. 这部分数据主要是图文交错以及image text-pairs数据。作者介绍了一下针对长尾数据（稀有物种识别）的数据构建方式&lt;/li>
&lt;li>OCR. 这部分数据有1B左右的sample，包括documents, scene texts, tables, charts以及flowcharts等。其中，作者通过SynthDog和Latex合成了200M左右的文本密集图片。作者还对合成的数据进行了增广;Chart数据包括公开数据集和合成的数据集，最终一共有100M的样本; Table数据集也是通过提取得到的，一共包含50M左右的图片。作者还基于OCR结果构建了VQA数据集，来提高模型理解图片文字的能力&lt;/li>
&lt;li>Visual Grounding &amp;amp; Counting: 提高模型识别和定位物体的能力。数据格式包括bounding box和center points. 其中bounding box主要是公开数据集；作者还是用Grounding DINO来标记了一部分数据，最后一共收集了200M的样本; point数据主要是PixMo, 作者还使用了Molmo以及CountGD标注了一部分数据，最后一共包括170M的指令以及110B的token; Counting数据从bounding box 和point数据集中筛选得到，包括了8M的样本和13B的token。最后在训练时，与InternVL一样，坐标会被normalize到 &lt;code>[1,1000]&lt;/code>&lt;/li>
&lt;li>3D spatial understanding. 主要包括相对深度，绝对深度和3D Grounding数据；对于相对深度，作者使用DepthAnything V2来标注，得到了3.2B的token; 对于绝对深度，作者使用了公开数据集，最终包含18M的指令以及28B的token;对于3D Grounding，作者使用了公开数据集，最终包含770的指令以及1.3B的token&lt;/li>
&lt;li>Video. 主要包含general video understanding数据, video temporal grounding and moment retrieval以及video streaming data.&lt;/li>
&lt;li>STEM. 主要包含image comprehension data和problem-solving data. 前者包括收集的3.2M教育相关的数据，以及10M结构化表格，4.5M化学结构表达式，1.5M坐标系数据。后者包括100M K12的习题&lt;/li>
&lt;li>GUI. 作者使用UI-TARS来合成数据。数据主要包括perception, grounding 以及 reasoning&lt;/li>
&lt;/ol>
&lt;p>预训练包括三个stage，其实验设置如下表所示：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Stages&lt;/th>
&lt;th>Stage 0&lt;/th>
&lt;th>Stage 1&lt;/th>
&lt;th>Stage 2&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Training budget (tokens)&lt;/td>
&lt;td>16B&lt;/td>
&lt;td>3T&lt;/td>
&lt;td>240B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Sequence length&lt;/td>
&lt;td>32,768&lt;/td>
&lt;td>32,768&lt;/td>
&lt;td>131,072&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Trainable components&lt;/td>
&lt;td>MLP adaptor&lt;/td>
&lt;td>all&lt;/td>
&lt;td>all&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Batch sizes (tokens)&lt;/td>
&lt;td>8.4M&lt;/td>
&lt;td>71M&lt;/td>
&lt;td>71M&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LR warmup steps&lt;/td>
&lt;td>100&lt;/td>
&lt;td>500&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Maximum LR&lt;/td>
&lt;td>$2.52 \times 10^{-4}$&lt;/td>
&lt;td>$5.22 \times 10^{-5}$&lt;/td>
&lt;td>$5.22 \times 10^{-6}$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Minimum LR&lt;/td>
&lt;td>$4.50 \times 10^{-5}$&lt;/td>
&lt;td>$5.22 \times 10^{-6}$&lt;/td>
&lt;td>$5.22 \times 10^{-6}$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;ul>
&lt;li>Stage 0: 仅训练MLP，用于对齐vision encoder和LLM,使用了16B token&lt;/li>
&lt;li>Stage 1：训练全部参数，用于获取知识以及掌握grounding和OCR能力，使用了3T token&lt;/li>
&lt;li>Stage 2：训练全部参数，提升模型的长上下文能力以及对下游任务的适应性，包括视频理解，coding和3D空间理解，上下文长度从32,768提升到131,072&lt;/li>
&lt;/ul>
&lt;p>作者还进行了消融实验，采取了和Qwen2-VL一样的训练方式，结果发现，本文的训练方式效果更好。作者认为，解冻vision encoder，可能让vision encoder弥补LLM的不足，进而损害其自身的感知能力。&lt;/p>
&lt;p>作者还探究了模型的scaling law，使用的建模函数为：&lt;/p>
$$
\log(\hat{L}) \sim \log(B) - \beta \log(D) = -a \log(D) + b
$$&lt;p>
这里 $L$ 是NLL loss, D是训练的token数，$a$和$b$是待拟合的参数。实验结果如下图所示。
&lt;img src="https://maosong.website/p/notes-on-seed1.5-vl/scaling_law.png"
width="1392"
height="1065"
loading="lazy"
alt="scaling law"
class="gallery-image"
data-flex-grow="130"
data-flex-basis="313px"
>&lt;/p>
&lt;p>结论是在特定领域数据训练的loss可以作为模型在下游任务上表现的一个估计&lt;/p>
&lt;h1 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h1>&lt;p>post-training分为两个阶段：SFT和RL，其中RL包括RLHF, RLVR. 总流程如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-seed1.5-vl/post_training.png"
width="1367"
height="598"
loading="lazy"
alt="post training"
class="gallery-image"
data-flex-grow="228"
data-flex-basis="548px"
>&lt;/p>
&lt;h2 id="sft">&lt;a href="#sft" class="header-anchor">&lt;/a>SFT
&lt;/h2>&lt;p>SFT数据主要包括General Instruction data以及Long Chain-of-Thought (LongCoT) data，劝着途胜模拟徐保国的指令跟随能力，后者提升模型的reasoning能力。&lt;/p>
&lt;p>通用指令微调数据包括13,000条收集的数据和40,0000高质量的公开数据集。&lt;/p>
&lt;h2 id="rlhf">&lt;a href="#rlhf" class="header-anchor">&lt;/a>RLHF
&lt;/h2>&lt;p>RLHF的数据包括人类标注数据和合成数据。对于reward model，作者使用了LVLM作为reward model，来给不同的response进行打分。最后，使用变体的PPO算法进行训练。&lt;/p>
&lt;h2 id="rlvr">&lt;a href="#rlvr" class="header-anchor">&lt;/a>RLVR
&lt;/h2>&lt;p>RLVR的训练数据主要包含Visual STEM和Visual Perception and Reasoning. 前者包含1M左右的问题，主要是STEM和K12的相关问题。后者包括grounding, visual instruction following以及visual puzzles &amp;amp; games等&lt;/p>
&lt;h2 id="hybrid-rl">&lt;a href="#hybrid-rl" class="header-anchor">&lt;/a>Hybrid RL
&lt;/h2>&lt;p>RL的训练过程包含了RLHF和RLVR两个RL算法。作者介绍了以下细节：&lt;/p>
&lt;ol>
&lt;li>format reward. 要求模型输出格式为 &lt;code>&amp;lt;think&amp;gt;{thought}&amp;lt;/think&amp;gt;{solution}&lt;/code>&lt;/li>
&lt;li>hybrid reward. reward包括RM和verifier两部分&lt;/li>
&lt;li>Shared critic. 使用一个critic model来估计value function&lt;/li>
&lt;li>KL coefficients. 对于RLHF和RLVE，使用不同的KL divergence稀疏&lt;/li>
&lt;/ol>
&lt;h2 id="iterative-update-by-rejection-sampling-fine-tuning">&lt;a href="#iterative-update-by-rejection-sampling-fine-tuning" class="header-anchor">&lt;/a>Iterative Update by Rejection Sampling Fine-tuning
&lt;/h2>&lt;p>在训练的过程中，作者使用了一个迭代训练策略。一个开始模型在低质量的Long-CoT数据上进行SFT，在RL过后，作者会重新评估问题的难度，然后过程掉一部分数据，再进行SFT，这样反复进行迭代训练以提高模型的表现。&lt;/p>
&lt;blockquote>
&lt;p>Infra部分好像都是一些细节，这里就跳过了。&lt;/p>
&lt;/blockquote>
&lt;h1 id="evaluation">&lt;a href="#evaluation" class="header-anchor">&lt;/a>Evaluation
&lt;/h1>&lt;p>作者评估了&lt;/p>
&lt;ol>
&lt;li>Seed-ViT的表现&lt;/li>
&lt;li>Seed1.5-VL的通用视觉理解和推理能力&lt;/li>
&lt;li>Seed1.5-VL的视频相关能力&lt;/li>
&lt;li>Seed1.5-VL的agent能力&lt;/li>
&lt;li>Seed1.5-VL在内部benchmark上的表现&lt;/li>
&lt;/ol>
&lt;p>由于Seed1.5-VL是一个API模型，并且其内部benchmark不公开，因此Seed-ViT以及其在内部benchmark上的表现我们就不列出来了。我们主要看一下其和领先多模态大模型的对比&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-seed1.5-vl/vision_benchmarks.png"
width="1240"
height="1184"
loading="lazy"
alt="vision benchmarks, 论文表6"
class="gallery-image"
data-flex-grow="104"
data-flex-basis="251px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-seed1.5-vl/video_benchmarks.png"
width="1254"
height="1025"
loading="lazy"
alt="Video benchmarks, 论文表7"
class="gallery-image"
data-flex-grow="122"
data-flex-basis="293px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-seed1.5-vl/gui_benchmarks.png"
width="1190"
height="393"
loading="lazy"
alt="GUI benchmarks"
class="gallery-image"
data-flex-grow="302"
data-flex-basis="726px"
>&lt;/p>
&lt;h1 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h1>&lt;p>在本文中，作者介绍了Seed1.5-VL，一个领先的多模态大模型，可以处理图片和视频输入。作者介绍了Seed-ViT以及Seed1.5-VL的数据，训练和评估。&lt;/p>
&lt;p>作者还分析了以下Seed1.5-VL的不足以及未来的改进方向&lt;/p>
&lt;ol>
&lt;li>Seed1.5-VL在复杂视觉感知任务中，其计数能力会下降。&lt;/li>
&lt;li>Seed1.5-VL的高阶推理能力还尚有不足，作者拿Klotski举了一个例子&lt;/li>
&lt;li>Seed1.5-VL的3D spatial reasoning能力不足&lt;/li>
&lt;li>Seed1.5-VL的temporal reasoning能力不足&lt;/li>
&lt;li>Seed1.5-VL与其他模型一样，存在幻觉问题&lt;/li>
&lt;/ol>
&lt;p>未来，有以下值得探索的方向：&lt;/p>
&lt;ol>
&lt;li>涌现现象(emergent abilities)&lt;/li>
&lt;li>scaling law，进一步探索模型参数量，算力以及数据对模型表现的影响&lt;/li>
&lt;li>提高模型的3D spatial reasoning能力，降低模型的幻觉，提高模型分组合搜索能力&lt;/li>
&lt;/ol>
&lt;h1 id="参考文献">&lt;a href="#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae" class="header-anchor">&lt;/a>参考文献
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2505.07062" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>分布式训练：参数量与计算量分析</title><link>https://maosong.website/p/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E5%8F%82%E6%95%B0%E9%87%8F%E4%B8%8E%E8%AE%A1%E7%AE%97%E9%87%8F%E5%88%86%E6%9E%90/</link><pubDate>Tue, 13 May 2025 11:26:36 +0800</pubDate><guid>https://maosong.website/p/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E5%8F%82%E6%95%B0%E9%87%8F%E4%B8%8E%E8%AE%A1%E7%AE%97%E9%87%8F%E5%88%86%E6%9E%90/</guid><description>&lt;p>在本文中，我们将要分析与大语言模型相关的参数量和计算量。在计算之前，我们会首先回顾一下大语言模型的架构&lt;/p>
&lt;h1 id="大语言模型架构">&lt;a href="#%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e6%9e%b6%e6%9e%84" class="header-anchor">&lt;/a>大语言模型架构
&lt;/h1>&lt;h1 id="大语言模型参数计算">&lt;a href="#%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e5%8f%82%e6%95%b0%e8%ae%a1%e7%ae%97" class="header-anchor">&lt;/a>大语言模型参数计算
&lt;/h1>&lt;h1 id="计算量估计">&lt;a href="#%e8%ae%a1%e7%ae%97%e9%87%8f%e4%bc%b0%e8%ae%a1" class="header-anchor">&lt;/a>计算量估计
&lt;/h1>&lt;h1 id="checkpointing">&lt;a href="#checkpointing" class="header-anchor">&lt;/a>checkpointing
&lt;/h1>&lt;h1 id="kv-cache">&lt;a href="#kv-cache" class="header-anchor">&lt;/a>KV cache
&lt;/h1>&lt;h1 id="参考文献">&lt;a href="#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae" class="header-anchor">&lt;/a>参考文献
&lt;/h1>&lt;ol>
&lt;li>&lt;a class="link" href="https://zhuanlan.zhihu.com/p/624740065" target="_blank" rel="noopener"
>回旋托马斯x 文章&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>分布式训练：如何训练一个模型</title><link>https://maosong.website/p/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AA%E6%A8%A1%E5%9E%8B/</link><pubDate>Tue, 13 May 2025 11:26:36 +0800</pubDate><guid>https://maosong.website/p/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AA%E6%A8%A1%E5%9E%8B/</guid><description>&lt;p>本节中，我们将介绍模型训练的基本数学原理，以及在分布式训练中我们需要考虑的精度，优化器等问题。&lt;/p>
&lt;h1 id="训练的数学原理">&lt;a href="#%e8%ae%ad%e7%bb%83%e7%9a%84%e6%95%b0%e5%ad%a6%e5%8e%9f%e7%90%86" class="header-anchor">&lt;/a>训练的数学原理
&lt;/h1>&lt;p>在最优化里面，我们需要解决的问题一般有如下形式：&lt;/p>
$$
\min_x\ f(x)
$$&lt;p>
这里 $f$是我们的目标函数, $x$是我们的变量。一个比较简单的例子就是求一个给定函数的最小值。&lt;/p>
&lt;p>如果说，我们想要基于数据来训练一个模型，这个时候，我们目标函数的输入就包括两部分，一部分是模型参数，另一部分是数据，为了方便起见，我们使用$\theta$来代表模型的参数，用 $\{x_i,y_i\}_{i=1}^N$ 来表示模型的训练集。上述的优化问题改写如下：&lt;/p>
$$
\min_{\theta}\ \frac{1}{N}\sum_{i=1}^Nf(x_i,y_i\;\theta)
$$&lt;p>比如我们训练 resnet 作为分类器，那么 resnet 的模型参数就是这里的 $\theta$, 训练集就是我们的图片和对应的标签，比如ImageNet等，对应的$f$可以设定为 cross entropy loss.&lt;/p>
&lt;p>现在有了优化问题之后，我们就需要设计算法求解这个优化问题。一个最简单的优化算法就是梯度下降算法：&lt;/p>
$$
\theta^{k+1} = \theta^k - \alpha_k\frac{1}{N}\sum_{i=1}^N\nabla_{\theta}f(x_i,y_i;\theta^k)
$$&lt;p>
这里 $\nabla_{\theta}f(x;\theta^k)$ 是 $f$ 相对于 $\theta$ 在 $\theta^k$ 处的梯度。&lt;/p>
&lt;p>但是，当我们模型过于复杂的时候，梯度往往计算起来非常复杂。为了简化模型的训练，现在的框架如tensorflow和pytorch都支持自动微分。因此，我们只需要定义如何从输入 $(x_i,y_i)$计算得到 $f(x_i,y_i;\theta)$ 就可以了，框架会帮我们计算参数的梯度。&lt;/p>
&lt;h1 id="自动微分">&lt;a href="#%e8%87%aa%e5%8a%a8%e5%be%ae%e5%88%86" class="header-anchor">&lt;/a>自动微分
&lt;/h1>&lt;p>自动微分的目的是将求导的过程交给框架，从而让用户专注于模型的开发（也就是设计&lt;code>forward&lt;/code>函数）。&lt;/p>
&lt;p>自动微分的核心思想就是链式法则.&lt;/p>
$$
\frac{dy}{dx} = \frac{dy}{df}\frac{df}{dg}\frac{dg}{dh}\frac{dh}{dx}
$$&lt;p>
如果我们的中间函数 $g$, $h$非常复杂的话，那么整个求导过程就会非常复杂。而链式法则则是将这样一个全局过程给分解成了若干个局部过程。我们将 $y$ 表示为：&lt;/p>
$$
\begin{aligned}
y &amp;= f(y_1)\\
y_1&amp;=g(y_2)\\
y_2&amp;=h(y_3)\\
y_3&amp;=x
\end{aligned}
$$&lt;p>接下来，&lt;/p>
&lt;h1 id="总结">&lt;a href="#%e6%80%bb%e7%bb%93" class="header-anchor">&lt;/a>总结
&lt;/h1>&lt;p>在本文中，我们简单介绍了一下如何训练一个模型，我们使用pytorch作为例子展示了现在训练框架的工作方式。在下一篇博客中，我们将会探究训练精度和优化器。训练精度和优化器是模型在训练过程中需要考虑的重点之一。&lt;/p>
&lt;h1 id="训练">&lt;a href="#%e8%ae%ad%e7%bb%83" class="header-anchor">&lt;/a>训练
&lt;/h1>&lt;h1 id="优化器">&lt;a href="#%e4%bc%98%e5%8c%96%e5%99%a8" class="header-anchor">&lt;/a>优化器
&lt;/h1>&lt;h2 id="sgd">&lt;a href="#sgd" class="header-anchor">&lt;/a>SGD
&lt;/h2>&lt;h2 id="adam">&lt;a href="#adam" class="header-anchor">&lt;/a>Adam
&lt;/h2>&lt;h2 id="adamw">&lt;a href="#adamw" class="header-anchor">&lt;/a>AdamW
&lt;/h2>&lt;h1 id="精度">&lt;a href="#%e7%b2%be%e5%ba%a6" class="header-anchor">&lt;/a>精度
&lt;/h1></description></item><item><title>Distributed training--Basic</title><link>https://maosong.website/p/distributed-training--basic/</link><pubDate>Mon, 12 May 2025 10:15:17 +0800</pubDate><guid>https://maosong.website/p/distributed-training--basic/</guid><description>&lt;blockquote>
&lt;p>说明：本文参考了 &lt;a class="link" href="https://huggingface.co/spaces/nanotron/ultrascale-playbook" target="_blank" rel="noopener"
>nanotron/ultrascale-playbook&lt;/a> 和 &lt;a class="link" href="https://colossalai.org/docs/concepts/distributed_training" target="_blank" rel="noopener"
>Colossal-AI Concepts&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;h1 id="什么是分布式系统">&lt;a href="#%e4%bb%80%e4%b9%88%e6%98%af%e5%88%86%e5%b8%83%e5%bc%8f%e7%b3%bb%e7%bb%9f" class="header-anchor">&lt;/a>什么是分布式系统
&lt;/h1>&lt;p>分布式系统允许一个软件的多个组件运行在不同的机器上。与传统集中式系统不一样，分布式系统可以有效提高系统的稳健性。
一个比较比较经典的分布式就是Git，Git允许我们把代码保存在多个remote上。这样当一个remote宕机时，其他remote也能提供服务。&lt;/p>
&lt;p>评估一个分布式系统的重要标准就是规模效益(scalablity)，也就是说，我们希望使用8台设备应该要比4台设备快2倍。但是，由于通信带宽等原因，实际上加速比并不是和设备数量成线性关系。因此，我们需要设计分布式算法，来有效提高分布式系统的效率。&lt;/p>
&lt;h1 id="为什么需要分布式训练">&lt;a href="#%e4%b8%ba%e4%bb%80%e4%b9%88%e9%9c%80%e8%a6%81%e5%88%86%e5%b8%83%e5%bc%8f%e8%ae%ad%e7%bb%83" class="header-anchor">&lt;/a>为什么需要分布式训练
&lt;/h1>&lt;p>我们需要分布式训练的原因主要是以下几点：&lt;/p>
&lt;ol>
&lt;li>模型越来越大。当下（2025）领先模型如Qwen，LLaMA系列的最大模型都超过了100B [2][3]。LLaMA系列最大的模型甚至超过了1000B。Scaling law告诉我们模型表现与参数量，算力，数据量成正相关关系。&lt;/li>
&lt;li>数据集越来越大。现在领先的模型需要的数据量基本都需要100M以上，而大语言模型训练需要的token数量也都超过了10T的量级 [2][3].&lt;/li>
&lt;li>算力越来越强。现有最强的GPU H100其显存为80GB，拥有3.35TB/s 的带宽 (PcIe)，这让训练大规模模型成为可能。&lt;/li>
&lt;/ol>
&lt;p>超大的模型使得我们很难在一张GPU上进行训练，甚至我们都很难使用单张GPU进行部署。而10T级的数据也也需要几个月的时间才能训练完毕。因此，如何高效利用多张GPU在大规模数据上训练超大模型就是我们需要解决的问题。&lt;/p>
&lt;h1 id="基本概念">&lt;a href="#%e5%9f%ba%e6%9c%ac%e6%a6%82%e5%bf%b5" class="header-anchor">&lt;/a>基本概念
&lt;/h1>&lt;p>我们先来熟悉一下分布式训练中的一些基本概念：&lt;/p>
&lt;ul>
&lt;li>Host: host (master address)是分布式训练中通信网络的主设备(main device). 一般我们需要对其进行初始化&lt;/li>
&lt;li>Node: 一个物理或虚拟的计算单元，可以是一台机器，一个容器或者一个虚拟机&lt;/li>
&lt;li>Port: port (master port)主要是用于通信的master port&lt;/li>
&lt;li>Rank: rank是通信网络中每个设备唯一的ID&lt;/li>
&lt;li>world size: world size是通信网络中设备的数量&lt;/li>
&lt;li>process group: 一个process group是通信网络中所有设备集合的一个子集。通过process group, 我们可以限制device只在group内部进行通信&lt;/li>
&lt;/ul>
&lt;p>我们以下图为例：
&lt;img src="https://maosong.website/p/distributed-training--basic/basic_concepts.png"
width="1893"
height="1098"
loading="lazy"
alt="basic concepts"
class="gallery-image"
data-flex-grow="172"
data-flex-basis="413px"
>
上图中一共包含2个node (2台机器)，每台机器包含4个GPU (device)，当我们初始化分布式环境时，我们一共启动了8个进程（每台机器4个进程），每个进程绑定一个GPU。&lt;/p>
&lt;p>在初始化分布式环境之间，我们需要指定host和port。假设我们指定host为&lt;code>node 0&lt;/code>和port为 &lt;code>29500&lt;/code>，接下来，所有的进程都会基于这个host和port来与其他进程连接。默认的process group（包含所有device）的world size 为8. 其细节展示如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>process ID&lt;/th>
&lt;th>rank&lt;/th>
&lt;th>Node index&lt;/th>
&lt;th>GPU index&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>0&lt;/td>
&lt;td>0&lt;/td>
&lt;td>0&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;td>0&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2&lt;/td>
&lt;td>2&lt;/td>
&lt;td>0&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>3&lt;/td>
&lt;td>3&lt;/td>
&lt;td>0&lt;/td>
&lt;td>3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4&lt;/td>
&lt;td>4&lt;/td>
&lt;td>1&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>5&lt;/td>
&lt;td>5&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>6&lt;/td>
&lt;td>6&lt;/td>
&lt;td>1&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>7&lt;/td>
&lt;td>7&lt;/td>
&lt;td>1&lt;/td>
&lt;td>3&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>我们可以创建一个新的process group，使其仅包含ID为偶数的process：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>process ID&lt;/th>
&lt;th>rank&lt;/th>
&lt;th>Node index&lt;/th>
&lt;th>GPU index&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>0&lt;/td>
&lt;td>0&lt;/td>
&lt;td>0&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2&lt;/td>
&lt;td>1&lt;/td>
&lt;td>0&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4&lt;/td>
&lt;td>2&lt;/td>
&lt;td>1&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>6&lt;/td>
&lt;td>3&lt;/td>
&lt;td>1&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Remark&lt;/strong>: 注意，rank与process group相关，一个process在不同的process group里可能会有不同的rank.&lt;/p>
&lt;h1 id="通信方式">&lt;a href="#%e9%80%9a%e4%bf%a1%e6%96%b9%e5%bc%8f" class="header-anchor">&lt;/a>通信方式
&lt;/h1>&lt;p>接下来，我们需要介绍一下设备间的通信方式，这是我们后面分布式训练算法的基础。根据设备数量的不同，我们可以将设备间通信分为：&lt;/p>
&lt;ol>
&lt;li>one-to-one: 两个device之间互相进行通信&lt;/li>
&lt;li>one-to-many: 一个device与多个device进行通信&lt;/li>
&lt;li>many-to-one: 多个device与一个device之间进行通信&lt;/li>
&lt;li>many-to-many: 多个device之间互相进行通信&lt;/li>
&lt;/ol>
&lt;h2 id="one-to-one">&lt;a href="#one-to-one" class="header-anchor">&lt;/a>One-to-one
&lt;/h2>&lt;p>One-to-one的情况很简单，一个process与另一个process进行通信，通信通过 &lt;code>send&lt;/code> 和 &lt;code>recv&lt;/code> 完成。还有对应的 immediate版本，即 &lt;code>isend&lt;/code> 和 &lt;code>irecv&lt;/code>，示意图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/distributed-training--basic/send_recv.png"
width="364"
height="411"
loading="lazy"
alt="point to point communication"
class="gallery-image"
data-flex-grow="88"
data-flex-basis="212px"
>&lt;/p>
&lt;p>测试代码如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># send_recv.py&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">os&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">torch&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">torch.distributed&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">dist&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">init_process&lt;/span>&lt;span class="p">():&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">init_process_group&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">backend&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;nccl&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cuda&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">set_device&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">example_send&lt;/span>&lt;span class="p">():&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">tensor&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">4&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cuda&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">send&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dst&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">elif&lt;/span> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">tensor&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zeros&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cuda&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;Before send on rank &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">recv&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">src&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;After send on rank &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">init_process&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">example_send&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># run with&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># torchrun --nproc_per_node=2 send_recv.py&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>结果输出&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Before send on rank 1: tensor([0., 0., 0., 0., 0.], device=&amp;#39;cuda:1&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">After send on rank 1: tensor([1., 2., 3., 4., 5.], device=&amp;#39;cuda:1&amp;#39;)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;blockquote>
&lt;p>注：为了方便，后续代码仅定义函数和运行方式，&lt;code>init_process()&lt;/code>和import部分省略&lt;/p>
&lt;/blockquote>
&lt;p>&lt;code>send/recv&lt;/code>的特点是在完成通信之前，两个process是锁住的。与之相反，&lt;code>isend/irecv&lt;/code> 则不会加锁，代码会继续执行然后返回&lt;code>Work&lt;/code>对象，为了让通信顺利进行，我们可以在返回之前加入&lt;code>wait()&lt;/code>&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># isend_irecv.py&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">example_isend&lt;/span>&lt;span class="p">():&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">req&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">None&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">tensor&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">4&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cuda&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">req&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">isend&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dst&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Rank 0 is sending&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">elif&lt;/span> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">tensor&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zeros&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cuda&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;Before irecv on rank &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">req&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">irecv&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">src&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Rank 1 is receiving&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">req&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">wait&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;After isend on rank &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">init_process&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">example_isend&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># run with&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># torchrun --nproc_per_node=2 isend_irecv.py&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>结果输出&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Before irecv on rank 1: tensor([0., 0., 0., 0., 0.], device=&amp;#39;cuda:1&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Rank 0 is sending
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Rank 1 is receiving
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">After isend on rank 1: tensor([1., 2., 3., 4., 5.], device=&amp;#39;cuda:1&amp;#39;)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>由于&lt;code>isend/irecv&lt;/code>这种不锁的特性，我们不应该&lt;/p>
&lt;ol>
&lt;li>在&lt;code>dist.isend()&lt;/code>之前修改发送的内容&lt;code>tensor&lt;/code>&lt;/li>
&lt;li>在&lt;code>dist.irecv()&lt;/code>之后读取接受的内容&lt;code>tensor&lt;/code>&lt;/li>
&lt;/ol>
&lt;p>&lt;code>req.wait()&lt;/code> 可以保证这次通信顺利完成，因此我们可以在&lt;code>req.wait()&lt;/code>之后再进行修改和读取。&lt;/p>
&lt;h2 id="one-to-many">&lt;a href="#one-to-many" class="header-anchor">&lt;/a>One-to-many
&lt;/h2>&lt;p>One-to-many 情形下，可以分为两种：scatter 和 broadcast&lt;/p>
&lt;p>scatter的作用是将一个process的数据均分并散布到其他process。broadcast的作用是将一个process的数据广播到其他process。两者不同的地方在于其他process获取到的是全量数据(copy)还是部分数据(slice)，其示意图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/distributed-training--basic/scatter_broadcast.png"
width="1641"
height="413"
loading="lazy"
alt="scatter and broadcast"
class="gallery-image"
data-flex-grow="397"
data-flex-basis="953px"
>&lt;/p>
&lt;p>scatter 测试代码：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># scatter.py&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">example_scatter&lt;/span>&lt;span class="p">():&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">scatter_list&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cuda&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_world_size&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;Rank 0 scatter list: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">scatter_list&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">scatter_list&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">None&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">tensor&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zeros&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cuda&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;Before scatter on rank &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">scatter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">scatter_list&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">src&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;After scatter on rank &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">init_process&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">example_broadcast&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># run with&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># torchrun --nproc_per_node=4 broadcast.py&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>结果输出以下内容（输出内容有优化，后续不再说明）：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Rank 0 scatter list: [
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([1., 1., 1., 1., 1.], device=&amp;#39;cuda:0&amp;#39;),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([2., 2., 2., 2., 2.], device=&amp;#39;cuda:0&amp;#39;),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([3., 3., 3., 3., 3.], device=&amp;#39;cuda:0&amp;#39;),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([4., 4., 4., 4., 4.], device=&amp;#39;cuda:0&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Before scatter on rank 2: tensor([0., 0., 0., 0., 0.], device=&amp;#39;cuda:2&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Before scatter on rank 1: tensor([0., 0., 0., 0., 0.], device=&amp;#39;cuda:1&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Before scatter on rank 3: tensor([0., 0., 0., 0., 0.], device=&amp;#39;cuda:3&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Before scatter on rank 0: tensor([0., 0., 0., 0., 0.], device=&amp;#39;cuda:0&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">After scatter on rank 0: tensor([1., 1., 1., 1., 1.], device=&amp;#39;cuda:0&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">After scatter on rank 2: tensor([3., 3., 3., 3., 3.], device=&amp;#39;cuda:2&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">After scatter on rank 3: tensor([4., 4., 4., 4., 4.], device=&amp;#39;cuda:3&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">After scatter on rank 1: tensor([2., 2., 2., 2., 2.], device=&amp;#39;cuda:1&amp;#39;)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>broadcast 测试代码:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># broadcast.py&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">example_broadcast&lt;/span>&lt;span class="p">():&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">tensor&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">4&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cuda&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">tensor&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zeros&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cuda&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;Before broadcast on rank &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">broadcast&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">src&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;After broadcast on rank &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">init_process&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">example_broadcast&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># run with&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># torchrun --nproc_per_node=3 broadcast.py&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>结果输出：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Before broadcast on rank 1: tensor([0., 0., 0., 0., 0.], device=&amp;#39;cuda:1&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Before broadcast on rank 2: tensor([0., 0., 0., 0., 0.], device=&amp;#39;cuda:2&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Before broadcast on rank 0: tensor([1., 2., 3., 4., 5.], device=&amp;#39;cuda:0&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">After broadcast on rank 0: tensor([1., 2., 3., 4., 5.], device=&amp;#39;cuda:0&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">After broadcast on rank 1: tensor([1., 2., 3., 4., 5.], device=&amp;#39;cuda:1&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">After broadcast on rank 2: tensor([1., 2., 3., 4., 5.], device=&amp;#39;cuda:2&amp;#39;)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="many-to-one">&lt;a href="#many-to-one" class="header-anchor">&lt;/a>Many-to-one
&lt;/h2>&lt;p>Many-to-one 情形下，也可以分为两种：gather 和 reduce, Gather对应one-to-many的scatter操作，负责将多个process的内容汇聚到一起，形成一个完整的向量。而reduce的操作则是通过一个函数 $f(\cdot)$ 来把数据进行汇总，常见的函数有求和以及求平均，示意图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/distributed-training--basic/gather_reduce.png"
width="1672"
height="413"
loading="lazy"
alt="gather and reduce"
class="gallery-image"
data-flex-grow="404"
data-flex-basis="971px"
>&lt;/p>
&lt;p>gather 测试代码：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># gather.py&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">example_gather&lt;/span>&lt;span class="p">():&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">tensor&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cuda&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">gather_list&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zeros&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cuda&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_world_size&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;Rank 0 gather list: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">gather_list&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">gather_list&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">None&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;Before gather on rank &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">gather&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">gather_list&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dst&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;After gather on rank &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">gather_list&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">init_process&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">example_gather&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># run with&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># torchrun --nproc_per_node=4 gather.py&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>结果输出：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Before gather on rank 3: tensor([4., 4., 4., 4., 4.], device=&amp;#39;cuda:3&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Before gather on rank 2: tensor([3., 3., 3., 3., 3.], device=&amp;#39;cuda:2&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Before gather on rank 0: tensor([1., 1., 1., 1., 1.], device=&amp;#39;cuda:0&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Before gather on rank 1: tensor([2., 2., 2., 2., 2.], device=&amp;#39;cuda:1&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">After gather on rank 0: [
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([1., 1., 1., 1., 1.], device=&amp;#39;cuda:0&amp;#39;),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([2., 2., 2., 2., 2.], device=&amp;#39;cuda:0&amp;#39;),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([3., 3., 3., 3., 3.], device=&amp;#39;cuda:0&amp;#39;),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([4., 4., 4., 4., 4.], device=&amp;#39;cuda:0&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>reduce 测试代码:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># example_reduce.py&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">example_reduce&lt;/span>&lt;span class="p">():&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">tensor&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cuda&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;Before reduce on rank &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reduce&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dst&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">op&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ReduceOp&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">SUM&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;After reduce on rank &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">init_process&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">example_reduce&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># run with&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># torchrun --nproc_per_node=3 example_reduce.py&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>这里我们使用求和&lt;code>dist.ReduceOp.SUM&lt;/code>作为我们的汇总操作，Pytorch还支持其他的&lt;a class="link" href="https://docs.pytorch.org/docs/stable/distributed.html#torch.distributed.ReduceOp" target="_blank" rel="noopener"
>reduce operations&lt;/a>. 结果输出以下内容：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Before reduce on rank 2: tensor([3., 3., 3., 3., 3.], device=&amp;#39;cuda:2&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Before reduce on rank 3: tensor([4., 4., 4., 4., 4.], device=&amp;#39;cuda:3&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Before reduce on rank 0: tensor([1., 1., 1., 1., 1.], device=&amp;#39;cuda:0&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Before reduce on rank 1: tensor([2., 2., 2., 2., 2.], device=&amp;#39;cuda:1&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">After reduce on rank 0: tensor([10., 10., 10., 10., 10.], device=&amp;#39;cuda:0&amp;#39;)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="many-to-many">&lt;a href="#many-to-many" class="header-anchor">&lt;/a>Many-to-many
&lt;/h2>&lt;p>Many-to-many 情形下的两种通信方式为：All-Reduce 和 All-Gather，分别是reduce和gather的升级版，all-reduce对所有process都执行一次reduce操作，而all-gather则对所有process执行一次gather操作，其示意图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/distributed-training--basic/all_gather_reduce.png"
width="1713"
height="411"
loading="lazy"
alt="all-gather and all-reduce"
class="gallery-image"
data-flex-grow="416"
data-flex-basis="1000px"
>&lt;/p>
&lt;p>all-gather 测试代码：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># example_all_gather.py&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">example_all_gather&lt;/span>&lt;span class="p">():&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">tensor&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cuda&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">gather_list&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zeros&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cuda&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_world_size&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;Before all gather on rank &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">all_gather&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">gather_list&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">tensor&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;After all gather on rank &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">gather_list&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">init_process&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">example_all_gather&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># run with&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># torchrun --nproc_per_node=3 example_all_gather.py&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>测试输出结果：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Before all gather on rank 2: tensor([3., 3., 3., 3., 3.], device=&amp;#39;cuda:2&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Before all gather on rank 3: tensor([4., 4., 4., 4., 4.], device=&amp;#39;cuda:3&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Before all gather on rank 0: tensor([1., 1., 1., 1., 1.], device=&amp;#39;cuda:0&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Before all gather on rank 1: tensor([2., 2., 2., 2., 2.], device=&amp;#39;cuda:1&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">After all gather on rank 0: [
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([1., 1., 1., 1., 1.], device=&amp;#39;cuda:0&amp;#39;),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([2., 2., 2., 2., 2.], device=&amp;#39;cuda:0&amp;#39;),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([3., 3., 3., 3., 3.], device=&amp;#39;cuda:0&amp;#39;),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([4., 4., 4., 4., 4.], device=&amp;#39;cuda:0&amp;#39;)]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">After all gather on rank 2: [
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([1., 1., 1., 1., 1.], device=&amp;#39;cuda:2&amp;#39;),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([2., 2., 2., 2., 2.], device=&amp;#39;cuda:2&amp;#39;),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([3., 3., 3., 3., 3.], device=&amp;#39;cuda:2&amp;#39;),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([4., 4., 4., 4., 4.], device=&amp;#39;cuda:2&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">After all gather on rank 3: [
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([1., 1., 1., 1., 1.], device=&amp;#39;cuda:3&amp;#39;),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([2., 2., 2., 2., 2.], device=&amp;#39;cuda:3&amp;#39;),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([3., 3., 3., 3., 3.], device=&amp;#39;cuda:3&amp;#39;),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([4., 4., 4., 4., 4.], device=&amp;#39;cuda:3&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">After all gather on rank 1: [
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([1., 1., 1., 1., 1.], device=&amp;#39;cuda:1&amp;#39;),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([2., 2., 2., 2., 2.], device=&amp;#39;cuda:1&amp;#39;),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([3., 3., 3., 3., 3.], device=&amp;#39;cuda:1&amp;#39;),
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> tensor([4., 4., 4., 4., 4.], device=&amp;#39;cuda:1&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>all-reduce 测试代码：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># example_all_reduce.py&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">example_all_reduce&lt;/span>&lt;span class="p">():&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">tensor&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cuda&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;Before all reduce on rank &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">all_reduce&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">op&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ReduceOp&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">SUM&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;After all reduce on rank &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">init_process&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">example_all_reduce&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># run with&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># torchrun --nproc_per_node=3 example_all_reduce.py&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>测试输出结果&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Before all reduce on rank 1: tensor([2., 2., 2., 2., 2.], device=&amp;#39;cuda:1&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Before all reduce on rank 0: tensor([1., 1., 1., 1., 1.], device=&amp;#39;cuda:0&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Before all reduce on rank 2: tensor([3., 3., 3., 3., 3.], device=&amp;#39;cuda:2&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Before all reduce on rank 3: tensor([4., 4., 4., 4., 4.], device=&amp;#39;cuda:3&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">After all reduce on rank 0: tensor([10., 10., 10., 10., 10.], device=&amp;#39;cuda:0&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">After all reduce on rank 2: tensor([10., 10., 10., 10., 10.], device=&amp;#39;cuda:2&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">After all reduce on rank 3: tensor([10., 10., 10., 10., 10.], device=&amp;#39;cuda:3&amp;#39;)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">After all reduce on rank 1: tensor([10., 10., 10., 10., 10.], device=&amp;#39;cuda:1&amp;#39;)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="barrier">&lt;a href="#barrier" class="header-anchor">&lt;/a>Barrier
&lt;/h2>&lt;p>除了之前这些传输数据的方式之外，我们还有Barrier，用于在所有process之间进行同步。Barrier会确保所有的process在同一时间点完成某些操作。其流程为，先让每个process完成各自的任务，然后当process到达barrier时，process会通知系统自己已到达。最后当所有process都到达barrier之后，阻塞会解除，所有process继续执行下一步操作。&lt;/p>
&lt;p>barrier 测试代码&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># example_barrier.py&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">example_barrier&lt;/span>&lt;span class="p">():&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kn">import&lt;/span> &lt;span class="nn">time&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">rank&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_rank&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">t_start&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">time&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">time&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;Rank &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">rank&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2"> sleeps &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">rank&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2"> seconds&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">time&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sleep&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">rank&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">barrier&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;Rank &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">rank&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2"> is done at &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">time&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">time&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">t_start&lt;/span>&lt;span class="si">:&lt;/span>&lt;span class="s2">.4f&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2"> seconds&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">init_process&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">example_barrier&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># run with&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># torchrun --nproc_per_node=3 example_barrier.py&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>结果输出&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;span class="lnt">9
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Rank 2 sleeps 2 seconds
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Rank 0 sleeps 0 seconds
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Rank 1 sleeps 1 seconds
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Rank 3 sleeps 3 seconds
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Rank 3 is done at 3.3046 seconds
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Rank 1 is done at 3.3229 seconds
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Rank 2 is done at 3.8437 seconds
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Rank 0 is done at 3.6613 seconds
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>可以看到，四个process的到达时间都在3s左右，这是因为rank 3需要3s才能完成当前任务&lt;/p>
&lt;h2 id="advanced">&lt;a href="#advanced" class="header-anchor">&lt;/a>Advanced
&lt;/h2>&lt;p>除了前面的通信方式之外，还有 Reduce-Scatter和Ring All-Reduce，这两个通信方式等我们学习ZeRO的时候再一并讲解。&lt;/p>
&lt;h1 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h1>&lt;ol>
&lt;li>&lt;a class="link" href="https://colossalai.org/docs/concepts/distributed_training" target="_blank" rel="noopener"
>Colossal-AI&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/" target="_blank" rel="noopener"
>LLaMA 4 blog&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://qwenlm.github.io/blog/qwen3/" target="_blank" rel="noopener"
>Qwen3 blog&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://docs.pytorch.org/tutorials/intermediate/dist_tuto.html" target="_blank" rel="noopener"
>Pytorch tutorial&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://huggingface.co/spaces/nanotron/ultrascale-playbook" target="_blank" rel="noopener"
>nanotron/ultrascale-playbook&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>Notes on LLaMA4 blog</title><link>https://maosong.website/p/notes-on-llama4-blog/</link><pubDate>Wed, 30 Apr 2025 10:44:19 +0800</pubDate><guid>https://maosong.website/p/notes-on-llama4-blog/</guid><description>&lt;h1 id="介绍">&lt;a href="#%e4%bb%8b%e7%bb%8d" class="header-anchor">&lt;/a>介绍
&lt;/h1>&lt;p>Meta在2025年4月10号发布了LLaMA4系列，包含三个模型：Llama 4 Scout, Llama 4 Maverick 以及Llama 4 Behemoth, 三个模型都基于MoE架构，且支持多模态&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Layers&lt;/th>
&lt;th>Heads (Q / KV)&lt;/th>
&lt;th>Context Length&lt;/th>
&lt;th>#Parameters (activated/total)&lt;/th>
&lt;th>#Experts (activated/total)&lt;/th>
&lt;th>#Tokens&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>LLaMA 4 Behemoth&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>288B / 2T&lt;/td>
&lt;td>(1 shared + 1 routed) / 16&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LLaMA 4 Maverick&lt;/td>
&lt;td>48&lt;/td>
&lt;td>40/8&lt;/td>
&lt;td>1M&lt;/td>
&lt;td>17B / 109B&lt;/td>
&lt;td>(1 shared + 1 routed) / 128&lt;/td>
&lt;td>~22T&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LLaMA 4 Scout&lt;/td>
&lt;td>48&lt;/td>
&lt;td>40/8&lt;/td>
&lt;td>10M&lt;/td>
&lt;td>17B / 400B&lt;/td>
&lt;td>(1 shared + 1 routed) / 16&lt;/td>
&lt;td>~40T&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>训练数据截止到2024年8月。LLaMa4支持200多种语言，其中100多种语言的训练token数超过了1B&lt;/p>
&lt;h1 id="亮点">&lt;a href="#%e4%ba%ae%e7%82%b9" class="header-anchor">&lt;/a>亮点
&lt;/h1>&lt;ol>
&lt;li>原生多模态
LLaMA 4是一个原生多模态架构&lt;/li>
&lt;li>超长上下文
LLaMA 4的上下文超过了1M&lt;/li>
&lt;li>iRoPE
通过交替dense和MoE MLP来提高整体推理效率&lt;/li>
&lt;li>基于MetaCLIP的vision encoder&lt;/li>
&lt;li>MetaP
使用MetaP来调整超参数&lt;/li>
&lt;li>FP8精度训练&lt;/li>
&lt;/ol>
&lt;h1 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h1>&lt;p>LLaMA 4 仍然是一个基于transformer的架构，但是引入了MoE，其示意图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-llama4-blog/architecture.png"
width="1920"
height="1308"
loading="lazy"
alt="architecture of LLaMA 4"
class="gallery-image"
data-flex-grow="146"
data-flex-basis="352px"
>&lt;/p>
&lt;p>MoE架构中包含1个shared expert以及1个routed expert. 并且，与其他LLM不同，LLaMA 4使用了一个交替MLP个MoE的架构，即iRoPE，即特定的transformer layer是MoE架构，其余的是MLP架构，其&lt;a class="link" href="https://github.com/huggingface/transformers/blob/5f8d17268ced2ca5f51b0216782356b16be0d6f4/src/transformers/models/llama4/modeling_llama4.py#L380" target="_blank" rel="noopener"
>核心代码&lt;/a>如下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">is_moe_layer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">layer_idx&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">moe_layers&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">if&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">is_moe_layer&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="c1"># the 128E model interleaves dense / sparse&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">feed_forward&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Llama4TextMoe&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">feed_forward&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Llama4TextMLP&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">intermediate_size&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">intermediate_size_mlp&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>early fusion. LLaMA 4称其一个原生多模态大模型，但是其架构仍然是 Vision Encoder-MLP-LLM 的形式，其不同点在于patch embedding没有使用convolution, 而是使用 &lt;code>nn.Unfold&lt;/code>直接进行展平，然后使用一个线性层与vision encoder进行对齐。&lt;a class="link" href="https://github.com/huggingface/transformers/blob/5f8d17268ced2ca5f51b0216782356b16be0d6f4/src/transformers/models/llama4/modeling_llama4.py#L1409" target="_blank" rel="noopener"
>代码&lt;/a>如下&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Llama4UnfoldConvolution&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="fm">__init__&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">kernel_size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">patch_size&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="nb">isinstance&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">kernel_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">kernel_size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">kernel_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">kernel_size&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">unfold&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Unfold&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">kernel_size&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">kernel_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">stride&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">patch_size&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">linear&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_channels&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">kernel_size&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">kernel_size&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">unfold&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">permute&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">hidden_states&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>其他训练优化技巧如下：&lt;/p>
&lt;ol>
&lt;li>MetaP：用于选择超参数&lt;/li>
&lt;li>FP8 precision：与 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v3/" target="_blank" rel="noopener"
>DeepSeek-V3&lt;/a> 一样，使用FP8精度进行训练&lt;/li>
&lt;li>mid-training：在预训练阶段之后，额外增加了一个训练阶段，来提高模型的长上下文等关键能力&lt;/li>
&lt;/ol>
&lt;h1 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h1>&lt;p>post-training包括三个阶段：&lt;/p>
&lt;ol>
&lt;li>SFT&lt;/li>
&lt;li>online RL&lt;/li>
&lt;li>&lt;a class="link" href="https://maosong.website/p/notes-on-dpo/" target="_blank" rel="noopener"
>DPO&lt;/a>&lt;/li>
&lt;/ol>
&lt;p>作者发现SFT和DPO会限制模型的探索能力，特别是在math, coding等domain。为了解决这个问题，作者使用LlaMA对问题进行难度分级，然后移除了50%的简单数据。&lt;/p>
&lt;p>在online RL阶段，作者设计了一个continuous online RL策略，让模型在训练和筛选问题两种模式之间进行切换，以平衡效率和准确率。&lt;/p>
&lt;p>DPO的目的是为了提升模型输出的质量&lt;/p>
&lt;h1 id="评测">&lt;a href="#%e8%af%84%e6%b5%8b" class="header-anchor">&lt;/a>评测
&lt;/h1>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>benchmark&lt;/th>
&lt;th>LLaMA 4 Maverick&lt;/th>
&lt;th>LLaMA 4 Maverick&lt;/th>
&lt;th>LLaMA 4 Scout&lt;/th>
&lt;th>Gemeni 2.0 Flash&lt;/th>
&lt;th>GPT-4o&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>MMMU&lt;/td>
&lt;td>76.1&lt;/td>
&lt;td>73.4&lt;/td>
&lt;td>69.4&lt;/td>
&lt;td>71.7&lt;/td>
&lt;td>69.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Math Vista&lt;/td>
&lt;td>73.7&lt;/td>
&lt;td>70.7&lt;/td>
&lt;td>73.1&lt;/td>
&lt;td>63.8&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ChartQA&lt;/td>
&lt;td>-&lt;/td>
&lt;td>90.0&lt;/td>
&lt;td>88.8&lt;/td>
&lt;td>88.3&lt;/td>
&lt;td>85.7&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DocVQA&lt;/td>
&lt;td>-&lt;/td>
&lt;td>94.4&lt;/td>
&lt;td>94.4&lt;/td>
&lt;td>-&lt;/td>
&lt;td>92.8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LiveCodeBench&lt;/td>
&lt;td>49.4&lt;/td>
&lt;td>43.4&lt;/td>
&lt;td>32.8&lt;/td>
&lt;td>34.5&lt;/td>
&lt;td>32.3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MMLU Pro&lt;/td>
&lt;td>82.2&lt;/td>
&lt;td>80.5&lt;/td>
&lt;td>74.3&lt;/td>
&lt;td>77.6&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GPQA Diamond&lt;/td>
&lt;td>73.7&lt;/td>
&lt;td>69.8&lt;/td>
&lt;td>57.2&lt;/td>
&lt;td>60.1&lt;/td>
&lt;td>53.6&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h1 id="结论">&lt;a href="#%e7%bb%93%e8%ae%ba" class="header-anchor">&lt;/a>结论
&lt;/h1>&lt;p>LLaMA 4 采用了MoE架构，是一个原生的多模态大模型系列。在架构上，与DeepSeek-MoE, aria和OLMoE不同，LLaMA4并没有增加expert granularity，&lt;a class="link" href="https://maosong.website/p/notes-on-olmoe/" target="_blank" rel="noopener"
>OLMoE&lt;/a>分析认为，增加granularity可以提高模型的flexibility，
下面总结了一下相关模型的参数&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Layers&lt;/th>
&lt;th>Heads (Q / KV)&lt;/th>
&lt;th>Context Length&lt;/th>
&lt;th>#Parameters (activated/total)&lt;/th>
&lt;th>#Experts (activated/total)&lt;/th>
&lt;th>#Tokens&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>DeepSeek-MoE(144.6B)&lt;/td>
&lt;td>62&lt;/td>
&lt;td>32/32&lt;/td>
&lt;td>2048&lt;/td>
&lt;td>22.2B/144.6B&lt;/td>
&lt;td>(1 shared + 7 routed)/64&lt;/td>
&lt;td>245B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DeepSeek-V3&lt;/td>
&lt;td>61&lt;/td>
&lt;td>128(MLA)&lt;/td>
&lt;td>128K&lt;/td>
&lt;td>37B/671B&lt;/td>
&lt;td>(1 shared + 8 routed)/257&lt;/td>
&lt;td>14.8T&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Aria&lt;/td>
&lt;td>28&lt;/td>
&lt;td>20/20&lt;/td>
&lt;td>64K&lt;/td>
&lt;td>3.5B/24.9B&lt;/td>
&lt;td>(2 shared+ 6 routed)/66&lt;/td>
&lt;td>6.4T(text)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OLMoE&lt;/td>
&lt;td>16&lt;/td>
&lt;td>16/16&lt;/td>
&lt;td>4096&lt;/td>
&lt;td>1.3B/6.9B&lt;/td>
&lt;td>8/64&lt;/td>
&lt;td>5T&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LLaMA 4 Maverick&lt;/td>
&lt;td>48&lt;/td>
&lt;td>40/8&lt;/td>
&lt;td>1M&lt;/td>
&lt;td>17B / 109B&lt;/td>
&lt;td>(1 shared + 1 routed) / 128&lt;/td>
&lt;td>~22T&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LLaMA 4 Scout&lt;/td>
&lt;td>48&lt;/td>
&lt;td>40/8&lt;/td>
&lt;td>10M&lt;/td>
&lt;td>17B / 400B&lt;/td>
&lt;td>(1 shared + 1 routed) / 16&lt;/td>
&lt;td>~40T&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h1 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md" target="_blank" rel="noopener"
>LLaMA 4 Model Card&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/" target="_blank" rel="noopener"
>LLaMA 4 Blog&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Qwen3 blog</title><link>https://maosong.website/p/notes-on-qwen3-blog/</link><pubDate>Tue, 29 Apr 2025 11:23:04 +0800</pubDate><guid>https://maosong.website/p/notes-on-qwen3-blog/</guid><description>&lt;h1 id="介绍">&lt;a href="#%e4%bb%8b%e7%bb%8d" class="header-anchor">&lt;/a>介绍
&lt;/h1>&lt;p>Qwen3发布，包含两种架构的模型，每种架构均包含对应的base model和post-trained model.&lt;/p>
&lt;p>&lt;strong>MoE架构&lt;/strong>：上下文长度为128K，128个专家，每个token由8个专家负责处理&lt;/p>
&lt;ul>
&lt;li>Qwen3-235B-A22B, 总参数235B，激活参数22B，&lt;/li>
&lt;li>Qwen3-30B-A3B, 总参数30B，激活参数3B&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Models&lt;/th>
&lt;th>Layers&lt;/th>
&lt;th>Heads (Q / KV)&lt;/th>
&lt;th># Experts (Total / Activated)&lt;/th>
&lt;th>Context Length&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Qwen3-30B-A3B&lt;/td>
&lt;td>48&lt;/td>
&lt;td>32 / 4&lt;/td>
&lt;td>128 / 8&lt;/td>
&lt;td>128K&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen3-235B-A22B&lt;/td>
&lt;td>94&lt;/td>
&lt;td>64 / 4&lt;/td>
&lt;td>128 / 8&lt;/td>
&lt;td>128K&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>dense架构&lt;/strong>: Qwen3-32B, Qwen3-14B, Qwen3-8B, Qwen3-4B, Qwen3-1.7B, and Qwen3-0.6B&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Models&lt;/th>
&lt;th>Layers&lt;/th>
&lt;th>Heads (Q / KV)&lt;/th>
&lt;th>Tie Embedding&lt;/th>
&lt;th>Context Length&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Qwen3-0.6B&lt;/td>
&lt;td>28&lt;/td>
&lt;td>16 / 8&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>32K&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen3-1.7B&lt;/td>
&lt;td>28&lt;/td>
&lt;td>16 / 8&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>32K&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen3-4B&lt;/td>
&lt;td>36&lt;/td>
&lt;td>32 / 8&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>32K&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen3-8B&lt;/td>
&lt;td>36&lt;/td>
&lt;td>32 / 8&lt;/td>
&lt;td>No&lt;/td>
&lt;td>128K&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen3-14B&lt;/td>
&lt;td>40&lt;/td>
&lt;td>40 / 8&lt;/td>
&lt;td>No&lt;/td>
&lt;td>128K&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen3-32B&lt;/td>
&lt;td>64&lt;/td>
&lt;td>64 / 8&lt;/td>
&lt;td>No&lt;/td>
&lt;td>128K&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h1 id="亮点">&lt;a href="#%e4%ba%ae%e7%82%b9" class="header-anchor">&lt;/a>亮点
&lt;/h1>&lt;ol>
&lt;li>Hybrid Thinking modes
qwen3支持两种思考模式： thinking mode 和 non-thinking mode，前者用于解决复杂的问题，后者用于解决简单的问题&lt;/li>
&lt;li>multilingual support
qwen3支持119中语言和方言&lt;/li>
&lt;li>Improved agentic capabilities
提升了qwen3的coding和agentic能力，并支持MCP&lt;/li>
&lt;/ol>
&lt;h1 id="训练">&lt;a href="#%e8%ae%ad%e7%bb%83" class="header-anchor">&lt;/a>训练
&lt;/h1>&lt;h2 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h2>&lt;p>Qwen3使用了36T token进行训练 （与之相比，Qwen2.5使用方的token数量为18T），数据来源于互联网和PDF，作者使用Qwen2.5-VL来提取内容，然后使用Qwen2.5来提升内容质量。作者还基于Qwen2.5-Math和Qwen2.5-Coder来合成数学以及代码数据&lt;/p>
&lt;p>训练包含三个stage：&lt;/p>
&lt;ol>
&lt;li>上下文长度为4K tokens，训练数据为30T tokens, 目标是让模型掌握初步的语言能力和知识&lt;/li>
&lt;li>上下文长度为4K tokens，训练数据为5T tokens,这部分数据主要是knowledge intensive的数据，比如STEM, coding和reasoning等&lt;/li>
&lt;li>上下文长度扩展到32K tokens，训练数据主要是高质量长上下文的数据&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen3-blog/qwen3-base.jpg"
width="1554"
height="1058"
loading="lazy"
alt="performance of qwen3 base"
class="gallery-image"
data-flex-grow="146"
data-flex-basis="352px"
>&lt;/p>
&lt;h2 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h2>&lt;p>Post-training包含四个阶段，如下图所示
&lt;img src="https://maosong.website/p/notes-on-qwen3-blog/post-training.png"
width="4143"
height="1640"
loading="lazy"
alt="post training of qwen3"
class="gallery-image"
data-flex-grow="252"
data-flex-basis="606px"
>&lt;/p>
&lt;ul>
&lt;li>Stage 1, Long CoT code start: 作者基于math, coding, logical reasoning, STEM等domain的Long CoT数据来微调模型，让模型拥有初步的推理能力，这和Kimi-VL是类似的&lt;/li>
&lt;li>Stage 2, reasoning-based RL: 作者使用rule-based rewards来提供奖励，然后使用RL来训练模型，经一部提高模型的exploration和exploitation能力&lt;/li>
&lt;li>Stage 3, thinking mode fusion: 作者混合了一部分instruction-following和Long CoT数据来提升模型的non-thinking能力，这样可以让模型在两种思考模式之间切换&lt;/li>
&lt;li>Stage 4,general RL： 作者使用RL在20多个general-domain任务上进一步提高模型的通用能力，包括instruction following, format following以及agent capability等&lt;/li>
&lt;/ul>
&lt;h1 id="future-work">&lt;a href="#future-work" class="header-anchor">&lt;/a>Future work
&lt;/h1>&lt;p>作者希望在未来能够在模型架构和训练方式上进行提升，包括：scaling data, increasing model size, extending context length, broadening modalities, advancing RL with environmental feedback for long-horizon reasoning.&lt;/p>
&lt;h1 id="结论">&lt;a href="#%e7%bb%93%e8%ae%ba" class="header-anchor">&lt;/a>结论
&lt;/h1>&lt;p>与Gemini2.5 pro，Kimi-VL等reaosning model不同，qwen3可以在快思考和慢思考之间进行转换。感觉未来有两个趋势，一个是如何在快思考和慢思考之间进行切换，切换的逻辑是什么？第二个就是qwen3以及qwen2.5-vl都在强调的agent能力，也就是我们不仅仅是在做一个LLM，而是逐步延伸到了agent这个层面。&lt;/p>
&lt;h1 id="参考文献">&lt;a href="#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae" class="header-anchor">&lt;/a>参考文献
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://qwenlm.github.io/blog/qwen3/" target="_blank" rel="noopener"
>blog&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Data mixture in MLLM</title><link>https://maosong.website/p/data-mixture-in-mllm/</link><pubDate>Fri, 25 Apr 2025 10:25:48 +0800</pubDate><guid>https://maosong.website/p/data-mixture-in-mllm/</guid><description>&lt;h1 id="介绍">&lt;a href="#%e4%bb%8b%e7%bb%8d" class="header-anchor">&lt;/a>介绍
&lt;/h1>&lt;p>简单总结一下已有的介绍了训练数据配比的多模态大模型，方便后续使用。
根据之前看的一些论文进行总结，如有缺漏，欢迎批评指正。&lt;/p>
&lt;h1 id="相关工作">&lt;a href="#%e7%9b%b8%e5%85%b3%e5%b7%a5%e4%bd%9c" class="header-anchor">&lt;/a>相关工作
&lt;/h1>&lt;h2 id="llava">&lt;a href="#llava" class="header-anchor">&lt;/a>LLaVA
&lt;/h2>&lt;p>&lt;a class="link" href="https://arxiv.org/abs/2310.03744" target="_blank" rel="noopener"
>LLaVA 1.5&lt;/a> SFT数据配比如下：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/data-mixture-in-mllm/llava_v1_5_sft_mixture.png"
width="693"
height="569"
loading="lazy"
alt="LLaVA 1.5 SFT data mixture"
class="gallery-image"
data-flex-grow="121"
data-flex-basis="292px"
>&lt;/p>
&lt;h2 id="llava-onevision">&lt;a href="#llava-onevision" class="header-anchor">&lt;/a>LLaVA-OneVision
&lt;/h2>&lt;p>&lt;a class="link" href="http://arxiv.org/abs/2408.03326" target="_blank" rel="noopener"
>LLaVA OneVision&lt;/a> 的数据集统计在表16里，这里总结一下数据配比(总量为3.15M)&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">Category&lt;/th>
&lt;th style="text-align: right">Ratio&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">general vqa&lt;/td>
&lt;td style="text-align: right">36.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Doc/Chart/Screen&lt;/td>
&lt;td style="text-align: right">20.6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Math/Reasoning&lt;/td>
&lt;td style="text-align: right">20.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">general OCR&lt;/td>
&lt;td style="text-align: right">8.9&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">text only&lt;/td>
&lt;td style="text-align: right">14.3&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="apollo">&lt;a href="#apollo" class="header-anchor">&lt;/a>Apollo
&lt;/h2>&lt;p>&lt;a class="link" href="http://arxiv.org/abs/2412.10360" target="_blank" rel="noopener"
>Apollo&lt;/a> 是Meta发布的一个视频理解多模态大模型，其数据集没有开源，论文中给出了其训练数据配比
&lt;img src="https://maosong.website/p/data-mixture-in-mllm/apollo.png"
width="1389"
height="589"
loading="lazy"
alt="apollo data mixture"
class="gallery-image"
data-flex-grow="235"
data-flex-basis="565px"
>&lt;/p>
&lt;h2 id="cambiran-1">&lt;a href="#cambiran-1" class="header-anchor">&lt;/a>Cambiran-1
&lt;/h2>&lt;p>&lt;a class="link" href="http://arxiv.org/abs/2406.16860" target="_blank" rel="noopener"
>Cambiran-1&lt;/a> 是纽约大学提出了一个多模态大模型系列，论文发表在了 NeurIPS 2024(Oral) 上，作者给出了 Cambrain-10M 和 Cambrain-7M 两个数据集，Cambrain-7M 的数据分布如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/data-mixture-in-mllm/cambrain-7M.png"
width="1294"
height="513"
loading="lazy"
alt="Cambrain-7M"
class="gallery-image"
data-flex-grow="252"
data-flex-basis="605px"
>&lt;/p>
&lt;h2 id="idefics">&lt;a href="#idefics" class="header-anchor">&lt;/a>Idefics
&lt;/h2>&lt;p>Idefics系列(1/2/3)是huggingface提出的视觉多模态大模型系列，在 &lt;a class="link" href="https://arxiv.org/abs/2405.02246" target="_blank" rel="noopener"
>Idefics2&lt;/a> 中，作者构建了The Cauldron数据集，其数据配比在表14里面。总结如下：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">Category&lt;/th>
&lt;th style="text-align: right">Ratio&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">general vqa&lt;/td>
&lt;td style="text-align: right">11.02&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">captioning&lt;/td>
&lt;td style="text-align: right">5.14&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">OCR&lt;/td>
&lt;td style="text-align: right">17.47&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">chart/figures&lt;/td>
&lt;td style="text-align: right">14.05&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">table&lt;/td>
&lt;td style="text-align: right">11.3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">reasoning&lt;/td>
&lt;td style="text-align: right">10.32&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">textbook&lt;/td>
&lt;td style="text-align: right">1.58&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">difference&lt;/td>
&lt;td style="text-align: right">2.38&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">screenshot2code&lt;/td>
&lt;td style="text-align: right">0.31&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">text only&lt;/td>
&lt;td style="text-align: right">26.41&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>在 &lt;a class="link" href="http://arxiv.org/abs/2408.12637" target="_blank" rel="noopener"
>Idefics3&lt;/a> 中，作者基于The Cauldron进行了扩展，最终数据集的比例如下：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/data-mixture-in-mllm/mixture_the_cauldron.png"
width="3430"
height="886"
loading="lazy"
alt="data mixture of SmolVLM blog"
class="gallery-image"
data-flex-grow="387"
data-flex-basis="929px"
>&lt;/p>
&lt;h2 id="molmo">&lt;a href="#molmo" class="header-anchor">&lt;/a>Molmo
&lt;/h2>&lt;p>&lt;a class="link" href="http://arxiv.org/abs/2409.17146" target="_blank" rel="noopener"
>Molmo&lt;/a> 是Allen AI发布的一个多模态大模型，其SFT数据配比如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/data-mixture-in-mllm/molmo_sft_data_mixture.png"
width="693"
height="785"
loading="lazy"
alt="SFT data mixture of Molmo"
class="gallery-image"
data-flex-grow="88"
data-flex-basis="211px"
>&lt;/p>
&lt;h2 id="eagle-225">&lt;a href="#eagle-225" class="header-anchor">&lt;/a>Eagle 2/2.5
&lt;/h2>&lt;p>Eagle (1/2/2.5) 是NVLab提出了系列多模态大模型，&lt;a class="link" href="http://arxiv.org/abs/2501.14818" target="_blank" rel="noopener"
>Eagle 2&lt;/a> 给出了 stage 1.5 和 stage 2的数据配比&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/data-mixture-in-mllm/eagle2_data_mixture.png"
width="693"
height="400"
loading="lazy"
alt="Eagle 2 data mixture"
class="gallery-image"
data-flex-grow="173"
data-flex-basis="415px"
>&lt;/p>
&lt;p>&lt;a class="link" href="http://arxiv.org/abs/2504.15271" target="_blank" rel="noopener"
>Eagle 2.5&lt;/a> 在 Eagle 2的基础上加入了一些 long context 数据，其数据列表在表11里&lt;/p>
&lt;h2 id="smolvlm">&lt;a href="#smolvlm" class="header-anchor">&lt;/a>SmolVLM
&lt;/h2>&lt;p>&lt;a class="link" href="http://arxiv.org/abs/2504.05299" target="_blank" rel="noopener"
>SmolVLM&lt;/a> 是huggingface开发的一款轻量化视觉多模态大模型，论文中的数据配比如下：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/data-mixture-in-mllm/smol_vlm_data_mixture.png"
width="1382"
height="560"
loading="lazy"
alt="data mixture of SmolVLM paper"
class="gallery-image"
data-flex-grow="246"
data-flex-basis="592px"
>&lt;/p>
&lt;h2 id="mm115">&lt;a href="#mm115" class="header-anchor">&lt;/a>MM1/1.5
&lt;/h2>&lt;p>&lt;a class="link" href="http://arxiv.org/abs/2403.09611" target="_blank" rel="noopener"
>MM1&lt;/a> 通过实验确定了训练数据的配比：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">Category&lt;/th>
&lt;th style="text-align: right">Ratio&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">interleaved image-text&lt;/td>
&lt;td style="text-align: right">45&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">imagetext&lt;/td>
&lt;td style="text-align: right">45&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">text only&lt;/td>
&lt;td style="text-align: right">10&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;a class="link" href="http://arxiv.org/abs/2409.20566" target="_blank" rel="noopener"
>MM1.5&lt;/a> 的SFT数据配比如下：
&lt;img src="https://maosong.website/p/data-mixture-in-mllm/MM1_5_data_mixture.png"
width="1183"
height="845"
loading="lazy"
alt="MM1.5 SFT data mixture"
class="gallery-image"
data-flex-grow="140"
data-flex-basis="336px"
>&lt;/p>
&lt;h2 id="internvl">&lt;a href="#internvl" class="header-anchor">&lt;/a>InternVL
&lt;/h2>&lt;p>&lt;a class="link" href="http://arxiv.org/abs/2412.05271" target="_blank" rel="noopener"
>InternVL2.5&lt;/a> 在论文里总结了其使用的pretraining数据和SFT数据，但是没有具体的数据配比，请参考论文的表4和表5&lt;/p>
&lt;p>SFT数据配比&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">Category&lt;/th>
&lt;th style="text-align: left">Ratio&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">single-image&lt;/td>
&lt;td style="text-align: left">45.92%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">multi-image&lt;/td>
&lt;td style="text-align: left">9.37%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">video&lt;/td>
&lt;td style="text-align: left">39.79%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">pure-text&lt;/td>
&lt;td style="text-align: left">4.92%&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="minicpm-v">&lt;a href="#minicpm-v" class="header-anchor">&lt;/a>MiniCPM V
&lt;/h2>&lt;p>&lt;a class="link" href="http://arxiv.org/abs/2408.01800" target="_blank" rel="noopener"
>MiniCPM V&lt;/a> 是 OpenBMB发布的轻量化多模态大模型，其在表1和表2列出了ptraining和SFT数据的具体量级和类别。&lt;/p>
&lt;h2 id="flash-vl">&lt;a href="#flash-vl" class="header-anchor">&lt;/a>Flash-VL
&lt;/h2>&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">Category&lt;/th>
&lt;th style="text-align: right">Ratio&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">Special Enhancement&lt;/td>
&lt;td style="text-align: right">4%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Text&lt;/td>
&lt;td style="text-align: right">21%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Caption&lt;/td>
&lt;td style="text-align: right">4%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Chart&lt;/td>
&lt;td style="text-align: right">16%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Math&lt;/td>
&lt;td style="text-align: right">11%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">OCR&lt;/td>
&lt;td style="text-align: right">3%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Code&lt;/td>
&lt;td style="text-align: right">8%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">General&lt;/td>
&lt;td style="text-align: right">33%&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>具体数据参见&lt;a class="link" href="http://arxiv.org/abs/2505.09498" target="_blank" rel="noopener"
>原论文&lt;/a>&lt;/p>
&lt;h1 id="参考文献">&lt;a href="#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae" class="header-anchor">&lt;/a>参考文献
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2310.03744" target="_blank" rel="noopener"
>LLaVA 1.5&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2408.03326" target="_blank" rel="noopener"
>LLaVA OneVision&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2412.10360" target="_blank" rel="noopener"
>Apollo&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2406.16860" target="_blank" rel="noopener"
>Cambiran-1&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2405.02246" target="_blank" rel="noopener"
>Idefics2&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2408.12637" target="_blank" rel="noopener"
>Idefics3&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2409.17146" target="_blank" rel="noopener"
>Molmo&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2501.14818" target="_blank" rel="noopener"
>Eagle 2&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2504.15271" target="_blank" rel="noopener"
>Eagle 2.5&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2504.05299" target="_blank" rel="noopener"
>SmolVLM&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2403.09611" target="_blank" rel="noopener"
>MM1&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2409.20566" target="_blank" rel="noopener"
>MM1.5&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2412.05271" target="_blank" rel="noopener"
>InternVL2.5&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2408.01800" target="_blank" rel="noopener"
>MiniCPM V&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>随笔-身体健康</title><link>https://maosong.website/p/%E9%9A%8F%E7%AC%94-%E8%BA%AB%E4%BD%93%E5%81%A5%E5%BA%B7/</link><pubDate>Wed, 23 Apr 2025 13:24:02 +0800</pubDate><guid>https://maosong.website/p/%E9%9A%8F%E7%AC%94-%E8%BA%AB%E4%BD%93%E5%81%A5%E5%BA%B7/</guid><description>&lt;p>上大学以及读研的时候，从没生过病，女朋友还说我身体素质强大。工作之后，先是新冠，然后轻度脂肪肝，脂肪肝，今年体检又有颈椎曲度变直，腰椎间盘突出一系列问题。说实话，这些毛病说大也不算大，工作久了都有类似的问题。但是前两天因为睡觉着凉感冒了两天，就忽然觉得年少时以为身体强健，不过是身体在替你硬抗。现在工作之后，久坐，不健康饮食，熬夜等坏习惯就开始了反击。都说人类理想的寿命不过25岁，25岁你应该已经完成结婚生子，可以安享晚年了。而也是在这个年龄段，身体开始摆烂，让你自己硬抗。现在来看，果然如此。&lt;/p>
&lt;p>有时候，还在那里想，人要完成一个什么样的目标才算是成功呢，是发很多顶会成为学术大佬，还是赚很多钱，实现人生自由。现在来看，身体健康，家庭美满，有一份说得过去的工作就已经很好了，不要好高骛远。人可以向前看，向前走，但是不能不看自己的身体状况。&lt;/p>
&lt;p>想起王立群老师的话，“人的一生就像一场马拉松，只要你在路上，就还有机会”。但是现在来看，看着身边那么多优秀的人，很容易就把马拉松跑成百米赛跑，没跑过人家不说，百米过了，还浑身酸痛。&lt;/p>
&lt;p>又想起高职务的那句话，“有时候想想这官当多大才叫大啊”。现在看，人也是这样啊，赚多少的钱才算人生自由，发多少的论文才算成功。向上看，总是会给自己无形的压力，让自己陷入无尽的焦虑之中。&lt;/p>
&lt;p>我觉得我已经算是比较豁达的人了，只会跟自己比，就算这样也还是会有一些焦虑。我的焦虑主要是来自于自己啥都没做，就已经周三了。周日晚上着凉，周一周二脑子就完全宕机，一周的计划直接泡汤一半。这种计划被打乱的挫败感对于T人来说确实难以接受。而细细一想，学生时代对于时间真的没什么概念，每天就瞎搞，没有目标，反而时间就过得慢。现在则是，一低头一抬头可能两个小时就过去了。难说这是年龄大了之后，对时间的迟钝性，还是工作之后，磨平了人心中的激情。&lt;/p>
&lt;p>最近开始了健身，希望能花半年时间降低体脂，我越来越感觉到身体健康的重要性，很多东西失去了才真正懂得拥有是多么难得，感冒鼻塞了才知道呼吸到空气不是那么一件简单的事情。摔伤之后才知道洗澡也可能是奢望。&lt;/p>
&lt;p>在明朝那些事的结尾，当年明月没有提到书中的大人物，而是以徐霞客收尾，“所谓百年功名、千秋霸业、万古流芳，与一件事情相比，其实算不了什么。这件事情就是——用你喜欢的方式度过一生。”希望大家都有健康的体魄，毕竟身体是革命的本钱。与诸君共勉。&lt;/p></description></item><item><title>Notes on VAPO</title><link>https://maosong.website/p/notes-on-vapo/</link><pubDate>Thu, 17 Apr 2025 09:41:51 +0800</pubDate><guid>https://maosong.website/p/notes-on-vapo/</guid><description>&lt;h1 id="abstract">&lt;a href="#abstract" class="header-anchor">&lt;/a>Abstract
&lt;/h1>&lt;p>字节Seed团队提出了 Value-based Augmented Proximal Policy Optimization (VAPO), 用于提高 reasoning model 的表现。 VAPO 通过集成 DAPO 和 VC-PPO 的优点，进一步提高了 value-based 方法的表现。&lt;/p>
&lt;h1 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h1>&lt;p>现有的RL 训练方法可以分为 value-free 和 value-based 两大类。
其中 value-free 方法不需要使用 value model, 比如 GRPO 和 GRPO 的变体 DAPO, 这类方法通过多次采样，然后使用 leave-one-out estimate 来代替 value model. 这类方法的优点是不需要训练value model, 但是缺点是在复杂的任务中表现不是很稳定。&lt;/p>
&lt;p>另一方面，value-based 方法需要训练一个 value model, 比如 &lt;a class="link" href="https://maosong.website/p/notes-on-vc-ppo/" target="_blank" rel="noopener"
>VC-PPO&lt;/a>, 这类方法的优点是：&lt;/p>
&lt;ol>
&lt;li>提供更细粒度的奖励信号&lt;/li>
&lt;li>提供lower-varaince value estimation, 从而提高训练的稳定性&lt;/li>
&lt;li>拥有更好的泛化能力&lt;/li>
&lt;/ol>
&lt;p>但是，value-based 方法在训练过程中存在一些问题：&lt;/p>
&lt;ol>
&lt;li>训练一个low-bias 的 value model 比较困难， 尤其是在long trajectory 上，因为 bias 会随着 trajectory 的长度增加而增加&lt;/li>
&lt;li>在heterogeneous sequence lengths during training 中表现不佳，对于短文本和长文本，我们需要考虑 bias-variance 的trade-off&lt;/li>
&lt;li>在sparse reward signal 中表现不佳&lt;/li>
&lt;/ol>
&lt;p>为了解决这些问题，字节Seed团队提出了 VAPO, 一个基于value-based 的 RL 训练方法，VAPO 通过结合 DAPO 和 &lt;a class="link" href="https://maosong.website/p/notes-on-vc-ppo/" target="_blank" rel="noopener"
>VC-PPO&lt;/a> 的优点，进一步提高了 value-based 方法的表现。&lt;/p>
&lt;h1 id="preliminary">&lt;a href="#preliminary" class="header-anchor">&lt;/a>Preliminary
&lt;/h1>&lt;p>Preliminary包括 token-level MDP, RLHF, PPO 三个部分，这部分请参考 &lt;a class="link" href="https://maosong.website/p/notes-on-vc-ppo/" target="_blank" rel="noopener"
>VC-PPO&lt;/a>. 这里不做重复。&lt;/p>
&lt;h1 id="vapo">&lt;a href="#vapo" class="header-anchor">&lt;/a>VAPO
&lt;/h1>&lt;p>作者针对value-based 方法在训练过程中存在的三个问题，在VAPO中分别进行了解决。&lt;/p>
&lt;h2 id="mitigating-value-model-bias-over-long-sequences">&lt;a href="#mitigating-value-model-bias-over-long-sequences" class="header-anchor">&lt;/a>Mitigating Value Model Bias over Long Sequences
&lt;/h2>&lt;p>在 &lt;a class="link" href="https://maosong.website/p/notes-on-vc-ppo/" target="_blank" rel="noopener"
>VC-PPO&lt;/a> 中，作者提到了 value model 和 reward model 的不一致性，这种不一致性会导致 bias, 尤其是在long sequences上。 在VAPO中，作者就直接使用了 VC-PPO的做法，包括value pretraining 和 decoupled GAE 来解决这个问题。&lt;/p>
&lt;h2 id="managing-heterogeneous-sequence-lengths-during-training">&lt;a href="#managing-heterogeneous-sequence-lengths-during-training" class="header-anchor">&lt;/a>Managing Heterogeneous Sequence Lengths during Training
&lt;/h2>&lt;p>针对heterogeneous sequence的问题，作者提出了 &lt;strong>Length-Adaptive GAE&lt;/strong>. 在&lt;a class="link" href="https://maosong.website/p/notes-on-vc-ppo/" target="_blank" rel="noopener"
>VC-PPO&lt;/a>中， $\lambda_{\mathrm{policy}}$ 被设置为 $0.95$. 但是当 sequence 非常长时， TD-error会迅速下降，导致GAE被一部分TD-error所主导，从而不利于模型的训练。&lt;/p>
&lt;p>为了解决这个问题，作者将 $\lambda_{\mathrm{policy}}$ 与 sequence 的长度 $\ell$ 联系起来，具体来说， 两者的关系如下：&lt;/p>
$$
\sum_{t=0}^{\infty}\lambda_{\mathrm{policy}}^t = \frac{1}{1-\lambda_{\mathrm{policy}}} := \alpha\ell
$$&lt;p>其中 $\alpha$ 是一个超参数，用来控制 bias-variance 的trade-off. 给定 $\ell$, $\lambda_{\mathrm{policy}}$ 可以被计算为：&lt;/p>
$$
\lambda_{\mathrm{policy}} = 1 - \frac{1}{\alpha\ell}
$$&lt;p>同时，为了平衡短文本和长文本的贡献，基于 &lt;a class="link" href="https://maosong.website/p/notes-on-dapo/" target="_blank" rel="noopener"
>DAPO&lt;/a>, 作者构建了 token-level policy gradient loss， 其具体形式如下：&lt;/p>
$$
\mathcal{L}_{\mathrm{PPO}}(\theta) = \frac{1}{\sum_{t=1}^G|o_i|}\sum_{i=1}^G\sum_{t=1}^{|o_i|}\min\left(r_{i,t}(\theta)\hat{A}_{i,t},\mathrm{clip}\left(r_{i,t}(\theta), 1-\epsilon, 1+\epsilon\right)\hat{A}_{i,t}\right)
$$&lt;h2 id="dealing-with-sparsity-of-reward-signal-in-verifier-based-tasks">&lt;a href="#dealing-with-sparsity-of-reward-signal-in-verifier-based-tasks" class="header-anchor">&lt;/a>Dealing with Sparsity of Reward Signal in Verifier-based Tasks
&lt;/h2>&lt;p>与DAPO一致，为了解决reward signal的稀疏性问题，作者提出了Clip-Higher, 来让更小概率的输出也能获得较大的更新概率。其更新公式如下：&lt;/p>
$$
\mathcal{L}_{\mathrm{PPO}}(\theta) = \frac{1}{\sum_{t=1}^G|o_i|}\sum_{i=1}^G\sum_{t=1}^{|o_i|}\min\left(r_{i,t}(\theta)\hat{A}_{i,t},\mathrm{clip}\left(r_{i,t}(\theta), 1-\epsilon_{low}, 1+\epsilon_{high}\right)\hat{A}_{i,t}\right)
$$&lt;p>Clip-Higher的介绍见&lt;a class="link" href="https://maosong.website/p/notes-on-dapo/" target="_blank" rel="noopener"
>DAPO&lt;/a>&lt;/p>
&lt;p>然后，作者还将next-token prediction loss 和 PPO loss 结合起来，来降低奖励的稀疏程度。&lt;/p>
$$
\mathcal{L}_{\mathrm{NTP}}(\theta) = -\frac{1}{N}\sum_{o_i\in\mathcal{T}}\sum_{t=1}^{|o_i|}\log \pi_{\theta}(a_{i,t}|s_{i,t})
$$&lt;p>其中 $\mathcal{T}$ 是正确答案的集合。 最终的loss为：&lt;/p>
$$
\mathcal{L}_{\mathrm{VAPO}}(\theta) = \mathcal{L}_{\mathrm{PPO}}(\theta) +\mu \mathcal{L}_{\mathrm{NTP}}(\theta)
$$&lt;p>其中 $\mu$ 是一个超参数，用来平衡PPO loss和NTP loss。&lt;/p>
&lt;h1 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h1>&lt;p>模型使用Qwen-32B来进行训练, 大部分细节与&lt;a class="link" href="https://maosong.website/p/notes-on-vc-ppo/" target="_blank" rel="noopener"
>VC-PPO&lt;/a>和DAPO一致，这里不再赘述。最后与DAPO以及R1的对比结果如下：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-vapo/performance.png"
width="1396"
height="600"
loading="lazy"
alt="performance"
class="gallery-image"
data-flex-grow="232"
data-flex-basis="558px"
>&lt;/p>
&lt;h2 id="ablation-study">&lt;a href="#ablation-study" class="header-anchor">&lt;/a>Ablation study
&lt;/h2>&lt;p>针对本文使用的模块，作者进行了消融实验，结果如下：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-vapo/ablation_results.png"
width="768"
height="547"
loading="lazy"
alt="ablation study"
class="gallery-image"
data-flex-grow="140"
data-flex-basis="336px"
>&lt;/p>
&lt;p>从实验结果可以看到：&lt;/p>
&lt;ol>
&lt;li>value pretraining 和 decoupled GAE 可以显著提高模型的表现&lt;/li>
&lt;li>clip-higer 可以提升模型的探索能力&lt;/li>
&lt;li>length-adaptive GAE 可以平衡模型在短文本和长文本上的表现&lt;/li>
&lt;/ol>
&lt;h2 id="training-dynamics">&lt;a href="#training-dynamics" class="header-anchor">&lt;/a>Training Dynamics
&lt;/h2>&lt;p>与DAPO类似，作者也分析了VAPO的训练动态，结果如下：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-vapo/mean_response_length.png"
width="623"
height="463"
loading="lazy"
alt="Mean Response Length"
class="gallery-image"
data-flex-grow="134"
data-flex-basis="322px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-vapo/reward_score.png"
width="620"
height="466"
loading="lazy"
alt="reward score"
class="gallery-image"
data-flex-grow="133"
data-flex-basis="319px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-vapo/generation_entropy.png"
width="634"
height="472"
loading="lazy"
alt="generation entropy"
class="gallery-image"
data-flex-grow="134"
data-flex-basis="322px"
>&lt;/p>
&lt;p>从上面三张图可以看到：&lt;/p>
&lt;ol>
&lt;li>VAPO相比于DAPO来说，其训练更加稳定&lt;/li>
&lt;li>从response length来看，VAPO的response length更长，说明VAPO的length scaling更强&lt;/li>
&lt;li>从reward score来看，VAPO的reward score更高，说明VAPO的reward signal提供的指导信息更多&lt;/li>
&lt;li>从generation entropy来看，在训练末期，VAPO的generation entropy更低，说明VAPO的生成多样性要更低一些，但这也说明了VAPO的生成更加稳定。 作者认为这个时候稳定性更重要。&lt;/li>
&lt;/ol>
&lt;h1 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h1>&lt;p>作者在DAPO和VC-PPO的基础上，提出了VAPO，一个基于value-based 的 RL 训练方法，VAPO 通过结合 DAPO 和 &lt;a class="link" href="https://maosong.website/p/notes-on-vc-ppo/" target="_blank" rel="noopener"
>VC-PPO&lt;/a> 的优点，进一步提高了 value-based 方法的表现。 后续，作者又提出了Seed-thiking-1.5的技术报告。可以说，这一系列论文的连贯性是非常高的。&lt;/p>
&lt;h1 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h1>&lt;ol>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2504.05118" target="_blank" rel="noopener"
>Arxiv VAPO: Efficient and Reliable Reinforcement Learning for Advanced Reasoning Tasks&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://maosong.website/p/notes-on-dapo/" target="_blank" rel="noopener"
>Notes onDAPO&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://maosong.website/p/notes-on-vc-ppo/" target="_blank" rel="noopener"
>Notes on VC-PPO&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>Notes on VC-PPO</title><link>https://maosong.website/p/notes-on-vc-ppo/</link><pubDate>Mon, 14 Apr 2025 17:36:15 +0800</pubDate><guid>https://maosong.website/p/notes-on-vc-ppo/</guid><description>&lt;h1 id="abstract">&lt;a href="#abstract" class="header-anchor">&lt;/a>Abstract
&lt;/h1>&lt;p>字节Seed团队提出了 Value-Calibrated PPO (VC-PPO), 用于解决PPO的value initialization bias 以及 reward signal decay 问题. 具体来讲：&lt;/p>
&lt;ol>
&lt;li>VC-PPO增加了 value pretraining 来解决 value initialization bias 的问题&lt;/li>
&lt;li>VC-PPO分离了 actor 和 critic 的GAE的计算，避免了 reward signal decay 的问题&lt;/li>
&lt;/ol>
&lt;h1 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h1>&lt;p>已有的reasoning model的训练方法，主要包括两个个stage:&lt;/p>
&lt;ol>
&lt;li>SFT: 这个阶段主要使用了一些标注好的long CoT数据，初步激活模型的reasoning能力(参考KImi-VL)&lt;/li>
&lt;li>RL: 这个阶段使用收集到的数据使用RL算法进行训练，任务包括math, code, logic reasoning等&lt;/li>
&lt;/ol>
&lt;p>已有PPO算法在处理Long CoT任务时，存在的问题在 &lt;a class="link" href="https://maosong.website/p/notes-on-dapo/" target="_blank" rel="noopener"
>DAPO&lt;/a> 中已经介绍过了，GRPO的解决方式为
使用leave-one-out estimate来替换value model. 但是GRPO相比于PPO能够提供token级别的奖励来说，只能提供response level的奖励，因此限制了模型的性能。&lt;/p>
&lt;h1 id="preliminary">&lt;a href="#preliminary" class="header-anchor">&lt;/a>Preliminary
&lt;/h1>&lt;p>Preliminary包括MDP, RLHF, PPO三个部分，RLHF和PPO我们在 &lt;a class="link" href="https://maosong.website/p/notes-on-dapo/" target="_blank" rel="noopener"
>DAPO&lt;/a> 中已经介绍过了，这里不再赘述。&lt;/p>
&lt;h2 id="token-level-mdp">&lt;a href="#token-level-mdp" class="header-anchor">&lt;/a>Token-level MDP
&lt;/h2>&lt;p>给定prompt $x$ 和 response $y$, 我们可以将 $x$ 和 $y$ 分解为 token 序列，比如 $y = y_1, y_2, \cdots, y_n$, 其中 $y_i\in\mathcal{A}$, $\mathcal{A}$ 是我们的词表。&lt;/p>
&lt;p>我们将 token-level MDP定义为：&lt;/p>
$$
\mathcal{M} = \langle \mathcal{S}, \mathcal{A}, P, r, d_0, \omega \rangle
$$&lt;p>其中：&lt;/p>
&lt;ul>
&lt;li>$\mathcal{S}$ 是状态空间，表示当前的token序列， $t$时刻的状态可以表示为 $s_t = (x, y_1, y_2, \cdots, y_t)$&lt;/li>
&lt;li>$\mathcal{A}$ 是动作空间，表示下一个token， $t$时刻的动作可以表示为 $a_t = y_{t+1}\in\mathcal{A}$&lt;/li>
&lt;li>$P$ 是状态转移概率，表示在 $t$时刻，从状态 $s_t$ 转移到状态 $s_{t+1}$ 的概率&lt;/li>
&lt;li>$r$ 是奖励函数，表示在 $t$时刻，从状态 $s_t$ 采取动作 $a_t$ 转移到状态 $s_{t+1}$ 的奖励&lt;/li>
&lt;li>$d_0$ 是初始状态分布，表示初始状态 $s_0$ 的概率分布&lt;/li>
&lt;li>$\omega$ 是终止状态分布，表示终止状态 $s_n$ 的概率分布，通常表示 &lt;code>&amp;lt;eos&amp;gt;&lt;/code> token&lt;/li>
&lt;/ul>
&lt;h1 id="方法">&lt;a href="#%e6%96%b9%e6%b3%95" class="header-anchor">&lt;/a>方法
&lt;/h1>&lt;p>首先作者分析了一下为什么 PPO 在处理 long CoT 任务时效果不佳。&lt;/p>
&lt;p>PPO的传统设置为：&lt;/p>
&lt;ul>
&lt;li>将GAE的参数 $\lambda$ 设置为 0.95&lt;/li>
&lt;li>使用一个 reward model 来初始化 value model&lt;/li>
&lt;/ul>
&lt;p>一方面，作者认为，$\lambda=0.95$ 是为了减少模型在 Mujoco 以及 Atari 等环境中的variance，但是对于Long CoT任务，这个设置会导致模型缺乏足够的探索能力。&lt;/p>
&lt;p>另一方面，作者认为 reward model 和 value model 虽然都是提供关于 response 的信息，但是他们之间还是存在一些差距的。作者给出了使用PPO来进行 long CoT 任务相关的实验结果&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-vc-ppo/failue_PPO.png"
width="1375"
height="582"
loading="lazy"
alt="failue_PPO"
class="gallery-image"
data-flex-grow="236"
data-flex-basis="567px"
>&lt;/p>
&lt;p>可以看到，随着训练的进行，模型的context length以及在AIME benchmark上的表现都出现了下降。&lt;/p>
&lt;p>在本文中，作者主要是在 verifiable tasks 上进行实验，答案的正确性与 response length 长度关系不大，因此, response length 可以反应模型的训练情况。 作者绘制了训练过程中， value 和 advantage 与 token position 的关系图&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-vc-ppo/value_against_pos.png"
width="1390"
height="592"
loading="lazy"
alt="value_advantage_position"
class="gallery-image"
data-flex-grow="234"
data-flex-basis="563px"
>&lt;/p>
&lt;p>从上图可以看到，value 和 advantage 更倾向与给最初的 token 更高的 bias, 作者认为出现这个情况的原因是 value model 和 reward model 的目标并不匹配。 reward model 的目标是给 $\omega$ 也就是 &lt;code>&amp;lt;eos&amp;gt;&lt;/code> token reward, 对于一开始的 token, reward model会给出比较低的奖励；而 value model 的目标是给整个 response 一个奖励，因此，value model 会倾向于给最初的 token 更高的奖励。我们对 GAE 进行改写得到：&lt;/p>
$$
\hat{A}_t = \sum_{i=t}^{T-t-1} \lambda^{i} \left( r_{t+i} + V(s_{t+i+1}) - V(s_{t+i}) \right)
$$&lt;p>从上式可以看到，一开始 $r_{t+i}$ 的值会比较小，而 $V(s_{t+i})$ 的变化比较大，因此，一开始的 token 的 advantage 会比较大，这个 bias 会持续影响整个 trajectory。&lt;/p>
&lt;p>为了解决这个问题，作者提出了 value-pretraining, 也就是对value model 进行离线与训练，直到其收敛到一个具体的 policy 上。具体训练步骤为：&lt;/p>
&lt;ol>
&lt;li>基于一个policy， 如 $\pi_{\mathrm{sft}}$ 进行采样，然后更新value model ($\lambda=1.0$)&lt;/li>
&lt;li>基于收集到的数据训练 value model, 直到 value loss 或者 explain variance 收敛&lt;/li>
&lt;/ol>
&lt;p>接下来，在训练 value model 时，我们还需要考虑 variance reduction 的问题。作者首先改写了 GAE 的公式：&lt;/p>
$$
\hat{A}_t = \begin{cases}
\sum_{i=0}^{T-t-1} \lambda^{i} \left( r_{t+i} + V(s_{t+i+1}) - V(s_{t+i}) \right)+V(s_t) &amp; \text{if } \lambda &lt; 1.0 \\
\sum_{i=0}^{T-t-1} r_{t+i} &amp; \text{if } \lambda=1.0
\end{cases}
$$&lt;p>可以看到，当 $\lambda &lt; 1.0$ 并且 $T-t-1$ 比较大时，&lt;code>&amp;lt;eos&amp;gt;&lt;/code> token 的reward就非常接近于0了，作者通过实验验证了这一点。&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-vc-ppo/reward_signal_decay.png"
width="1117"
height="623"
loading="lazy"
alt="reward_signal_decay"
class="gallery-image"
data-flex-grow="179"
data-flex-basis="430px"
>&lt;/p>
&lt;p>可以看到，当我们降低 $\lambda$ 时，reward signal 的稀疏性会显著增加，从而提高了模型的训练难度。&lt;/p>
&lt;p>但是，我们又不能不使用 variance reduction, 因为这会导致训练的不稳定性。 作者从 TD error 的角度分析了 variance:&lt;/p>
$$
\begin{aligned}
\mathrm{Var}[A_{t}^{\lambda}] &amp;= \mathrm{Var}\left[\sum_{i=0}^{T-t-1} \lambda^{i}\delta_{t+i}\right] \\
&amp;= \sum_{i=1}^{T-t-1} \lambda^{2i} \mathrm{Var}[\delta_{t+i}] + 2\sum_{i=1}^{T-t-1}\sum_{j=0}^{i-1} \lambda^{i+j} \mathrm{Cov}[\delta_{t+i}, \delta_{t+j}]
\end{aligned}
$$&lt;p>因为 $\lambda\in[0,1]$, 因此未来的 TD error 会衰减的更快，因此就降低了 advantage 的 variance.&lt;/p>
&lt;p>那么如何在降低variance的同时，又不至于使得 reward signal 过于稀疏呢？作者的解决方案是分离GAE的计算，也就是将 GAE 的计算分为两部分：&lt;/p>
$$
G_{t:t+h} = \begin{cases}
\sum_{i=0}^{h-1} r_{t+i} + \bar{V}(s_{t+h}) &amp; \text{if } t+h&lt;T \\
\sum_{i=0}^{T-h} r_{t+i} &amp; \text{if } t+h=T
\end{cases}
$$&lt;p>基于这个公式，我们可以写出policy gradient的公式：&lt;/p>
$$
\begin{aligned}
\nabla_{\theta} J(\theta) &amp;= \mathbb{E}_{t}[\nabla_{\theta} \log \pi_{\theta}(a_t|s_t)A_t] \\
&amp;= \mathbb{E}_{t}\left[\nabla_{\theta} \log \pi_{\theta}(a_t|s_t) \sum_{i=0}^{T-t-1} \lambda^{i}\left( r_{t+i} + \bar{V}(s_{t+i+1}) - \bar{V}(s_{t+i}) \right)\right] \\
&amp;= \mathbb{E}_{t}\left[\nabla_{\theta} \log \pi_{\theta}(a_t|s_t) \left((1-\lambda) \sum_{i=1}^{T-t-1} \lambda^{i-1}G_{t:t+i}+\lambda^{T-t-1}G_{t:T}- \bar{V}(s_{t})\right)\right] \\
&amp;= \mathbb{E}_{t}\left[\nabla_{\theta} \log \pi_{\theta}(a_t|s_t) \left((1-\lambda) \sum_{i=1}^{T-t-1} \lambda^{i-1}G_{t:t+i}+\lambda^{T-t-1}G_{t:T}\right)\right]
\end{aligned}
$$&lt;p>通过这种方式，我们就可以避免 value function 对 policy gradient 的影响，因而我们可以对 value model 和 policy model 使用不同的 $\lambda$ 进行训练。&lt;/p>
&lt;p>最终，我们就可以得到 VC-PPO 的算法：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-vc-ppo/vc-ppo.png"
width="1376"
height="713"
loading="lazy"
alt="vc_ppo"
class="gallery-image"
data-flex-grow="192"
data-flex-basis="463px"
>&lt;/p>
&lt;h1 id="实验">&lt;a href="#%e5%ae%9e%e9%aa%8c" class="header-anchor">&lt;/a>实验
&lt;/h1>&lt;h2 id="setup">&lt;a href="#setup" class="header-anchor">&lt;/a>setup
&lt;/h2>&lt;ol>
&lt;li>作者在AIME, GPQA 以及Codeforces三个数据集上进行评测&lt;/li>
&lt;li>作者首先进行了code-start，作者构建了一批样本然后要求模型在 &lt;code>&amp;lt;thinking&amp;gt;&lt;/code> 和 &lt;code>&amp;lt;/thinking&amp;gt;&lt;/code> 之间生成推理过程，然后使用Verifier来针对答案部分提供奖励，正确则奖励为1，错误则奖励为-1&lt;/li>
&lt;li>RL的Baseline使用的是PPO&lt;/li>
&lt;li>value pretraining 时，作者将 GAE的 $\lambda$ 设置为 1.0， 其他参数与PPO一致&lt;/li>
&lt;li>对于decoupled GAE，作者使用 $\lambda_{\text{critic}}=1.0$, $\lambda_{\text{actor}}=0.95$&lt;/li>
&lt;/ol>
&lt;h2 id="实验结果">&lt;a href="#%e5%ae%9e%e9%aa%8c%e7%bb%93%e6%9e%9c" class="header-anchor">&lt;/a>实验结果
&lt;/h2>&lt;p>实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-vc-ppo/results.png"
width="905"
height="260"
loading="lazy"
alt="results"
class="gallery-image"
data-flex-grow="348"
data-flex-basis="835px"
>&lt;/p>
&lt;p>作者还分析了以下模型在AIME数据集上随着训练步数增加准确率的变化情况&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-vc-ppo/AIME_acc_dynamics.png"
width="1119"
height="619"
loading="lazy"
alt="results_aime"
class="gallery-image"
data-flex-grow="180"
data-flex-basis="433px"
>&lt;/p>
&lt;h2 id="ablation-study">&lt;a href="#ablation-study" class="header-anchor">&lt;/a>Ablation Study
&lt;/h2>&lt;p>作者首先探究了 value pretraining 以及 decoupled GAE 对于模型性能的影响&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-vc-ppo/ablation_vc_ppo_componenets.png"
width="1013"
height="319"
loading="lazy"
alt="ablation_study"
class="gallery-image"
data-flex-grow="317"
data-flex-basis="762px"
>&lt;/p>
&lt;p>从上图可以看到，直接使用PPO并不能提升模型的表现，而使用value pretraining 以及 decoupled GAE 能够显著提升模型的表现。&lt;/p>
&lt;p>作者接下来探究了不同的value pretraining steps对模型的影响，结果如下图&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-vc-ppo/ablation_value_pretraining.png"
width="954"
height="297"
loading="lazy"
alt="ablation_study_2"
class="gallery-image"
data-flex-grow="321"
data-flex-basis="770px"
>&lt;/p>
&lt;p>从上表可以看到，value pretraining 的训练步数并不是越多越好，随着训练步数的增加，模型可能会出现过拟合的现象。&lt;/p>
&lt;p>最后作者还分析了以下 $\lambda_{\text{actor}}$ 对于模型性能的影响，结果如下图&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-vc-ppo/ablation_lambda_actor.png"
width="712"
height="321"
loading="lazy"
alt="ablation_study_3"
class="gallery-image"
data-flex-grow="221"
data-flex-basis="532px"
>&lt;/p>
&lt;p>可以看到， $\lambda_{\text{actor}} =1.0$ 的效果是最差的，但是 $\lambda_{\text{actor}}$ 也不是越小越好，实验发现当 $\lambda_{\text{actor}} \in [0.95, 1.0)$ 时结果比较好&lt;/p>
&lt;h2 id="findings">&lt;a href="#findings" class="header-anchor">&lt;/a>Findings
&lt;/h2>&lt;p>作者还提供了一些发现。&lt;/p>
&lt;ol>
&lt;li>作者认为，在LLM中进行RL的训练与传统的RL训练不同，我们不再是从一个随机policy开始，而是从一个SFT之后的policy开始，因此，这就会引入 prior，我们需要将 value model 与 policy model 进行对齐，才能使得训练更加稳定。&lt;/li>
&lt;li>作者认为，value pretraining 可以王value model 中注入先验知识，作者通过实验发现，value pretraining 的过程可以分为两个阶段，第一个阶段是random alignment，这个和传统的RL训练类似，第二个阶段是knowledge injection，这个阶段，value model 开始学习如何给重要的token 更高的权重。
&lt;img src="https://maosong.website/p/notes-on-vc-ppo/value-pretraining.png"
width="634"
height="532"
loading="lazy"
alt="value-pretraining"
class="gallery-image"
data-flex-grow="119"
data-flex-basis="286px"
>&lt;/li>
&lt;li>作者发现， value model 倾向于更大的 $\lambda$, 因此结果会导致更小的 bias 和 更大的 variance. 而 policy model 倾向于更小的 $\lambda$. 这种差异启发我们需要使用一些基于policy gradient 的目标来训练 value model.&lt;/li>
&lt;/ol>
&lt;h1 id="结论">&lt;a href="#%e7%bb%93%e8%ae%ba" class="header-anchor">&lt;/a>结论
&lt;/h1>&lt;p>本文提出了VC-PPO，一个通过使用value-pretraining 以及 decoupled GAE 来解决PPO 的 value initialization bias 以及 reward signal decay 问题的算法。&lt;/p>
&lt;h1 id="参考文献">&lt;a href="#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae" class="header-anchor">&lt;/a>参考文献
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2503.01491" target="_blank" rel="noopener"
>VC-PPO&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://maosong.website/p/notes-on-dapo/" target="_blank" rel="noopener"
>DAPO&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on DAPO</title><link>https://maosong.website/p/notes-on-dapo/</link><pubDate>Wed, 09 Apr 2025 21:40:33 +0800</pubDate><guid>https://maosong.website/p/notes-on-dapo/</guid><description>&lt;h1 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h1>&lt;p>字节Seed团队和清华合作提出了DAPO，一种全开源的，基于PPO的强化学习方法，一用于提升LLM的reasoning能力。&lt;/p>
&lt;h1 id="preliminary">&lt;a href="#preliminary" class="header-anchor">&lt;/a>Preliminary
&lt;/h1>&lt;p>Preliminary包括PPO，GRPO还有KL divergence&lt;/p>
&lt;h2 id="ppo">&lt;a href="#ppo" class="header-anchor">&lt;/a>PPO
&lt;/h2>&lt;p>PPO的训练目标为：&lt;/p>
$$
\mathcal{J}_{\mathrm{PPO}}(\theta) = \mathbb{E}_{(q,a)\sim\mathcal{D},o_{\leq t}\sim \pi_{\theta_{old}}(\cdot\mid q)}\left[ \min\left(r_t(\theta)\hat{A}_t,\mathrm{clip}\left(r_t(\theta), 1-\epsilon, 1+\epsilon\right)\hat{A}_t\right) \right]
$$&lt;p>其中&lt;/p>
$$
r_t(\theta) = \frac{\pi_{\theta}(o_t\mid q, o_{&lt; t})}{\pi_{\theta_{old}}(o_t\mid q, o_{&lt; t})}
$$&lt;p>$(q,a)$ 是从数据集 $\mathcal{D}$ 采样的QA pair，$\epsilon>0$ 是一个超参数，$\hat{A}_t$ 是 $t$时刻的优势估计 (advantage estimator). 给定 value function $V$ 以及 reward function $R$, $\hat{A}_t$ 通过计算GAE得到：&lt;/p>
$$
\hat{A}_t^{\mathrm{GAE}(\gamma, \lambda)}=\sum_{k=0}^{\infty}(\gamma\lambda)^k\delta_{t+k}
$$&lt;p>其中&lt;/p>
$$
\delta_k = R_k + \gamma V(s_{k+1})-V(s_k),\quad 0\leq \gamma,\lambda\leq 1
$$&lt;h2 id="grpo">&lt;a href="#grpo" class="header-anchor">&lt;/a>GRPO
&lt;/h2>&lt;p>相比于PPO，GRPO不依赖于value function, 因此不需要使用reward model. GRPO通过一组输出来估计value $V(s)$, 然后进行更新。具体来说，给定 QA pair $(q,a)$, 我们从$\pi_{\theta_{old}}$中采样$G$个输出 $\{o_i\}_{i=1}^G$, 接下来我们基于reward $\{R_i\}_{i=1}^G$ 使用如下表达式来估计group-level reward:&lt;/p>
$$
\hat{A}_{i,t} = \frac{r_i - \mathrm{mean}(\{R_i\}_{i=1}^G)}{\mathrm{std}(\{R_i\}_{i=1}^G)}
$$&lt;p>最后，GRPO的训练目标与PPO类似，只不过将 $\hat{A}_t$ 替换为 $\hat{A}_{i,t}$, 然后在分组上进行了归一化：&lt;/p>
$$
\mathcal{J}_{\mathrm{GRPO}}(\theta) = \mathbb{E}_{(q,a)\sim\mathcal{D},\{o_i\}_{i=1}^G\sim \pi_{\theta_{old}}(\cdot\mid q)}\left[ \frac{1}{G}\sum_{i=1}^G\frac{1}{|o_i|}\sum_{t=1}^{|o_i|}\min\left(r_{i,t}(\theta)\hat{A}_{i,t},\mathrm{clip}\left(r_{i,t}(\theta), 1-\epsilon, 1+\epsilon\right)\hat{A}_{i,t}\right) \right]
$$&lt;p>其中，&lt;/p>
$$
r_{i,t}(\theta) = \frac{\pi_{\theta}(o_{i,t}\mid q, o_{i,&lt; t})}{\pi_{\theta_{old}}(o_{i,t}\mid q, o_{i,&lt; t})}
$$&lt;h2 id="kl-divergence">&lt;a href="#kl-divergence" class="header-anchor">&lt;/a>KL divergence
&lt;/h2>&lt;p>在传统的RLHF框架中，我们再PPO的基础上，增加了一个KL divergence正则项，用于约束新旧策略的差异。具体来说，给定旧策略 $\pi_{\theta_{old}}$ 和新策略 $\pi_{\theta}$，我们实际上优化的损失函数为&lt;/p>
$$
\mathcal{J}_{\mathrm{RLHF}}(\theta) = \mathcal{J}_{\mathrm{PPO}}(\theta) - \beta\mathrm{KL}\left(\pi_{\theta_{old}}(\cdot\mid q)\|\pi_{\theta}(\cdot\mid q)\right)
$$&lt;p>其中，$\beta$ 是一个超参数，用于平衡PPO损失和KL divergence损失。上面的PPO损失函数也可以改为GRPO损失函数。&lt;/p>
&lt;p>作者在本文中认为，reasoning model和RLHF的训练目标是不一样的，RLHF加上KL divergence正则项，会使得模型在推理时过于保守，偏向于explotition而reasoning model则需要exploration。因此，作者在本文中去掉了这一项。&lt;/p>
&lt;h2 id="rule-based-reward-modeling">&lt;a href="#rule-based-reward-modeling" class="header-anchor">&lt;/a>Rule-based reward modeling
&lt;/h2>&lt;p>作者基于final accuracy来作为outcome reward, 避免模型训练出现reward hacking问题。reward function 如下：&lt;/p>
$$
R(\hat{y},y) = \begin{cases}
1, &amp; \text{if is\_equivalent}(\hat{y},y) \\
-1, &amp; \text{otherwise}
\end{cases}
$$&lt;h1 id="dapo">&lt;a href="#dapo" class="header-anchor">&lt;/a>DAPO
&lt;/h1>&lt;p>DAPO基于GRPO改进，其优化的目标函数为：&lt;/p>
$$
\mathcal{J}_{\mathrm{DAPO}}(\theta) = \mathbb{E}_{(q,a)\sim\mathcal{D},\{o_i\}_{i=1}^G\sim \pi_{\theta_{old}}(\cdot\mid q)}\left[ \frac{1}{\sum_{t=1}^G|o_i|}\sum_{i=1}^G\sum_{t=1}^{|o_i|}\min\left(r_{i,t}(\theta)\hat{A}_{i,t},\mathrm{clip}\left(r_{i,t}(\theta), 1-\epsilon_{low}, 1+\epsilon_{high}\right)\hat{A}_{i,t}\right) \right]
s.t. \quad 0&lt; \vert \{o_i\mid \text{is\_equivalent}(o_i,a)\} \vert &lt; G
$$&lt;p>其中，$\alpha$ 是一个超参数，用于约束输出数量, $r_{i,t}(\theta)$ 和 $\hat{A}_{i,t}$ 的定义与GRPO相同。&lt;/p>
&lt;p>接下来就是DAPO算法的几个关键点：&lt;/p>
&lt;h2 id="clip-higher">&lt;a href="#clip-higher" class="header-anchor">&lt;/a>Clip-Higher
&lt;/h2>&lt;p>作者认为，PPO和GRPO存在entropy collapse问题，也就是policy的entropy会迅速的下降，导致最终每个group的输出基本都差不多，模型很难探索到新的知识。因此，作者提出了Clip-Higher策略，也就是说，让模型能够充分探索。&lt;/p>
&lt;p>作者举了一个例子，当 $\epsilon=0.2$ 时，如果一个group的输出为 $[0.01, 0.9]$, 那么在clip之后，最大的更新幅度为 $[0.01, 0.9]\times 0.2=[0.002, 0.18]$. 这对于概率比较小的输出来说，很难帮助模型提升。因此，作者修改了其阈值。进一步地，作者分离了clip的阈值，分别使用 $\epsilon_{low}$ 和 $\epsilon_{high}$ 来约束更新幅度。作者通过实验验证了这个例子，结果如下图所示：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-dapo/entropy.png"
width="1395"
height="581"
loading="lazy"
alt="entropy collapse"
class="gallery-image"
data-flex-grow="240"
data-flex-basis="576px"
>&lt;/p>
&lt;p>最后，加入Clip-Higher策略的DAPO的训练目标为，与GRPO的训练目标相比，我们使用 $\epsilon_{low}$ 和 $\epsilon_{high}$ 来约束更新幅度。&lt;/p>
$$
\mathcal{J}_{\mathrm{DAPO}}(\theta) = \mathbb{E}_{(q,a)\sim\mathcal{D},\{o_i\}_{i=1}^G\sim \pi_{\theta_{old}}(\cdot\mid q)}\left[ \frac{1}{\sum_{t=1}^G|o_i|}\sum_{i=1}^G\sum_{t=1}^{|o_i|}\min\left(r_{i,t}(\theta)\hat{A}_{i,t},\mathrm{clip}\left(r_{i,t}(\theta), 1-\epsilon_{low}, 1+\epsilon_{high}\right)\hat{A}_{i,t}\right) \right]
s.t. \quad 0&lt; \vert \{o_i\mid \text{is\_equivalent}(o_i,a)\} \vert &lt; G
$$&lt;p>在实际训练中，作者使用了一个比较大的 $\epsilon_{high}$ 来保证概率较小的token也能有较大的概率被采样到。
Clip-Higher的实验结果如下图所示：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-dapo/clip-higher.png"
width="1396"
height="598"
loading="lazy"
alt="Clip-Higher"
class="gallery-image"
data-flex-grow="233"
data-flex-basis="560px"
>&lt;/p>
&lt;h2 id="dynamic-sampling">&lt;a href="#dynamic-sampling" class="header-anchor">&lt;/a>Dynamic Sampling
&lt;/h2>&lt;p>作者认为，在GRPO中，如果一个group的输出全都是对的，那么其advantage会趋近于0，结果导致 policy 不会得到更新，这样就降低了采样效率。&lt;/p>
&lt;p>为了解决这个问题，坐着提出了over-sample以及filtering策略，来过滤accracy等于1或者0的prompts,也就是DAPO中的约束项：&lt;/p>
$$
s.t. \quad 0&lt; \vert \{o_i\mid \text{is\_equivalent}(o_i,a)\} \vert &lt; G
$$&lt;p>通过增加这个约束项，我们可以保证每个group的输出不会趋同，从保证模型能够得到更新，以提高采样效率。实验结果如下图所示：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-dapo/dynamic-sampling.png"
width="1169"
height="554"
loading="lazy"
alt="Dynamic Sampling"
class="gallery-image"
data-flex-grow="211"
data-flex-basis="506px"
>&lt;/p>
&lt;h2 id="token-level-policy-gradient-loss">&lt;a href="#token-level-policy-gradient-loss" class="header-anchor">&lt;/a>Token-level policy gradient loss
&lt;/h2>&lt;p>GRPO采用了一个sample-level的loss计算方式，也就是GRPO首先在每个sample中计算每个token的损失并进行平均，然后在不同的sample中在此进行平均。在这种方式下，每个sample对最终的loss贡献是相同的。这样就导致两个问题：&lt;/p>
&lt;ol>
&lt;li>对于高质量的long answer，通过在token层面进行平均，因为answe比较长，因此整体loss会比较低，也就会降低这个sample的贡献，导致模型很难学习到长答案。&lt;/li>
&lt;li>一些非常长的sample，其往往包含gibberish或者repetitive tokens，这些sample对模型训练是有害的&lt;/li>
&lt;/ol>
&lt;p>因此，为了解决这个问题，作者提出了Token-level policy gradient loss，也就是我们改变了求和方式&lt;/p>
$$
\frac{1}{G}\sum_{i=1}^G\frac{1}{|o_i|}\sum_{t=1}^{|o_i|}\left(\cdot\right)\to \frac{1}{\sum_{t=1}^G|o_i|}\sum_{i=1}^G\sum_{t=1}^{|o_i|}\left(\cdot\right)
$$&lt;p>通过这种方式，我们让长回答的loss贡献更大，从而提高模型学习长回答的能力。另一方面，现在每个token对整体loss的贡献是相同的，一些关键token的loss也会被放大，从而提高模型的表现。&lt;/p>
&lt;p>实验结果如下图所示：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-dapo/token-loss.png"
width="1391"
height="564"
loading="lazy"
alt="Token-level policy gradient loss"
class="gallery-image"
data-flex-grow="246"
data-flex-basis="591px"
>&lt;/p>
&lt;h2 id="overlong-reward-shaping">&lt;a href="#overlong-reward-shaping" class="header-anchor">&lt;/a>Overlong reward shaping
&lt;/h2>&lt;p>与Kimi-k1.5类似，DAPO也增加了length penalty，用于惩罚过长的回答。作者首先使用了一个overlong filtering技巧，来mask掉truncated samples的loss，作者发现通过这个技巧，可以提升模型的训练稳定性以及表现，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-dapo/overlong-filtering.png"
width="1394"
height="605"
loading="lazy"
alt="Overlong filtering"
class="gallery-image"
data-flex-grow="230"
data-flex-basis="552px"
>&lt;/p>
&lt;p>作者还提出了soft overlong punishment， 用于reshape truncated samples的reward，表达式如下：&lt;/p>
$$
R_{length}(y) = \begin{cases}
0, &amp; \text{if } |y|\leq L_{\max}-L_{cache} \\
\frac{(L_{\max}-L_{cache})-|y|}{L_{cache}}, &amp; \text{if } L_{\max}-L_{cache}&lt;|y|\leq L_{\max} \\
-1, &amp; \text{if } |y|>L_{\max}
\end{cases}
$$&lt;h2 id="算法">&lt;a href="#%e7%ae%97%e6%b3%95" class="header-anchor">&lt;/a>算法
&lt;/h2>&lt;p>DAPO的算法流程如下：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-dapo/dapo-algorithm.png"
width="1391"
height="536"
loading="lazy"
alt="DAPO"
class="gallery-image"
data-flex-grow="259"
data-flex-basis="622px"
>&lt;/p>
&lt;h1 id="实验">&lt;a href="#%e5%ae%9e%e9%aa%8c" class="header-anchor">&lt;/a>实验
&lt;/h1>&lt;h2 id="数据集">&lt;a href="#%e6%95%b0%e6%8d%ae%e9%9b%86" class="header-anchor">&lt;/a>数据集
&lt;/h2>&lt;p>作者构建了DAPO-Math-17K数据集用于DAPO的训练，该数据集从AoPS得到，包含了17K prompts，每个prompt都对应一个整数作为答案。&lt;/p>
&lt;h2 id="训练细节">&lt;a href="#%e8%ae%ad%e7%bb%83%e7%bb%86%e8%8a%82" class="header-anchor">&lt;/a>训练细节
&lt;/h2>&lt;p>作者以GRPO作为baseline， $G=16$, $L_{\max}=16384$, $L_{cache}=4096$, $\epsilon_{low}=0.2$, $\epsilon_{high}=0.28$&lt;/p>
&lt;p>评估时，使用AIME作为benchmark，以 avg@32作为指标，temperature设置为1.0, topp设置为0.7。&lt;/p>
&lt;h2 id="实验结果">&lt;a href="#%e5%ae%9e%e9%aa%8c%e7%bb%93%e6%9e%9c" class="header-anchor">&lt;/a>实验结果
&lt;/h2>&lt;p>DAPO与GRPO的对比如下图所示：
&lt;img src="https://maosong.website/p/notes-on-dapo/dapo-performance.png"
width="1381"
height="605"
loading="lazy"
alt="dapo performance"
class="gallery-image"
data-flex-grow="228"
data-flex-basis="547px"
>&lt;/p>
&lt;h2 id="ablation-study">&lt;a href="#ablation-study" class="header-anchor">&lt;/a>Ablation study
&lt;/h2>&lt;p>作者探究了每一个部分对最终表现的影响，结果如下：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-dapo/ablation-study.png"
width="787"
height="415"
loading="lazy"
alt="ablation study"
class="gallery-image"
data-flex-grow="189"
data-flex-basis="455px"
>&lt;/p>
&lt;h2 id="traing-dynamics">&lt;a href="#traing-dynamics" class="header-anchor">&lt;/a>traing dynamics
&lt;/h2>&lt;p>作者还探究了mean response length, reward score, generation entropy以及mean probability随训练轮数的变化，其中：&lt;/p>
&lt;ul>
&lt;li>mean response length: 在一定程度上反应了模型训练的稳定性和表现&lt;/li>
&lt;li>reward score: 反应模型的表现，作者认为，给定一个reliable reward signal， LLM可以很好的fit到训练数据集上，但是作者发现最终的reward与val score相关性比较大，很可能是因为模型过拟合了&lt;/li>
&lt;li>generation entropy &amp;amp; mean probability: 代表了模型的探索能力，通过实验结果可以看到，DAPO初期的探索能力比较强，但是随着训练的进行，探索能力下降，利用能力增强。&lt;/li>
&lt;/ul>
&lt;p>结果如下：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-dapo/training-dynamics.png"
width="1402"
height="1027"
loading="lazy"
alt="training dynamics"
class="gallery-image"
data-flex-grow="136"
data-flex-basis="327px"
>&lt;/p>
&lt;h1 id="结论">&lt;a href="#%e7%bb%93%e8%ae%ba" class="header-anchor">&lt;/a>结论
&lt;/h1>&lt;p>作者基于GRPO提出了DAPO，一种全开源的，基于PPO的强化学习方法，用于提升LLM的reasoning能力。作者首先分析了GRPO损失函数存在的不足，然后进行了针对性改进，包括Clip-Higher, Dynamic Sampling, Token-level policy gradient loss以及Overlong reward shaping。作者通过实验验证了DAPO的有效性，并探究了DAPO的训练动态。&lt;/p>
&lt;h1 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h1>&lt;ol>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2503.14476" target="_blank" rel="noopener"
>DAPO: An Open-Source LLM Reinforcement Learning System at Scale&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>Notes on Qwen2.5 omni</title><link>https://maosong.website/p/notes-on-qwen2.5-omni/</link><pubDate>Tue, 01 Apr 2025 10:29:00 +0800</pubDate><guid>https://maosong.website/p/notes-on-qwen2.5-omni/</guid><description>&lt;h1 id="介绍">&lt;a href="#%e4%bb%8b%e7%bb%8d" class="header-anchor">&lt;/a>介绍
&lt;/h1>&lt;p>2025年3月26号，Qwen团队发布了Qwen2.5 omni，Qwen2.5 omni是一个多模态模型，支持文本、音频、视频、图像等多个模态的输入，支持文本，音频的输出。
作者提出了TMRoPE来对齐不同的模态，然后还是用了Thinker-Talker的架构来同时生成文本和音频。
Thinker是一个大语言模型，Talker是一个dual-track自回归模型，Talker基于Thinker的hideen states来生成audio token，最后通过一个sliding-window DiT来解码audio token&lt;/p>
&lt;p>现有omni-model存在的问题：&lt;/p>
&lt;ol>
&lt;li>缺乏一个系统的，联合训练多个不同模态的方法&lt;/li>
&lt;li>需要处理不同模态输出时互相干扰的问题&lt;/li>
&lt;li>不能实时理解或者流式输出多模态信息&lt;/li>
&lt;/ol>
&lt;p>Qwen2.5 omni的解决方法：&lt;/p>
&lt;ol>
&lt;li>使用TMRoPE来对齐不同的模态。作者将audio以及video frame按照时间顺序组织成一个交替的架构，然后使用TMRoPE来对齐&lt;/li>
&lt;li>使用Thinker-Talker的架构来同时生成文本和音频。Thinker负责文本输出，Talker负责音频的流式输出。&lt;/li>
&lt;li>在multimodal encoder中使用Block-wise streaming处理技巧来实现实时理解；在输出时，实现了一个dual-track自回归架构来生成audio token，以及一个DiT模型来解码audio token&lt;/li>
&lt;/ol>
&lt;p>Qwen2.5 omni的贡献：&lt;/p>
&lt;ol>
&lt;li>提出了Qwen2.5-omni,可以实时理解多模态信息，并流式输出文本和音频&lt;/li>
&lt;li>提出了TMRoPE，一个可以对齐不同模态的RoPE算法&lt;/li>
&lt;li>提出了Thinker-Talker的架构，来完成实时理解和音频输出&lt;/li>
&lt;li>在多个benchmark上取得了SOTA&lt;/li>
&lt;/ol>
&lt;h1 id="架构">&lt;a href="#%e6%9e%b6%e6%9e%84" class="header-anchor">&lt;/a>架构
&lt;/h1>&lt;h2 id="总览">&lt;a href="#%e6%80%bb%e8%a7%88" class="header-anchor">&lt;/a>总览
&lt;/h2>&lt;p>Qwen2.5-omni的架构如下图所示
&lt;img src="https://maosong.website/p/notes-on-qwen2.5-omni/architecture.png"
width="1346"
height="1110"
loading="lazy"
alt="architecture of Qwen2.5-omni"
class="gallery-image"
data-flex-grow="121"
data-flex-basis="291px"
>&lt;/p>
&lt;p>其中Talker类似于人的嘴，负责基于脑中的信息来生成对话；Thinker类似于人的大脑，负责理解输入的信息，并生成对话的上下文。&lt;/p>
&lt;h2 id="输入">&lt;a href="#%e8%be%93%e5%85%a5" class="header-anchor">&lt;/a>输入
&lt;/h2>&lt;ul>
&lt;li>text: Qwen的tokenizer&lt;/li>
&lt;li>audio： Qwen2-Audio的tokenizer&lt;/li>
&lt;li>vision： Qwen2.5-VL的vision tokenizer&lt;/li>
&lt;/ul>
&lt;p>视频以及TMRoPE。 对于视频来说，Qwen2.5-omni采取了和Qwen2.5-VL一样的MRoPE算法。只是这里的temporal ID对应40ms&lt;/p>
&lt;p>音频：对于音频来说，Qwen2.5-omni也是以40ms进行采样然后encoding。&lt;/p>
&lt;p>视频+音频：对于视频+音频来说，Qwen2.5-omni将视频和音频的token进行交替的排列，来保证时间上信息一致&lt;/p>
&lt;h2 id="输出">&lt;a href="#%e8%be%93%e5%87%ba" class="header-anchor">&lt;/a>输出
&lt;/h2>&lt;ul>
&lt;li>Text：text由Thinker直接输出，这和LLM的输出方式是一样的。&lt;/li>
&lt;li>Speech：Qwen2.5-omni首先构建了&lt;code>qwen-tts-tokenizer&lt;/code>，一个speech codec，用于表示speech的关键信息。
然后基于这些关键信息，Talker通过自回归的方式生成audio tokens以及text tokens。&lt;/li>
&lt;/ul>
&lt;h2 id="流式输出">&lt;a href="#%e6%b5%81%e5%bc%8f%e8%be%93%e5%87%ba" class="header-anchor">&lt;/a>流式输出
&lt;/h2>&lt;p>对于流式输出来说，现在的模型存在latency，具体影响因素如下：&lt;/p>
&lt;ol>
&lt;li>处理多模态输入的延迟&lt;/li>
&lt;li>TTFT （time to first token）&lt;/li>
&lt;li>将第一段speech token解码为audio的时间&lt;/li>
&lt;li>模型架构导致的latency，如模型参数等&lt;/li>
&lt;/ol>
&lt;p>为了解决这个问题，Qwen2.5-omni采取了以下措施：&lt;/p>
&lt;ol>
&lt;li>Support prefilling. 作者修改了audio以及vision encoder来在temporal dimension上支持block-wise attention.
具体来讲，audio encoder只关注2秒内的信息，而vision encoder和Qwen2.5-VL一样，使用了一个MLP patch merger来压缩视觉token&lt;/li>
&lt;li>Streaming Codec generation. 为了提高流式输出的效率，作者还是用了基于Flow-Matching的DiT，输出的code收线使用Flow-Matching转化为一个mel-spectrogram.
然后再通过一个BigVGAN来重建得到对应的waveform. 作者在这里修改了DiT的attention，将其receptive field 限制为4个block，包括lookback of 2 blocks以及lookahead of 1 block.&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen2.5-omni/DiT-sliding-window.png"
width="410"
height="433"
loading="lazy"
alt="Sliding Window DiT"
class="gallery-image"
data-flex-grow="94"
data-flex-basis="227px"
>&lt;/p>
&lt;h1 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre Training
&lt;/h1>&lt;p>模型参数初始化：&lt;/p>
&lt;ul>
&lt;li>LLM： 从Qwen2.5进行初始化&lt;/li>
&lt;li>vision encoder：从Qwen2.5-VL的vision encoder进行初始化&lt;/li>
&lt;li>audio encoder：从Whisper-large-v3进行初始化&lt;/li>
&lt;/ul>
&lt;p>Pretraining和Qwen2.5-VL的训练过程类似，分成了3个stage：&lt;/p>
&lt;ol>
&lt;li>冻结LLM的参数，仅训练vision encoder和audio encoder， 该阶段使用了audio-text以及image-text pairs来进行训练。 数据：image-text, video-text, video-audio, audio-text以及text corpus，
跟Qwen2-Audio类似，作者将hierarchical tags用自然语言prompt进行替换&lt;/li>
&lt;li>训练所有参数，使用更多的多模态数据进行训练。数据包括800B token的image和Video相关数据，300B token的audio数据，100B token的video-audio相关数据。&lt;/li>
&lt;li>将模型的上下文扩展到32K。前两个阶段的上下文长度为8192，本阶段使用了long video data等数据来将模型的上下文扩展到32K。&lt;/li>
&lt;/ol>
&lt;h1 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post Training
&lt;/h1>&lt;h2 id="thinker">&lt;a href="#thinker" class="header-anchor">&lt;/a>Thinker
&lt;/h2>&lt;p>数据格式为ChatML，数据类型包括pure text-based dialogue data, visual-modality conversation data, audio-modality conversation data and mix-modality conversation data.&lt;/p>
&lt;h2 id="talker">&lt;a href="#talker" class="header-anchor">&lt;/a>Talker
&lt;/h2>&lt;p>包括三个阶段&lt;/p>
&lt;ol>
&lt;li>ICL training: 训练Talker学习context continuation&lt;/li>
&lt;li>&lt;a class="link" href="https://maosong.website/p/notes-on-dpo/" target="_blank" rel="noopener"
>DPO&lt;/a> training: 提升speech generation的stability&lt;/li>
&lt;li>multi-speaker instruction fine-tuning: 提升模型的naturalness和controllability&lt;/li>
&lt;/ol>
&lt;h1 id="evaluation">&lt;a href="#evaluation" class="header-anchor">&lt;/a>Evaluation
&lt;/h1>&lt;p>Qwen2.5-omni在两类benchmark上进行来的评测，分别时多模态理解（X-&amp;gt;text）以及音频生成(X-&amp;gt; Speech)&lt;/p>
&lt;p>我们这里主要关注一下speech understanding以及speech generation的benchmark.&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen2.5-omni/audio-to-text-performance.png"
width="1093"
height="1125"
loading="lazy"
alt="Audio-to-Text Performance"
class="gallery-image"
data-flex-grow="97"
data-flex-basis="233px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen2.5-omni/zero-shot-speech-generation.png"
width="904"
height="685"
loading="lazy"
alt="Speech Generation Performance"
class="gallery-image"
data-flex-grow="131"
data-flex-basis="316px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen2.5-omni/single-speaker-speech-generation.png"
width="864"
height="513"
loading="lazy"
alt="single speaker speech generation"
class="gallery-image"
data-flex-grow="168"
data-flex-basis="404px"
>&lt;/p>
&lt;h1 id="总结">&lt;a href="#%e6%80%bb%e7%bb%93" class="header-anchor">&lt;/a>总结
&lt;/h1>&lt;p>Qwen2.5-omni相当于是结合了Qwen2.5-VL以及Mini-omni，跟mini-omni2的输入输出是类似的。不同的点在于，Qwen2.5-omni使用了Thinker-Talker的架构，然后还使用了TMRoPE来对齐不同的模态。总的来说，感觉模型还是更偏重于audio的理解与生成。&lt;/p>
&lt;h1 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2503.20215" target="_blank" rel="noopener"
>Qwen2.5-omni&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2408.16725" target="_blank" rel="noopener"
>Mini-omni&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2410.11190" target="_blank" rel="noopener"
>Mini-omni2&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Understanding Sigmoid Loss in SigLip</title><link>https://maosong.website/p/understanding-sigmoid-loss-in-siglip/</link><pubDate>Fri, 28 Mar 2025 14:55:50 +0800</pubDate><guid>https://maosong.website/p/understanding-sigmoid-loss-in-siglip/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>A simple note to understand Sigmoid Loss in SigLip &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Supported by DeepSeek&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;h2 id="binary-cross-entropy-loss">&lt;a href="#binary-cross-entropy-loss" class="header-anchor">&lt;/a>Binary cross entropy loss
&lt;/h2>&lt;p>Suppose we want to solve the binary classification problem, with label $y\in\{0, 1\}$, a common option is to use binary cross entropy loss:&lt;/p>
$$\mathcal{L}(x, y) = -[y\log (\sigma(z)) + (1-y)\log (1-\sigma(z))]$$&lt;p>where $z=f_\theta(x)$ is the logits predicted by our model $f_\theta$, and $\sigma$ is the sigmoid function:&lt;/p>
$$\sigma(z) := \frac{1}{1 + e^{-z}}$$&lt;p>Let $\sigma(\cdot)$ be the sigmoid function, then we have:&lt;/p>
$$
\sigma(-z) = \frac{1}{1 + e^{z}} = \frac{e^{-z}}{1 + e^{-z}} = 1 - \frac{1}{1 + e^{-z}} = 1- \sigma(z)
$$&lt;p>Now we substitute $\sigma(-z)=1-\sigma(z)$ into the loss function, we obtain:&lt;/p>
$$\mathcal{L}(x, y) = -[y\log (\sigma(z)) + (1-y)\log (\sigma(-z))]$$&lt;p>Note that $y\in\{0, 1\}$ thus for each instance, there are two cases:&lt;/p>
&lt;ul>
&lt;li>If $y=0$, then $\mathcal{L}(x, y) =-\log (\sigma(-z))$&lt;/li>
&lt;li>If $y=1$, then $\mathcal{L}(x, y) =-\log (\sigma(z))$&lt;/li>
&lt;/ul>
&lt;p>Now we want to use a unified expression to express these two cases. Note that this requires fitting a curve that passes two points $(0, -1)$ and $(1, 1)$. The simplest curve is a straight line $y=2x-1$. So, we can further simplify the loss expression into:&lt;/p>
$$\mathcal{L}(x, y) = -\log\left[\sigma((2y-1)z)\right]$$&lt;h2 id="sigmoid-loss-in-siglip">&lt;a href="#sigmoid-loss-in-siglip" class="header-anchor">&lt;/a>Sigmoid Loss in SigLip
&lt;/h2>&lt;p>Now we recall the sigmoid loss in SigLip:&lt;/p>
$$\mathcal{L}(\{\bm{x}, \bm{y}\}_{i=1}^N)=-\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^N\log \frac{1}{1+\exp\left[z_{ij}(-t\bm{x}_i\cdot \bm{y_j}+b)\right]}$$&lt;p>where $t, b$ are learnable parameters, and $z_{ij}=1$ if $i=j$ and $z_{ij}=-1$ otherwise.&lt;/p>
&lt;p>To understand Sigmoid loss, notice that $z_{ij}=2\mathbb{I}_{i=j}-1$, which exactly matches the form we derived earlier.&lt;/p>
&lt;h2 id="why-use-sigmoid-loss">&lt;a href="#why-use-sigmoid-loss" class="header-anchor">&lt;/a>Why Use Sigmoid Loss?
&lt;/h2>&lt;ol>
&lt;li>More stable: avoids $\log 0$.&lt;/li>
&lt;li>More efficient: Compute Sigmoid once.&lt;/li>
&lt;li>More Precise: one line of code without condition checking.&lt;/li>
&lt;/ol>
&lt;h1 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h1>&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>&lt;a class="link" href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhai_Sigmoid_Loss_for_Language_Image_Pre-Training_ICCV_2023_paper.pdf" target="_blank" rel="noopener"
>SigLip&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>&lt;a class="link" href="https://chat.deepseek.com/" target="_blank" rel="noopener"
>DeepSeek&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Notes on Aya Vision</title><link>https://maosong.website/p/notes-on-aya-vision/</link><pubDate>Mon, 17 Mar 2025 17:58:24 +0800</pubDate><guid>https://maosong.website/p/notes-on-aya-vision/</guid><description>&lt;p>Aya Vision是一个多模态大语言模型，包含8B, 32B两个size，支持23种语言。Aya Vision基于 Aya Expanse大语言模型。&lt;/p>
&lt;h2 id="模型架构">&lt;a href="#%e6%a8%a1%e5%9e%8b%e6%9e%b6%e6%9e%84" class="header-anchor">&lt;/a>模型架构
&lt;/h2>&lt;p>Aya Vision的模型架构如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-aya-vision/architecture.png"
width="3624"
height="1316"
loading="lazy"
alt="Aya Vision模型架构"
class="gallery-image"
data-flex-grow="275"
data-flex-basis="660px"
>&lt;/p>
&lt;ul>
&lt;li>Vision Encoder: SigLip2-patch14-384&lt;/li>
&lt;li>Vision-text connector: 2 layer MLP&lt;/li>
&lt;li>LLM: Aya Expanse 8B/ 32B&lt;/li>
&lt;/ul>
&lt;h2 id="训练">&lt;a href="#%e8%ae%ad%e7%bb%83" class="header-anchor">&lt;/a>训练
&lt;/h2>&lt;p>训练包含两个stage：&lt;/p>
&lt;ol>
&lt;li>Vision-language alignment: 仅训练vision-text connector，基于image-text pairs进行训练&lt;/li>
&lt;li>SFT：训练connector和LLM，基于合成的多语种数据进行训练&lt;/li>
&lt;/ol>
&lt;h2 id="多语种数据">&lt;a href="#%e5%a4%9a%e8%af%ad%e7%a7%8d%e6%95%b0%e6%8d%ae" class="header-anchor">&lt;/a>多语种数据
&lt;/h2>&lt;p>为了提高模型的多语种能力，作者先基于English的高质量数据集合成了annotation，然后作者讲这些数据转化为22中语言对应的文本&lt;/p>
&lt;h2 id="model-merging">&lt;a href="#model-merging" class="header-anchor">&lt;/a>Model merging
&lt;/h2>&lt;p>最后为了提高模型在纯文本任务上的表现，作者还使用了model merging的技巧。具体做法就是merge使用的base language model和SFT之后的vision-language model&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ol>
&lt;li>&lt;a class="link" href="https://huggingface.co/blog/aya-vision" target="_blank" rel="noopener"
>Aya Vision Blog&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>Notes on Gemma3</title><link>https://maosong.website/p/notes-on-gemma3/</link><pubDate>Sat, 15 Mar 2025 11:15:29 +0800</pubDate><guid>https://maosong.website/p/notes-on-gemma3/</guid><description>&lt;img src="https://maosong.website/p/notes-on-gemma3/cover.png" alt="Featured image of post Notes on Gemma3" />&lt;p>作者提出了Gemma3系列大模型，包括1B, 4B, 12B, 27B四个size。4B, 12B, 27B三个size均支持多模态，128K的上下文长度以及140种语言。&lt;/p>
&lt;h1 id="方法">&lt;a href="#%e6%96%b9%e6%b3%95" class="header-anchor">&lt;/a>方法
&lt;/h1>&lt;h2 id="数据处理">&lt;a href="#%e6%95%b0%e6%8d%ae%e5%a4%84%e7%90%86" class="header-anchor">&lt;/a>数据处理
&lt;/h2>&lt;h3 id="数据格式">&lt;a href="#%e6%95%b0%e6%8d%ae%e6%a0%bc%e5%bc%8f" class="header-anchor">&lt;/a>数据格式
&lt;/h3>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">[BOS]&amp;lt;start_of_turn&amp;gt;user
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Who are you?&amp;lt;end_of_turn&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&amp;lt;start_of_turn&amp;gt;model
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">My name is Gemma!&amp;lt;end_of_turn&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&amp;lt;start_of_turn&amp;gt;user
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">What is 2+2?&amp;lt;end_of_turn&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&amp;lt;start_of_turn&amp;gt;model
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">2+2=4.&amp;lt;end_of_turn&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>pretrain和SFT的区别在于,pretrain时模型输出以&lt;code>&amp;lt;eos&amp;gt;&lt;/code>结束,SFT时模型输出以&lt;code>&amp;lt;end_of_turn&amp;gt;&lt;/code>结束.&lt;/p>
&lt;h3 id="图片处理">&lt;a href="#%e5%9b%be%e7%89%87%e5%a4%84%e7%90%86" class="header-anchor">&lt;/a>图片处理
&lt;/h3>&lt;p>输入的图片都会被resize到896x896，如果图片精度过大或者不是正方形，则会通过Pan &amp;amp; Scan技巧裁剪为多个子图，然后每个子图分别进行resize。核心处理代码为：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">crop_size_w&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">math&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ceil&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">width&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">num_crops_w&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">crop_size_h&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">math&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ceil&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">height&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">num_crops_h&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Don&amp;#39;t apply PaS if crop size is too small.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">if&lt;/span> &lt;span class="nb">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">crop_size_w&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">crop_size_h&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">pan_and_scan_min_crop_size&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">crop_positions_w&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">crop_size_w&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num_crops_w&lt;/span>&lt;span class="p">)]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">crop_positions_h&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">crop_size_h&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num_crops_h&lt;/span>&lt;span class="p">)]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">image_crops&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">image&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">pos_h&lt;/span> &lt;span class="p">:&lt;/span> &lt;span class="n">pos_h&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">crop_size_h&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">pos_w&lt;/span> &lt;span class="p">:&lt;/span> &lt;span class="n">pos_w&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">crop_size_w&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">pos_h&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">pos_w&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">itertools&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">product&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">crop_positions_h&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">crop_positions_w&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="模型架构">&lt;a href="#%e6%a8%a1%e5%9e%8b%e6%9e%b6%e6%9e%84" class="header-anchor">&lt;/a>模型架构
&lt;/h2>&lt;p>模型包括4个size，分别是1B, 4B, 12B, 27B。1B的模型是单模态的，4B, 12B, 27B的模型是多模态的。对于多模态模型来说：&lt;/p>
&lt;ul>
&lt;li>Vision encoder: Siglip-400M&lt;/li>
&lt;li>Projection layer: linear layer&lt;/li>
&lt;li>LLM: Gemma 3&lt;/li>
&lt;/ul>
&lt;h2 id="模型参数">&lt;a href="#%e6%a8%a1%e5%9e%8b%e5%8f%82%e6%95%b0" class="header-anchor">&lt;/a>模型参数
&lt;/h2>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Vision Encoder&lt;/th>
&lt;th>Embedding Parameters&lt;/th>
&lt;th>Non-embedding Parameters&lt;/th>
&lt;th>context length&lt;/th>
&lt;th>multilingual&lt;/th>
&lt;th>training data&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>1B&lt;/td>
&lt;td>0&lt;/td>
&lt;td>302M&lt;/td>
&lt;td>698M&lt;/td>
&lt;td>32K&lt;/td>
&lt;td>English&lt;/td>
&lt;td>2T tokens&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4B&lt;/td>
&lt;td>417M&lt;/td>
&lt;td>675M&lt;/td>
&lt;td>3,209M&lt;/td>
&lt;td>128K&lt;/td>
&lt;td>140+ languages&lt;/td>
&lt;td>4T tokens&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>12B&lt;/td>
&lt;td>417M&lt;/td>
&lt;td>1,012M&lt;/td>
&lt;td>10,759M&lt;/td>
&lt;td>128K&lt;/td>
&lt;td>140+ languages&lt;/td>
&lt;td>12T tokens&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>27B&lt;/td>
&lt;td>417M&lt;/td>
&lt;td>1,416M&lt;/td>
&lt;td>25,600M&lt;/td>
&lt;td>128K&lt;/td>
&lt;td>140+ languages&lt;/td>
&lt;td>14T tokens&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="attention-layers">&lt;a href="#attention-layers" class="header-anchor">&lt;/a>Attention layers
&lt;/h2>&lt;p>为了提高效率，作者将部分layer的self-attention替换为sliding window attention。论文里是将6 layers为一组，替换一组中最后一层为sliding window attention，其余层为self attention。判断某layer是否为sliding window attention的代码为：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># config.sliding_window_pattern = 6&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">is_sliding&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">bool&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">layer_idx&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sliding_window_pattern&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="long-context">&lt;a href="#long-context" class="header-anchor">&lt;/a>Long context
&lt;/h2>&lt;p>作者将global self-attention的RoPE的base frequency从10K提升到了1M，对于sliding window attention，RoPE的base frequency保持10k不变。&lt;/p>
&lt;h2 id="预训练">&lt;a href="#%e9%a2%84%e8%ae%ad%e7%bb%83" class="header-anchor">&lt;/a>预训练
&lt;/h2>&lt;ol>
&lt;li>数据在模型参数里进行了汇总&lt;/li>
&lt;li>tokenizer使用的是Gemini2.0的tokenizer，基于SentencePiece，vocab大小为262K&lt;/li>
&lt;li>使用知识蒸馏的方法进行训练，每个token按照教师模型的概率采样256个logits，然后使用cross-entropy loss进行训练&lt;/li>
&lt;li>4B, 12B, 27B的模型先在32K的context length上进行训练，然后在128K的context length上进行训练&lt;/li>
&lt;/ol>
&lt;h2 id="quantization-aware-training">&lt;a href="#quantization-aware-training" class="header-anchor">&lt;/a>Quantization Aware training
&lt;/h2>&lt;p>作者提供了quantized版本的模型，模型基于预训练好的模型使用QAT方法进行SFT 5000 steps左右。最后是各个模型的内存占用情况&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Raw (GB)&lt;/th>
&lt;th>&lt;/th>
&lt;th>Quantized (GB)&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Model&lt;/td>
&lt;td>bf16&lt;/td>
&lt;td>Int4&lt;/td>
&lt;td>Int4(blocks=32)&lt;/td>
&lt;td>SFP8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1B&lt;/td>
&lt;td>2.0&lt;/td>
&lt;td>0.5&lt;/td>
&lt;td>0.7&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>+KV&lt;/td>
&lt;td>2.9&lt;/td>
&lt;td>1.4&lt;/td>
&lt;td>1.6&lt;/td>
&lt;td>1.9&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4B&lt;/td>
&lt;td>8.0&lt;/td>
&lt;td>2.6&lt;/td>
&lt;td>2.9&lt;/td>
&lt;td>4.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>+KV&lt;/td>
&lt;td>12.7&lt;/td>
&lt;td>7.3&lt;/td>
&lt;td>7.6&lt;/td>
&lt;td>9.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>12B&lt;/td>
&lt;td>24.0&lt;/td>
&lt;td>6.6&lt;/td>
&lt;td>7.1&lt;/td>
&lt;td>12.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>+KV&lt;/td>
&lt;td>38.9&lt;/td>
&lt;td>21.5&lt;/td>
&lt;td>22.0&lt;/td>
&lt;td>27.3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>27B&lt;/td>
&lt;td>54.0&lt;/td>
&lt;td>14.1&lt;/td>
&lt;td>15.3&lt;/td>
&lt;td>27.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>+KV&lt;/td>
&lt;td>72.7&lt;/td>
&lt;td>32.8&lt;/td>
&lt;td>34.0&lt;/td>
&lt;td>46.1&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="后训练">&lt;a href="#%e5%90%8e%e8%ae%ad%e7%bb%83" class="header-anchor">&lt;/a>后训练
&lt;/h2>&lt;p>数据过滤：筛选掉包含PII，不安全的或者有毒的输出等。留下上下文依赖高，幻觉小的数据&lt;/p>
&lt;p>训练包括升级版的知识蒸馏和基于RL的finetuning，其中RL的reward来自于weight averaged reward models, code execution feedback, ground-truth rewards&lt;/p>
&lt;h1 id="实验">&lt;a href="#%e5%ae%9e%e9%aa%8c" class="header-anchor">&lt;/a>实验
&lt;/h1>&lt;h2 id="表现">&lt;a href="#%e8%a1%a8%e7%8e%b0" class="header-anchor">&lt;/a>表现
&lt;/h2>&lt;p>Gemma3的表现如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gemma3/performance.png"
width="1762"
height="767"
loading="lazy"
alt="performance of Gemma3"
class="gallery-image"
data-flex-grow="229"
data-flex-basis="551px"
>&lt;/p>
&lt;p>与PaliGemma 2的表现对比&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gemma3/comparison_with_pali_gemma_2.png"
width="843"
height="933"
loading="lazy"
alt="Comparison with PaliGemma 2"
class="gallery-image"
data-flex-grow="90"
data-flex-basis="216px"
>&lt;/p>
&lt;h2 id="消融实验">&lt;a href="#%e6%b6%88%e8%9e%8d%e5%ae%9e%e9%aa%8c" class="header-anchor">&lt;/a>消融实验
&lt;/h2>&lt;h3 id="local-attention-layers">&lt;a href="#local-attention-layers" class="header-anchor">&lt;/a>Local attention layers
&lt;/h3>&lt;ol>
&lt;li>&lt;code>sliding_window_pattern&lt;/code>对模型的表现影响不大&lt;/li>
&lt;li>sliding window size对模型的表现也不是很大，如下图&lt;/li>
&lt;li>使用sliding window attention可以降低KV cache的内存占用&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gemma3/sliding_window_size.png"
width="865"
height="618"
loading="lazy"
alt="sliding window size"
class="gallery-image"
data-flex-grow="139"
data-flex-basis="335px"
>&lt;/p>
&lt;h3 id="long-context-ablation">&lt;a href="#long-context-ablation" class="header-anchor">&lt;/a>Long context ablation
&lt;/h3>&lt;p>结论为long context会降低模型的性能&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gemma3/long_context.png"
width="864"
height="887"
loading="lazy"
alt="long context"
class="gallery-image"
data-flex-grow="97"
data-flex-basis="233px"
>&lt;/p>
&lt;h3 id="知识蒸馏">&lt;a href="#%e7%9f%a5%e8%af%86%e8%92%b8%e9%a6%8f" class="header-anchor">&lt;/a>知识蒸馏
&lt;/h3>&lt;p>训练token个数比较少的时候，使用小的教师模型效果更好；训练token个数比较多的时候，使用大的教师模型效果更好。&lt;/p>
&lt;h3 id="pan--scan">&lt;a href="#pan--scan" class="header-anchor">&lt;/a>Pan &amp;amp; Scan
&lt;/h3>&lt;p>使用图片原始的aspect ratio训练的模型效果更好。&lt;/p>
&lt;h3 id="memorization">&lt;a href="#memorization" class="header-anchor">&lt;/a>memorization
&lt;/h3>&lt;p>memorization指模型输出的文本与训练数据中文本的重复率。结果发现Gemma3的memorization要更低一些。&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gemma3/memorization.png"
width="865"
height="824"
loading="lazy"
alt="memorization"
class="gallery-image"
data-flex-grow="104"
data-flex-basis="251px"
>&lt;/p>
&lt;h1 id="结论">&lt;a href="#%e7%bb%93%e8%ae%ba" class="header-anchor">&lt;/a>结论
&lt;/h1>&lt;ol>
&lt;li>sliding window attention在Qwen2.5-VL里已经验证过有效,这里在Gemma3上同样验证了有效性.&lt;/li>
&lt;li>模型架构与PaliGemma系列基本一致，只是attention改变了，然后LLM从Gemma 2升级到了Gemma 3。&lt;/li>
&lt;/ol>
&lt;h1 id="参考文献">&lt;a href="#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae" class="header-anchor">&lt;/a>参考文献
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf" target="_blank" rel="noopener"
>Gemma3 Technical Report&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/huggingface/transformers/tree/main/src/transformers/models/gemma3" target="_blank" rel="noopener"
>transformers-Gemma3 code&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Overview of Qwen-VL series</title><link>https://maosong.website/p/overview-of-qwen-vl-series/</link><pubDate>Sun, 09 Mar 2025 15:11:29 +0800</pubDate><guid>https://maosong.website/p/overview-of-qwen-vl-series/</guid><description>&lt;h1 id="timeline">&lt;a href="#timeline" class="header-anchor">&lt;/a>Timeline
&lt;/h1>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>时间&lt;/th>
&lt;th>模型系列&lt;/th>
&lt;th>Feature&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>2023.08&lt;/td>
&lt;td>Qwen-VL&lt;/td>
&lt;td>Vision-centric understanding&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>Multi-lingual&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>Multi-image&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>Fine-grained visual understanding&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2024.09&lt;/td>
&lt;td>Qwen2-VL&lt;/td>
&lt;td>Image understanding&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>Video understanding (20min)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>Agent capability&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>Multilingual support&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2025.02&lt;/td>
&lt;td>Qwen2.5-VL&lt;/td>
&lt;td>Document parsing&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>Object grounding&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>Long video understadning and grouding (1 hour)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>Agent functionality&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h1 id="models">&lt;a href="#models" class="header-anchor">&lt;/a>Models
&lt;/h1>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>name&lt;/th>
&lt;th>size&lt;/th>
&lt;th>Vision&lt;/th>
&lt;th>Adapter&lt;/th>
&lt;th>LLM&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>1.0&lt;/td>
&lt;td>Qwen-VL&lt;/td>
&lt;td>9.6B&lt;/td>
&lt;td>ViT(1.9B)&lt;/td>
&lt;td>Cross-attention(0.08B)&lt;/td>
&lt;td>Qwen-LLM(7B)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2.0&lt;/td>
&lt;td>Qwen2-VL-2B&lt;/td>
&lt;td>2B&lt;/td>
&lt;td>ViT(675M)&lt;/td>
&lt;td>MLP(0.0341B)&lt;/td>
&lt;td>Qwen2-LLM(1.5B)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Qwen2-VL-7B&lt;/td>
&lt;td>7B&lt;/td>
&lt;td>&lt;/td>
&lt;td>MLP(0.0446B)&lt;/td>
&lt;td>Qwen2-LLM(7.6B)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Qwen2-VL-72B&lt;/td>
&lt;td>72B&lt;/td>
&lt;td>&lt;/td>
&lt;td>MLP(0.0682B)&lt;/td>
&lt;td>Qwen2-LLM(72B)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2.5&lt;/td>
&lt;td>Qwen2.5-VL-3B&lt;/td>
&lt;td>3B&lt;/td>
&lt;td>ViT(675M)&lt;/td>
&lt;td>MLP(0.1817B)&lt;/td>
&lt;td>Qwen2.5-LLM(3B)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Qwen2.5-VL-7B&lt;/td>
&lt;td>7B&lt;/td>
&lt;td>&lt;/td>
&lt;td>MLP(0.3179B)&lt;/td>
&lt;td>Qwen2.5-LLM(7.6B)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Qwe2.5n-VL-72B&lt;/td>
&lt;td>72B&lt;/td>
&lt;td>&lt;/td>
&lt;td>MLP(0.7267B)&lt;/td>
&lt;td>Qwen2.5-LLM(72B)&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Remark&lt;/p>
&lt;ul>
&lt;li>Qwen2-VL的MLP使用的是LayerNorm&lt;/li>
&lt;li>Qwen2.5-VL的MLP使用的是RMSNorm&lt;/li>
&lt;li>出了Qwen-VL之外，所有模型均有对应的Instruct版本，这里为了方便没有列出。Qwen-VL对应的是Qwen-VL-chat。&lt;/li>
&lt;/ul>
&lt;h1 id="data--training">&lt;a href="#data--training" class="header-anchor">&lt;/a>Data &amp;amp; training
&lt;/h1>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model series&lt;/th>
&lt;th>stage&lt;/th>
&lt;th>data&lt;/th>
&lt;th>Vision&lt;/th>
&lt;th>Adapter&lt;/th>
&lt;th>LLM&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Qwen-VL&lt;/td>
&lt;td>pretraining&lt;/td>
&lt;td>1.4B&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Multi-task pretraining&lt;/td>
&lt;td>96.8M&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>SFT&lt;/td>
&lt;td>350K&lt;/td>
&lt;td>&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen2-VL&lt;/td>
&lt;td>pretraining&lt;/td>
&lt;td>600b tokens&lt;/td>
&lt;td>✅？&lt;/td>
&lt;td>✅？&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Multi-task pretraining&lt;/td>
&lt;td>800b tokens&lt;/td>
&lt;td>✅？&lt;/td>
&lt;td>✅？&lt;/td>
&lt;td>✅？&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>SFT&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>✅？&lt;/td>
&lt;td>✅？&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen2.5-VL&lt;/td>
&lt;td>Visual Pre-Training&lt;/td>
&lt;td>1.5T token&lt;/td>
&lt;td>✅？&lt;/td>
&lt;td>✅？&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Multimodal Pre-Training&lt;/td>
&lt;td>2T token&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Long-Context Pre-Training&lt;/td>
&lt;td>0.6T token&lt;/td>
&lt;td>✅？&lt;/td>
&lt;td>✅？&lt;/td>
&lt;td>✅？&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>SFT&lt;/td>
&lt;td>2M samples&lt;/td>
&lt;td>&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>&lt;a class="link" href="https://maosong.website/p/notes-on-dpo/" target="_blank" rel="noopener"
>DPO&lt;/a>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Remark&lt;/p>
&lt;ol>
&lt;li>Qwen2-VL按照技术报告的说法是follow了Qwen-VL的训练方式，因此这里也使用了同样的表示&lt;/li>
&lt;li>带问号的地方代表不确定，为个人猜测。请谨慎参考。&lt;/li>
&lt;li>难以确定的原因是Qwen-VL系列将projection layer和ViT作为一个模块，因此比较难区分是否训练了projection layer&lt;/li>
&lt;/ol>
&lt;p>技术报告的训练框架图&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/overview-of-qwen-vl-series/qwen-vl-training.png"
width="1908"
height="868"
loading="lazy"
alt="qwen-vl-training"
class="gallery-image"
data-flex-grow="219"
data-flex-basis="527px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/overview-of-qwen-vl-series/qwen2-5-training.png"
width="1354"
height="468"
loading="lazy"
alt="qwen2-5-training"
class="gallery-image"
data-flex-grow="289"
data-flex-basis="694px"
>&lt;/p>
&lt;p>References&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2308.12966" target="_blank" rel="noopener"
>Qwen-VL&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2409.12191" target="_blank" rel="noopener"
>Qwen2-VL&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2502.13923" target="_blank" rel="noopener"
>Qwen2.5-VL&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on QwQ-32B</title><link>https://maosong.website/p/notes-on-qwq-32b/</link><pubDate>Sat, 08 Mar 2025 09:46:16 +0800</pubDate><guid>https://maosong.website/p/notes-on-qwq-32b/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>通义团队在3月6号发布了QwQ-32B，一个基于RL的，32B参数的reasoning model，QwQ-32B的表现可以与DeepSeek-R1相比&lt;/p>
&lt;h2 id="模型架构">&lt;a href="#%e6%a8%a1%e5%9e%8b%e6%9e%b6%e6%9e%84" class="header-anchor">&lt;/a>模型架构
&lt;/h2>&lt;p>QwQ-32B基于Qwen2.5-32B开发。主要通过基于outcome-based rewards的RL进行训练。训练过程包括两个stage&lt;/p>
&lt;ol>
&lt;li>Stage 1：本阶段在math和coding domain上进行RL的训练，作者使用了一个verifier来保证最终数学问题结果的正确性，使用了一个code execution server来保证最终生成代码的正确性。&lt;/li>
&lt;li>Stage 2：本阶段在通用领域上进行RL的训练。模型基于general reward model和一些rule-based verifier进行训练。应该是类似 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-r1/" target="_blank" rel="noopener"
>DeepSeek-R1&lt;/a> 的规则。&lt;/li>
&lt;/ol>
&lt;h2 id="实验结果">&lt;a href="#%e5%ae%9e%e9%aa%8c%e7%bb%93%e6%9e%9c" class="header-anchor">&lt;/a>实验结果
&lt;/h2>&lt;p>实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwq-32b/qwq-32b-final.jpg"
width="3035"
height="1713"
loading="lazy"
alt="QwQ_evaluation"
class="gallery-image"
data-flex-grow="177"
data-flex-basis="425px"
>&lt;/p>
&lt;h2 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://qwenlm.github.io/blog/qwq-32b/" target="_blank" rel="noopener"
>blog&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://chat.qwen.ai/?models=Qwen2.5-Plus" target="_blank" rel="noopener"
>demo&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>compression is intelligence</title><link>https://maosong.website/p/compression-is-intelligence/</link><pubDate>Thu, 06 Mar 2025 17:57:51 +0800</pubDate><guid>https://maosong.website/p/compression-is-intelligence/</guid><description>&lt;p>我们知道，基于decoder-only transformer的LLM的训练目标是最小化next-token-prediction loss，即给定sequence $x=(x_1,\dots, x_n)\in D$，我们的目标为求解以下优化问题&lt;/p>
$$
\min_{\theta} -\sum_{x\in D}\log P_{\theta}(x_i|x_1,\dots,x_{i-1})
$$&lt;p>这里 $\theta$ 就是我们的模型参数，$D$ 是我们的训练数据集。&lt;/p>
&lt;p>无数模型通过实际效果告诉我们，这个优化目标可以很好地训练出具有良好泛化能力的大语言模型。但是，我们的问题是，为什么这个优化目标可以训练出智能的模型？ 本文将从压缩即智能的角度来理解这个问题。&lt;/p>
&lt;h1 id="压缩即智能">&lt;a href="#%e5%8e%8b%e7%bc%a9%e5%8d%b3%e6%99%ba%e8%83%bd" class="header-anchor">&lt;/a>压缩即智能
&lt;/h1>&lt;h2 id="一个例子">&lt;a href="#%e4%b8%80%e4%b8%aa%e4%be%8b%e5%ad%90" class="header-anchor">&lt;/a>一个例子
&lt;/h2>&lt;p>我们首先来看一个简单的例子。给定如下三个0-1字符串：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">01010101010101010101
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">01001000100001000001
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">01101000101010100101
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>我们该如何描述这三个字符串的规律？显然，第一个字符串最简单，它是&lt;code>01&lt;/code>字符串重复得到的结果；第二个字符串稍微复杂一些，它在每个&lt;code>1&lt;/code>之前插入重复次数的&lt;code>0&lt;/code>；第三个字符串则最复杂，它是我随手写的一个字符串，基本没有任何规律，因此，我们只能直接存储这个字符串。&lt;/p>
&lt;p>这个例子告诉我们，一个字符串的规律越简单，我们越容易描述它，因此，我们越容易压缩它。实际上，大语言模型做的也是类似的事情。它们的核心思想是，压缩即智能。&lt;/p>
&lt;h2 id="heading">&lt;a href="#heading" class="header-anchor">&lt;/a>
&lt;/h2>&lt;h1 id="结论">&lt;a href="#%e7%bb%93%e8%ae%ba" class="header-anchor">&lt;/a>结论
&lt;/h1>&lt;p>本文中，我们从压缩即智能的角度来理解大语言模型的原理。我们发现，大语言模型的next-token-prediction其实就是压缩。我们通过压缩让大语言模型学习到了语言中的规律，从而让模型具有了智能。&lt;/p>
&lt;h1 id="参考文献">&lt;a href="#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae" class="header-anchor">&lt;/a>参考文献
&lt;/h1></description></item><item><title>Notes on Qwen2.5 VL</title><link>https://maosong.website/p/notes-on-qwen2.5-vl/</link><pubDate>Tue, 04 Mar 2025 10:46:42 +0800</pubDate><guid>https://maosong.website/p/notes-on-qwen2.5-vl/</guid><description>&lt;h1 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h1>&lt;p>2025年2月20号Qwen团队发布了Qwen2.5 VL技术报告，Qwen2.5 VL包括3B，7B， 72B三个size。Qwen2.5-VL主要在架构，数据上进行了改进。通过评测，Qwen2.5-VL在多个benchmark上取得了SOTA。&lt;/p>
&lt;p>Qwen2.5 VL认为已有模型的缺点为：&lt;/p>
&lt;ul>
&lt;li>计算复杂度高&lt;/li>
&lt;li>上下文理解能力有限&lt;/li>
&lt;li>细粒度visual perception能力不足&lt;/li>
&lt;li>在不同上下文长度下表现不一致&lt;/li>
&lt;/ul>
&lt;p>Qwen2.5 VL的贡献为：&lt;/p>
&lt;ol>
&lt;li>使用了一个从零开始训练的ViT作为vision encoder，并且在ViT中使用了window attention，来提高计算效率&lt;/li>
&lt;li>使用了dynamic FPS sampling，用于处理不同采样率的视频输入&lt;/li>
&lt;li>将MRoPE扩展到了temporal domain上，进一步提高了模型在与时间相关任务上的表现&lt;/li>
&lt;li>使用了更高质量的数据集，其中预训练阶段使用了4.1T的token&lt;/li>
&lt;/ol>
&lt;p>Qwen2.5 VL的主要亮点为：&lt;/p>
&lt;ul>
&lt;li>优秀的document parsing能力&lt;/li>
&lt;li>精确的object grounding能力&lt;/li>
&lt;li>针对长视频的理解和细粒度grounding能力&lt;/li>
&lt;li>针对UI的agent functionality&lt;/li>
&lt;/ul>
&lt;h1 id="模型架构">&lt;a href="#%e6%a8%a1%e5%9e%8b%e6%9e%b6%e6%9e%84" class="header-anchor">&lt;/a>模型架构
&lt;/h1>&lt;h2 id="总览">&lt;a href="#%e6%80%bb%e8%a7%88" class="header-anchor">&lt;/a>总览
&lt;/h2>&lt;p>Qwen2.5 VL和Qwen2 VL的架构基本一致，包括Vision Encoder，Language Encoder，以及projector layer三个部分，其架构图如下：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen2.5-vl/architecture.png"
width="1345"
height="889"
loading="lazy"
alt="Qwen2.5 VL架构图"
class="gallery-image"
data-flex-grow="151"
data-flex-basis="363px"
>&lt;/p>
&lt;ul>
&lt;li>LLM： 使用Qwen2.5 LLM作为LLM，并且将1D-RoPE升级为了MRoPE&lt;/li>
&lt;li>Vision Encoder： 使用一个从零开始训练的ViT架构，patch_size为14，position embedding为2D-RoPE， attention为window attention和self-attention的混合，其中，只有四层使用的是self-attention.对于输入的图片，ViT会将图片resize到28的整数倍。&lt;/li>
&lt;li>Projector Layer： 使用的是一个两层的MLP&lt;/li>
&lt;/ul>
&lt;p>模型的参数配置如下图&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen2.5-vl/model_config.png"
width="1180"
height="861"
loading="lazy"
alt="Qwen2.5 VL模型参数配置"
class="gallery-image"
data-flex-grow="137"
data-flex-basis="328px"
>&lt;/p>
&lt;h2 id="vision-encoder">&lt;a href="#vision-encoder" class="header-anchor">&lt;/a>Vision Encoder
&lt;/h2>&lt;p>Vision encoder的主要改进点为：&lt;/p>
&lt;ol>
&lt;li>使用了window attention来提升计算效率，window attention的size为 $112\times 112$， 对应为 $8\times 8$ 个patch. 这样做的好处是可以不用对图片做scaling&lt;/li>
&lt;li>使用了2D RoPE来捕捉空间信息，使用3D RoPE来捕捉视频输入的时间信息&lt;/li>
&lt;li>与LLM的结构进行对齐，包括使用RMSNorm替换LayerNorm，使用SwiGLU替换ReLU&lt;/li>
&lt;/ol>
&lt;h2 id="输入处理">&lt;a href="#%e8%be%93%e5%85%a5%e5%a4%84%e7%90%86" class="header-anchor">&lt;/a>输入处理
&lt;/h2>&lt;p>对于图片输入，Qwen2.5 VL使用原始图片的空间信息来构建坐标，而不是将坐标normalize到[0, 1000]之间，这样让模型可以处理不同精度的图片输入。&lt;/p>
&lt;p>对于视频输入，因为使用了3D RoPE，因此Qwen2.5 VL可以处理不同帧率的视频输入，这样就避免了不同帧率视频对模型视频理解带来的影响。这一点和Apollo里的想法是一样的。具体来说，Qwen2.5 VL首先将连续的两帧group到了一起，然后使用了temporal ID来将position和视频所对应的时间进行对齐。这里可以看Qwen2.5 VL的代码：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Temporal (Time): 3 patches, representing different segments of the video in time.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Height: 2 patches, dividing each frame vertically.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Width: 2 patches, dividing each frame horizontally.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">We also have some important parameters:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">fps (Frames Per Second): The video&amp;#39;s frame rate, set to 1. This means one frame is processed each second.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">tokens_per_second: This is a crucial parameter. It dictates how many &amp;#34;time-steps&amp;#34; or &amp;#34;temporal tokens&amp;#34; are conceptually packed into a one-second interval of the video. In this case, we have 25 tokens per second. So each second of the video will be represented with 25 separate time points. It essentially defines the temporal granularity.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">temporal_patch_size: The number of frames that compose one temporal patch. Here, it&amp;#39;s 2 frames.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">interval: The step size for the temporal position IDs, calculated as tokens_per_second * temporal_patch_size / fps. In this case, 25 * 2 / 1 = 50. This means that each temporal patch will be have a difference of 50 in the temporal position IDs.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">input_ids: [V V V V V V V V V V V V T T T T T], here V is for vision.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">vision temporal position_ids: [0, 0, 0, 0, 50, 50, 50, 50, 100, 100, 100, 100]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">vision height position_ids: [0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">vision width position_ids: [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">text temporal position_ids: [101, 102, 103, 104, 105]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">text height position_ids: [101, 102, 103, 104, 105]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">text width position_ids: [101, 102, 103, 104, 105]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Here we calculate the text start position_ids as the max vision position_ids plus 1.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>这里&lt;code>fps&lt;/code>为1，表示每一帧对应一秒，&lt;code>tokens_per_second&lt;/code>为25，表示每秒包含25个token，&lt;code>temporal_patch_size&lt;/code>为2，表示每个temporal patch包含2个frame。因此一个patch里面，就包含了2frame, 对应50tokens. 然后前面提到连续两帧会被group到一起，因此每个temporal patch对应4个spatial patches. 其position_ids为：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="p">[(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="训练">&lt;a href="#%e8%ae%ad%e7%bb%83" class="header-anchor">&lt;/a>训练
&lt;/h1>&lt;h2 id="预训练">&lt;a href="#%e9%a2%84%e8%ae%ad%e7%bb%83" class="header-anchor">&lt;/a>预训练
&lt;/h2>&lt;h3 id="数据">&lt;a href="#%e6%95%b0%e6%8d%ae" class="header-anchor">&lt;/a>数据
&lt;/h3>&lt;p>预训练阶段使用了4.1T的token，包括image captions, interleaved image-text data, optical character recognition (OCR) data, visual knowledge (e.g., celebrity, landmark, flora, and fauna identification), multi-modal academic questions, localization data, document parsing data, video descriptions, video localization, and agent-based interaction data. 作者详细介绍了以下几种数据：&lt;/p>
&lt;ul>
&lt;li>Interleaved image-text data： 主要1. 提高模型的上下文学习能力；2. 保持模型的text-only能力；3.包含一些通用信息。数据清洗包括：1. 基于text-only quality过滤； 2. 基于image-text相关性过滤；3. 基于image-text互补程度过滤；4.基于information density balance过滤.&lt;/li>
&lt;li>Grounding data：作者使用了Grounding DINO, SAM等模型来生成一些grounding data.为了提升模型在open-vocabulary detection上的能力，作者将训练数据集扩展到了1万个object category上，作者还使用了一些point-based object grounding data来提升模型的能力&lt;/li>
&lt;li>Document Parsing data：作者基于text,image, music sheets和chemical formulas合成了一些HTML格式的数据，然后根据tag和bounding box来提升模型document parsing的能力&lt;/li>
&lt;li>OCR data：作者使用了开源，合成和in-house的数据集，数据集主要提到multi-lingual以及1M的chart-type数据&lt;/li>
&lt;li>Video data：作者通过pipeline构建了long video caption来提升模型长视频的理解能力&lt;/li>
&lt;li>Agent data：作者收集了一些mobile device和网页的screenshots,然后基于agent框架来合成控制轨迹&lt;/li>
&lt;/ul>
&lt;h3 id="训练-1">&lt;a href="#%e8%ae%ad%e7%bb%83-1" class="header-anchor">&lt;/a>训练
&lt;/h3>&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen2.5-vl/pretraining.png"
width="1217"
height="428"
loading="lazy"
alt="Qwen2.5 VL预训练阶段"
class="gallery-image"
data-flex-grow="284"
data-flex-basis="682px"
>&lt;/p>
&lt;p>如上图所示，Qwen2.5 VL的预训练阶段包括了三个阶段：&lt;/p>
&lt;ol>
&lt;li>Visual Pretraining:这一阶段使用了image-caption, knowledge, OCR数据集，旨在提升ViT提取视觉特征的能力&lt;/li>
&lt;li>multimodal pretraining:这一阶段在第一阶段的基础上增加了pure-text, interleaved data, VQA, Video grounding, agent data, 旨在提升模型处理复杂视觉信息的能力&lt;/li>
&lt;li>long-context pretraining: 这一阶段，在第二阶段的基础上，增加了long video, long agent, long document data，旨在提升模型处理长上下文的能力&lt;/li>
&lt;/ol>
&lt;h2 id="后训练">&lt;a href="#%e5%90%8e%e8%ae%ad%e7%bb%83" class="header-anchor">&lt;/a>后训练
&lt;/h2>&lt;h3 id="数据-1">&lt;a href="#%e6%95%b0%e6%8d%ae-1" class="header-anchor">&lt;/a>数据
&lt;/h3>&lt;p>SFT阶段使用大概2M的样本进行训练，其中纯文本和多模态数据占比为1:1，语言主要是中文和英文。&lt;/p>
&lt;p>为了保证数据质量，作者提供了一个数据清洗的pipeline，包括：&lt;/p>
&lt;ol>
&lt;li>domain-specific categorization: 作者基于Qwen2-VL-72B构建了Qwen2-VL-Instag，用于将QA pair分为8个大类别，30个小类别&lt;/li>
&lt;li>Domain-tailed filtering:作者使用了rule-based和model-based方法来提升数据的质量
&lt;ul>
&lt;li>rule-based filtering: 重复性检测，格式检测等&lt;/li>
&lt;li>model-based filtering: 使用基于Qwen2.5-VL训练的reward model来从多个维度评估QA pair的质量&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>为了进一步提升模型的推理能力，作者还是用了rejection sampling来refine 数据集。&lt;/p>
&lt;h3 id="训练-2">&lt;a href="#%e8%ae%ad%e7%bb%83-2" class="header-anchor">&lt;/a>训练
&lt;/h3>&lt;p>post-training阶段分为SFT和&lt;a class="link" href="https://maosong.website/p/notes-on-dpo/" target="_blank" rel="noopener"
>DPO&lt;/a>两个小阶段，这个阶段都会冻结VIT. SFT阶段使用大多数的训练数据，而DPO阶段专注于image-text data和pure text data，以更好地进行对齐。具体做法就是基于Grounding truth，使用checkpoint来评估数据的质量，然后只保留答案正确的数据来训练。&lt;/p>
&lt;h1 id="评测">&lt;a href="#%e8%af%84%e6%b5%8b" class="header-anchor">&lt;/a>评测
&lt;/h1>&lt;ol>
&lt;li>通用VQA
&lt;img src="https://maosong.website/p/notes-on-qwen2.5-vl/general_vqa.png"
width="1356"
height="656"
loading="lazy"
alt="通用VQA"
class="gallery-image"
data-flex-grow="206"
data-flex-basis="496px"
>&lt;/li>
&lt;li>文档理解和OCR
&lt;img src="https://maosong.website/p/notes-on-qwen2.5-vl/document_understanding.png"
width="1358"
height="655"
loading="lazy"
alt="文档理解和OCR"
class="gallery-image"
data-flex-grow="207"
data-flex-basis="497px"
>&lt;/li>
&lt;li>空间理解
&lt;img src="https://maosong.website/p/notes-on-qwen2.5-vl/grounding.png"
width="1324"
height="714"
loading="lazy"
alt="空间理解"
class="gallery-image"
data-flex-grow="185"
data-flex-basis="445px"
>&lt;/li>
&lt;li>视频理解和Grounding
&lt;img src="https://maosong.website/p/notes-on-qwen2.5-vl/video.png"
width="1320"
height="672"
loading="lazy"
alt="视频理解和Grounding"
class="gallery-image"
data-flex-grow="196"
data-flex-basis="471px"
>&lt;/li>
&lt;li>Agent
&lt;img src="https://maosong.website/p/notes-on-qwen2.5-vl/agent.png"
width="1374"
height="337"
loading="lazy"
alt="Agent"
class="gallery-image"
data-flex-grow="407"
data-flex-basis="978px"
>&lt;/li>
&lt;/ol>
&lt;h1 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://github.com/QwenLM/Qwen2.5-VL" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2502.13923" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Git authentication error</title><link>https://maosong.website/p/git-authentication-error/</link><pubDate>Sat, 22 Feb 2025 10:51:27 +0800</pubDate><guid>https://maosong.website/p/git-authentication-error/</guid><description>&lt;h1 id="问题描述">&lt;a href="#%e9%97%ae%e9%a2%98%e6%8f%8f%e8%bf%b0" class="header-anchor">&lt;/a>问题描述
&lt;/h1>&lt;p>在使用 &lt;code>git clone&lt;/code> 命令时，可能会遇到认证错误。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&amp;gt; git clone https://github.com/user-name/some-repository.git
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">正克隆到 &lt;span class="s1">&amp;#39;some-repository&amp;#39;&lt;/span>...
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">remote: Invalid username or password.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">fatal: &lt;span class="s1">&amp;#39;https://github.com/user-name/some-repository.git/&amp;#39;&lt;/span> 鉴权失败
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="问题原因解决方案">&lt;a href="#%e9%97%ae%e9%a2%98%e5%8e%9f%e5%9b%a0%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88" class="header-anchor">&lt;/a>问题原因&amp;amp;解决方案
&lt;/h1>&lt;p>本地git配置问题，需要重新认证本地和GitHub的连接。一般是因为本地Git无法和Github的SSH服务器连接。&lt;/p>
&lt;h2 id="方案1ssh">&lt;a href="#%e6%96%b9%e6%a1%881ssh" class="header-anchor">&lt;/a>方案1：SSH
&lt;/h2>&lt;p>本方法将Github视为一个SSH服务器，本机通过SSH连接到Github。该方法主要需要在本地生成SSH密钥，并将其添加到Github的SSH密钥列表中。最后通过SSH的方式进行Git操作&lt;/p>
&lt;ol>
&lt;li>本地生成SSH秘钥&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">ssh-keygen
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>默认的秘钥地址是&lt;code>~/.ssh/id_rsa&lt;/code>，对应的公钥就是&lt;code>~/.ssh/id_rsa.pub&lt;/code>。如果需要指定秘钥地址，可以使用&lt;code>-f&lt;/code>选项，对于指定的秘钥地址，需要通过以下命令将秘钥地址加入到搜索列表中.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 1. Start the SSH agent&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&amp;gt; &lt;span class="nb">eval&lt;/span> &lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="k">$(&lt;/span>ssh-agent -s&lt;span class="k">)&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 2. Add your custom key file&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&amp;gt; ssh-add path/to/id_rsa
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ol start="2">
&lt;li>
&lt;p>将公钥添加到Github的SSH密钥列表中。使用 &lt;code>cat ~/.ssh/id_rsa.pub&lt;/code> 复制公钥，然后打开&lt;code>Github-&amp;gt;设置-&amp;gt;SSH and GPG keys-&amp;gt;New SSH key&lt;/code>，将公钥粘贴到&lt;code>Key&lt;/code>中，然后点击&lt;code>Add SSH key&lt;/code>。将公钥&lt;code>Key&lt;/code>中并保存。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>本机通过SSH连接到Github，可以通过以下方式验证&lt;/p>
&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&amp;gt; ssh -T git@github.com
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># ssh -vT git@github.com # 查看ssh连接的秘钥文件搜索列表&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># ssh -i ~/.ssh/id_rsa git@github.com # 自定义密钥的地址，如果使用默认的密钥地址，则不需要指定密钥地址，如果之前已经加入到搜索列表中，则也不需要指定密钥地址&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Hi user-name! You&lt;span class="err">&amp;#39;&lt;/span>ve successfully authenticated, but GitHub does not provide shell access.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ol start="4">
&lt;li>克隆仓库：在clone的时候选择&lt;code>SSH&lt;/code>的方式（不是&lt;code>HTTPS&lt;/code>的方式），即repo的地址为&lt;code>git@github.com:user-name/some-repository.git&lt;/code>&lt;/li>
&lt;li>设置remote repo：&lt;code>git remote set-url origin git@github.com:user-name/some-repository.git&lt;/code>&lt;/li>
&lt;/ol>
&lt;p>通过以上步骤，即可正常使用Git进行操作。&lt;/p>
&lt;h2 id="方案2personal-access-token">&lt;a href="#%e6%96%b9%e6%a1%882personal-access-token" class="header-anchor">&lt;/a>方案2：Personal Access Token
&lt;/h2>&lt;p>该方法通过生成一个Personal Access Token来认证本地和GitHub的连接。该方法需要在Github的设置中生成一个Personal Access Token，然后通过该Token来连接remote repo&lt;/p>
&lt;ol>
&lt;li>生成Personal Access Token. 打开&lt;code>Github-&amp;gt;设置-&amp;gt;Developer settings-&amp;gt;Personal access tokens-&amp;gt;Token(classic)-&amp;gt;Generate new token&lt;/code>，在&lt;code>Note&lt;/code>中输入本机相关信息，在&lt;code>Expiration&lt;/code>中选择token失效日期，在&lt;code>Permissions&lt;/code>中选择&lt;code>repo&lt;/code>（必选，其他可选），然后点击&lt;code>Generate token&lt;/code>。将生成的&lt;code>&amp;lt;token&amp;gt;&lt;/code>复制到本地。&lt;/li>
&lt;li>克隆仓库：&lt;code>git clone https://user-name:&amp;lt;token&amp;gt;@github.com/user-name/some-repository.git&lt;/code>&lt;/li>
&lt;li>设置remote repo：&lt;code>git remote set-url origin https://user-name:&amp;lt;token&amp;gt;@github.com/user-name/some-repository.git&lt;/code>&lt;/li>
&lt;/ol>
&lt;p>通过以上步骤，即可正常使用Git进行操作。&lt;/p>
&lt;h1 id="参考资料">&lt;a href="#%e5%8f%82%e8%80%83%e8%b5%84%e6%96%99" class="header-anchor">&lt;/a>参考资料
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://docs.github.com/en/authentication/troubleshooting-ssh/error-permission-denied-publickey" target="_blank" rel="noopener"
>Error: Permission denied (publickey)
&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens" target="_blank" rel="noopener"
>Managing your personal access tokens
&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Kimi k1.5</title><link>https://maosong.website/p/notes-on-kimi-k1.5/</link><pubDate>Sat, 08 Feb 2025 10:09:52 +0800</pubDate><guid>https://maosong.website/p/notes-on-kimi-k1.5/</guid><description>&lt;p>论文提出了Kimi k1.5， 一个基于强化学习训练的多模态推理模型。作者介绍了Kimi k1.5的训练方法，以及infra上的优化。作者的主要贡献如下：&lt;/p>
&lt;ol>
&lt;li>作者发现，模型的上下文长度可以有效提升LLM的推理能力。&lt;/li>
&lt;li>作者提出了基于online mirror descent的强化学习训练方法。&lt;/li>
&lt;li>作者发现可以通过long context scaling和policy optimization来提升模型的推理能力。&lt;/li>
&lt;li>作者提出了long2short的推理方法，可以有效提升short CoT模型的推理能力。&lt;/li>
&lt;/ol>
&lt;h2 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h2>&lt;p>论文中包括两个模型，一个是k1.5 base model， 其是一个多模态大模型。另一个是Kimi-1.5 reasoning model（简称为k1.5），k1.5是基于kimi-1.5 base model， 通过强化学习训练得到的推理模型。&lt;/p>
&lt;h3 id="kimi-15-base-model">&lt;a href="#kimi-15-base-model" class="header-anchor">&lt;/a>Kimi-1.5 base model
&lt;/h3>&lt;p>k1.5 base model是一个基于transformer的多模态大模型，论文没有给出详细的模型结构，只有如下的示意图&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k1.5/k1-5-model-arch.png"
width="1210"
height="287"
loading="lazy"
alt="Architecture of k1.5 base model"
class="gallery-image"
data-flex-grow="421"
data-flex-basis="1011px"
>&lt;/p>
&lt;p>k1.5 base model的训练包括三个阶段：&lt;/p>
&lt;ol>
&lt;li>Vision-language pretraining stage: 这一阶段的目的是让模型拥有语言和视觉的表征能力。训练时，模型先在纯文本的数据上进行训练，然后提高加入图文交错的数据进行训练，直到训练数据中图文交错的数据占比达到30%。模型的更新方式为先冻结LLM更新visual tower, 然后解冻LLM更新所有参数。&lt;/li>
&lt;li>Vision-language cooldown stage: 这一阶段的目的是让模型保持多模态理解能力。作者发现，使用合成数据可以提高模型在这一阶段的表现。因此，作者使用闭源大语言模型，基于math, knowledge和code domain的pretraining data来合成了一些QA pair数据对，然后讲这些QA pair数据对加入到训练数据中。&lt;/li>
&lt;li>Long-context activation stage: 这一阶段的目的是让模型在长上下文的数据上进行训练，以提升模型在长上下文上的理解能力。这一阶段包含了40%的full attention data以及60%的partial attention data。其中，full attention data是基于真实数据和合成的QA以及summary数据得到的，partial attention data是在cooldown data找那个通过均匀采样得到的。这一阶段，模型的上下文长度从4096逐步增加到131072。&lt;/li>
&lt;/ol>
&lt;h3 id="kimi-k15">&lt;a href="#kimi-k15" class="header-anchor">&lt;/a>Kimi k1.5
&lt;/h3>&lt;p>Kimi k1.5是基于k1.5 base model， 通过进一步训练得到的推理模型。k1.5的训练包括四个阶段：&lt;/p>
&lt;ol>
&lt;li>pretraining：这一步就是k1.5 base model的训练， 包括三个小阶段。前面已经介绍过了。&lt;/li>
&lt;li>vanilla SFT：这一步的目的是使得k1.5 base model具有指令跟随能力。对于non-reasoning任务，作者先使用人类标注产生一个seed dataset，然后基于seed dataset训练一个seed model，最后根据收集的prompt来使用seed model来生成回答。对于reasoning任务，作者使用了rejection sampling来扩展SFT数据集&lt;/li>
&lt;li>long-CoT SFT： 这一步的目的是让模型能够像人类一样进行推理，能够掌握最基本的推理策略，即：planning，evaluation，reflection和exploration。从而为RL训练提供一个良好的初始化。&lt;/li>
&lt;li>RL：这一步就是通过RL来提高模型的推理能力。我们在下一节中进行详细的介绍。&lt;/li>
&lt;/ol>
&lt;h2 id="rl">&lt;a href="#rl" class="header-anchor">&lt;/a>RL
&lt;/h2>&lt;h3 id="problem-definition">&lt;a href="#problem-definition" class="header-anchor">&lt;/a>Problem Definition
&lt;/h3>&lt;p>给定一个训练数据集 $\mathcal{D} = \{(x_i, y_i^\star)\}_ {i=1}^n$， 其中 $x_ i$ 是问题， $y_{i}^\star$ 是ground truth。我们希望找到一个模型 $\pi_{\theta}$ ，来解决这个问题。通常问题比较难，因此我们使用chain of thought（CoT）的方法来解决这个问题。具体做法就是让模型输出中间步骤 $z=(z_1, z_2, ..., z_m)$， 来连接问题 $x_i$ 和答案 $y$。其中， $z_j$ 是模型在第 $j$ 步的推理结果，即 $z_t\sim\pi_{\theta}(x_i, z_{&lt;t})$, $y\sim \pi_{\theta}(x_i, z)$。&lt;/p>
&lt;p>通常，我们还会使用一个reward model $r_{\phi}$，来评估模型输出的答案的质量。一个常用的reward model是基于答案的正确性来评估的，即 $r_{\phi}(x_i, y, y^\star) = \mathbb{I}[y=y_i^\star]$ 。这样，我们要求解的问题就变成了最大化中间步骤和最终答案的reward，即&lt;/p>
$$
\max_{\theta} \mathbb{E}_ {(x, y^\star)\sim\mathcal{D},(y,z,)\sim\pi_{\theta}} r_{\phi}(x, y, y^\star)
$$&lt;h3 id="policy-optimization">&lt;a href="#policy-optimization" class="header-anchor">&lt;/a>Policy Optimization
&lt;/h3>&lt;p>作者使用了online policy mirror descent来解决上面提到的优化问题。在每个iteration中，存在一个reference model $\pi_{\theta_r}$ ， 以及一个当前要更新的模型 $\pi_{\theta}$ 。online policy mirror descent要解决的优化问题为：&lt;/p>
$$
\min_{\theta} \mathbb{E}_ {(x, y^\star)\sim\mathcal{D}} \left[\mathbb{E}_ {(y,z,)\sim\pi_{\theta}}\left[r_{\phi}(x, y, y^\star) - \beta \mathcal{D}_ {KL}(\pi_{\theta}(y,z\mid x)\Vert \pi_{\theta_r}(y,z\mid x))\right]\right]
$$&lt;p>其中， $\beta$ 是超参数，用于平衡reward和KL散度 $\mathcal{D}_{KL}(\cdot)$.&lt;/p>
&lt;p>上述问题有一个闭式解，即：&lt;/p>
$$
\pi^\star(y,z\mid x)=\pi_{\theta_r}(y,z\mid x)\exp\left(\frac{r_{\phi}(x, y, y^\star)}{\beta}\right)/Z(x;\beta)
$$&lt;p>其中，&lt;/p>
$$
Z(x;\beta):=\sum_{y,z}\pi_{\theta_r}(y,z\mid x)\exp\left(\frac{r_{\phi}(x, y, y^\star)}{\beta}\right)
$$&lt;p>是归一化因子。在闭式解的表达式中，我们对两边取对数，得到：&lt;/p>
$$
r_{\phi}(x, y, y^\star) - \beta \log Z - \beta \log\frac{\pi^\star(y,z\mid x)}{\pi_{\theta_r}(y,z\mid x)}=0
$$&lt;p>这是最优策略满足的等式。作者这里使用了L2 loss来优化当前策略 $\pi_{\theta}$ ，即：&lt;/p>
$$
\min_{\theta} \mathbb{E}_ {(x, y^\star)\sim\mathcal{D}} \left[\mathbb{E}_ {(y,z,)\sim\pi_ {\theta_r}}\left[r_{\phi}(x, y, y^\star) - \beta \log Z - \beta \log\frac{\pi_{\theta}(y,z\mid x)}{\pi_{\theta_r}(y,z\mid x)}\right]^2\right]
$$&lt;p>这里需要注意的一点是response $(y,z)$ 是基于reference model $\pi_{\theta_r}$ 生成的，而不再是基于当前策略 $\pi_{\theta}$ 生成的。这是Kimi-1.5的RL训练和传统RL训练的一个不同点。这也是为什么k1.5说他是off-policy的原因。&lt;/p>
&lt;p>接下来，我们就可以对上面的目标函数求梯度，我们将里层的函数展开为 $(a-b-c)^2=(a-b)^2-2(a-b)c+c^2$ 的形式，然后对 $\theta$ 求梯度，得到：&lt;/p>
$$
\mathbb{E}_ {(x, y^\star)\sim\mathcal{D}} \left[\mathbb{E}_ {(y,z,)\sim\pi_{\theta_r}}\left[\nabla_{\theta}\log \pi_{\theta}(y,z\mid x)(r_{\phi}(x, y, y^\star) - \beta \log Z) - \frac{\beta}{2} \nabla_{\theta}\left(\log\frac{\pi_{\theta}(y,z\mid x)}{\pi_{\theta_r}(y,z\mid x)}\right)^2\right]\right]
$$&lt;p>如果我们对每个问题$x$,采样 $k$ 个response $(y,z)$ ，那么我们可以近似上式中内部的期望，得到：&lt;/p>
$$
\frac{1}{k}\sum_{i=1}^k\left[\nabla_{\theta}\log \pi_{\theta}(y_i,z_i\mid x)(r_{\phi}(x, y_i, y^\star) - \bar{r}) - \frac{\beta}{2} \nabla_{\theta}\left(\log\frac{\pi_{\theta}(y_i,z_i\mid x)}{\pi_{\theta_r}(y_i,z_i\mid x)}\right)^2\right]
$$&lt;p>这里， $\bar{r}$ 是 $\beta \log Z$ 的估计值。作者提供了两种估计方法：&lt;/p>
&lt;ol>
&lt;li>基于采样的 $(y_i,z_i)\sim \pi_{\theta_r}$ 对 $\beta \log Z$ 进行估计：&lt;/li>
&lt;/ol>
$$
\bar{r}=\beta\log\frac{1}{k}\sum_{j=1}^k\exp\left(\frac{r_{\phi}(x, y_j, y^\star)}{\beta}\right)
$$&lt;ol start="2">
&lt;li>直接使用samples rewards的平均值来近似：&lt;/li>
&lt;/ol>
$$
\bar{r}=\frac{1}{k}\sum_{i=1}^k r_{\phi}(x, y_i, y^\star)
$$&lt;p>作者提到，第二种方法效果很好并且计算效率更高，因此作者在实验中使用了第二种方法。&lt;/p>
&lt;p>训练时，作者每次从数据集中采样一个batch的样本，然后使用上述的梯度更新模型。模型参数更新完之后，作者将新的模型作为reference model，然后重复上述过程。&lt;/p>
&lt;blockquote>
&lt;p>Remark
作者还探究了为什么不使用value network。作者认为，在强化学习中，value network通常用于评估当前状态的价值，但是在reasoning model中，训练的目的是让模型能够充分探索，以提高模型在不同任务中的泛化性。因此，作者认为使用value network可能会限制模型的探索能力。&lt;/p>
&lt;/blockquote>
&lt;h3 id="training-strategy">&lt;a href="#training-strategy" class="header-anchor">&lt;/a>Training Strategy
&lt;/h3>&lt;p>作者提出了两个提高采样效率的方法：&lt;/p>
&lt;ol>
&lt;li>Curriculum Sampling. 这种采样方式会将问题按照难度进行排序，然后按照难度从易到难进行采样。以逐步提高模型的推理能力。&lt;/li>
&lt;li>Prioritized Sampling.作者还采用了prioritized sampling来提高采样效率。也就是说，对于回答错误的问题，模型会进行更多的采样。&lt;/li>
&lt;/ol>
&lt;h2 id="infra">&lt;a href="#infra" class="header-anchor">&lt;/a>Infra
&lt;/h2>&lt;h3 id="training-system">&lt;a href="#training-system" class="header-anchor">&lt;/a>Training System
&lt;/h3>&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k1.5/k1-5-training-system.png"
width="1112"
height="530"
loading="lazy"
alt="Large Scale Reinforcement Learning Training System for LLM"
class="gallery-image"
data-flex-grow="209"
data-flex-basis="503px"
>&lt;/p>
&lt;p>为了高效地进行RL训练，作者构建了一个iterative synchronous RL训练框架。框架如上图所示。框架的创新之处在于&lt;strong>Partial Rollout&lt;/strong>，该方法可以更高效的处理复杂的推理轨迹。框架包含了以下几部分：&lt;/p>
&lt;ol>
&lt;li>rollout worker: 该部分负责生成推理轨迹。然后将生成的推理轨迹加入到replay buffer中。&lt;/li>
&lt;li>master: 管理data flow和replay buffer和train worker之间的通信。&lt;/li>
&lt;li>train worker: 根据获取的rollout轨迹更新模型。&lt;/li>
&lt;li>code execution service: 在代码相关任务中，该部分负责执行代码，并返回执行结果。&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Partial Rollout&lt;/strong>：Partial Rollout的目的是为了整体的训练效率。其做法就是给定一个固定的outpu token budget，然后限制推理轨迹的长度。当长度超过token限制时，没有完成的部分就会被存入到replay buffer中，以供后续的训练使用。&lt;/p>
&lt;h3 id="deployment-framework">&lt;a href="#deployment-framework" class="header-anchor">&lt;/a>Deployment Framework
&lt;/h3>&lt;p>根据前面的介绍，我们知道k1.5每个iteration的训练包括两步：&lt;/p>
&lt;ol>
&lt;li>inference phase: 基于Reference model进行采样，生成推理轨迹。&lt;/li>
&lt;li>training phase: 基于采样得到的推理轨迹，更新模型。&lt;/li>
&lt;/ol>
&lt;p>如果我们直接使用Megatron和vLLM来实现上述的训练和推理，那么会存在以下问题：&lt;/p>
&lt;ol>
&lt;li>Megatron和vLLM的并行策略可能并不一致，因此很难进行参数共享&lt;/li>
&lt;li>在训练过程中，vLLM可能会保留一些GPU，这就导致了训练和推理的资源分配不均。&lt;/li>
&lt;li>推理和训练的资源需求可能并不一致，无法动态调控显卡资源&lt;/li>
&lt;/ol>
&lt;p>因此，作者构建了一个hybrid deployment framework来解决上述问题，其框架如下图所示。&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k1.5/k1-5-hybrid-deployment-framework.png"
width="867"
height="560"
loading="lazy"
alt="Hybrid Deployment Framework"
class="gallery-image"
data-flex-grow="154"
data-flex-basis="371px"
>&lt;/p>
&lt;p>该框架的主要优势在于使用Kubernetes Sidecar containers来在一个pod中共享所有的GPU资源。这样就避免了资源分配不均的问题。&lt;/p>
&lt;h2 id="数据">&lt;a href="#%e6%95%b0%e6%8d%ae" class="header-anchor">&lt;/a>数据
&lt;/h2>&lt;h3 id="数据处理">&lt;a href="#%e6%95%b0%e6%8d%ae%e5%a4%84%e7%90%86" class="header-anchor">&lt;/a>数据处理
&lt;/h3>&lt;ol>
&lt;li>RL prompt set. 作者认为prompt的quality和diversity对RL训练至关重要。因此作者基于diverse coverage, balanced difficuty以及accurate evaluation来构建了一个RL prompt set。对于多样性，作者确保数据集覆盖多个domain。对于平衡难度，作者通过多次采样，根据通过率来给每个样本的难易程度打分。对于准确性，作者筛掉了多选题，正确题和证明题，确保模型能够生成正确的推理步骤。最后，作者还让模型进行猜测，如果模型猜测的正确率比较高，那么就认为该样本是easy-to-hack的，就会筛掉。&lt;/li>
&lt;li>Test Case Generation for Coding. 作者使用&lt;a class="link" href="https://github.com/luogu-dev/cyaron" target="_blank" rel="noopener"
>CYaRon&lt;/a>来生成coding任务的测试用例。通过与Kimi k1.5结合，作者构建了一个包含323个测试用例的训练集。&lt;/li>
&lt;li>Reward modeling for Math. 作者使用了两种方法来提高reward model的准确性。第一个是classic RM，作者基于InstructGPT，构造了大约800K的数据训练了一个value-head based RM。模型会基于问题，参考答案和模型生成的答案来判断模型生成的答案是否正确。第二个是Chain of Thought RM，作者基于收集的800k CoT数据对kimi进行了finetune，然后对模型生成的推理步骤进行打分。最终发现，Chain of Thought RM的效果更好。因此，作者在RL训练中使用了Chain of Thought RM。&lt;/li>
&lt;li>Vision RL Data。 作者从三个方面构建了vision RL数据集：real-world data, synthetic visual reasoning data以及text-rendered data.&lt;/li>
&lt;/ol>
&lt;h3 id="数据集">&lt;a href="#%e6%95%b0%e6%8d%ae%e9%9b%86" class="header-anchor">&lt;/a>数据集
&lt;/h3>&lt;ol>
&lt;li>pretraining：其中，纯文本数据包括Engligh, Chinese, Code, math reasoning以及Knowledge这5个领域。多模态数据包括captioning, image-text interleaving, OCR, Knowledge以及QA数据集等。附录B介绍了数据集的来源和处理过程。&lt;/li>
&lt;li>SFT： vanilla SFT数据集包含1M的纯文本数据，其中包含500K的general QA数据，200K的coding数据，200K的数学和科学数据，5K的创作写作数据以及20K的长上下文任务（总结，QA，翻译和写作等）。作者还构建了1M的图文数据，包括chart理解，OCR，grounding，visual coding, visual reasoning以及visual aided math/science problems等&lt;/li>
&lt;li>long-CoT SFT：该阶段的数据基于refined RL prompt set，使用了prompt engineering来构建了一个少量但高质量的long-CoT数据集。用于将reasoning能力内化到模型中。&lt;/li>
&lt;/ol>
&lt;h2 id="long2short">&lt;a href="#long2short" class="header-anchor">&lt;/a>Long2short
&lt;/h2>&lt;p>为了降低推理成本，作者提出了long2short的推理方法。该方法通过将长上下文推理转化为短上下文推理，让模型能够更加高效的完成推理。作者尝试了几种方法来实现long2short：&lt;/p>
&lt;ol>
&lt;li>Model merging:将long-CoT模型和short-CoT模型进行合并，然后进行推理。这里采用的办法是对两个模型的权重取平均。&lt;/li>
&lt;li>shortest rejection sampling：通过对同一个问题进行多次采样（论文中每个问题采样8次），然后选择最短的，正确的推理步骤作为最终答案。&lt;/li>
&lt;li>&lt;a class="link" href="https://maosong.website/p/notes-on-dpo/" target="_blank" rel="noopener"
>DPO&lt;/a>：通过对同一个问题进行多次采样，选择最短的正确的回答作为正样本，最长的回答作为负样本，然后使用DPO进行训练。&lt;/li>
&lt;li>Long2short RL：作者在RL阶段，选择一个在performance和token efficiency之间达到平衡的模型作为 base model，然后加入了一个long2short RL训练阶段。该阶段，作者加入了length penalty以及降低了maximum rollout length来鼓励模型生成更短的推理步骤。&lt;/li>
&lt;/ol>
&lt;h3 id="length-penalty">&lt;a href="#length-penalty" class="header-anchor">&lt;/a>Length penalty
&lt;/h3>&lt;p>Length penalty的目的是降低模型的overthinking和训练成本。作者加入了一个length reward，对于问题 $x$ 和采样的回答 $(y_i,z_i)$ ，回答 $(y_i,z_i)$ 的length定义为 $len(x_i)=\text{length}([z_i, y_i])$ length reward定义为：&lt;/p>
$$
r_{\text{length}}(x, (y_i,z_i))=\begin{cases}
\lambda, &amp; \text{if } r(x, y_i, y^\star) = 1 \\
\min(0,\lambda), &amp; \text{if } r(x, y_i, y^\star) = 0 \\
\end{cases}
$$&lt;p>其中&lt;/p>
$$
\lambda = 0.5 - \frac{len(x_i)-\min_{j}len(x_j)}{\max_{j}len(x_j)-\min_{j}len(x_j)}
$$&lt;p>也就是说，当回答正确时，我们鼓励模型生成更长的推理步骤。反之，我们鼓励模型生成更短的推理步骤。&lt;/p>
&lt;h2 id="实验">&lt;a href="#%e5%ae%9e%e9%aa%8c" class="header-anchor">&lt;/a>实验
&lt;/h2>&lt;h3 id="实验结果">&lt;a href="#%e5%ae%9e%e9%aa%8c%e7%bb%93%e6%9e%9c" class="header-anchor">&lt;/a>实验结果
&lt;/h3>&lt;ol>
&lt;li>K1.5在long-CoT任务上的表现&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k1.5/k1-5-long-cot-benchmark.png"
width="1118"
height="425"
loading="lazy"
alt="Long-CoT-benchmark"
class="gallery-image"
data-flex-grow="263"
data-flex-basis="631px"
>&lt;/p>
&lt;ol start="2">
&lt;li>K1.5在Short-CoT任务上的表现&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k1.5/k1-5-short-cot-benchmark.png"
width="1357"
height="566"
loading="lazy"
alt="Short-CoT-benchmark"
class="gallery-image"
data-flex-grow="239"
data-flex-basis="575px"
>&lt;/p>
&lt;ol start="3">
&lt;li>
&lt;p>Long Context Scaling
作者在实验中还探究了上下文长度对模型推理能力的影响。作者针对几个模型进行了实验，结果在图5和图6里面。结果发现，随着上下文长度的增加，模型的推理能力会逐渐提升。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Long2short&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k1.5/k1-5-long2short.png"
width="1354"
height="590"
loading="lazy"
alt="Long2short"
class="gallery-image"
data-flex-grow="229"
data-flex-basis="550px"
>&lt;/p>
&lt;h3 id="消融实验">&lt;a href="#%e6%b6%88%e8%9e%8d%e5%ae%9e%e9%aa%8c" class="header-anchor">&lt;/a>消融实验
&lt;/h3>&lt;ol>
&lt;li>Scaling of model size and context length. 作者探究了模型大小和上下文长度对模型推理能力的影响，结果在图8里面。结果发现
&lt;ol>
&lt;li>小模型可以通过Long CoT来达到大模型的效果&lt;/li>
&lt;li>大模型具有更高的token效率&lt;/li>
&lt;li>理想情况下，使用具有更长上下文的大模型，可以同时达到更高的推理能力和更高的token效率，否则，使用上下文长度更长的小模型&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Effects of using negative gradients. 作者探究了使用负梯度对模型推理能力的影响。作者与ReST方法进行了对比，结果在图10里面。结果发现，ReST回到只模型产生更长的推理步骤。作者认为，选取合适的优化方法，可以有效提升模型的推理能力。&lt;/li>
&lt;li>Sampling strategies. 作者还探究了采样策略对模型推理能力的影响。作者与均匀采样进行了对比，结果在图9里面。结果发现，论文提出的采样策略可以有效提升模型的推理能力。&lt;/li>
&lt;/ol>
&lt;h2 id="结论">&lt;a href="#%e7%bb%93%e8%ae%ba" class="header-anchor">&lt;/a>结论
&lt;/h2>&lt;p>作者提出了Kimi k1.5，一个基于强化学习训练的多模态推理模型。作者介绍了Kimi k1.5的训练方法，数据集，infra上的优化以及long2short的推理方法。作者的主要贡献在于：&lt;/p>
&lt;ol>
&lt;li>作者发现，模型的上下文长度可以有效提升LLM的推理能力。&lt;/li>
&lt;li>作者提出了基于online mirror descent的强化学习训练方法。&lt;/li>
&lt;li>作者提出了long2short的推理方法，可以有效提升short CoT模型的推理能力。&lt;/li>
&lt;/ol>
&lt;p>论文的问题在于对于多模态的内容介绍较少，感觉还是更倾向于文本推理任务。&lt;/p>
&lt;h2 id="参考文献">&lt;a href="#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae" class="header-anchor">&lt;/a>参考文献
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2501.12599" target="_blank" rel="noopener"
>Kimi k1.5: Scaling Reinforcement Learning with LLMs&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Screen usage</title><link>https://maosong.website/p/screen-usage/</link><pubDate>Wed, 05 Feb 2025 15:14:43 +0800</pubDate><guid>https://maosong.website/p/screen-usage/</guid><description>&lt;h1 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h1>&lt;p>Screen is a terminal multiplexer, which allows you to create multiple sessions in a single terminal window.&lt;/p>
&lt;h1 id="installation">&lt;a href="#installation" class="header-anchor">&lt;/a>Installation
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">sudo apt-get install screen
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="usage">&lt;a href="#usage" class="header-anchor">&lt;/a>Usage
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">screen &lt;span class="o">[&lt;/span>-option&lt;span class="o">]&lt;/span> &lt;span class="o">[&lt;/span>command&lt;span class="o">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Options:&lt;/p>
&lt;ul>
&lt;li>&lt;code>-S&lt;/code>: specify the session name&lt;/li>
&lt;li>&lt;code>-d&lt;/code>: detach from the session&lt;/li>
&lt;li>&lt;code>-r&lt;/code>: reattach to the session&lt;/li>
&lt;li>&lt;code>-c&lt;/code>: execute the command in the session&lt;/li>
&lt;li>&lt;code>-L&lt;/code>: list all sessions&lt;/li>
&lt;li>&lt;code>-wipe&lt;/code>: wipe out all sessions&lt;/li>
&lt;li>&lt;code>-x&lt;/code>: execute the command in the session&lt;/li>
&lt;li>&lt;code>-p&lt;/code>: specify the port number&lt;/li>
&lt;li>&lt;code>-m&lt;/code>: specify the mode&lt;/li>
&lt;li>&lt;code>-t&lt;/code>: specify the title&lt;/li>
&lt;li>&lt;code>-v&lt;/code>: specify the version&lt;/li>
&lt;li>&lt;code>-h&lt;/code>: display help information&lt;/li>
&lt;li>&lt;code>-v&lt;/code>: display version information&lt;/li>
&lt;li>&lt;code>-q&lt;/code>: quit the session&lt;/li>
&lt;/ul>
&lt;h1 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://www.gnu.org/software/screen/manual/screen.html" target="_blank" rel="noopener"
>Screen usage&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Phi-4</title><link>https://maosong.website/p/notes-on-phi-4/</link><pubDate>Mon, 16 Dec 2024 17:33:52 +0800</pubDate><guid>https://maosong.website/p/notes-on-phi-4/</guid><description>&lt;h1 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h1>&lt;p>微软在12月12号发布了Phi-4的技术报告,主要关注了数据的构建，特别是。Phi-4是一个14B的大语言模型，主要在STEM相关的QA任务以及推理相关的任务上表现比较好。Phi-4主要在三个方面进行了改进：&lt;/p>
&lt;ol>
&lt;li>在pre-training和mid-training阶段使用了合成数据进行训练&lt;/li>
&lt;li>&lt;/li>
&lt;/ol>
&lt;h1 id="data">&lt;a href="#data" class="header-anchor">&lt;/a>Data
&lt;/h1>&lt;h1 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h1>&lt;h2 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h2>&lt;h1 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h1>&lt;h2 id="dpo">&lt;a href="#dpo" class="header-anchor">&lt;/a>DPO
&lt;/h2>&lt;h2 id="sft">&lt;a href="#sft" class="header-anchor">&lt;/a>SFT
&lt;/h2>&lt;h1 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://techcommunity.microsoft.com/blog/aiplatformblog/introducing-phi-4-microsoft%E2%80%99s-newest-small-language-model-specializing-in-comple/4357090" target="_blank" rel="noopener"
>Blog&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2412.08905" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>An overview of adaption layer in multimodal large language models.</title><link>https://maosong.website/p/an-overview-of-adaption-layer-in-multimodal-large-language-models./</link><pubDate>Sat, 09 Nov 2024 09:53:43 +0800</pubDate><guid>https://maosong.website/p/an-overview-of-adaption-layer-in-multimodal-large-language-models./</guid><description>&lt;h1 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h1>&lt;p>A multimodal large language model (MLLM) usually consists of three parts: an encoder $E$ that ingests the information from different modality, a large language model (LLM) that is corresponds to complete various of downstream tasks given multimodal input such as image and text, and an adaption layer $C$ that aligns features of different modality to word embedding space of the LLM.
Below is an example MLLM adopting aforementioned architecture: LLaVA [1]&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/an-overview-of-adaption-layer-in-multimodal-large-language-models./llava.png"
width="1032"
height="360"
loading="lazy"
alt="Architecture of LlaVA"
class="gallery-image"
data-flex-grow="286"
data-flex-basis="688px"
>&lt;/p>
&lt;p>Efforts have been made to improve the performance of MLLMs. In this post, we aim to review the design of adaption layer and its potential effect on the downstream tasks.&lt;/p>
&lt;h1 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h1>&lt;p>Suppose the hidden size of the LLM is $d$, the feature produced by encoder $E$ is $V\in\mathbb{R}^{P\times d_v}$, where $P$ is the number of features (number of visual patches if $E$ is an visual encoder) and $d_v$ is the channel dimension.
The adaption layer $C$ then aligns the feature $V$ with the word embedding space with $x=C(V)\in\mathbb{R}^{Q\times d}$, where $Q$ is the number of tokens. As we can see, $C$ is actually a mapping from $\mathbb{R}^{P\times d_v}$ to $\mathbb{R}^{Q\times d}$.&lt;/p>
&lt;p>Based on relationship between $d_v$ and $d$, we can divide projection layers into two types:&lt;/p>
&lt;ol>
&lt;li>Feature-preserving adaption layer, where $P=Q$&lt;/li>
&lt;li>Feature-compressing adaption layer, where $P>Q$.&lt;/li>
&lt;/ol>
&lt;h2 id="feature-preserving-adaption-layer">&lt;a href="#feature-preserving-adaption-layer" class="header-anchor">&lt;/a>Feature-preserving adaption layer
&lt;/h2>$$ x = VW^T, \text{ where } W\in\mathbb{R}^{d\times d_v}$$&lt;p>
the code reads as:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># linear layer&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">adaption_layer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_features&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>$$ x = \phi(VW_1^T)W_2^T$$&lt;p>
where $W_1\in\mathbb{R}^{d\times d_v}$, $W_2\in\mathbb{R}^{d\times d}$, $\phi$ is a activation function, specified as &lt;code>nn.GELU()&lt;/code>. The code reads as:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># two-layer MLP&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">adaption_layer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Sequential&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_features&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">GELU&lt;/span>&lt;span class="p">(),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="feature-compressing-adaption-layer">&lt;a href="#feature-compressing-adaption-layer" class="header-anchor">&lt;/a>Feature-compressing adaption layer
&lt;/h2>&lt;p>The feature compression adaption layers can be categorized into three types:&lt;/p>
&lt;ol>
&lt;li>average pooling&lt;/li>
&lt;li>attention pooling&lt;/li>
&lt;li>convolution mapping&lt;/li>
&lt;/ol>
&lt;p>They usually comprise two steps:&lt;/p>
&lt;ol>
&lt;li>reduce the number of features from $P$ to $Q$ with a pooling operation:
$$ f' = \mathcal{P}(f)\in\mathbb{R}^{Q\times d_v} $$&lt;/li>
&lt;li>project compressed features $f'$ to word embedding space with a transformation $\mathcal{T}$:
$$ x = \mathcal{T}(f')\in\mathbb{R}^{Q\times d} $$&lt;/li>
&lt;/ol>
$$ f'_i = \frac{1}{n}\sum_{j=1}^{n}f_{(i-1)n+j}, i=1,\dots,Q $$$$ K = W_kf\in\mathbb{R}^{d_c}, V=W_vf\in\mathbb{R}^{d_c}, f'=\mathrm{softmax}\left(\frac{QK^T}{\sqrt{d_c}}\right)V\in\mathbb{R}^{Q\times d_v} $$&lt;p>
where $W_k, W_v\in\mathbb{R}^{d_c\times d_v}$ and $Q\in\mathbb{R}^{Q\times d_c}$ is a learnable query.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">PerceiverResampler&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_queries&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_features&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_heads&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_queries&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">num_queries&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">hidden_size&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_features&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">num_features&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">query_tokens&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Parameter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zeros&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_queries&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_features&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">requires_grad&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">query_tokens&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">data&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">normal_&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">mean&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">std&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.02&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">attention&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">MultiheadAttention&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_heads&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">layer_norm_kv&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">LayerNorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">layer_norm_q&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">LayerNorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">attention_mask&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">layer_norm_kv&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">permute&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">N&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">layer_norm_q&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">query_tokens&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">q&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">unsqueeze&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">repeat&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">N&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">out&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">attention&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">q&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">v&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">attention_mask&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">attention_mask&lt;/span>&lt;span class="p">)[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">out&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">out&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">permute&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">adaption_layer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Sequential&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">PerceiverResampler&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num_queries&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_features&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_heads&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">MLP&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">intermediate_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_size&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>$$ f_i' = \frac{1}{n}\sum_{j=1}^n w_jf_{(i-1)n+j},\quad x_i = \sum_{k=-K}^Kw_k'f_{i+k}' $$&lt;p>
where $W=[w_1,\dots,w_n]^T\in\mathbb{R}^n$ and $W'=[w_1,\dots,w_n]^T\in\mathbb{R}^{2K}$ are the weights of the convolution layers.&lt;/p>
&lt;p>&lt;strong>D-Abstractor&lt;/strong> aa&lt;/p>
&lt;h1 id="usages">&lt;a href="#usages" class="header-anchor">&lt;/a>Usages
&lt;/h1>&lt;h1 id="comparisons">&lt;a href="#comparisons" class="header-anchor">&lt;/a>Comparisons
&lt;/h1>&lt;h1 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h1>&lt;ol>
&lt;li>&lt;a class="link" href="https://papers.nips.cc/paper_files/paper/2023/hash/6dcf277ea32ce3288914faf369fe6de0-Abstract-Conference.html" target="_blank" rel="noopener"
>LLaVA&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Improved_Baselines_with_Visual_Instruction_Tuning_CVPR_2024_paper.pdf" target="_blank" rel="noopener"
>LLaVA 1.5&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/haotian-liu/LLaVA/blob/main/llava/model/multimodal_projector/builder.py" target="_blank" rel="noopener"
>LLaVA adaption layer code&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2405.10739v1" target="_blank" rel="noopener"
>survey&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>592. Fraction Addition and Subtraction</title><link>https://maosong.website/p/592.-fraction-addition-and-subtraction/</link><pubDate>Fri, 23 Aug 2024 20:16:54 +0800</pubDate><guid>https://maosong.website/p/592.-fraction-addition-and-subtraction/</guid><description>&lt;p>Given a string expression representing an expression of fraction addition and subtraction, return the calculation result in string format.&lt;/p>
&lt;h1 id="intuition">&lt;a href="#intuition" class="header-anchor">&lt;/a>Intuition
&lt;/h1>&lt;p>Simulate the calculation of fraction addition and subtraction process.&lt;/p>
&lt;h1 id="approach">&lt;a href="#approach" class="header-anchor">&lt;/a>Approach
&lt;/h1>&lt;p>First, to simplify addition and subtraction, we move the &lt;code>-&lt;/code> to numerator part, for example, &lt;code>3/4-2/3&lt;/code> becomes &lt;code>3/4+(-2)/3&lt;/code>, this step makes all operations become addition.&lt;/p>
&lt;p>Second, we use a initial value &lt;code>0/1&lt;/code> to record result to prevent from parsing the first fraction. Thus &lt;code>3/4-2/3&lt;/code> is actually equivalent to &lt;code>0/1+3/4-2/3&lt;/code>.&lt;/p>
&lt;p>Now, in each loop, we record the sign, the numerator, the denominator. Then we compute the result with the previous result with the formula&lt;/p>
$$
\frac{a}{b} + \frac{c}{d} = \frac{ad-bc}{bd}
$$&lt;p>after computation, we use &lt;code>gcd()&lt;/code> function to make the resulted fraction irreducible.&lt;/p>
&lt;h1 id="complexity">&lt;a href="#complexity" class="header-anchor">&lt;/a>Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n)$$&lt;/li>
&lt;li>
$$O(1)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">string&lt;/span> &lt;span class="n">fractionAddition&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">string&lt;/span> &lt;span class="n">expression&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">expression&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">numerator1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dominator1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">numerator2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dominator2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">sign&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">expression&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="sc">&amp;#39;-&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sign&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">expression&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="sc">&amp;#39;+&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sign&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">numerator2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="sc">&amp;#39;0&amp;#39;&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">expression&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="n">expression&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="sc">&amp;#39;9&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">numerator2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">numerator2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">10&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">expression&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="sc">&amp;#39;0&amp;#39;&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">numerator2&lt;/span> &lt;span class="o">*=&lt;/span> &lt;span class="n">sign&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="c1">// move sign to numerator
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="n">sign&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="c1">// reset sign
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="c1">// division operator
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="n">dominator2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="sc">&amp;#39;0&amp;#39;&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">expression&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="n">expression&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="sc">&amp;#39;9&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dominator2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">dominator2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">10&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">expression&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="sc">&amp;#39;0&amp;#39;&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">numerator1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">numerator1&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">dominator2&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">numerator2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">dominator1&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dominator1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">dominator1&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">dominator2&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">numerator1&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="n">dominator1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">gcd_num&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">gcd&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">numerator1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dominator1&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">numerator1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">numerator1&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">gcd_num&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dominator1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">dominator1&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">gcd_num&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// corner case
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">numerator1&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="s">&amp;#34;0/1&amp;#34;&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="nf">to_string&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">numerator1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="s">&amp;#34;/&amp;#34;&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">to_string&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dominator1&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/fraction-addition-and-subtraction/description" target="_blank" rel="noopener"
>leetcode 592&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>506. Relative Ranks</title><link>https://maosong.website/p/506.-relative-ranks/</link><pubDate>Thu, 22 Aug 2024 21:36:01 +0800</pubDate><guid>https://maosong.website/p/506.-relative-ranks/</guid><description>&lt;p>Given an integer, flip all bits in its binary representation.&lt;/p>
&lt;h1 id="intuition">&lt;a href="#intuition" class="header-anchor">&lt;/a>Intuition
&lt;/h1>&lt;p>Use XOR operation to complete this task.&lt;/p>
&lt;h1 id="approach">&lt;a href="#approach" class="header-anchor">&lt;/a>Approach
&lt;/h1>&lt;p>We can use the XOR operation to complete this task. However, notice that we cannot simply use &lt;code>num ^ INT_MAX&lt;/code> since the leading &lt;code>1&lt;/code>s are still &lt;code>1&lt;/code>s if &lt;code>num&lt;/code> is too small. Instead, we need to compute the number of digits in &lt;code>num&lt;/code>, and use the corresponding masks.&lt;/p>
&lt;p>Notice that if &lt;code>2^30 &amp;lt; num &amp;lt;= 2^31-1&lt;/code>, in this case the corresponding mask will exceeds the integer ranges, thus we use &lt;code>INT_MAX&lt;/code> directly in this case.&lt;/p>
&lt;h1 id="complexity">&lt;a href="#complexity" class="header-anchor">&lt;/a>Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(\log n)$$&lt;/li>
&lt;li>
$$O(1)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">findComplement&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">num&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">num_bits&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">temp&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">num&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">temp&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">temp&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">++&lt;/span>&lt;span class="n">num_bits&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">num_bits&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">31&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">num&lt;/span> &lt;span class="o">^&lt;/span> &lt;span class="n">INT_MAX&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">mask&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span> &lt;span class="o">&amp;lt;&amp;lt;&lt;/span> &lt;span class="n">num_bits&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">num&lt;/span> &lt;span class="o">^&lt;/span> &lt;span class="n">mask&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/number-complement/description" target="_blank" rel="noopener"
>leetcode 476&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>664. Strange Printer</title><link>https://maosong.website/p/664.-strange-printer/</link><pubDate>Wed, 21 Aug 2024 15:16:41 +0000</pubDate><guid>https://maosong.website/p/664.-strange-printer/</guid><description>&lt;h1 id="approach">&lt;a href="#approach" class="header-anchor">&lt;/a>Approach
&lt;/h1>&lt;p>We can use a &lt;code>dp&lt;/code> matrix, where element &lt;code>dp[i][j]&lt;/code> represents the minimum turn to print substring &lt;code>s[i...j]&lt;/code>.&lt;/p>
&lt;p>Then, there are two cases:&lt;/p>
&lt;ul>
&lt;li>case 1: We print &lt;code>s[i]&lt;/code> separately, now we have &lt;code>dp[i][j] = 1 + dp[i + 1][j]&lt;/code>.&lt;/li>
&lt;li>case 2: There is a char &lt;code>s[k] == s[i]&lt;/code>, then the string &lt;code>s[i...k]&lt;/code> can be obtained by printing some new substrings on the range &lt;code>s[i...k]&lt;/code>, now we have &lt;code>dp[i][j] = dp[i][k-1]+dp[k+1][j]&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Combining case 1 and case 2, we can now write the update formula for dynamic programming:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">k&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">k&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>the base case is when &lt;code>i &amp;gt; j&lt;/code>, &lt;code>d[pi][j] = 0&lt;/code> and when &lt;code>i=j&lt;/code>, &lt;code>dp[i][j] = 1&lt;/code>.&lt;/p>
&lt;h1 id="complexity">&lt;a href="#complexity" class="header-anchor">&lt;/a>Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n^3)$$&lt;/li>
&lt;li>
$$O(n^2)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">dp&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">start&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">end&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="k">const&lt;/span> &lt;span class="n">string&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">start&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">end&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">return&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">start&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">end&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">return&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">start&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">return&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">start&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">ans&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dp&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">start&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">end&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">k&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">start&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">k&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">end&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">start&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ans&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ans&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dp&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">start&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dp&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">end&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">start&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ans&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">ans&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="nf">strangePrinter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">string&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dp&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/strange-printer/description/" target="_blank" rel="noopener"
>Leetcode 664&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>264. Ugly Number II</title><link>https://maosong.website/p/264.-ugly-number-ii/</link><pubDate>Sun, 18 Aug 2024 10:37:20 +0000</pubDate><guid>https://maosong.website/p/264.-ugly-number-ii/</guid><description>&lt;p>A number is &lt;code>ugly&lt;/code> if its prime factors is a subset of $\{2, 3, 5\}$. We are required to find the $n$-th &lt;code>ugly&lt;/code> number&lt;/p>
&lt;h1 id="intuition">&lt;a href="#intuition" class="header-anchor">&lt;/a>Intuition
&lt;/h1>&lt;p>Each ugly number is generated from a previous ugly number by multiplying $2$, $3$ or $5$.&lt;/p>
&lt;h1 id="approach">&lt;a href="#approach" class="header-anchor">&lt;/a>Approach
&lt;/h1>&lt;p>We use three pointers &lt;code>index_2&lt;/code>, &lt;code>index_3&lt;/code> and &lt;code>index_5&lt;/code> to record the index of previous ugly number. For example, $4$ is generated from $2$ by multiplying $2$, so the index &lt;code>index_2&lt;/code> is the index of &lt;code>2&lt;/code>.&lt;/p>
&lt;p>Then, we take the minimum of three generated numbers:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">index_2&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">index_3&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">index_5&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Finally, we need to update the pointers so that there is no duplication.&lt;/p>
&lt;h1 id="complexity">&lt;a href="#complexity" class="header-anchor">&lt;/a>Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n)$$&lt;/li>
&lt;li>
$$O(n)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">nthUglyNumber&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">index_2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">index_3&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">index_5&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">result_2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">index_2&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">result_3&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">index_3&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">result_5&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">index_5&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">current_ugly_num&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">result_2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">result_3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">result_5&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">current_ugly_num&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">result_2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">index_2&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">current_ugly_num&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">result_3&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">index_3&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">current_ugly_num&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">result_5&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">index_5&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/ugly-number-ii/description/" target="_blank" rel="noopener"
>Leetcode 264&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on VITA</title><link>https://maosong.website/p/notes-on-vita/</link><pubDate>Tue, 13 Aug 2024 14:36:45 +0800</pubDate><guid>https://maosong.website/p/notes-on-vita/</guid><description>&lt;h1 id="tldr">&lt;a href="#tldr" class="header-anchor">&lt;/a>TLDR
&lt;/h1>&lt;p>This paper proposes a Multimodal Large Language Model VITA (Video, Image, Text, Audio).
VITA supports non-awakening interaction and audio interruption for better interactive experience.
VITA aims to be an open-sourced version of GPT-4o.&lt;/p>
&lt;h1 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h1>&lt;p>Features of GPT-4o:&lt;/p>
&lt;ol>
&lt;li>a unified framework that processes text, vision, and audio signals in an end-to-end manner,&lt;/li>
&lt;li>the capability to enable natural multimodal human-computer interaction.&lt;/li>
&lt;/ol>
&lt;p>Similar to Mini-GPT4, this paper tries to proposed an open-sourced version of GPT-4o.&lt;/p>
&lt;h1 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h1>&lt;h2 id="model">&lt;a href="#model" class="header-anchor">&lt;/a>Model
&lt;/h2>&lt;p>The architecture of VITA is shown as follows:
&lt;img src="https://maosong.website/p/notes-on-vita/VITA_architecture.png"
width="1282"
height="1069"
loading="lazy"
alt="Architecture of VITA"
class="gallery-image"
data-flex-grow="119"
data-flex-basis="287px"
>&lt;/p>
&lt;ul>
&lt;li>LLM: Mixtral $8\times 7$ B&lt;/li>
&lt;li>Visual Encoder: InternViT-300M-448px&lt;/li>
&lt;li>Audio Encoder: Mel Filter Bank block&lt;/li>
&lt;/ul>
&lt;p>To support audio interruption, the author uses two model at the same time, where the generation model is responsible for handling user queries and the other model monitors the environment. The other models starts to work is there is an audio interruption.&lt;/p>
&lt;h2 id="data">&lt;a href="#data" class="header-anchor">&lt;/a>Data
&lt;/h2>&lt;h3 id="multimodal-instruction-tuning">&lt;a href="#multimodal-instruction-tuning" class="header-anchor">&lt;/a>multimodal instruction tuning
&lt;/h3>&lt;p>Training data of multimodal instruction tuning is given as follows:&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-vita/VITA_SFT_data.png"
width="1286"
height="672"
loading="lazy"
alt="Training data of multimodal instruction tuning"
class="gallery-image"
data-flex-grow="191"
data-flex-basis="459px"
>&lt;/p>
&lt;p>Improvements are made:&lt;/p>
&lt;ol>
&lt;li>The questions are randomly (about half) replaced with their audio versions, using TTS technique such as GPT-SoVITS&lt;/li>
&lt;li>Different system prompts are set to avoid conflicts between different types of data&lt;/li>
&lt;/ol>
&lt;p>To support human-AI interaction, the noisy audio data are also constructed. Noisy audio samples are generated from existed QA data. These negative sample texts aim to improve the ability of VITA to not respond to non-query-related content.&lt;/p>
&lt;p>To distinguish three types of queries, the author uses three state tokens:&lt;/p>
&lt;ul>
&lt;li>Token &lt;code>&amp;lt;1&amp;gt;&lt;/code> denotes that the question input is the query audio&lt;/li>
&lt;li>Token &lt;code>&amp;lt;2&amp;gt;&lt;/code> denotes that the question input is the noisy audio.&lt;/li>
&lt;li>Token &lt;code>&amp;lt;3&amp;gt;&lt;/code> signifies the question of pure text.&lt;/li>
&lt;/ul>
&lt;h2 id="training-pipeline">&lt;a href="#training-pipeline" class="header-anchor">&lt;/a>Training pipeline
&lt;/h2>&lt;p>Training pipeline of VITA consists of three stages:&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-vita/VITA_training_pipeline.png"
width="1301"
height="852"
loading="lazy"
alt="Training pipeline of VITA"
class="gallery-image"
data-flex-grow="152"
data-flex-basis="366px"
>&lt;/p>
&lt;h3 id="non-awakening-interaction">&lt;a href="#non-awakening-interaction" class="header-anchor">&lt;/a>Non-awakening Interaction
&lt;/h3>&lt;p>There are following requirements and solutions:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Real-time Tracking of Environmental Sounds. This paper uses SileroVAD to complete the Voice Activity Detection (VAD) task.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Filtering out noisy audio. This is done by making use of token &lt;code>&amp;lt;2&amp;gt;&lt;/code>.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="audio-interrupt-interaction">&lt;a href="#audio-interrupt-interaction" class="header-anchor">&lt;/a>Audio Interrupt Interaction
&lt;/h3>&lt;p>There are following requirements and solutions:&lt;/p>
&lt;ol>
&lt;li>Real-time Tracking and Filtering of External Queries. This is done by use another VITA model as stated in Model section.&lt;/li>
&lt;/ol>
&lt;h1 id="evaluation">&lt;a href="#evaluation" class="header-anchor">&lt;/a>Evaluation
&lt;/h1>&lt;p>&lt;img src="https://maosong.website/p/notes-on-vita/VITA_evaluation.png"
width="1297"
height="536"
loading="lazy"
alt="Evaluation on image and video understanding"
class="gallery-image"
data-flex-grow="241"
data-flex-basis="580px"
>&lt;/p>
&lt;h1 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h1>&lt;p>The paper points out three limitations of VITA:&lt;/p>
&lt;ol>
&lt;li>Enhancement of Foundational Capabilities.&lt;/li>
&lt;li>Refinement of Noisy Audio Construction.&lt;/li>
&lt;li>Building end-to-end TTS in conjunction with LLM.&lt;/li>
&lt;/ol>
&lt;h1 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2408.05211" target="_blank" rel="noopener"
>Arxiv paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://vita-home.github.io" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>78. Subsets</title><link>https://maosong.website/p/78.-subsets/</link><pubDate>Tue, 21 May 2024 22:16:49 +0800</pubDate><guid>https://maosong.website/p/78.-subsets/</guid><description>&lt;p>Given an array, find all subsets in the array&lt;/p>
&lt;h1 id="intuition">&lt;a href="#intuition" class="header-anchor">&lt;/a>Intuition
&lt;/h1>&lt;p>Use DFS to iterate over all subsets and record them.&lt;/p>
&lt;h1 id="approach">&lt;a href="#approach" class="header-anchor">&lt;/a>Approach
&lt;/h1>&lt;p>A subset can be represented as a binary number of &lt;code>n&lt;/code> digits. Each digit is either &lt;code>0&lt;/code> or &lt;code>1&lt;/code>. We can use DFS to iterate over all possible such format of binary numbers.&lt;/p>
&lt;h1 id="complexity">&lt;a href="#complexity" class="header-anchor">&lt;/a>Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(2^n)$$&lt;/li>
&lt;li>
$$O(2^n)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">void&lt;/span> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">index&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">index&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// digit is 1
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nums&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">index&lt;/span>&lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nums&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">index&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">s&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">pop_back&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// digit is 0
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nums&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">index&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">subsets&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nums&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/subsets/description/" target="_blank" rel="noopener"
>Leetcode&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>2812. Find the Safest Path in a Grid</title><link>https://maosong.website/p/2812.-find-the-safest-path-in-a-grid/</link><pubDate>Wed, 15 May 2024 20:24:56 +0800</pubDate><guid>https://maosong.website/p/2812.-find-the-safest-path-in-a-grid/</guid><description>&lt;p>Given a matrix, we wish to find a path that from start to end, such that the path is as far as from the dangerous position.&lt;/p>
&lt;h1 id="intuition">&lt;a href="#intuition" class="header-anchor">&lt;/a>Intuition
&lt;/h1>&lt;p>We can convert the problem into finding a path with minimum weights in a graph.&lt;/p>
&lt;h1 id="approach">&lt;a href="#approach" class="header-anchor">&lt;/a>Approach
&lt;/h1>&lt;p>We first convert the matrix into a graph, with node as position &lt;code>(i, j)&lt;/code> and weight &lt;code>w[i,j]\min_k(|i-thief[k][0]| + |j - thief[k][1]|)&lt;/code>, where &lt;code>thief[k]=(thief[k][0], thief[k][1])&lt;/code> is the position of the thief &lt;code>thief[k]&lt;/code>. To do this, we can use DFS, starting from each thief and iterate through the grids.&lt;/p>
&lt;p>Then, we need to find a path from the start &lt;code>(0, 0)&lt;/code> to target &lt;code>(n - 1, n - 1)&lt;/code>. This can be done via Dijkstra&amp;rsquo;s Algorithm. We use a priority queue to keep track of minimum to-be-visited nodes, this ensures that the newly added nodes are always with the maximum safe factor.&lt;/p>
&lt;h1 id="complexity">&lt;a href="#complexity" class="header-anchor">&lt;/a>Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n^2\log n)$$&lt;/li>
&lt;li>
$$O(n^2)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;span class="lnt">49
&lt;/span>&lt;span class="lnt">50
&lt;/span>&lt;span class="lnt">51
&lt;/span>&lt;span class="lnt">52
&lt;/span>&lt;span class="lnt">53
&lt;/span>&lt;span class="lnt">54
&lt;/span>&lt;span class="lnt">55
&lt;/span>&lt;span class="lnt">56
&lt;/span>&lt;span class="lnt">57
&lt;/span>&lt;span class="lnt">58
&lt;/span>&lt;span class="lnt">59
&lt;/span>&lt;span class="lnt">60
&lt;/span>&lt;span class="lnt">61
&lt;/span>&lt;span class="lnt">62
&lt;/span>&lt;span class="lnt">63
&lt;/span>&lt;span class="lnt">64
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">dirs&lt;/span>&lt;span class="p">{{&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">},&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">},&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">},&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">}};&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">maximumSafenessFactor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="o">||&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">n&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">n&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">scores&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">INT_MAX&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">queue&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">pair&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">q&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">make_pair&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">scores&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="o">!&lt;/span>&lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">empty&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">const&lt;/span> &lt;span class="k">auto&lt;/span> &lt;span class="n">node&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">front&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">first&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">second&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="k">auto&lt;/span>&lt;span class="o">&amp;amp;&lt;/span> &lt;span class="nl">dir&lt;/span> &lt;span class="p">:&lt;/span> &lt;span class="n">dirs&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">new_x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">new_y&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">new_x&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="mi">0&lt;/span> &lt;span class="o">||&lt;/span> &lt;span class="n">new_x&lt;/span> &lt;span class="o">&amp;gt;=&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">||&lt;/span> &lt;span class="n">new_y&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="mi">0&lt;/span> &lt;span class="o">||&lt;/span> &lt;span class="n">new_y&lt;/span> &lt;span class="o">&amp;gt;=&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">scores&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">new_x&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">new_y&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">scores&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">scores&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">new_x&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">new_y&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">scores&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">make_pair&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">new_x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">new_y&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">pop&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">bool&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">visited&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">bool&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">false&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">priority_queue&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">pair&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">pair&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">pq&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pq&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">make_pair&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">scores&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">make_pair&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="o">!&lt;/span>&lt;span class="n">pq&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">empty&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">auto&lt;/span> &lt;span class="n">node&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">pq&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">top&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pq&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">pop&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">safe_factor&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">first&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">auto&lt;/span> &lt;span class="n">pos&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">second&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">pos&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">first&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="n">pos&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">second&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">safe_factor&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">visited&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">pos&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">first&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">pos&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">second&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">true&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="k">auto&lt;/span>&lt;span class="o">&amp;amp;&lt;/span> &lt;span class="nl">dir&lt;/span> &lt;span class="p">:&lt;/span> &lt;span class="n">dirs&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">new_x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">pos&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">first&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">new_y&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">pos&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">second&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">new_x&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="mi">0&lt;/span> &lt;span class="o">||&lt;/span> &lt;span class="n">new_x&lt;/span> &lt;span class="o">&amp;gt;=&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">||&lt;/span> &lt;span class="n">new_y&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="mi">0&lt;/span> &lt;span class="o">||&lt;/span> &lt;span class="n">new_y&lt;/span> &lt;span class="o">&amp;gt;=&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">visited&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">new_x&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">new_y&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">score&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">safe_factor&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">scores&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">new_x&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">new_y&lt;/span>&lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pq&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">make_pair&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">score&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">make_pair&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">new_x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">new_y&lt;/span>&lt;span class="p">)));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">visited&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">new_x&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">new_y&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">true&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/find-the-safest-path-in-a-grid/description" target="_blank" rel="noopener"
>Leetcode&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>1219. Path with Maximum Gold</title><link>https://maosong.website/p/1219.-path-with-maximum-gold/</link><pubDate>Tue, 14 May 2024 20:39:03 +0800</pubDate><guid>https://maosong.website/p/1219.-path-with-maximum-gold/</guid><description>&lt;p>Given a matrix, whose element representing the number of golds. Find a path such that the sum is maximized and without crossing the grids that has not gold elements.&lt;/p>
&lt;h1 id="intuition">&lt;a href="#intuition" class="header-anchor">&lt;/a>Intuition
&lt;/h1>&lt;p>Use backtracking to find all possible paths, and update the results.&lt;/p>
&lt;h1 id="approach">&lt;a href="#approach" class="header-anchor">&lt;/a>Approach
&lt;/h1>&lt;p>We use two variables &lt;code>current&lt;/code> and &lt;code>result&lt;/code> to store results, &lt;code>current&lt;/code> stores the sum of golds from start to current position, &lt;code>result&lt;/code> stores the final result.&lt;/p>
&lt;h1 id="complexity">&lt;a href="#complexity" class="header-anchor">&lt;/a>Complexity
&lt;/h1>&lt;ul>
&lt;li>Time complexity: In worst case, each grid contains gold, for each position, there are $\binom{m+n}{m}$ possible paths, so the overall complexity is&lt;/li>
&lt;/ul>
$$O\left(mn\binom{m+n}{m}\right)$$&lt;ul>
&lt;li>Space complexity: No extra spaces needed (without considering recursive stack)&lt;/li>
&lt;/ul>
$$O(1)$$&lt;h1 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">dirs&lt;/span>&lt;span class="p">{{&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">},&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">},&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">},&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">}};&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">void&lt;/span> &lt;span class="n">backtracking&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">current&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">result&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">return&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">current&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// update result
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">max&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">result&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">current&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// mark as visited
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">temp&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="k">auto&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="nl">dir&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">dirs&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">row&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">col&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">row&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="mi">0&lt;/span> &lt;span class="o">||&lt;/span> &lt;span class="n">row&lt;/span> &lt;span class="o">&amp;gt;=&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">||&lt;/span> &lt;span class="n">col&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="mi">0&lt;/span> &lt;span class="o">||&lt;/span> &lt;span class="n">col&lt;/span> &lt;span class="o">&amp;gt;=&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">].&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">backtracking&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">grid&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">row&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">col&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">current&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// retrieve the state
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">temp&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">current&lt;/span> &lt;span class="o">-=&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="nf">getMaximumGold&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">].&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">current&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">backtracking&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">grid&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">current&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/path-with-maximum-gold/description/" target="_blank" rel="noopener"
>Leetcode 1219&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>861. Score After Flipping Matrix</title><link>https://maosong.website/p/861.-score-after-flipping-matrix/</link><pubDate>Mon, 13 May 2024 20:52:02 +0800</pubDate><guid>https://maosong.website/p/861.-score-after-flipping-matrix/</guid><description>&lt;p>Given a binary matrix, we can flip one column or one row, the goal is to flip zero or more times such that the sum of the number represented by the rows are maximized.&lt;/p>
&lt;h1 id="intuition">&lt;a href="#intuition" class="header-anchor">&lt;/a>Intuition
&lt;/h1>&lt;p>The leading &lt;code>1&lt;/code>s are always important than the trailing &lt;code>1&lt;/code>s. So we make sure that &lt;code>1&lt;/code> appears before &lt;code>0&lt;/code>s.&lt;/p>
&lt;h1 id="approach">&lt;a href="#approach" class="header-anchor">&lt;/a>Approach
&lt;/h1>&lt;p>The challenge is to determine when to flip the rows and flip the columns. From the intuition, we know that:&lt;/p>
&lt;ol>
&lt;li>a row is flipped only if its first bit is &lt;code>0&lt;/code>, after flipping, the number becomes larger and cannot be flipped again.&lt;/li>
&lt;li>a column is flipped only if the number of &lt;code>1&lt;/code>s are smaller than &lt;code>0&lt;/code>s.&lt;/li>
&lt;/ol>
&lt;p>So we flip the rows first and the columns second.&lt;/p>
&lt;h1 id="complexity">&lt;a href="#complexity" class="header-anchor">&lt;/a>Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$ O(mn) $$&lt;/li>
&lt;li>
$$ O(1) $$&lt;/li>
&lt;/ul>
&lt;h1 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">matrixScore&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">m&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">].&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">m&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// flip row
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// check column by column
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">count&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">m&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">count&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">count&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">m&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// flip column
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">k&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">k&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">m&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">m&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span> &lt;span class="o">&amp;lt;&amp;lt;&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/score-after-flipping-matrix/description/" target="_blank" rel="noopener"
>Leetcode&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>786. K-th Smallest Prime Fraction</title><link>https://maosong.website/p/786.-k-th-smallest-prime-fraction/</link><pubDate>Fri, 10 May 2024 22:39:54 +0800</pubDate><guid>https://maosong.website/p/786.-k-th-smallest-prime-fraction/</guid><description>&lt;p>Given an integer array of size $n$ containing prime integers, it can form $n(n-1)/2$ fractions, we are required to find the $k$-th smallest prime fraction.&lt;/p>
&lt;h1 id="intuition">&lt;a href="#intuition" class="header-anchor">&lt;/a>Intuition
&lt;/h1>&lt;p>We can use a priority queue to store prime integers, then we maintain the priority queue.&lt;/p>
&lt;h1 id="approach1-brute-force">&lt;a href="#approach1-brute-force" class="header-anchor">&lt;/a>Approach1: Brute force
&lt;/h1>&lt;h2 id="complexity">&lt;a href="#complexity" class="header-anchor">&lt;/a>Complexity
&lt;/h2>&lt;ul>
&lt;li>
$$O(n^2\log k)$$&lt;/li>
&lt;li>
$$O(k)$$&lt;/li>
&lt;/ul>
&lt;h2 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h2>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">class&lt;/span> &lt;span class="nc">Compare&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">bool&lt;/span> &lt;span class="k">operator&lt;/span>&lt;span class="p">()(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">a&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="k">const&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">){&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="mf">1.0&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">a&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">a&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="mf">1.0&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">];&lt;/span> &lt;span class="c1">// the root is the biggest
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">kthSmallestPrimeFraction&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">arr&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">priority_queue&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Compare&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">pq&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">arr&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">r&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">r&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">k&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">r&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">r&lt;/span> &lt;span class="o">&amp;gt;=&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">break&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pq&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="n">arr&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">arr&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">r&lt;/span>&lt;span class="p">]});&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">pq&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pq&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">pop&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">pq&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">top&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="approach2-simplification">&lt;a href="#approach2-simplification" class="header-anchor">&lt;/a>Approach2: Simplification
&lt;/h1>&lt;p>Notice that Approach1 requires iterating over all fractions, can we reduce the time complexity?&lt;/p>
&lt;p>The solution is by considering the relative order, if we write a matrix whose element &lt;code>a[i][j]=nums[i]/nums[j]&lt;/code> (&lt;code>i&amp;lt;j&lt;/code>), then we know that &lt;code>a[i][i+1]&amp;gt;...&amp;gt;a[n-1][n]&lt;/code> since the array &lt;code>nums&lt;/code> are increasing. So, the smallest fraction are in &lt;code>a[1][2], ..., a[n-1][n]&lt;/code>. If we take the smallest fraction, and add its successive elements (same column, last row), then we can find the second smallest fraction and so on. This solution requires iterating over $\max(n, k)$ fractions and $O(n)$ spaces.&lt;/p>
&lt;h2 id="complexity-1">&lt;a href="#complexity-1" class="header-anchor">&lt;/a>Complexity
&lt;/h2>&lt;ul>
&lt;li>
$$O(\max(n, k)\log n)$$&lt;/li>
&lt;li>
$$O(n)$$&lt;/li>
&lt;/ul>
&lt;h2 id="code-1">&lt;a href="#code-1" class="header-anchor">&lt;/a>Code
&lt;/h2>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">kthSmallestPrimeFraction&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">A&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">K&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// (fraction, (i, j))
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="n">priority_queue&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">pair&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">double&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">pair&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">pq&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">A&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pq&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push&lt;/span>&lt;span class="p">({&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mf">1.0&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">A&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">A&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">back&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">A&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">}});&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="o">--&lt;/span>&lt;span class="n">K&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">auto&lt;/span> &lt;span class="n">t&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">pq&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">top&lt;/span>&lt;span class="p">().&lt;/span>&lt;span class="n">second&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">pop&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">--&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">second&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pq&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push&lt;/span>&lt;span class="p">({&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mf">1.0&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">A&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">first&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">A&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">second&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">first&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">t&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">second&lt;/span>&lt;span class="p">}});&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="n">A&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">pq&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">top&lt;/span>&lt;span class="p">().&lt;/span>&lt;span class="n">second&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">first&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">A&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">pq&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">top&lt;/span>&lt;span class="p">().&lt;/span>&lt;span class="n">second&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">second&lt;/span>&lt;span class="p">]};&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/k-th-smallest-prime-fraction/description/" target="_blank" rel="noopener"
>leetcode&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>3068. Find the Maximum Sum of Node Values</title><link>https://maosong.website/p/3068.-find-the-maximum-sum-of-node-values/</link><pubDate>Thu, 09 May 2024 20:21:07 +0800</pubDate><guid>https://maosong.website/p/3068.-find-the-maximum-sum-of-node-values/</guid><description>&lt;p>Given a graph and an integer $k$, where the nodes represent values, we can choose an edge, and perform XOR operations on its nodes corresponding to $k$. The goal is the find the maximum sum of the values after 0 or more XOR operations.&lt;/p>
&lt;h1 id="intuition">&lt;a href="#intuition" class="header-anchor">&lt;/a>Intuition
&lt;/h1>&lt;p>Since XOR operations satisfies the property that &lt;code>a XOR b XOR b = a&lt;/code>, we can record the gain after XOR operation on an edge and finally obtain the result.&lt;/p>
&lt;h1 id="approach">&lt;a href="#approach" class="header-anchor">&lt;/a>Approach
&lt;/h1>&lt;p>We use &lt;code>total_sum&lt;/code> to record the sum of the values of the original trees.&lt;/p>
&lt;p>Then for each node, we perform the XOR operation and record the &lt;code>change&lt;/code> if the &lt;code>change &amp;gt; 0&lt;/code>, and this change requires one operation, which we add to &lt;code>count&lt;/code>. Meanwhile, we use &lt;code>positive_min&lt;/code> and &lt;code>negative_max&lt;/code> to record the minimum absolute change for reverting use.&lt;/p>
&lt;p>Now after all nodes are computed, we need to compute the result.&lt;/p>
&lt;ol>
&lt;li>If &lt;code>count&lt;/code> is even, it means the operations satisfied the requirement that the nodes of an edge changes simultaneously&lt;/li>
&lt;li>If &lt;code>count&lt;/code> is odd, then there is one invalid operation and we need to revert the operation. To make the final sum maximum, we can either subtract the &lt;code>positive_min&lt;/code> or add &lt;code>negative_max&lt;/code>, the result is then obtained by taking the maximum of them.&lt;/li>
&lt;/ol>
&lt;h1 id="complexity">&lt;a href="#complexity" class="header-anchor">&lt;/a>Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n)$$&lt;/li>
&lt;li>
$$O(1)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">long&lt;/span> &lt;span class="kt">long&lt;/span> &lt;span class="n">maximumValueSum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">edges&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">long&lt;/span> &lt;span class="kt">long&lt;/span> &lt;span class="n">total_sum&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">count&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">positive_min&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">INT_MAX&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">negative_max&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">INT_MIN&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="nl">num&lt;/span> &lt;span class="p">:&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">new_num&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">num&lt;/span> &lt;span class="o">^&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">total_sum&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">num&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">change&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">new_num&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">num&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">change&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">positive_min&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">positive_min&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">change&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">total_sum&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">change&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">++&lt;/span>&lt;span class="n">count&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">negative_max&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">max&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">negative_max&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">change&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">count&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">total_sum&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="nf">max&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">total_sum&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">positive_min&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">total_sum&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">negative_max&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/find-the-maximum-sum-of-node-values/description/" target="_blank" rel="noopener"
>Leetcode&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>3075. Maximize Happiness of Selected Children</title><link>https://maosong.website/p/3075.-maximize-happiness-of-selected-children/</link><pubDate>Thu, 09 May 2024 20:21:07 +0800</pubDate><guid>https://maosong.website/p/3075.-maximize-happiness-of-selected-children/</guid><description>&lt;p>Given an integer array representing the happiness of the children, select $k$ children such that the sum of their happiness is maximized.&lt;/p>
&lt;h1 id="intuition">&lt;a href="#intuition" class="header-anchor">&lt;/a>Intuition
&lt;/h1>&lt;p>Since happiness of all rest children after choosing one child will decrease by 1, their relative order will still the same. So this problem is actually requiring us to select $k$ most happy children.&lt;/p>
&lt;h1 id="approach">&lt;a href="#approach" class="header-anchor">&lt;/a>Approach
&lt;/h1>&lt;p>Use sorting.&lt;/p>
&lt;h1 id="complexity">&lt;a href="#complexity" class="header-anchor">&lt;/a>Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n\log n)$$&lt;/li>
&lt;li>
$$O(1)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">long&lt;/span> &lt;span class="kt">long&lt;/span> &lt;span class="n">maximumHappinessSum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">happiness&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sort&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">happiness&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">begin&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">happiness&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">());&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">happiness&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">long&lt;/span> &lt;span class="kt">long&lt;/span> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">max&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">happiness&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">n&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/maximize-happiness-of-selected-children/description/" target="_blank" rel="noopener"
>Leetcode&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>ROUGE (Recall-Oriented Understudy)</title><link>https://maosong.website/p/rouge-recall-oriented-understudy/</link><pubDate>Thu, 09 May 2024 17:35:20 +0800</pubDate><guid>https://maosong.website/p/rouge-recall-oriented-understudy/</guid><description>&lt;p>ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes several automatic evaluation methods that measure the similarity between summaries.&lt;/p>
&lt;h1 id="preliminaries">&lt;a href="#preliminaries" class="header-anchor">&lt;/a>Preliminaries
&lt;/h1>&lt;h1 id="rouge-n-n-gram-co-occurrence-statistics">&lt;a href="#rouge-n-n-gram-co-occurrence-statistics" class="header-anchor">&lt;/a>ROUGE-N: N-gram co-occurrence statistics
&lt;/h1>&lt;h2 id="definitions">&lt;a href="#definitions" class="header-anchor">&lt;/a>Definitions
&lt;/h2>&lt;p>Given any string $y=y_1y_2\cdots y_K$, where $y_i,i\in{1,\dots,K}$ are characters and an integer $n\geq1$, we define the &lt;strong>set of $n$-gram&lt;/strong> to be&lt;/p>
$$ G_n(y) = \{ y_1\cdots y_n, y_2\cdots y_{n+1}, \dots, y_{K-n+1}\cdots y_K\} $$&lt;p>NOTE that this is a set with unique elements, for example, $G_2(abab)=\{ab, ba\}$.&lt;/p>
&lt;p>Given any two strings $s$ and $y$, we define &lt;strong>substring count&lt;/strong> $C(s,y)$ to tbe the number of appearances of $s$ as a substring of $y$. For example, $C(ab, abcbab)=2$ since $ab$ appear in $abcbab$ twice in position $1$ and $5$.&lt;/p>
&lt;h2 id="base-version">&lt;a href="#base-version" class="header-anchor">&lt;/a>Base Version
&lt;/h2>&lt;p>ROUGE-N is an n-gram recall between a candidate summary $\hat{y}$ and a set of reference summaries $S=\{y_1,\dots,y_n\}$. ROUGE-N is defined as follows:&lt;/p>
$$ \text{ROUGE-N}(\hat{y}, S) = \frac{\sum_{i=1}^n\sum_{s\in G_N(y_i)}C(s,\hat{y})}{\sum_{i=1}^n\sum_{s\in G_N(y_i)}C(s, y_i)} $$&lt;p>Features of ROUGE-N:&lt;/p>
&lt;ol>
&lt;li>the denominator increases as we add more references, since there might exists multiple good summaries.&lt;/li>
&lt;li>A candidate summary that contains words shared by more references is favored by the ROUGE-N measure.&lt;/li>
&lt;/ol>
&lt;h2 id="multiple-references">&lt;a href="#multiple-references" class="header-anchor">&lt;/a>Multiple references
&lt;/h2>&lt;p>When there are multiple references for a candidate summary, it is suggests to use the following formula:&lt;/p>
$$ \text{ROUGE-N}(\hat{y}, S) = \arg\max_{i=1,\dots,n}\text{ROUGE-N}(\hat{y}, \{y_i\}) $$&lt;p>the above formula is favored for the following reasons:&lt;/p>
&lt;ol>
&lt;li>There is no single &amp;ldquo;best&amp;rdquo; reference summary, the multiple reference formula allows ROUGE-N to take into account of all the possible reference summaries and provide a more accurate measure of the quality of the generated summary.&lt;/li>
&lt;li>The multiple reference formula is more robust. If a reference summary contains a typo or a grammatical error, this can affect the ROUGE-N score.&lt;/li>
&lt;li>The multiple reference formula can provide a more comprehensive evaluation of the generated summary, since it can allow ROUGE-N to evaluate the generated summary against a wider range pf possible reference summaries.&lt;/li>
&lt;/ol>
&lt;h1 id="rouge-l">&lt;a href="#rouge-l" class="header-anchor">&lt;/a>ROUGE-L
&lt;/h1>&lt;p>A sequence $Z=[z_1,\dots,z_m]$ is a subsequence of another sequence $X=[x_1,\dots,x_n]$ if there exists a strict increasing sequence $[i_1,\dots,i_k]$ of indices of $X$ such that for all $j=1,\dots,k$, we have $x_{i_j}=z_j$.&lt;/p>
&lt;p>Given two sequences $X$ and $Y$, the longest common subsequences (LCS) of $X$ and $Y$ is a common subsequences with maximum length.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">longest_common_subsequence&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;compute the length of LCS of x and y
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> Args:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> x (str): a string of length m
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> y (str): a string of length n
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> Return:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> int: the length of LCS of x and y
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">m&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dp&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[[&lt;/span>&lt;span class="mi">0&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">m&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)]&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">m&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">max&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">m&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="sentence-level-lcs">&lt;a href="#sentence-level-lcs" class="header-anchor">&lt;/a>Sentence-level LCS
&lt;/h2>&lt;p>The intuition of sentence-level LCS is that the longer the LCS of two summary sentences is, the more similar the two summaries are.&lt;/p>
&lt;p>Given two summaries $X$ of length $m$ and $Y$ of length $n$, assuming $X$ is a reference summary sentence and $Y$ is a candidate summary sentence, the LCS-based recall, precision and F-measure are defined as follows:&lt;/p>
$$ R_{LCS} = \frac{LCS(X, Y)}{m}, P_{LCS} = \frac{LCS(X, Y)}{n}, R_{LCS} = \frac{(1+\beta^2)R_{LCS}P_{LCS}}{R_{LCS}+\beta^2P_{LCS}} $$&lt;p>the above formula is called ROUGE-L. $\beta$ is a hyperparameter.&lt;/p>
&lt;p>Features of ROUGE-L are listed as follows:&lt;/p>
&lt;ol>
&lt;li>It doesn&amp;rsquo;t require consecutive matches but in-sequence matches, which is more reasonable than n-grams.&lt;/li>
&lt;li>It automatically includes longest in-sequence common n-grams, there fore no predefined n-gram length is necessary.&lt;/li>
&lt;li>It&amp;rsquo;s value is less tan or equal to the minimum of unigram F-measure of $X$ and $Y$.&lt;/li>
&lt;li>The disadvantage of ROUGE-L is that it only counts the main in-sequences words, therefore, other alternative LCSes and shorter sequences are not reflected in the final score.&lt;/li>
&lt;/ol>
&lt;h2 id="summary-level-lcs">&lt;a href="#summary-level-lcs" class="header-anchor">&lt;/a>Summary-level LCS
&lt;/h2>&lt;p>We can apply sentence-level LCS-based F-measure score to summary level. Given a reference summary of $u$ sentences $\{r_1,\dots,r_u\}$ containing a total of $m$ words and a candidate summary of $v$ sentences $\{c_1,\dots,c_v\}$ containing a total of $n$ words, the summary-level LCS-based recall, precision and F-measure are defined as follows:&lt;/p>
$$ R_{LCS} = \frac{\sum_{i=1}^u\max_{j}LCS(r_i, c_j)}{m}, P_{LCS} = \frac{\sum_{i=1}^v \max_{j}LCS(r_i, c_j)}{n}, R_{LCS} = \frac{(1+\beta^2)R_{LCS}P_{LCS}}{R_{LCS}+\beta^2P_{LCS}} $$&lt;h1 id="rouge-w">&lt;a href="#rouge-w" class="header-anchor">&lt;/a>ROUGE-W
&lt;/h1>&lt;p>The basic LCS has a problem that it doesn&amp;rsquo;t differentiate LCSes of different spatial relations within their embedding sequences.&lt;/p>
&lt;p>To improve the basic LSC method, we can simply remember the length of consecutive matches encountered so fat to a regular two dimensional dynamic program table computing LCS. we call this &lt;em>weighted LCS (WLCS)&lt;/em> and use $k$ to indicate the length of the current consecutive matches ending at words $x_i$ and $y_j$.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">weighted_longest_common_subsequence&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">f&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Callable&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;&amp;#34;&amp;#34;use dynamic programming to compute WLCS with a weighted function f
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> Args:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> x (str): a candidate summary containing m words
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> y (str): a reference summary containing n words
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> f (Callable): a function satisfies f(x+y) &amp;gt; f(x) + f(y)
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> Return:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> the WLCS score
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> Reference:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> https://aclanthology.org/W04-1013
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">split&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">split&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">m&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">c&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[[&lt;/span>&lt;span class="mi">0&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">m&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)]&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">w&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[[&lt;/span>&lt;span class="mi">0&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">m&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)]&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">m&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># the length of consecutive matches at (i - 1, j - 1)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">k&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">w&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">c&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">f&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">k&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">f&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># remember the length of consecutive matches at (i - 1, j - 1)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">w&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">k&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">c&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">w&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span> &lt;span class="c1"># no match at (i, j)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">elif&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">w&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span> &lt;span class="c1"># no match at (i, j)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">m&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>where &lt;code>c[i][j]&lt;/code> stores the WLCS score ending at word &lt;code>x[i]&lt;/code> of &lt;code>x&lt;/code> and &lt;code>y[i]&lt;/code> of &lt;code>y&lt;/code>. &lt;code>w&lt;/code> stores the length of consecutive matches at &lt;code>c[i][j]&lt;/code>. &lt;code>f&lt;/code> is a function of consecutive matches at &lt;code>c[i][j]&lt;/code>.&lt;/p>
&lt;p>Recall, precision, F-score based on WLCS can be computed as follows:&lt;/p>
$$ R_{WLCS} = f^{-1}\left(\frac{WLCS(X, Y)}{f(m)}\right), P_{WLCS} = f^{-1}\left(\frac{WLCS(X, Y)}{f(n)}\right), R_{LCS} = \frac{(1+\beta^2)R_{WLCS}P_{WLCS}}{R_{WLCS}+\beta^2P_{WLCS}} $$&lt;p>where $f$ is the inverse function of $f$. We call the WLCS-based F-measure as ROUGE-W. Usually, a function $f$ that has a close form inverse is preferred.&lt;/p>
&lt;h1 id="rouge-s">&lt;a href="#rouge-s" class="header-anchor">&lt;/a>ROUGE-S
&lt;/h1>&lt;p>Skip-bigram is any pair of words in their sentence order, allowing for arbitrary gaps. Skip-bigram co-occurrence statistics measure the overlap of skip-bigrams between a candidate translation and a set of reference translations.
A sentence with $n$ words will have $\binom{n}{2}=n(n-1)/2$ skip-bigrams.&lt;/p>
&lt;p>Recall, precision, F-score based on skip-bigram can be computed as follows:&lt;/p>
$$ R_{\mathrm{SKIP2}} = \frac{\mathrm{SKIP2}(X, Y)}{m}, P_{\mathrm{SKIP2}} = \frac{\mathrm{SKIP2}(X, Y)}{n}, R_{\mathrm{SKIP2}} = \frac{(1+\beta^2)R_{\mathrm{SKIP2}}P_{\mathrm{SKIP2}}}{R_{\mathrm{SKIP2}}+\beta^2P_{\mathrm{SKIP2}}} $$&lt;p>where $\mathrm{SKIP2}(X, Y)$ is the number of skip-bigram matches between $X$ and $Y$. The F-score is called ROUGE-S.&lt;/p>
&lt;h1 id="rouge-su">&lt;a href="#rouge-su" class="header-anchor">&lt;/a>ROUGE-SU
&lt;/h1>&lt;p>One problem of ROUGE-S is that it doesn&amp;rsquo;t given any credit to a candidate sentence if the sentence doesn&amp;rsquo;t have any word pair co-occurring with its references.&lt;/p>
&lt;p>To fix this problem, we extend ROUGE-S with the addition of unigram as counting unit. The extended version is called ROUGE-SU. We can also obtain ROUGE-SU from ROUGE-S by adding a begin-of-sentence marker at the beginning of candidate and reference sentences.&lt;/p>
&lt;h1 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://aclanthology.org/W04-1013.pdf" target="_blank" rel="noopener"
>ROUGE: A Package for Automatic Evaluation of Summaries&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/Yale-LILY/SummerTime/blob/e49058d928b4bd5b1017b7d774bea984bbdf5006/summertime/model/third_party/HMNet/Evaluation/OldROUGEEval.py" target="_blank" rel="noopener"
>ROUGE Eval&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/google-research/google-research/tree/master/rouge" target="_blank" rel="noopener"
>google-research rouge&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>506. Relative Ranks</title><link>https://maosong.website/p/506.-relative-ranks/</link><pubDate>Wed, 08 May 2024 20:17:53 +0800</pubDate><guid>https://maosong.website/p/506.-relative-ranks/</guid><description>&lt;p>Given an array of scores, assign different ranks based on their position in the sorted array&lt;/p>
&lt;h1 id="intuition">&lt;a href="#intuition" class="header-anchor">&lt;/a>Intuition
&lt;/h1>&lt;p>Sort the array and assign based on the sorted array.&lt;/p>
&lt;h1 id="approach">&lt;a href="#approach" class="header-anchor">&lt;/a>Approach
&lt;/h1>&lt;p>in C++, we can use the property of the container &lt;code>map&lt;/code> to solve this problem, the map is constructed so that the key is the score and the value is the index of the score.&lt;/p>
&lt;h1 id="complexity">&lt;a href="#complexity" class="header-anchor">&lt;/a>Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n\log n)$$&lt;/li>
&lt;li>
$$O(n)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">string&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">findRelativeRanks&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">score&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">map&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">m&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">score&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">m&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">score&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">string&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">score&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">());&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">index&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="k">auto&lt;/span> &lt;span class="n">iter&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">m&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">rbegin&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="n">iter&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="n">m&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">rend&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">iter&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">index&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">index&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">iter&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">second&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;Gold Medal&amp;#34;&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span> &lt;span class="nf">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">index&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">iter&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">second&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;Silver Medal&amp;#34;&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span> &lt;span class="nf">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">index&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">iter&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">second&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;Bronze Medal&amp;#34;&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">iter&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">second&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">to_string&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">index&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/relative-ranks/description" target="_blank" rel="noopener"
>leetcode 506&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>2487. Remove Nodes From Linked List</title><link>https://maosong.website/p/2487.-remove-nodes-from-linked-list/</link><pubDate>Mon, 06 May 2024 19:47:28 +0800</pubDate><guid>https://maosong.website/p/2487.-remove-nodes-from-linked-list/</guid><description>&lt;p>Given a linked list, we are required to remove some nodes, such that for each node in the result linked list, the value of the node is the greatest from the node to the end of the linked list.&lt;/p>
&lt;h1 id="intuition">&lt;a href="#intuition" class="header-anchor">&lt;/a>Intuition
&lt;/h1>&lt;p>We construct the result linked list from right to left, that is, the last node in the linked list is kept, then the pointer goes from right to left util there is a node with greater value. This process can be done via post traversal as tree.&lt;/p>
&lt;h2 id="complexity">&lt;a href="#complexity" class="header-anchor">&lt;/a>Complexity
&lt;/h2>&lt;ul>
&lt;li>
$$O(n)$$&lt;/li>
&lt;li>
$$O(n)$$&lt;/li>
&lt;/ul>
&lt;h2 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h2>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * Definition for singly-linked list.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * struct ListNode {
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * int val;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * ListNode *next;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * ListNode() : val(0), next(nullptr) {}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * ListNode(int x) : val(x), next(nullptr) {}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * ListNode(int x, ListNode *next) : val(x), next(next) {}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * };
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">void&lt;/span> &lt;span class="n">reverse_traversal&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ListNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">head&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">stack&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="o">!&lt;/span>&lt;span class="n">head&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">return&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">reverse_traversal&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">head&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">next&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">empty&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">||&lt;/span> &lt;span class="n">head&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">val&lt;/span> &lt;span class="o">&amp;gt;=&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">top&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">s&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">head&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">val&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ListNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="nf">removeNodes&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ListNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">head&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">stack&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">reverse_traversal&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">head&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ListNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">dummyhead&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">ListNode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="k">nullptr&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ListNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">cur&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">dummyhead&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="o">!&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">empty&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">cur&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">next&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">ListNode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">top&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="k">nullptr&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">cur&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">cur&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">next&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">s&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">pop&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">dummyhead&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">next&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">``&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cp"># References
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cp">&lt;/span>&lt;span class="o">-&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">Leetcode&lt;/span>&lt;span class="p">](&lt;/span>&lt;span class="nl">https&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="c1">//leetcode.com/problems/remove-nodes-from-linked-list/description/)
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>2816. Double a Number Represented as a Linked List</title><link>https://maosong.website/p/2816.-double-a-number-represented-as-a-linked-list/</link><pubDate>Mon, 06 May 2024 19:47:28 +0800</pubDate><guid>https://maosong.website/p/2816.-double-a-number-represented-as-a-linked-list/</guid><description>&lt;p>Given a linked list representing a non-negative integer, we are required to double this integer and convert it back to a linked list.&lt;/p>
&lt;h1 id="intuition">&lt;a href="#intuition" class="header-anchor">&lt;/a>Intuition
&lt;/h1>&lt;p>We can just simulate the process, that is:&lt;/p>
&lt;ol>
&lt;li>Retrieve the integer represented by the linked list&lt;/li>
&lt;li>Double the integer&lt;/li>
&lt;li>Construct the result linked list from the result integer.&lt;/li>
&lt;/ol>
&lt;h1 id="approach">&lt;a href="#approach" class="header-anchor">&lt;/a>Approach
&lt;/h1>&lt;p>We can just simulate the process as above. However, if the integer is very large, retrieve the integer may cause overflow. To simplify this process, we can use a stack to store the digits of the original integer, then we double the integer by operating on the top of the stack.&lt;/p>
&lt;p>In each step, we pop an element from the stack, doubling it and adding it with the carry digit. Then we construct the result linked list with inserting the new node in the front of the head.&lt;/p>
&lt;h1 id="complexity">&lt;a href="#complexity" class="header-anchor">&lt;/a>Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n)$$&lt;/li>
&lt;li>
$$O(n)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * Definition for singly-linked list.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * struct ListNode {
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * int val;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * ListNode *next;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * ListNode() : val(0), next(nullptr) {}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * ListNode(int x) : val(x), next(nullptr) {}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * ListNode(int x, ListNode *next) : val(x), next(next) {}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * };
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ListNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">doubleIt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ListNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">head&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">stack&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ListNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">cur&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">head&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">cur&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">s&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">cur&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">val&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">cur&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">cur&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">next&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">carry&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="o">!&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">empty&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">num&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">top&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">carry&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ListNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">pre&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">ListNode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="mi">10&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">cur&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">carry&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">num&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="mi">10&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">cur&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">pre&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">s&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">pop&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">carry&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ListNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">pre&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">ListNode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">carry&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">cur&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">cur&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">pre&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">cur&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/double-a-number-represented-as-a-linked-list/description" target="_blank" rel="noopener"
>Leetcode&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>237. Delete Node in a Linked List</title><link>https://maosong.website/p/237.-delete-node-in-a-linked-list/</link><pubDate>Sun, 05 May 2024 20:13:45 +0800</pubDate><guid>https://maosong.website/p/237.-delete-node-in-a-linked-list/</guid><description>&lt;p>Given a linked list and the node to be deleted, delete the node without accessing the head of the linked list&lt;/p>
&lt;h1 id="intuition">&lt;a href="#intuition" class="header-anchor">&lt;/a>Intuition
&lt;/h1>&lt;p>This deletion is same as deleting a node from an array.&lt;/p>
&lt;h1 id="approach">&lt;a href="#approach" class="header-anchor">&lt;/a>Approach
&lt;/h1>&lt;p>We use two pointers, &lt;code>pre&lt;/code> and &lt;code>node&lt;/code> to represent the previous and current node of the linked list, and we update the value of &lt;code>pre&lt;/code> and &lt;code>node&lt;/code> at each iteration. Finally, we delete the last node in the linked list.&lt;/p>
&lt;h1 id="complexity">&lt;a href="#complexity" class="header-anchor">&lt;/a>Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n)$$&lt;/li>
&lt;li>
$$O(1)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * Definition for singly-linked list.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * struct ListNode {
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * int val;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * ListNode *next;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * ListNode(int x) : val(x), next(NULL) {}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * };
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">void&lt;/span> &lt;span class="n">deleteNode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ListNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ListNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">pre&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">nullptr&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">next&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">node&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">val&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">next&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">val&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pre&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">node&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">next&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pre&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">next&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">nullptr&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/delete-node-in-a-linked-list/description/" target="_blank" rel="noopener"
>Leetcode&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>881. Boats to Save People</title><link>https://maosong.website/p/881.-boats-to-save-people/</link><pubDate>Sat, 04 May 2024 09:06:48 +0800</pubDate><guid>https://maosong.website/p/881.-boats-to-save-people/</guid><description>&lt;p>Given an array &lt;code>weight&lt;/code> where &lt;code>weight[i]&lt;/code> representing the weight of person &lt;code>i&lt;/code>, now given the capacity of the boat and the constraint that each boat can carry at most two people, find the minimum number of boats to carry all people.&lt;/p>
&lt;h1 id="intuition">&lt;a href="#intuition" class="header-anchor">&lt;/a>Intuition
&lt;/h1>&lt;p>Always pair the lightest person abd the heaviest person to a boat.&lt;/p>
&lt;h1 id="approach">&lt;a href="#approach" class="header-anchor">&lt;/a>Approach
&lt;/h1>&lt;p>We first sort the array &lt;code>weight&lt;/code> in ascending order. Then we use two pointers &lt;code>left=0&lt;/code> and &lt;code>right=n-1&lt;/code> to iterate through the array. In each step, there are two cases:&lt;/p>
&lt;ol>
&lt;li>If &lt;code>weight[left]+weight[right] &amp;gt; limit&lt;/code>, then we cannot find a peer who can take one boat with &lt;code>right&lt;/code>, in this case, &lt;code>right&lt;/code> occupies a single boat alone.&lt;/li>
&lt;li>If &lt;code>weight[left]+weight[right] &amp;lt;= limit&lt;/code>, then these two people can take one boat.&lt;/li>
&lt;/ol>
&lt;h1 id="complexity">&lt;a href="#complexity" class="header-anchor">&lt;/a>Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$ O(n\log n) $$&lt;/li>
&lt;li>
$$ O(1) $$&lt;/li>
&lt;/ul>
&lt;h1 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">numRescueBoats&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">people&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">limit&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sort&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">people&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">begin&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">people&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">());&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">left&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">right&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">people&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">num&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">left&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">right&lt;/span>&lt;span class="p">){&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">people&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">right&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">people&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">left&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">limit&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">++&lt;/span>&lt;span class="n">left&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">--&lt;/span>&lt;span class="n">right&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">++&lt;/span>&lt;span class="n">num&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">num&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/boats-to-save-people/description/" target="_blank" rel="noopener"
>Leetcode&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>165. Compare Version Numbers</title><link>https://maosong.website/p/165.-compare-version-numbers/</link><pubDate>Fri, 03 May 2024 15:13:23 +0800</pubDate><guid>https://maosong.website/p/165.-compare-version-numbers/</guid><description>&lt;p>Given two strings containing digits and dot, compare them in the form of a version number.&lt;/p>
&lt;h1 id="intuition">&lt;a href="#intuition" class="header-anchor">&lt;/a>Intuition
&lt;/h1>&lt;p>Since the leading zeros are required to be ignored, we use dot character as separator and compute the integer of each part and compare them individually.&lt;/p>
&lt;h1 id="approach">&lt;a href="#approach" class="header-anchor">&lt;/a>Approach
&lt;/h1>&lt;p>We use two index &lt;code>i&lt;/code> and &lt;code>j&lt;/code> to iterate over &lt;code>s1&lt;/code> and &lt;code>s2&lt;/code>, we move the index &lt;code>i&lt;/code> to the next dot character and compute the integer &lt;code>num1&lt;/code> we have found. Same operations for &lt;code>j&lt;/code> to obtain the integer &lt;code>num2&lt;/code>. Then &lt;code>num1&lt;/code> and &lt;code>num2&lt;/code> are compared.&lt;/p>
&lt;h1 id="complexity">&lt;a href="#complexity" class="header-anchor">&lt;/a>Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n)$$&lt;/li>
&lt;li>
$$O(1)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">compareVersion&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">string&lt;/span> &lt;span class="n">version1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">string&lt;/span> &lt;span class="n">version2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">m&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">version1&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">version2&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">m&lt;/span> &lt;span class="o">||&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">num1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">m&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="n">version1&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="sc">&amp;#39;.&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">num1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">10&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">num1&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">version1&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">++&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="sc">&amp;#39;0&amp;#39;&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">j&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="n">version2&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="sc">&amp;#39;.&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">num2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">10&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">num2&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">version2&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="o">++&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="sc">&amp;#39;0&amp;#39;&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">num1&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">num2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">return&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">num1&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">num2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">return&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/compare-version-numbers/description/" target="_blank" rel="noopener"
>Leetcode&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Formal Algorithms for Transformer</title><link>https://maosong.website/p/formal-algorithms-for-transformer/</link><pubDate>Thu, 02 May 2024 13:13:12 +0800</pubDate><guid>https://maosong.website/p/formal-algorithms-for-transformer/</guid><description>&lt;p>This post is a notes on understanding how transformer works in an algorithm perspective.&lt;/p>
&lt;h1 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h1>&lt;p>Transformer is a neural architecture that is used for neural language processing. Transformer receives an embedding matrix, which represents a sentence as input, and outputs a matrix of the same size as the embedding matrix, then the output can be used for downstream tasks.&lt;/p>
&lt;h2 id="notation">&lt;a href="#notation" class="header-anchor">&lt;/a>Notation
&lt;/h2>&lt;ol>
&lt;li>We denote $V=[N_V]:=\{1,\dots,N_V\}$ as the &lt;em>vocabulary&lt;/em> of tokens or words or characters.&lt;/li>
&lt;li>We denote $\bm{x}=x[1...n]:=x[1]...x[n]\in V^n$ be a sequence of tokens, for example, a sentence or a paragraph.&lt;/li>
&lt;li>Given a matrix $M\in\mathbb{R}^{m\times n}$, $M[i,:]\in\mathbb{R}^n$ is the $i$-th row of $M$, $M[:, j]\in\mathbb{R}^m$ is the $j$-th column of $M$.&lt;/li>
&lt;/ol>
&lt;h1 id="tokenization">&lt;a href="#tokenization" class="header-anchor">&lt;/a>Tokenization
&lt;/h1>&lt;p>Tokenization determines how the text are represented. Given a piece of text, for example, &lt;code>&amp;quot;I have an apple&amp;quot;&lt;/code>, we seek to find a proper way to represent this sentence.&lt;/p>
&lt;ol>
&lt;li>Character level tokenization. In this setting, $V$ is the English alphabet plus punctuation. This tends to yield very long sequences (depends on the character contained in the raw text).&lt;/li>
&lt;li>Word level tokenization. In this setting, $V$ is the set of all English words plus punctuation. Word level tokenization is straightforward, but it tends to required a very large vocabulary and cannot handle new vocabulary at test time.&lt;/li>
&lt;li>Subword tokenization. This is the most common way used in nowadays, $V$ is the set containing the commonly used word segments like &amp;ldquo;ing&amp;rdquo;, &amp;ldquo;est&amp;rdquo;. This can be computed via Byte-Pair Encoding (BPE) algorithm.&lt;/li>
&lt;li>We suppose the length of the input text is $L$, if the length input text exceeds $L$, we chunk it.&lt;/li>
&lt;/ol>
&lt;p>After tokenization, each element in the vocabulary is assigned to a unique index $i\in\{1,\dots,N_V-3\}$, and a number of special tokens are added to the vocabulary. For example:&lt;/p>
&lt;ol>
&lt;li>&lt;code>mask_token&lt;/code>$=N_V-2$, used in masked language modeling&lt;/li>
&lt;li>&lt;code>bos_token&lt;/code>$=N_V-1$ and &lt;code>eos_token&lt;/code>$=N_V$, these two tokens are used to represent the beginning and the end of the sequence.&lt;/li>
&lt;/ol>
&lt;p>Finally, a piece of raw text is represented as a sequence of indices, often called &lt;em>token ID&lt;/em>s corresponding to its subwords, preceded by &lt;code>bos_token&lt;/code> and followed by &lt;code>eos_token&lt;/code>.&lt;/p>
&lt;h1 id="embedding">&lt;a href="#embedding" class="header-anchor">&lt;/a>Embedding
&lt;/h1>&lt;p>The embedding layer is used to represent each token as a vector that contains richer semantic information. The embedding contains two parts:&lt;/p>
&lt;ol>
&lt;li>token embedding, where each token is embedded into a vector space&lt;/li>
&lt;li>positional embedding, where embeds the position information of the tokens.&lt;/li>
&lt;/ol>
&lt;h2 id="token-embedding">&lt;a href="#token-embedding" class="header-anchor">&lt;/a>Token embedding
&lt;/h2>&lt;p>Given a sequence of token ID, we now need to represent each token as a vector in $\mathbb{R}^d$.&lt;/p>
&lt;p>The simplest way is to use &lt;em>one-hot embedding&lt;/em>, where each token $i$ is represented a vector $[0,\dots,1,\dots,0]\in\mathbb{R}^{N_V}$ whose elements are all $0$ excepts that $i$-th position is equal to $1$. However, the problem is that the vocabulary size $N_V$ is two large.&lt;/p>
&lt;p>To solve this problem, we can train a learnable embedding model, of which parameter is a matrix $W_e\in\mathbb{R}^{d\times N_V}$, its $i$-th row corresponds to vector representation of the token $i$:&lt;/p>
$$ \bm{e} = W_{e}[:, i]\in\mathbb{R}^d $$&lt;h2 id="position-embedding">&lt;a href="#position-embedding" class="header-anchor">&lt;/a>Position embedding
&lt;/h2>&lt;p>There is a problem in token embedding, that is, it doesn&amp;rsquo;t contain consider the order of tokens. In latter, we show that the self-attention mechanism is equivariant to a permutation matrix $X\Pi$ of data $X$, where $\Pi$ is a permutation matrix, that is,&lt;/p>
$$ \mathrm{Sa}(X\Pi) = \mathrm{Sa}(X)\Pi $$&lt;p>the above equation indicates that the self-attention layer learn no position information at all!&lt;/p>
&lt;p>To solve this problem, we add a positional embedding to token embedding. There are two kinds of embeddings:&lt;/p>
&lt;ol>
&lt;li>Absolute positional embeddings. In this setting, a matrix $W_P\in\mathbb{R}^{d\times N}$ is learned or design to indicate the position of tokens. Mathematically, we have&lt;/li>
&lt;/ol>
$$ \bm{e}_p = W_p[:, \mathrm{index}(i)]\in\mathbb{R}^{d} $$&lt;p>where $\mathrm{index}(i)$ is the index of token $i$ in the input sequence.
2. Relative positional embeddings. We leave this in latter notes. Compared to absolute positional embeddings, relative positional embeddings uses offset information, which performs well when the input sequence is too long.&lt;/p>
&lt;p>The final embedding of a token $i$ is given by&lt;/p>
$$ \bm{e} = W_e[:, i] + W_p[:, \mathrm{index}(i)]\in\mathbb{R}^{d} $$&lt;h1 id="attention">&lt;a href="#attention" class="header-anchor">&lt;/a>Attention
&lt;/h1>&lt;p>The idea of attention mechanism is: Given a sequence of token, to predict the current token, which token should I pay attention to? For example, &lt;code>I opened the door with my ___&lt;/code>, we may answer &lt;code>key&lt;/code>, &lt;code>password&lt;/code> or &lt;code>fingerprint&lt;/code> etc. This is because we notice that we &lt;code>opened the door&lt;/code>, so to predict the next token, we should make use of the information. What attention mechanism does is to quantify this process and make them parallel and learnable.&lt;/p>
&lt;h2 id="single-query-attention">&lt;a href="#single-query-attention" class="header-anchor">&lt;/a>Single query attention
&lt;/h2>&lt;p>We first consider a simple example. Given the embedding of the current token $\bm{e}\in\mathbb{R}^d$ and the list of context tokens $[\bm{e}_1,\dots,\bm{e}_N]\in\mathbb{R}^{d\times N}$, the attention is given as follows:&lt;/p>
&lt;ol>
&lt;li>compute query vector: $\bm{q}=W_q\bm{e}+b_q\in\mathbb{R}^{d}$&lt;/li>
&lt;li>compute key vectors: for $i=1,\dots,L$, $\bm{k}_i=W_k\bm{e}_i+b_k\in\mathbb{R}^{d}$&lt;/li>
&lt;li>compute value vectors: for $i=1,\dots,L$, $\bm{v}_i=W_v\bm{e}_i+b_v\in\mathbb{R}^{d}$&lt;/li>
&lt;li>compute attention weights: let $\bm{s}=[\bm{q}^T\bm{k}_1,\dots,\bm{q}^T\bm{k}_L]\in\mathbb{R}^{N}$, then:&lt;/li>
&lt;/ol>
$$ \bm\alpha = \mathrm{softmax}\left(\frac{\bm{s}}{\sqrt{d}}\right)\in\mathbb{R}^{N}$$&lt;p>
5. compute vector representation of the token and context combined:&lt;/p>
$$ \bm{v}'= \sum_{i=1}^N\alpha_i\bm{v}_i\in\mathbb{R}^{d} $$&lt;p>where $W_q,W_k,W_v\in\mathbb{R}^{d\times d}$, $b_q,b_k,b_v\in\mathbb{R}$.&lt;/p>
&lt;h2 id="general-attention">&lt;a href="#general-attention" class="header-anchor">&lt;/a>General attention
&lt;/h2>&lt;p>To extend the single query attention to general form, we consider the embedding matrix $X\in\mathbb{R}^{D\times N}$, the context matrix $Z\in\mathbb{R}^{d\times C}$ and a mask matrix $M\in\mathbb{R}^{D\times D}$, then the attention is computed as follows:&lt;/p>
&lt;ol>
&lt;li>compute query matrix:&lt;/li>
&lt;/ol>
$$ Q=W_qX+\bm{b}_q\in\mathbb{R}^{D\times N}$$&lt;p>
2. compute key matrix:&lt;/p>
$$ K=W_kZ+\bm{b}_k\in\mathbb{R}^{D\times C}$$&lt;p>
3. compute value vectors:&lt;/p>
$$ V=W_vZ+\bm{b}_v\in\mathbb{R}^{D\times C}$$&lt;p>
4. compute attention weights:&lt;/p>
$$\mathrm{Sa}(X) = \mathrm{softmax}\left(M\odot \frac{K^TQ}{\sqrt{D}}\right) \in\mathbb{R}^{C\times N} $$&lt;p>
where $\odot$ is the element-wise product.
5. output the updated representations of tokens in $X$, folding the information from tokens in $Z$&lt;/p>
$$ \tilde{V} = V\odot \mathrm{Sa}(X)\in\mathbb{R}^{D\times N} $$&lt;p>There are two kinds of mask matrices depending on which attention we are using:&lt;/p>
&lt;ol>
&lt;li>Bidirectional attention, in this case, $M=\bm{1}\bm{1}^T\in\mathbb{R}^{C\times N}$.&lt;/li>
&lt;li>Undirectional attention, in this case, $M[i,j] = \bm{1}_{i\leq j}$, where $\bm{1}_{i\leq j}$ is the &lt;em>indicator function&lt;/em>.&lt;/li>
&lt;/ol>
&lt;h2 id="multi-head-attention">&lt;a href="#multi-head-attention" class="header-anchor">&lt;/a>Multi-head Attention
&lt;/h2>&lt;p>The previous describes the operation of a &lt;em>single head&lt;/em>. In practice, transformers run multiple attention heads in parallel and combine their outputs, this is called &lt;em>multi-head attention&lt;/em>. The idea behind multi-head attention can be summarized as follows:&lt;/p>
&lt;ol>
&lt;li>In high dimensional space, two vectors are usually far from each other, with multiple attention heads, we can reduce the dimension of the representation.&lt;/li>
&lt;li>With multiple attention heads, each heads may focus on specific semantics of the representation. For example, one head focuses on positiveness, and one another head focuses on noun/verb semantics.&lt;/li>
&lt;/ol>
&lt;p>For simplicity, we denote the single-head attention as $\mathrm{attention}(X, Z\mid M)$, suppose we have $H$ heads, then we compute $\tilde{V}_i$ for each heads:&lt;/p>
$$ \tilde{V}_i = \mathrm{attention}(X, Z\mid M)\in\mathbb{R}^{D\times N},i=1,\dots,H $$&lt;p>
then attention representation are concatenated together:&lt;/p>
$$ V = [\tilde{V}_1^T, \dots, \tilde{V}_H^T]^T\in\mathbb{R}^{HD\times N} $$&lt;p>combined via an output matrix $W_o\in\mathbb{R}^{D\times HD}$:&lt;/p>
$$ \tilde{V} = W_oV+\bm{b}_o\in\mathbb{R}^{D\times N} $$&lt;p>We denote the multi head attention as $\mathrm{MhSa}(X, Z\mid M)$.&lt;/p>
&lt;h2 id="transformer-layer">&lt;a href="#transformer-layer" class="header-anchor">&lt;/a>Transformer layer
&lt;/h2>&lt;p>After computing the multi head attention, we can now construct a transformer layer, which can also be stacked as convolution neural networks. A transformer layer can be constructed by the following operations:&lt;/p>
&lt;ol>
&lt;li>Multi head attention (residual), $X\gets X + \mathrm{MhSa}(X, Z\mid M)$.&lt;/li>
&lt;li>Layer norm, $X \gets \mathrm{LayerNorm}(X)$&lt;/li>
&lt;li>Multi layer perception $\bm{x}_i\gets \bm{x}_i + \mathrm{mlp}(\bm{x}_i), i=1,\dots,N$&lt;/li>
&lt;li>Layer norm, $X \gets \mathrm{LayerNorm}(X)$&lt;/li>
&lt;/ol>
&lt;p>where $\mathrm{LayerNorm}$ is the layer norm operation. $\mathrm{mlp}$ is a multi layer perception, usually it consists of one hidden layer of size $4D$, that is, then umber of neurons in three layers are $D, 4D, D$.&lt;/p>
&lt;p>Usually, a large language model consists of multiple transformer layers.&lt;/p>
&lt;h1 id="unembedding">&lt;a href="#unembedding" class="header-anchor">&lt;/a>Unembedding
&lt;/h1>&lt;p>The unembedding learns to convert a vector representation of a token and its context $\bm{e}$ into a distribution over the vocabulary elements.&lt;/p>
$$ \bm{p} = \mathrm{softmax}(W_u\bm{e})\in \Delta(V)\subseteq \mathbb{R}^d $$&lt;p>
where $\Delta(V)$ is a simplex over the set $V$.&lt;/p>
&lt;h1 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://udlbook.github.io/udlbook/" target="_blank" rel="noopener"
>Understanding Deep Learning Chapter 12&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2207.09238" target="_blank" rel="noopener"
>Formal Algorithms for Transformers&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>MiniGPT-4-Enhancing Vision-Language Understanding with Advanced Large Language Models</title><link>https://maosong.website/p/minigpt-4-enhancing-vision-language-understanding-with-advanced-large-language-models/</link><pubDate>Thu, 02 May 2024 13:13:12 +0800</pubDate><guid>https://maosong.website/p/minigpt-4-enhancing-vision-language-understanding-with-advanced-large-language-models/</guid><description>&lt;h1 id="tldr">&lt;a href="#tldr" class="header-anchor">&lt;/a>TLDR
&lt;/h1>&lt;p>This paper proposed an open-sourced, GPT-4 liked multimodal language model (MLLM), mini-GPT4, aiming to provide inspiration of how to build an MLLM.&lt;/p>
&lt;h1 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h1>&lt;p>GPT-4 has exhibited remarkable vision language abilities, however, due to it is not open sourced, it is hard to explore the construction and how the GPT-4 is trained.&lt;/p>
&lt;p>To solve this problem, this paper builds mini-GPT4, which utilizes an advanced LLM, Vicuna and BLIP-2, to build an open sourced MLLM. The result show that mini-GPT4 possesses numerous capabilities similar to those demostrated by GPT-4.&lt;/p>
&lt;h1 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h1>&lt;p>The architecture of mini-GPT4 is shown as follows:&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/minigpt-4-enhancing-vision-language-understanding-with-advanced-large-language-models/mini-gpt4-architecture.png"
width="907"
height="600"
loading="lazy"
alt="mini-GPT4 architecture"
class="gallery-image"
data-flex-grow="151"
data-flex-basis="362px"
>&lt;/p>
&lt;p>mini-GPT4 consists of three parts:&lt;/p>
&lt;ol>
&lt;li>Vision Encoder same as used in BLIP-2, which is built upon a ViT backbone and a Q-former network.&lt;/li>
&lt;li>A single projection layer, which aligns the encoded visual features with the Vicuna language model.&lt;/li>
&lt;li>Language decoder: Vicuna.&lt;/li>
&lt;/ol>
&lt;h2 id="training">&lt;a href="#training" class="header-anchor">&lt;/a>Training
&lt;/h2>&lt;p>mini-GPT4 only trains the linear projection layer, this includes two stages:&lt;/p>
&lt;ol>
&lt;li>First pre-training stage: in this stage, the language decoder and vision encoder are remain frozen, only the linear projection layer is trained. The training dataset contains Conceptual Caption, SBU and LAION. The problem is that the output may be incoherent like repetitive words or sentences.&lt;/li>
&lt;li>Fine-tune dataset generation: the pre-trained mini-GPT4 is used to generate image-text pair as fine-tune data, to improve the text quality, the generated text is polished with the help of ChatGPT. Finally, the dataset is manually refined.&lt;/li>
&lt;li>Second-stage fine-tuning: Fine-tune the mini-GPT-4 with the high quality fine-tune data.&lt;/li>
&lt;/ol>
&lt;h1 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://github.com/Vision-CAIR/MiniGPT-4/tree/main" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://openreview.net/forum?id=1tZbq88f27" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://minigpt-4.github.io/" target="_blank" rel="noopener"
>Homepage&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on t-SNE</title><link>https://maosong.website/p/notes-on-t-sne/</link><pubDate>Thu, 02 May 2024 13:13:12 +0800</pubDate><guid>https://maosong.website/p/notes-on-t-sne/</guid><description>&lt;p>This post introduces how to understand t-SNE.&lt;/p>
&lt;h1 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h1>&lt;p>t-SNE an dimension reduction algorithm, which projects high-dimensional data into low-dimensional space. Thus the algorithm can be used to visualize the data distribution.&lt;/p>
&lt;p>To understand how t-SNE works, we first review the SNE algorithms, then we introduce the t-SNE algorithm.&lt;/p>
&lt;h1 id="sne">&lt;a href="#sne" class="header-anchor">&lt;/a>SNE
&lt;/h1>&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;p>Stochastic Neighbor Embedding, or SNE, is the previous version of t-SNE.&lt;/p>
&lt;p>The basic idea behind SNE is that: &lt;em>data points that are close in high-dimensional space should be close in lower-dimensional space too&lt;/em>.&lt;/p>
&lt;p>Formally speaking, given a data set $X\in\mathbb{R}^{D\times N}$ consisting of $N$ data points, with each data point lies in $D$ dimensional space. Our goal is to reduce the data points into $d&lt;&lt; D$ dimensional space $Y\in\mathbb{R}^{d\times N}$, that is, we seek to find a map $f:\mathbb{R}^{D\times N}\to \mathbb{R}^{d\times N}$ such that $f(X)=Y$. Usually, $d=2$ or $d=3$ for visualization use.&lt;/p>
&lt;p>SNE measures &amp;ldquo;close&amp;rdquo; in a probabilistic way. The similarity is represented by converting Euclidean distance between data points to condition probabilities:&lt;/p>
$$ p_{j\mid i} = \frac{\exp\left(-\Vert\bm{x}_i-\bm{x}_j\Vert^2/(2\sigma_i^2)\right)}{\sum_{k\neq i}\exp\left(-\Vert\bm{x}_i-\bm{x}_k\Vert^2/(2\sigma_i^2)\right)} $$&lt;p>the above equation can be interpreted as &lt;em>the probability of point $\bm{x}_j$ being a neighbor of point $\bm{i}$ is proportional to the distance between them&lt;/em>. $\sigma_i$ is the variance of the Gaussian distribution that is centered on data point $\bm{x}_i$. We introduce the method for determining $\sigma_i$ later.&lt;/p>
&lt;p>Similarly, we can construct a probability distribution $q$ based on $Y$.&lt;/p>
$$ q_{j\mid i} = \frac{\exp\left(-\Vert\bm{x}_i-\bm{x}_j\Vert^2\right)}{\sum_{k\neq i}\exp\left(-\Vert\bm{x}_i-\bm{x}_k\Vert^2\right)} $$&lt;p>where we set the variance as $1/\sqrt{2}$ following the original paper.&lt;/p>
&lt;p>$p_{i\mid i}$ and $q_{i\mid i}$ are set $0$ since we are only interested in modeling pairwise similarities.&lt;/p>
&lt;p>Now we want $q_{j\mid i}$ are as close as $p_{j\mid i}$, that is, we want two distributions are as close as to each other. This can be measured by &lt;strong>Kullback- Leibler divergence&lt;/strong>, which is written as:&lt;/p>
$$ C(P, Q) = \sum_{i=1}^N\mathrm{KL}(P_i\Vert Q_i)=\sum_{i=1}^N\sum_{j=1}^N p_{j\mid i}\log \frac{p_{i\mid j}}{q_{i\mid j}} $$&lt;p>where $P_i=[p_{1\mid i},\dots,p_{N\mid i}]\in\mathbb{R}^N$ and $Q_i=[q_{1\mid i},\dots,q_{N\mid i}]\in\mathbb{R}^N$.&lt;/p>
&lt;h2 id="choosing-sigma">&lt;a href="#choosing-sigma" class="header-anchor">&lt;/a>Choosing $\sigma$
&lt;/h2>&lt;p>Now we introduce how to choose $\sigma$. Note that $\sigma$ determines the distribution of data points, larger $\sigma$ indicates sparser distribution of data points. The original paper uses &lt;em>perplexity&lt;/em> to measure such sparsity. It is defined as&lt;/p>
$$ \mathrm{Perp}(P_i) = 2^{H(P_i)} $$&lt;p>where $H(P_i)$ is the &lt;em>Shannon entropy&lt;/em> of $P_i$ measured in bits:&lt;/p>
$$ H(P_i) = -\sum_{i=1}^N p_{j\mid i}\log p_{j\mid i} $$&lt;p>The perplexity can be interpreted as a smooth measure of the effective number of neighbors. The performance of SNE is fairly robust to changes in the perplexity, and typical values are between 5 and 50.&lt;/p>
&lt;p>Notice that $p_{j\mid i}$, by setting different value on $\mathrm{Perp}(P_i)$, we can obtain different $\sigma_i$ via binary search.&lt;/p>
&lt;h2 id="optimization">&lt;a href="#optimization" class="header-anchor">&lt;/a>Optimization
&lt;/h2>&lt;p>Our goal now becomes minimizing $C(p, q)$ over variables $\bm{y}_1,\dots,\bm{y}_N\in\mathbb{R}^d$, given $p$ and hyperparameter $\sigma_i, i=1,\dots,N$. This can be done via gradient descent methods. The gradient is now given by&lt;/p>
$$ \frac{d C}{d\bm{y}_i}=2\sum_{j=1}^N\left(p_{j\mid i} - q_{j\mid i} + p_{i\mid j}- q_{i\mid j} \right)(\bm{y}_i-\bm{y}_j)\in\mathbb{R}^d, \ i=1,\dots,N $$&lt;p>We can write it in matrix form and add a momentum term:&lt;/p>
$$ Y^{t+1} = Y^t + \beta\frac{dC}{dY} + \alpha_t\left(Y^{t-1}-Y^{t-2}\right) $$&lt;p>where $\beta$ is the step size and $\alpha_t$ is momentum parameter,&lt;/p>
$$ Y^t = [\bm{y}_1^t,\dots, \bm{y}_N^t]\in\mathbb{R}^{d\times N} ,\ \frac{dC}{dY} = \left[\frac{d C}{d\bm{y}_1},\dots,\frac{d C}{d\bm{y}_N}\right]\in\mathbb{R}^{d\times N} $$&lt;h1 id="t-sne">&lt;a href="#t-sne" class="header-anchor">&lt;/a>t-SNE
&lt;/h1>&lt;h2 id="symmetric-sne">&lt;a href="#symmetric-sne" class="header-anchor">&lt;/a>Symmetric SNE
&lt;/h2>&lt;p>The first difference between t-SNE and SNE is the probability, t-SNE uses symmetric version of SNE to simplify computations.&lt;/p>
&lt;p>Different from SNE, symmetric SNE uses a joint probability instead of a condition probability:&lt;/p>
$$ C(P, Q) = \sum_{i=1}^N\mathrm{KL}(P\Vert Q)=\sum_{i=1}^N\sum_{j=1}^N p_{ij}\log \frac{p_{ij}}{q_{ij}} $$&lt;p>where $p_{ij}$ and $q_{ij}$ are defined as&lt;/p>
$$ p_{ij} = \frac{\exp\left(-\Vert\bm{x}_i-\bm{x}_j\Vert^2/(2\sigma^2)\right)}{\sum_{k\neq r}\exp\left(-\Vert\bm{x}_r-\bm{x}_k\Vert^2/(2\sigma^2)\right)},q_{ij} = \frac{\exp\left(-\Vert\bm{x}_i-\bm{x}_j\Vert^2\right)}{\sum_{k\neq r}\exp\left(-\Vert\bm{x}_r-\bm{x}_k\Vert^2\right)} $$&lt;p>the problem if joint probability $p_{ij}$ is that if there is an outlier $\bm{x}_i$, then $p_{ij}$ will be extremely small for all $j$. This problem can be solved by defining $p_{ij}$ from the conditional probability $p_{i\mid j}$ and $p_{j\mid i}$&lt;/p>
$$ p_{ij} = \frac{p_{j\mid i}+p_{i\mid j}}{2N} $$&lt;p>This ensures that&lt;/p>
$$ \sum_{j=1}^N p_{ij} > \frac{1}{2N} $$&lt;p>for all $\bm{x}_i$, in result, each data point makes a significant contribution to the cost function.&lt;/p>
&lt;p>In this case, the gradient of the cost function is now given by&lt;/p>
$$ \frac{d C}{d\bm{y}_i}=4\sum_{j=1}^N\left(p_{ij} - q_{ij}\right)(\bm{y}_i-\bm{y}_j)\in\mathbb{R}^d, \ i=1,\dots,N $$&lt;h2 id="t-sne-1">&lt;a href="#t-sne-1" class="header-anchor">&lt;/a>t-SNE
&lt;/h2>&lt;p>Experiments show that symmetric SNE seems to produce maps that are just as good as asymmetric SNE, and sometimes even a little better.&lt;/p>
&lt;p>However, there is a problem with SNE, that is, the &lt;em>crowding problem&lt;/em>, which manifests as a tendency for points in the low-dimensional space to be clustered too closely together, particularly in high-density regions of the data.&lt;/p>
&lt;p>The causes of the crowding problems are:&lt;/p>
&lt;ol>
&lt;li>Data points in high dimensional space tend to far from each other, which makes the distance information less useful.&lt;/li>
&lt;li>SNE aims to preserve the local structure of the data points, but it can struggle with non-linear relationships. The projected data points will be closed to each other due to this reason.&lt;/li>
&lt;li>The optimization algorithm used by SNE can get stuck in local minimum.&lt;/li>
&lt;/ol>
&lt;p>To alleviate the crowding problem, t-SNE is introduced in the following way:
&lt;em>In the high-dimensional space, we convert distances into probabilities using a Gaussian distribution. In the low-dimensional map, we can use a probability distribution that has much heavier tails than a Gaussian to convert distances into probabilities.&lt;/em>
This allows a moderate distance in the high-dimensional space to be faithfully modeled by a much larger distance in the map and, as a result, it eliminates the unwanted attractive forces between map points that represent moderately dissimilar data points.&lt;/p>
&lt;p>t-SNE uses student t-distribution in low-dimensional map:&lt;/p>
$$ q_{ij} = \frac{\left(1+\Vert\bm{y}_i-\bm{y}_j\Vert^2\right)^{-1}}{\sum_{k\neq r}\left(1+\Vert\bm{y}_i-\bm{y}_j\Vert^2\right)^{-1}} $$&lt;p>A Student t-distribution with a single degree of freedom is used, because it has the particularly nice property that $\left(1+\Vert\bm{y}_i-\bm{y}_j\Vert^2\right)^{-1}$ approaches an inverse square law for large pairwise distances $\Vert\bm{y}_i-\bm{y}_j\Vert$ in the low-dimensional map.&lt;/p>
&lt;p>Compared to Gaussian distribution, t-distribution is heavily tailed。&lt;/p>
&lt;p>A computationally convenient property of t-SNE is that it is much faster to evaluate the density of a point under a Student t-distribution than under a Gaussian because it does not involve an exponential, even though the Student t-distribution is equivalent to an infinite mixture of Gaussians with different variances.&lt;/p>
&lt;p>The gradient of t-SNE is given by&lt;/p>
$$ \frac{d C}{d\bm{y}_i}=4\sum_{j=1}^N\left(p_{ij} - q_{ij}\right)(\bm{y}_i-\bm{y}_j)\left(1+\Vert\bm{y}_i-\bm{y}_j\Vert^2\right)^{-1}\in\mathbb{R}^d, \ i=1,\dots,N $$&lt;p>The advantages of t-SNE gradients over SNE are given by:&lt;/p>
&lt;ol>
&lt;li>The t-SNE gradient strongly repels dissimilar data points that are modeled by a small pairwise distance in the low-dimensional representation.&lt;/li>
&lt;li>Second, although t-SNE introduces strong repulsions between dissimilar data points that are modeled by small pairwise distances, these repulsions do not go to infinity.&lt;/li>
&lt;/ol>
&lt;p>The algorithm is given as follows:&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-t-sne/t-SNE-algorithm.png"
width="1910"
height="1096"
loading="lazy"
alt="algorithm"
class="gallery-image"
data-flex-grow="174"
data-flex-basis="418px"
>&lt;/p>
&lt;h2 id="optimization-1">&lt;a href="#optimization-1" class="header-anchor">&lt;/a>Optimization
&lt;/h2>&lt;p>There are some optimizations that can be used to improve performance of t-SNE:&lt;/p>
&lt;ol>
&lt;li>Early compression, which is used to force the map points to stay close together at the start of the optimization&lt;/li>
&lt;li>Early exaggeration, which is used to multiply all of the pi j’s by, for example, 4, in the initial stages of the optimization&lt;/li>
&lt;/ol>
&lt;h1 id="implementation">&lt;a href="#implementation" class="header-anchor">&lt;/a>Implementation
&lt;/h1>&lt;h1 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="http://jmlr.org/papers/v9/vandermaaten08a.html" target="_blank" rel="noopener"
>Visualizing Data using t-SNE&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>1915. Number of Wonderful Substrings</title><link>https://maosong.website/p/1915.-number-of-wonderful-substrings/</link><pubDate>Thu, 02 May 2024 10:51:29 +0800</pubDate><guid>https://maosong.website/p/1915.-number-of-wonderful-substrings/</guid><description>&lt;p>Given a string ,count the number of wonderful substrings, where a &lt;strong>wonderful string&lt;/strong> is a string that at most one character appears in an odd number of times.&lt;/p>
&lt;h1 id="intuition">&lt;a href="#intuition" class="header-anchor">&lt;/a>Intuition
&lt;/h1>&lt;p>We use the prefix sum and bitwise representation to solve the problem. The idea is that every string can be represented by a state, and we can count the number of wonderful substrings by performing operations among states&lt;/p>
&lt;h1 id="approach">&lt;a href="#approach" class="header-anchor">&lt;/a>Approach
&lt;/h1>&lt;p>According to the hint and the problem description, we have:&lt;/p>
&lt;ol>
&lt;li>there are $10$ possible variables, which represent &lt;code>a&lt;/code> to &lt;code>j&lt;/code>&lt;/li>
&lt;li>we only care about if the variable appears in odd number of times or in even number of times, which means that each variable has $2$ possible states.&lt;/li>
&lt;/ol>
&lt;p>so the number of overall possible states is $2^10=1024$.
We define the prefix substring as &lt;code>prefix[i] = s[1...i]&lt;/code>.&lt;/p>
&lt;p>For each substring, we can represent the state of the prefix as an index of the array, that is, &lt;code>state(prefix[i])&lt;/code> is a number between 0 and 1023, with each byte representing if the corresponding character appears in odd number of times (&lt;code>1&lt;/code>) or in even number of times (&lt;code>0&lt;/code>). A wonderful string is then defined as &lt;em>a string whose state representation is 0 or a power of 2&lt;/em>.&lt;/p>
&lt;p>now the substring &lt;code>s[i...j]&lt;/code> is defined as &lt;code>prefix[j]-prefix[i]&lt;/code>, in state representations, there are three cases:&lt;/p>
&lt;ol>
&lt;li>if one character appears in odd number of times in both prefix substring, then in the result substring, the character appears in even number of times, which is equivalent to &lt;code>(1, 1) -&amp;gt; 0&lt;/code>&lt;/li>
&lt;li>if one character appears in even number of times in both prefix substring, then in the result substring, the character appears in even number of times, which is equivalent to &lt;code>(0, 0) -&amp;gt; 0&lt;/code>&lt;/li>
&lt;li>If one character appears in even number of times in one prefix substring, and appears in odd number of times in the other prefix substring, then in the result substring, the character appears in even number of times, which is equivalent to &lt;code>(1, 0) -&amp;gt; 1&lt;/code>&lt;/li>
&lt;/ol>
&lt;p>so, according to analysis, the minus operation in string corresponding to the XOR operation in states. That is, the state of &lt;code>s[i...j]&lt;/code> is given by &lt;code>state(s[i...j])=state(prefix[j]) XOR state(prefix[i])&lt;/code>.&lt;/p>
&lt;p>Note that there is also a simplification since if &lt;code>a XOR b = c&lt;/code>, then &lt;code>a XOR c = b&lt;/code>. Instead of using a for loop to compute all substrings that ended with character &lt;code>j&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// substring s[i...j]
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">substr_state&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">state&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">^&lt;/span> &lt;span class="n">state&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// check if substr is a wonderful substring
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>with the property of XOR, wo directly check if &lt;code>state(prefix[j]) XOR 2^k&lt;/code> exists, that is:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">for (int i = 0; i &amp;lt; 10; ++i) {
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> int state = state[j] ^ (1 &amp;lt;&amp;lt; i);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> // check if state exists and add them.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="complexity">&lt;a href="#complexity" class="header-anchor">&lt;/a>Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n)$$&lt;/li>
&lt;li>
$$O(1)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">long&lt;/span> &lt;span class="kt">long&lt;/span> &lt;span class="n">wonderfulSubstrings&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">string&lt;/span> &lt;span class="n">word&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">bits&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1024&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">bits&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">long&lt;/span> &lt;span class="kt">long&lt;/span> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">prefix&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">char&lt;/span> &lt;span class="nl">c&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">word&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">prefix&lt;/span> &lt;span class="o">^=&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="o">&amp;lt;&amp;lt;&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">c&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="sc">&amp;#39;a&amp;#39;&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">bits&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">prefix&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="mi">10&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">bits&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">prefix&lt;/span> &lt;span class="o">^&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span> &lt;span class="o">&amp;lt;&amp;lt;&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">)];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">++&lt;/span>&lt;span class="n">bits&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">prefix&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>References:&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/number-of-wonderful-substrings/description/" target="_blank" rel="noopener"
>Leetcode&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>2441. Largest Positive Integer That Exists With Its Negative</title><link>https://maosong.website/p/2441.-largest-positive-integer-that-exists-with-its-negative/</link><pubDate>Thu, 02 May 2024 10:28:05 +0800</pubDate><guid>https://maosong.website/p/2441.-largest-positive-integer-that-exists-with-its-negative/</guid><description>&lt;p>Given a list containing integers, find the largest positive integer that its negative also exists in the list.&lt;/p>
&lt;h1 id="intuition">&lt;a href="#intuition" class="header-anchor">&lt;/a>Intuition
&lt;/h1>&lt;p>There are two ways to solve the problem.&lt;/p>
&lt;ul>
&lt;li>One way is to use two sum, we find the all pairs of integers such that their sum is zero and the one is the negative of the other.&lt;/li>
&lt;li>The second way is to use two pointer, we move left and right pointer utils their absolute values are equal.&lt;/li>
&lt;/ul>
&lt;h1 id="approach-1-two-sum">&lt;a href="#approach-1-two-sum" class="header-anchor">&lt;/a>Approach 1: two sum
&lt;/h1>&lt;p>Similar to Two Sum, for each &lt;code>num&lt;/code> in &lt;code>nums&lt;/code>, we store its negative &lt;code>-num&lt;/code> in the hash table,
however, notice that the added term can be determined by &lt;code>num&lt;/code>, we can use a set instead.&lt;/p>
&lt;h2 id="complexity">&lt;a href="#complexity" class="header-anchor">&lt;/a>Complexity
&lt;/h2>&lt;ul>
&lt;li>
$$O(n)$$&lt;/li>
&lt;li>
$$O(n)$$&lt;/li>
&lt;/ul>
&lt;h2 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h2>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">findMaxK&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">set&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="nl">num&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">find&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">num&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">max&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">result&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">abs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">s&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">insert&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="approach-2-two-pointer">&lt;a href="#approach-2-two-pointer" class="header-anchor">&lt;/a>Approach 2: two pointer
&lt;/h1>&lt;p>We first sort the lists, then we use &lt;code>left=0&lt;/code> and &lt;code>right=length(nums)&lt;/code> pointer to iterate the list, there are three cases:&lt;/p>
&lt;ol>
&lt;li>if &lt;code>nums[left] == -nums[right]&lt;/code>, then we directly return &lt;code>nums[right]&lt;/code> since this is the largest one (note that the list is sorted)&lt;/li>
&lt;li>if &lt;code>nums[left] &amp;lt; -nums[right]&lt;/code>, then we update &lt;code>left&lt;/code> to &lt;code>left + 1&lt;/code> since &lt;code>(nums[left], -nums[left])&lt;/code> cannot be found in the list.&lt;/li>
&lt;li>if &lt;code>nums[left] &amp;gt; -nums[right]&lt;/code>, then we update &lt;code>right&lt;/code> to &lt;code>right - 1&lt;/code> since &lt;code>(-nums[right], nums[right])&lt;/code> cannot be found in the list.&lt;/li>
&lt;/ol>
&lt;h2 id="complexity-1">&lt;a href="#complexity-1" class="header-anchor">&lt;/a>Complexity
&lt;/h2>&lt;ul>
&lt;li>
$$O(n\log n)$$&lt;/li>
&lt;li>
$$O(1)$$&lt;/li>
&lt;/ul>
&lt;h2 id="code-1">&lt;a href="#code-1" class="header-anchor">&lt;/a>Code
&lt;/h2>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">findMaxK&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sort&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nums&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">begin&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">());&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">left&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">right&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">left&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">right&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">nums&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">left&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="n">nums&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">right&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="k">return&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">right&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span> &lt;span class="nf">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">nums&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">left&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="n">nums&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">right&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">left&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">right&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="err">``&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cp"># References
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cp">&lt;/span>&lt;span class="o">-&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">Leetcode&lt;/span>&lt;span class="p">](&lt;/span>&lt;span class="nl">https&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="c1">//leetcode.com/problems/largest-positive-integer-that-exists-with-its-negative/description/)
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="o">-&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">Leetcode&lt;/span> &lt;span class="n">Two&lt;/span> &lt;span class="n">Sum&lt;/span>&lt;span class="p">](&lt;/span>&lt;span class="nl">https&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="c1">//leetcode.com/problems/two-sum/)
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>2000. Reverse Prefix of Word</title><link>https://maosong.website/p/2000.-reverse-prefix-of-word/</link><pubDate>Wed, 01 May 2024 09:01:22 +0800</pubDate><guid>https://maosong.website/p/2000.-reverse-prefix-of-word/</guid><description>&lt;p>Given a string ans a specified character, reverse the prefix that is ended with the specified character.&lt;/p>
&lt;h1 id="intuition">&lt;a href="#intuition" class="header-anchor">&lt;/a>Intuition
&lt;/h1>&lt;p>Simulate the process.&lt;/p>
&lt;h1 id="approach">&lt;a href="#approach" class="header-anchor">&lt;/a>Approach
&lt;/h1>&lt;p>find the first occurrence of the specified character, then reverse the prefix.&lt;/p>
&lt;h1 id="complexity">&lt;a href="#complexity" class="header-anchor">&lt;/a>Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n)$$&lt;/li>
&lt;li>
$$O(1)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;span class="lnt">9
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">string&lt;/span> &lt;span class="n">reversePrefix&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">string&lt;/span> &lt;span class="n">word&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">char&lt;/span> &lt;span class="n">ch&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">auto&lt;/span> &lt;span class="n">pos&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">word&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">find&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ch&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">pos&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">word&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">npos&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">return&lt;/span> &lt;span class="n">word&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">reverse&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">word&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">begin&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">word&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">begin&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">pos&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">word&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>834. Sum of Distances in Tree</title><link>https://maosong.website/p/834.-sum-of-distances-in-tree/</link><pubDate>Mon, 29 Apr 2024 21:24:06 +0800</pubDate><guid>https://maosong.website/p/834.-sum-of-distances-in-tree/</guid><description>&lt;p>Given a tree, return a vector, with each elements of the vector is the sum of the distances between the corresponding node and all other nodes.&lt;/p>
&lt;h1 id="intuition">&lt;a href="#intuition" class="header-anchor">&lt;/a>Intuition
&lt;/h1>&lt;p>Intuitively, I want to use DFS + memorization to solve the problem, however, such method exceeds te time limit.&lt;/p>
&lt;p>Then, I refer to some solutions, and solve the problem.&lt;/p>
&lt;h1 id="approach">&lt;a href="#approach" class="header-anchor">&lt;/a>Approach
&lt;/h1>&lt;h2 id="dp---tle">&lt;a href="#dp---tle" class="header-anchor">&lt;/a>DP -&amp;gt; TLE
&lt;/h2>&lt;p>In the first, I am thinking that we can use &lt;code>dp[i][j]&lt;/code> to represent the distance between the node &lt;code>i&lt;/code> and the node &lt;code>j&lt;/code>. Initially, &lt;code>dp&lt;/code> are initialized as follows:&lt;/p>
&lt;ol>
&lt;li>If there is an edge between &lt;code>i&lt;/code> and &lt;code>j&lt;/code>, then &lt;code>dp[i][j]=dp[j][i]=1&lt;/code>.&lt;/li>
&lt;li>If there is an edge between &lt;code>i&lt;/code> and &lt;code>j&lt;/code>, then &lt;code>dp[i][j]=dp[j][i]=INFINITY&lt;/code>.&lt;/li>
&lt;/ol>
&lt;p>we traversal from node &lt;code>0&lt;/code> to node &lt;code>n-1&lt;/code>, for each node &lt;code>i&lt;/code>, we compute the distance between &lt;code>i&lt;/code> and all other nodes &lt;code>j&lt;/code>. There are two cases:&lt;/p>
&lt;ol>
&lt;li>&lt;code>dp[i][j] != INFINITY&lt;/code>, then we directly returns &lt;code>dp[i][j]&lt;/code>&lt;/li>
&lt;li>&lt;code>dp[i][j] == INFINITY&lt;/code>, then it means the distance between node &lt;code>i&lt;/code> and node &lt;code>j&lt;/code> hasn&amp;rsquo;t been computed, we then update it as follows:&lt;/li>
&lt;/ol>
$$ dp[i][j] = \min_{k\in N(i)} (1 + dp[k][j]) $$&lt;p>where $N(i)$ is the nodes that adjacent to node &lt;code>i&lt;/code>. The &lt;code>min&lt;/code> operation is used here since the node &lt;code>k&lt;/code> and the node &lt;code>j&lt;/code> may not connected (without passing node &lt;code>i&lt;/code>).&lt;/p>
&lt;p>The code is given as follows. However, the time complexity is $O(n^2)$, which exceeds the time limit.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">adjs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">dp&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">bool&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">visited&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">start&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">end&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">start&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="n">INT_MAX&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">return&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">start&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">visited&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">start&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="k">return&lt;/span> &lt;span class="n">INT_MAX&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">visited&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">start&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">true&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">distance&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">INT_MAX&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="nl">next_node&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">adjs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">start&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">distance&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">distance&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">adjs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">visited&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">next_node&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">end&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">visited&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">start&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">false&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">distance&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="n">INT_MAX&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">start&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">distance&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">start&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">sumOfDistancesInTree&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">edges&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">INT_MAX&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">adjs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// initialize the distance matrix
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="k">auto&lt;/span>&lt;span class="o">&amp;amp;&lt;/span> &lt;span class="nl">edge&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">edges&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]][&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]][&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">adjs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]].&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">adjs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]].&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">bool&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">visited&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">false&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">adjs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">visited&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// the graph is undirected
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">j&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="tree--traversal">&lt;a href="#tree--traversal" class="header-anchor">&lt;/a>Tree + Traversal
&lt;/h2>&lt;p>The second way is not easy to figure out. We decompose it into two parts:&lt;/p>
&lt;ol>
&lt;li>compute the distance between the root node and all other nodes.&lt;/li>
&lt;li>Convert the root node from one to another and update the result.&lt;/li>
&lt;/ol>
&lt;h3 id="sum-of-distance-from-root-node-to-all-other-nodes">&lt;a href="#sum-of-distance-from-root-node-to-all-other-nodes" class="header-anchor">&lt;/a>Sum of distance from root node to all other nodes.
&lt;/h3>&lt;p>Consider one example tree with root set as &lt;code>0&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl"> &lt;span class="mi">0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">/&lt;/span> \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="mi">1&lt;/span> &lt;span class="mi">2&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">/&lt;/span> &lt;span class="o">|&lt;/span> \
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="mi">3&lt;/span> &lt;span class="mi">4&lt;/span> &lt;span class="mi">5&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>We first define &lt;code>dist[i]&lt;/code> as the distance of all child nodes of node &lt;code>i&lt;/code> to node &lt;code>i&lt;/code>. Then we have:&lt;/p>
&lt;ol>
&lt;li>$dist[i] = 0$ if &lt;code>i&lt;/code> is a leaf node.&lt;/li>
&lt;li>$dist[i] = \sum_{j\in C(i)}dist[j] + |C(i)|$ if &lt;code>i&lt;/code> is not a leaf node, where $C(i)$ is the offspring of node &lt;code>i&lt;/code>&lt;/li>
&lt;/ol>
&lt;p>We can now compute the sum of distances between root &lt;code>0&lt;/code> to all other nodes, which is &lt;code>result[0]&lt;/code>.&lt;/p>
&lt;p>Now we need to compute all other results. Repeating the above process is too time consuming, we need to reduce the time complexity. We are seeking a way to compute &lt;code>result[i]&lt;/code> from &lt;code>result[j]&lt;/code>, where $j\in C(i)$.&lt;/p>
&lt;p>Note that for $k\in C(j)$, when we compute &lt;code>result[i]&lt;/code>, we are computing distance from $k$ to $i$, so we need to reduce by 1 since their height decreases (the root changes from &lt;code>i&lt;/code> to &lt;code>j&lt;/code>). On the other hand, all other nodes, which are not offspring of node &lt;code>j&lt;/code>, is added by 1 since the height of them increases. Thus, the transformation reads:&lt;/p>
$$ result[j] = result[i] - |C(j)| + n - |C(j)| $$&lt;h1 id="complexity">&lt;a href="#complexity" class="header-anchor">&lt;/a>Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$ O(n) $$&lt;/li>
&lt;li>
$$ O(n) $$&lt;/li>
&lt;/ul>
&lt;h1 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">void&lt;/span> &lt;span class="n">post_order_traversal&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">adjs&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">count&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">parent&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="nl">next_node&lt;/span> &lt;span class="p">:&lt;/span> &lt;span class="n">adjs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">next_node&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">parent&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">post_order_traversal&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">adjs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">count&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">next_node&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">count&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">count&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">next_node&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">count&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">next_node&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">next_node&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">++&lt;/span>&lt;span class="n">count&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">void&lt;/span> &lt;span class="nf">pre_order_traversal&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">adjs&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">count&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">parent&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="nl">next_node&lt;/span> &lt;span class="p">:&lt;/span> &lt;span class="n">adjs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">next_node&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">parent&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">next_node&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">count&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">next_node&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">count&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">count&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">next_node&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pre_order_traversal&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">adjs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">count&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">next_node&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">sumOfDistancesInTree&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">edges&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">count&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">adjs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="k">auto&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="nl">edge&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">edges&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">adjs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]].&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">adjs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]].&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// compute result[0]
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="n">post_order_traversal&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">adjs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">count&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// compute other results from result[0]
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="n">pre_order_traversal&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">adjs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">count&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/sum-of-distances-in-tree/description/" target="_blank" rel="noopener"
>Leetcode&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.cnblogs.com/grandyang/p/11520804.html" target="_blank" rel="noopener"
>Solution&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>2997. Minimum Number of Operations to Make Array XOR Equal to K</title><link>https://maosong.website/p/2997.-minimum-number-of-operations-to-make-array-xor-equal-to-k/</link><pubDate>Mon, 29 Apr 2024 21:16:53 +0800</pubDate><guid>https://maosong.website/p/2997.-minimum-number-of-operations-to-make-array-xor-equal-to-k/</guid><description>&lt;p>Given an integer array, we can flip one bit of one element of the array at every step, we asked to compute the &lt;em>minimum&lt;/em> flips, such that the XOR of all elements of the array is equal to the given integer $k$.&lt;/p>
&lt;h1 id="intuition">&lt;a href="#intuition" class="header-anchor">&lt;/a>Intuition
&lt;/h1>&lt;p>XOR of multiple bits is always equal to either &lt;code>1&lt;/code> or &lt;code>0&lt;/code>, and changes one of the input bits will cause the result change to the other one.&lt;/p>
&lt;h1 id="approach">&lt;a href="#approach" class="header-anchor">&lt;/a>Approach
&lt;/h1>&lt;p>We first compute the XOR of all elements of the array, then we compare bit by bit with the XOR result and the given &lt;code>k&lt;/code>, utils all bits becomes the same.&lt;/p>
&lt;h1 id="complexity">&lt;a href="#complexity" class="header-anchor">&lt;/a>Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n)$$&lt;/li>
&lt;li>
$$O(1)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">minOperations&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">^=&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">bits_result&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bits_k&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">bits_k&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">k&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">k&lt;/span> &lt;span class="o">/=&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">result&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">bits_result&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">result&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">/=&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">count&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">bits_result&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">bits_k&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">bits_result&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="n">bits_k&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">++&lt;/span>&lt;span class="n">count&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">bits_result&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">bits_result&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">++&lt;/span>&lt;span class="n">count&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">bits_k&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">bits_k&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">++&lt;/span>&lt;span class="n">count&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">count&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>Regularization methods in deep learning</title><link>https://maosong.website/p/regularization-methods-in-deep-learning/</link><pubDate>Sat, 27 Apr 2024 18:02:02 +0800</pubDate><guid>https://maosong.website/p/regularization-methods-in-deep-learning/</guid><description>&lt;p>To reduce the gap of the performance of the model on the training dataset and the test dataset, we need use regularization methods.&lt;/p>
&lt;h1 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h1>&lt;p>Possible reasons for the discrepancy that the model performs worse on the test dataset than on the training dataset are:&lt;/p>
&lt;ol>
&lt;li>the model describes statistical peculiarities of the training dataset that are not representative of the true mapping from the input to the output, that is, overfitting.&lt;/li>
&lt;li>the model is unconstrained in areas with no training areas, leading to suboptimal predictions&lt;/li>
&lt;/ol>
&lt;h1 id="explicit-regularization-methods">&lt;a href="#explicit-regularization-methods" class="header-anchor">&lt;/a>Explicit Regularization Methods
&lt;/h1>&lt;p>Consider fitting a model $f(\bm{x}; \phi)$ with parameter $\phi$ using a training dataset $\{\bm{x}_i,y_i\}$, we seek to minimize the loss function $L(\phi)$:&lt;/p>
$$ \hat{\phi} = \arg\min_{\phi}L(\phi;\{\bm{x}_i,y_i\}) $$&lt;p>Now, to bias the minimization towards certain solutions, we add an additional term:&lt;/p>
$$ \hat{\phi} = \arg\min_{\phi}[L(\phi;\{\bm{x}_i,y_i\}) + \lambda g(\phi)] $$&lt;p>where $g(\phi)$ is called the regularization term and $\lambda>0$ is a hyperparameter.&lt;/p>
&lt;p>In the probabilistic perspective, we can construct the loss function from &lt;em>maximum likelihood estimation&lt;/em>, or MLE, that is&lt;/p>
$$ \hat{\phi} = \arg\max_{\phi}\left[ \prod_{i}\mathrm{Pr}(y_i\mid \bm{x}_i,\phi) \right] $$&lt;p>The regularization term can be considered as a &lt;em>prior&lt;/em> $\mathrm{Pr}(\phi)$， in this way, we are now using &lt;em>maximum a posteriori&lt;/em> criterion:&lt;/p>
$$ \hat{\phi} = \arg\max_{\phi}\left[ \prod_{i}\mathrm{Pr}(y_i\mid \bm{x}_i,\phi)\mathrm{Pr}(\phi) \right] $$&lt;h1 id="implicit-regularization-methods">&lt;a href="#implicit-regularization-methods" class="header-anchor">&lt;/a>Implicit Regularization Methods
&lt;/h1>&lt;p>Gradient descent and stochastic gradient descent are commonly used to minimize the loss functions, however, neither of them moves neutrally to the minimum of the loss function, thus the implicit regularization method is proposed to solve the problem.&lt;/p>
&lt;h2 id="implicit-regularization-in-gradient-descent">&lt;a href="#implicit-regularization-in-gradient-descent" class="header-anchor">&lt;/a>Implicit Regularization in gradient descent
&lt;/h2>&lt;p>The change of the parameters $\phi$ is defined by the differential equation:&lt;/p>
$$ \frac{d{\phi}}{d t} = -\frac{dL}{d\phi} $$&lt;p>gradient descent uses &lt;a class="link" href="https://en.wikipedia.org/wiki/Difference_quotient" target="_blank" rel="noopener"
>difference quotient&lt;/a> with increment (or learning rate) $\alpha$ to approximate the change of $\phi$:&lt;/p>
$$ \frac{\phi_{t+1}-\phi_{t}}{\alpha}=-\frac{dL}{d\phi} \Rightarrow \phi_{t+1} = \phi_{t} - \alpha\frac{dL}{d\phi} $$&lt;p>However, this discretization causes deviation from the continuous path.&lt;/p>
&lt;p>To fix the problem, and extra item is added to the loss to avoid the deviation caused by discretization:&lt;/p>
$$ \tilde{L}(\phi) = L(\phi) + \frac{\alpha}{4}\left\Vert \frac{dL}{d\phi}\right\Vert^2 $$&lt;h2 id="implicit-regularization-in-stochastic-gradient-descent">&lt;a href="#implicit-regularization-in-stochastic-gradient-descent" class="header-anchor">&lt;/a>Implicit Regularization in stochastic gradient descent
&lt;/h2>&lt;p>A similar approach can be applied to stochastic gradient descent, which reads as&lt;/p>
$$ \tilde{L}(\phi) = L(\phi) + \frac{\alpha}{4|B|}\sum_{i\in B}\left\Vert \frac{dL_{i}}{d\phi}-\frac{dL}{d\phi}\right\Vert^2 $$&lt;p>
where $L_{B}$ is the loss on the batch $B$.&lt;/p>
&lt;h1 id="heuristic-methods">&lt;a href="#heuristic-methods" class="header-anchor">&lt;/a>Heuristic Methods
&lt;/h1>&lt;h2 id="early-stopping">&lt;a href="#early-stopping" class="header-anchor">&lt;/a>Early stopping
&lt;/h2>&lt;p>Early stopping means that we stop the training procedure before the model becomes overfitting. By stopping early, we prevent the model captures the corner features of the training dataset.&lt;/p>
&lt;p>Early stopping has a single hyperparameter, the number of steps after which the training is stopped, this is chosen usually with the help of the validation dataset.&lt;/p>
&lt;h2 id="ensembling">&lt;a href="#ensembling" class="header-anchor">&lt;/a>Ensembling
&lt;/h2>&lt;p>Ensembling means we train multiple models on the training dataset, and during the inference time, we take the average inference result of each model. The technique improves the test performance with the sacrifices of training and storing multiple models.&lt;/p>
&lt;p>There are some ensembling methods:&lt;/p>
&lt;ol>
&lt;li>Use different random initializations. This leads the model reaches different local minimum and may help reduce the overfitting.&lt;/li>
&lt;li>Generate several different datasets by re-sampling the training dataset and train model on each of them.This is also known as &lt;em>bootstrap aggregating&lt;/em> or &lt;em>bagging&lt;/em>, this division can smooth out the data, since each model tries to predict the distribution of data that is not included in its training dataset.&lt;/li>
&lt;/ol>
&lt;h2 id="dropout">&lt;a href="#dropout" class="header-anchor">&lt;/a>Dropout
&lt;/h2>&lt;p>Drop out randomly clamps a subset of hidden units of the layer at each iteration of SGD. This makes the model depends on general feature instead of some specific feature, since the specific feature may be masked.&lt;/p>
&lt;p>At test time, we can run the network as usual with all the hidden units active; however, the network now has more hidden units than it was trained with at any given iteration, so we multiply the weights by one minus the dropout probability to compensate. This is known as the &lt;em>weight scaling inference rule&lt;/em>.&lt;/p>
&lt;h2 id="applying-noise">&lt;a href="#applying-noise" class="header-anchor">&lt;/a>Applying noise
&lt;/h2>&lt;p>Dropout can be interpreted as applying multiplicative Bernoulli noise to the network activations. We can apply noise to other parts of the model during training.&lt;/p>
&lt;ol>
&lt;li>We can add noise to the input data, this smooth out the learned function.&lt;/li>
&lt;li>We can also add noise to model parameters, this encourages the model to be robust to small perturbations of the weights.&lt;/li>
&lt;li>We can also perturb the labels. We can change the label of a portion of the training dataset, this can prevent the model from being overconfident.&lt;/li>
&lt;/ol>
&lt;h2 id="bayesian-inference">&lt;a href="#bayesian-inference" class="header-anchor">&lt;/a>Bayesian inference
&lt;/h2>&lt;p>The MLE approach tries to find a function $f(\bm{x};\phi)$ that fit the dataset $\{\bm{x}_i,y_i\}$, this approach may be overconfident about the task since the bias of the training data construction.&lt;/p>
&lt;p>To overcome such bias, we treats the parameters $\phi$ as unknown variables instead of scalars. Then we find a distribution over the parameters $\phi$ conditioned on the training data $\{\bm{x}_i,y_i\}$, using &lt;a class="link" href="https://en.wikipedia.org/wiki/Bayes%27_theorem" target="_blank" rel="noopener"
>Bayesian theorem&lt;/a>:&lt;/p>
$$ \mathrm{Pr}(\phi\mid \{\bm{x}_i,y_i\}) = \frac{\prod_{i}\mathrm{Pr}(y_i\mid \bm{x}_i,\phi)\mathrm{Pr}(\phi)}{\int\prod_{i}\mathrm{Pr}(y_i\mid \bm{x}_i,\phi)\mathrm{Pr}(\phi)d\phi} $$&lt;p>where $\mathrm{Pr}(\phi)$ us the prior probability of the parameters, and the denominator is a normalizing term.&lt;/p>
&lt;p>Then the prediction for unseen item $\{\bm{x},y\}$ is given by the following infinite weighted sum:&lt;/p>
$$ \mathrm{Pr}(y\mid x, \{\bm{x}_i,y_i\}) = \int \mathrm{Pr}(y\mid \bm{x},\phi)\mathrm{Pr}(\phi\mid \{\bm{x}_i,y_i\})d\phi $$&lt;p>This is an infinite weighted ensemble, where the weights depend on&lt;/p>
&lt;ol>
&lt;li>the prior probability of the parameters&lt;/li>
&lt;li>the agreement with the data&lt;/li>
&lt;/ol>
&lt;p>Though Bayesian approach is capable of representing the data more robust, it is hard to implement since there is no way to represent an distribution.
Current implementation simplifies the distribution as Gaussian distribution and each parameter is replaced with the mean $\mu$ and standard deviation $\sigma$ of the Gaussian distribution.&lt;/p>
&lt;h2 id="transfer-learning-and-multi-task-learning">&lt;a href="#transfer-learning-and-multi-task-learning" class="header-anchor">&lt;/a>Transfer learning and multi-task learning
&lt;/h2>&lt;p>In transfer learning, the model is first pre-trained before training or fine-tuning on the task we are interested in. The idea is that the model may learn some good representation of the data from the main task. Alternatively, we can think transfer learning as initializing the model parameters in a reasonable area such that the minimum is better compared to the random initialization.&lt;/p>
&lt;p>Multi-task learning is a related technique that the model is trained on multiple related tasks concurrently. In this way, the model can learn from multiple datasets and multiple objectives, this encourages the model to learn the essential part of the tasks.&lt;/p>
&lt;h2 id="self-supervised-learning">&lt;a href="#self-supervised-learning" class="header-anchor">&lt;/a>Self-supervised learning
&lt;/h2>&lt;p>In some cases, we do not have multiple datasets for pre-training or for multi-tasks. To solve this problem, we can use self-supervised learning to generate large amounts of label-free data. There are two families of self-supervised learning: generative and contrastive.&lt;/p>
&lt;p>In generative self-supervised learning, part of each data example is masked, and the task is to predict the masked part. For example, given a sentence, we can mask the verb and ask for the model to predict the correct verb, the helps the model to learning semantic meaning of a sentence.&lt;/p>
&lt;p>In contrastive self-supervised learning, we try to group related data and separated unrelated data. For example, a cat is more similar to another cat compared with a dog. In this way, the model can learn more robust representations and can be adapted to new tasks easily.&lt;/p>
&lt;h2 id="augmentation">&lt;a href="#augmentation" class="header-anchor">&lt;/a>Augmentation
&lt;/h2>&lt;p>Augmentation aims to expand the training dataset, we can perform transformation to each training data without changing the labels, for example we can rotate, flip a image of cat. The augmentation is to teach the model to be invariant to these irrelevant data transformations.&lt;/p>
&lt;h1 id="summary">&lt;a href="#summary" class="header-anchor">&lt;/a>Summary
&lt;/h1>&lt;p>To summarize the regularization methods, we use the following picture to depict the mechanisms.
&lt;img src="https://maosong.website/p/regularization-methods-in-deep-learning/regularization_overview.png"
width="1210"
height="718"
loading="lazy"
alt="Regularization methods"
class="gallery-image"
data-flex-grow="168"
data-flex-basis="404px"
>&lt;/p>
&lt;h1 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://udlbook.github.io/udlbook/" target="_blank" rel="noopener"
>Understanding Deep Learning Chapter 9&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>514. Freedom Trail</title><link>https://maosong.website/p/514.-freedom-trail/</link><pubDate>Sat, 27 Apr 2024 11:55:28 +0800</pubDate><guid>https://maosong.website/p/514.-freedom-trail/</guid><description>&lt;p>Given a string displaying in the ring format, we can move one character at each step either in clockwise or anticlockwise (or hold still), now we need to retrieve a given string with minimum steps.&lt;/p>
&lt;h1 id="intuition">&lt;a href="#intuition" class="header-anchor">&lt;/a>Intuition
&lt;/h1>&lt;p>We can construct a graph from the string and use DFS to search all possible paths, and find the path with minimum steps.&lt;/p>
&lt;h1 id="approach">&lt;a href="#approach" class="header-anchor">&lt;/a>Approach
&lt;/h1>&lt;h2 id="dfs---tle">&lt;a href="#dfs---tle" class="header-anchor">&lt;/a>DFS -&amp;gt; TLE
&lt;/h2>&lt;p>In the beginning, I am going to use DFS to find all possible paths and find the path with minimum steps.&lt;/p>
&lt;p>First, we need construct the graph, each character is adajcent to all other characters in &lt;code>ring&lt;/code>, so there are &lt;code>n=length(ring)&lt;/code> nodes and &lt;code>(n-1)^n&lt;/code> edges. each edge has a weight representing the distance between two characters: &lt;code>weight(i,j)=min(abs(i-j), n - abs(i-j))&lt;/code> (here we use index of the character to represent the node).&lt;/p>
&lt;p>Then, we can use DFS to search all possible paths, in each point, we have a state &lt;code>(ring_index, key_index)&lt;/code>, representing the current index on &lt;code>ring&lt;/code> and &lt;code>key&lt;/code>, there are two cases:&lt;/p>
&lt;ol>
&lt;li>&lt;code>ring[ring_index] == key[key_index]&lt;/code>, in this case, we change &lt;code>key_index&lt;/code> to &lt;code>key_index+1&lt;/code> and keeps &lt;code>ring_index&lt;/code> unchanged (hold still).&lt;/li>
&lt;li>&lt;code>ring[ring_index] != key[key_index]&lt;/code>, in this case, we need to rotate the string &lt;code>ring&lt;/code> to make &lt;code>ring[ring_index] == key[key_index]&lt;/code>, this takes step and notice that there may multiple choices, so we need to go over all of them.&lt;/li>
&lt;/ol>
&lt;p>Once &lt;code>key_index == len(key)&lt;/code>, we have found a path and we can now update the result.&lt;/p>
&lt;p>The code is given as follows:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">void&lt;/span> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">unordered_map&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">adjs&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">const&lt;/span> &lt;span class="n">string&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">ring&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="k">const&lt;/span> &lt;span class="n">string&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">key&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">ring_index&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">key_index&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">min_rotates&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">current_rotates&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">key_index&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">min_rotates&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">min_rotates&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">current_rotates&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// character matches
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">ring&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">ring_index&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">key_index&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">adjs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ring&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ring_index&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">key_index&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">min_rotates&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">current_rotates&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// character doesn&amp;#39;t match
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="k">auto&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="nl">next_node&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">adjs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">ring_index&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">next_ring_index&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">next_node&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">rotates&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">next_node&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">ring&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">next_ring_index&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">key_index&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">current_rotates&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">rotates&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">adjs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ring&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">next_ring_index&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">key_index&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">min_rotates&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">current_rotates&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">current_rotates&lt;/span> &lt;span class="o">-=&lt;/span> &lt;span class="n">rotates&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="nf">findRotateSteps&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">string&lt;/span> &lt;span class="n">ring&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">string&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">unordered_map&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">adjs&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ring&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">ring&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">ring&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">j&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">diff&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">abs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">rotates&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">diff&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">diff&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">adjs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">].&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">rotates&lt;/span>&lt;span class="p">});&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">ring_index&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">key_index&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">min_rotates&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">INT_MAX&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">current_rotates&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">adjs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ring&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">ring_index&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">key_index&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">min_rotates&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">current_rotates&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">spell&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">min_rotates&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">spell&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="dynamic-programming">&lt;a href="#dynamic-programming" class="header-anchor">&lt;/a>Dynamic programming
&lt;/h2>&lt;p>The problem of DFS is that, its time complexity grows exponentially if there have multiple repeat characters, which causes TLE (time limit exceeded) error.&lt;/p>
&lt;p>So, to reduce the complexity, we can construct the solution from bottom to up. That is, we remember the path to go from the current state, and now we now go one step back, util we back to the original state.&lt;/p>
&lt;p>We use &lt;code>dp[i][j]&lt;/code> to represent start from &lt;code>ring[j]&lt;/code>, the minimum rotate steps we need to recover the string &lt;code>key[i...m]&lt;/code>, where &lt;code>m=length(key)&lt;/code>, the target then becomes finding out &lt;code>dp[0][0]&lt;/code>.&lt;/p>
&lt;p>Note that we can easily compute &lt;code>dp[m-1][j]&lt;/code>, &lt;code>j=1,\dots,n&lt;/code>, since there is only one character we need to recover, so we start from &lt;code>ring[j]&lt;/code>, and rotate util we find a character &lt;code>ring[k]&lt;/code> such that &lt;code>ring[k]==key[m-1]&lt;/code>, the minimum steps is then updated. The update formula is then given by&lt;/p>
$$ dp[i][j] = \min_{k=1,\dots,n,\ ring[k]=key[i]}(dp[i][j],\ dp[i + 1][k] + step(j, k)) $$&lt;p>where $step(j,k)=\min(|j-k|,\ n-|j-k|)$.&lt;/p>
&lt;h1 id="complexity">&lt;a href="#complexity" class="header-anchor">&lt;/a>Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(mn^2)$$&lt;/li>
&lt;li>
$$O(mn)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">findRotateSteps&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">string&lt;/span> &lt;span class="n">ring&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">string&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">m&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ring&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">m&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">m&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;gt;=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">--&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// start from ring[j]
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">INT_MAX&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// Find the feasible target ring[k] == key[i]
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">k&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">k&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">ring&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">rotates&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">abs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">k&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">abs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">k&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">rotates&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">m&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/freedom-trail/description" target="_blank" rel="noopener"
>leetcode 514&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.cnblogs.com/grandyang/p/6675879.html" target="_blank" rel="noopener"
>Solution&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>1289. Minimum Falling Path Sum II</title><link>https://maosong.website/p/1289.-minimum-falling-path-sum-ii/</link><pubDate>Fri, 26 Apr 2024 20:44:17 +0800</pubDate><guid>https://maosong.website/p/1289.-minimum-falling-path-sum-ii/</guid><description>&lt;p>Given a matrix, find the minimum falling path sum from top to bottom, with no two adjacent rows sharing the same column.&lt;/p>
&lt;h1 id="intuition">&lt;a href="#intuition" class="header-anchor">&lt;/a>Intuition
&lt;/h1>&lt;p>Same as the previous version, we only change the update formula.&lt;/p>
&lt;h1 id="approach">&lt;a href="#approach" class="header-anchor">&lt;/a>Approach
&lt;/h1>&lt;p>We use &lt;code>dp[i][j]&lt;/code> to represent the minimum falling path sum that ends with &lt;code>grid[i][j]&lt;/code>. The update rules is then given by&lt;/p>
$$ dp[i][j] = \min_{k=1,\dots,n,k\neq j}(grid[i][j] + dp[i - 1][k]) $$&lt;h1 id="complexity">&lt;a href="#complexity" class="header-anchor">&lt;/a>Complexity
&lt;/h1>&lt;ul>
&lt;li>Time complexity: We need to iterate all elements of the matrix once, and each iterate requires to iterate its last row once, which is $O(n)$. This can be reduced to $O(n^2\log n)$ by compute the smallest, second smallest elements of the last row.&lt;/li>
&lt;/ul>
$$O(n^3)$$&lt;ul>
&lt;li>Space complexity: the &lt;code>dp&lt;/code> matrix is of size $n\times n$. This can be reduced to $O(n)$ by use a $n\times 2$ matrix since each row only relates to its last row.&lt;/li>
&lt;/ul>
$$O(n^2)$$&lt;h1 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">minFallingPathSum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">INT_MAX&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">row&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">row&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">row&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">col&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">col&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">col&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">last_col&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">last_col&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">last_col&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">last_col&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">col&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">row&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">col&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">row&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">col&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">row&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">col&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">row&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">last_col&lt;/span>&lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">INT_MAX&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">result&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">n&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://leetcode.com/problems/minimum-falling-path-sum-ii/description/" target="_blank" rel="noopener"
>Leetcode 1289&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>BLEU (Bilingual Evaluation Understudy)</title><link>https://maosong.website/p/bleu-bilingual-evaluation-understudy/</link><pubDate>Thu, 25 Apr 2024 22:46:53 +0800</pubDate><guid>https://maosong.website/p/bleu-bilingual-evaluation-understudy/</guid><description>&lt;p>BLEU (Bilingual Evaluation Understudy) is a widely used metric that evaluates the quality of the translated text with respect to the reference translations.&lt;/p>
&lt;h1 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h1>&lt;p>The formula of BLEU is defined as follows:&lt;/p>
$$ \mathrm{BLEU}_ {w_n}(\hat{S}, S) = \mathrm{BP} \cdot \exp\left(\sum_{n=1}^Nw_n\log p_n(\hat{S}, S) \right) $$&lt;p>where&lt;/p>
&lt;ol>
&lt;li>$\mathrm{BP}$ represents the Brevity Penalty to penalize the translations that are shorter than the reference translations.&lt;/li>
&lt;li>$p_n$ represents the modified $n$-gram precision score&lt;/li>
&lt;li>$w_n$ represents the weight for $p_n$, it satisfies $0\leq w_n\leq1$ and $\sum_{n=1}^Nw_n=1$.&lt;/li>
&lt;/ol>
&lt;p>Now we interprets three parts in detail.&lt;/p>
&lt;h1 id="interpretation">&lt;a href="#interpretation" class="header-anchor">&lt;/a>Interpretation
&lt;/h1>&lt;h2 id="definitions">&lt;a href="#definitions" class="header-anchor">&lt;/a>Definitions
&lt;/h2>&lt;p>Given any string $y=y_1y_2\cdots y_K$, where $y_i,i\in{1,\dots,K}$ are characters and an integer $n\geq1$, we define the &lt;strong>set of $n$-gram&lt;/strong> to be&lt;/p>
$$ G_n(y) = \{ y_1\cdots y_n, y_2\cdots y_{n+1}, \dots, y_{K-n+1}\cdots y_K\} $$&lt;p>NOTE that this is a set with unique elements, for example, $G_2(abab)=\{ab, ba\}$.&lt;/p>
&lt;p>Given any two strings $s$ and $y$, we define &lt;strong>substring count&lt;/strong> $C(s,y)$ to tbe the number of appearances of $s$ as a substring of $y$. For example, $C(ab, abcbab)=2$ since $ab$ appear in $abcbab$ twice in position $1$ and $5$.&lt;/p>
&lt;h2 id="modified-precision-score">&lt;a href="#modified-precision-score" class="header-anchor">&lt;/a>Modified precision score
&lt;/h2>&lt;p>We start from the one candidate translation $\hat{y}$ and one reference translation $y$. The modified $n$-gram is defined as&lt;/p>
$$ p_n(\hat{y}, y) = \frac{\sum_{s\in G_n(\hat{y})}\min (C(s,\hat{y}), C(s,y))}{\sum_{s\in G_n(\hat{y})}C(s,\hat{y})} $$&lt;p>The quantity measures how many $n$-grams of the reference translation $y$ appears in the candidate translation $\hat{y}$.
In case that $\hat{y}$ is too short, we take a minimum between $C(s,\hat{y})$ and $ C(s,y)$. Then we normalize to make $p_n(\hat{y}, y)$ comparable among multiple translation pairs.&lt;/p>
&lt;p>Now, suppose we have a candidate translation corpus, $\hat{S}=\{\hat{y}^1,\dots,\hat{y}^M\}$, and for each candidate translation $\hat{y}^i$, we have a reference translation corpus (there are multiple translations can represent the same meaning) $S_i=\{y^{i,1},\dots,y^{i,N_i}\}$. We define $S=\{S_1,\dots,S_M\}$, then our modified $n$-gram precision is defined as&lt;/p>
$$ p_n(\hat{S}, S) = \frac{\sum_{i=1}^M\sum_{s\in G_n(\hat{y})}\min (C(s,\hat{y}), \max_{y\in S_i}C(s,y))}{\sum_{i=1}^M\sum_{s\in G_n(\hat{y})}C(s,\hat{y})} $$&lt;p>note that we have replaced $\min (C(s,\hat{y}), C(s,y))$ with $\min (C(s,\hat{y}), \max_{y\in S_i}C(s,y))$ since there are multiple reference translation, we use the most similar one. So this is to say: &amp;ldquo;There are multiple answer, go to choose the best one and compute the score.&amp;rdquo;&lt;/p>
&lt;h2 id="bp-brevity-penalty">&lt;a href="#bp-brevity-penalty" class="header-anchor">&lt;/a>BP (Brevity Penalty)
&lt;/h2>&lt;p>Candidate translations longer than their references are already penalized by the modified $n$-gram precision measure.
Now, to penalize those translations shorter than the reference translations, we need add an penalty term. This is when brevity penalty comes out:&lt;/p>
$$
\mathrm{BP}=\begin{cases}1&amp;\text{ if }c > r\\
\exp(1-\frac{r}{c})&amp;\text{ if }c \leq r
\end{cases}
$$&lt;p>where&lt;/p>
&lt;ul>
&lt;li>$c$ is the number of words or tokens of the candidate corpus. That is,&lt;/li>
&lt;/ul>
$$ c = \sum_{i=1}^M\mathrm{length}(\hat{y}^i) $$&lt;ul>
&lt;li>$r$ is the number of words or tokens of the effective reference corpus length, where the effective reference is defined as the reference translation whose length is as close to the corresponding candidate translation as possible. That is&lt;/li>
&lt;/ul>
$$ r = \sum_{i=1}^M\mathrm{length}(y^{i,j}),\text{ where } y^{i,j}=\arg\min_{y\in S_i}|\mathrm{length}(\hat{y}^i)-\mathrm{length}(y)| $$&lt;p>with this penalty term, we wish the model to output the translations with the same length as the reference translations.&lt;/p>
&lt;h2 id="weight">&lt;a href="#weight" class="header-anchor">&lt;/a>Weight
&lt;/h2>&lt;p>The weight measures the importance of different precision score, in the original paper, the uniform weights are adopted, that is&lt;/p>
$$ w_i = \frac{1}{N}, \ \text{ for } i=1,\dots,N $$&lt;h2 id="final-definition">&lt;a href="#final-definition" class="header-anchor">&lt;/a>Final definition
&lt;/h2>&lt;p>The final definition of the BLEU is given by&lt;/p>
$$ \mathrm{BLEU}_ {w}(\hat{S}, S) = \mathrm{BP} \cdot \exp\left(\sum_{n=1}^\infty w_n\log p_n(\hat{S}, S) \right) $$&lt;p>usually, the upper-bound of the above summation can be reduced to $\max_{i=1,\dots,M}\mathrm{length}(\hat{y}^i)$.&lt;/p>
&lt;h1 id="analysis">&lt;a href="#analysis" class="header-anchor">&lt;/a>Analysis
&lt;/h1>&lt;p>Disadvantages of BLEU:&lt;/p>
&lt;ul>
&lt;li>BLEU compares overlap in tokens from the predictions and references, instead of comparing meaning. This can lead to discrepancies between BLEU scores and human ratings.&lt;/li>
&lt;li>BLEU scores are not comparable across different datasets, nor are they comparable across different languages.&lt;/li>
&lt;li>BLEU scores can vary greatly depending on which parameters are used to generate the scores, especially when different tokenization and normalization techniques are used.&lt;/li>
&lt;li>BLEU ignores synonym or similar expression, which causes refuses of reasonable translation.&lt;/li>
&lt;li>BLEU is affected by common words.&lt;/li>
&lt;/ul>
&lt;h1 id="python-implementation">&lt;a href="#python-implementation" class="header-anchor">&lt;/a>Python Implementation
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;span class="lnt">49
&lt;/span>&lt;span class="lnt">50
&lt;/span>&lt;span class="lnt">51
&lt;/span>&lt;span class="lnt">52
&lt;/span>&lt;span class="lnt">53
&lt;/span>&lt;span class="lnt">54
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">math&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">typing&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">Set&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">List&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">compute_n_gram_set&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">Set&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="nb">set&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="p">:&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">modified_n_gram_precision&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">S_hat&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">S&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">]],&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">float&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">S_hat&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="mf">0.0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">numerator&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">denominator&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">index&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y_hat&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">enumerate&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">S_hat&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">n_gram_set&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">compute_n_gram_set&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y_hat&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="n">n_gram_set&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">continue&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># print(n_gram_set)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">n_gram&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">n_gram_set&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">candidate_substr_count&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">y_hat&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">count&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n_gram&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">best_ref_substr_count&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">best_ref_len&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">max&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">[(&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">count&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n_gram&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">S&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">index&lt;/span>&lt;span class="p">]],&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="k">lambda&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">numerator&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="nb">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">candidate_substr_count&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">best_ref_substr_count&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">denominator&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">candidate_substr_count&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">numerator&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">denominator&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">brevity_penalty&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">S_hat&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">S&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">]])&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">float&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">r&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">c&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">index&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y_hat&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">enumerate&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">S_hat&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">c&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y_hat&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">best_match_ref&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">S&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">index&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="k">lambda&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">abs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y_hat&lt;/span>&lt;span class="p">)))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">r&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">best_match_ref&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="mf">1.0&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="n">c&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">r&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="n">math&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">exp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">r&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">compute_bleu_score&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">S_hat&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">S&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">List&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">]])&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">float&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">assert&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">S_hat&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">S&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># take N as a sufficiently large integer&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># N = max(len(y_hat) for y_hat in S_hat)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">N&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">sum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y_hat&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">y_hat&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">S_hat&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">bp&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">brevity_penalty&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">S_hat&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">S&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">precisions&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">modified_n_gram_precision&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">S_hat&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">S&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">N&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># bleu_score = bp * exp(p_n)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">bp&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">math&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">exp&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">sum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">math&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">log&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">precision&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">precision&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">precisions&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="n">precision&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://huggingface.co/spaces/evaluate-metric/bleu" target="_blank" rel="noopener"
>Hugging Face space&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://aclanthology.org/P02-1040.pdf" target="_blank" rel="noopener"
>Original Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://en.wikipedia.org/wiki/BLEU" target="_blank" rel="noopener"
>Wikipedia Documentation, Recommended&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>2370. Longest Ideal Subsequence</title><link>https://maosong.website/p/2370.-longest-ideal-subsequence/</link><pubDate>Thu, 25 Apr 2024 20:34:06 +0800</pubDate><guid>https://maosong.website/p/2370.-longest-ideal-subsequence/</guid><description>&lt;p>Given a string consisting of lower-case characters, find the longest subsequence such that the distance between adjacent characters in the subsequence are less than a given threshold.&lt;/p>
&lt;h1 id="intuition">&lt;a href="#intuition" class="header-anchor">&lt;/a>Intuition
&lt;/h1>&lt;p>Same as the longest increasing subsequence, we can use dynamic programming to solve this problem.&lt;/p>
&lt;h1 id="approach">&lt;a href="#approach" class="header-anchor">&lt;/a>Approach
&lt;/h1>$$ dp[i] = \max_{j=1,\dots,i-1, \mathrm{abs}(s[i - 1]-s[j - 1])\leq k}(dp[i],\ dp[j] + 1) ,\ i=1,\dots,n$$&lt;p>However, it turns out that the above solution is of complexity $O(n^2)$, which leads to &lt;em>Time Exceed Limit&lt;/em>, so we need to optimize it.&lt;/p>
&lt;p>Now, consider the property of ideal sequence, we only care about those characters that is within the range &lt;code>(s[i - 1] - k, s[i - 1] + k)&lt;/code>. So, we can use a map &lt;code>record&lt;/code>, whose key is all lowercase characters, to remember the result that is used to update &lt;code>dp[i]&lt;/code>, that is, for given index &lt;code>i&lt;/code>:&lt;/p>
$$ record[l] = \max_{j=1,\dots,i - 1, s[j] - 'a' = l}dp[j] $$$$ dp[i] = \max_{\mathrm{abs}((s[i]-'a') - l)\leq k}(dp[i],\ record[l] + 1),\ i=1,\dots,n $$&lt;p>
notice that &lt;code>len(record)=26&lt;/code>, so the complexity now reduces to $O(n)$.&lt;/p>
&lt;h1 id="complexity">&lt;a href="#complexity" class="header-anchor">&lt;/a>Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n)$$&lt;/li>
&lt;li>
$$O(n)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">longestIdealString&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">string&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">record&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">26&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">index&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="sc">&amp;#39;a&amp;#39;&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="mi">26&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">abs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">index&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">max&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">record&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">record&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">index&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">max&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">record&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">index&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">max&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">result&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dp&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>1137. N-th Tribonacci Number</title><link>https://maosong.website/p/1137.-n-th-tribonacci-number/</link><pubDate>Wed, 24 Apr 2024 18:53:26 +0800</pubDate><guid>https://maosong.website/p/1137.-n-th-tribonacci-number/</guid><description>&lt;p>Compute the $n$-th tribonacci number.&lt;/p>
&lt;h1 id="intuition">&lt;a href="#intuition" class="header-anchor">&lt;/a>Intuition
&lt;/h1>&lt;p>Same as compute the $n$-th fibonacci number, we use three numbers to remember the state.&lt;/p>
&lt;h1 id="approach">&lt;a href="#approach" class="header-anchor">&lt;/a>Approach
&lt;/h1>&lt;p>We use three numbers to represent $n-2$, $n-1$ and $n$-th tribonacci number respectively&lt;/p>
&lt;h1 id="complexity">&lt;a href="#complexity" class="header-anchor">&lt;/a>Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n)$$&lt;/li>
&lt;li>
$$O(1)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">tribonacci&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">return&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">temp&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">nums&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">nums&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">nums&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">temp&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">nums&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>310. Minimum Height Trees</title><link>https://maosong.website/p/310.-minimum-height-trees/</link><pubDate>Tue, 23 Apr 2024 21:02:07 +0800</pubDate><guid>https://maosong.website/p/310.-minimum-height-trees/</guid><description>&lt;p>Given a tree, reorganize the tree such that the height of the tree is minimized.&lt;/p>
&lt;h1 id="intuition">&lt;a href="#intuition" class="header-anchor">&lt;/a>Intuition
&lt;/h1>&lt;p>We construct the tree from bottom to top, util we find the root of the tree&lt;/p>
&lt;h1 id="approach">&lt;a href="#approach" class="header-anchor">&lt;/a>Approach
&lt;/h1>&lt;p>We use topological sort to order the nodes of the tree, then we iteratively construct the tree from bottom to top with BFS.&lt;/p>
&lt;p>We use a different stop criteria to avoid missing possible solutions.&lt;/p>
&lt;blockquote>
&lt;p>The result contains at most two possible roots, since if there are three, then the degree of one node must be lower than the other two nodes.&lt;/p>
&lt;/blockquote>
&lt;h1 id="complexity">&lt;a href="#complexity" class="header-anchor">&lt;/a>Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n)$$&lt;/li>
&lt;li>
$$O(n)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">findMinHeightTrees&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">edges&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// boundary check
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">return&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">adjs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">in_degrees&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="k">auto&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="nl">edge&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">edges&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">adjs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]].&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">adjs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]].&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">++&lt;/span>&lt;span class="n">in_degrees&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">++&lt;/span>&lt;span class="n">in_degrees&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// leaf nodes
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="n">queue&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">q&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">in_degrees&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// construct next layer
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">n&lt;/span> &lt;span class="o">-=&lt;/span> &lt;span class="n">size&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">size&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">node&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">front&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="nl">next_node&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">adjs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="o">--&lt;/span>&lt;span class="n">in_degrees&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">next_node&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">next_node&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">pop&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="o">!&lt;/span>&lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">empty&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">front&lt;/span>&lt;span class="p">());&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">pop&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>752. Open the Lock</title><link>https://maosong.website/p/752.-open-the-lock/</link><pubDate>Mon, 22 Apr 2024 19:01:23 +0800</pubDate><guid>https://maosong.website/p/752.-open-the-lock/</guid><description>&lt;p>Given a four-digit string, change one digit (plus or minus 1) at a time, find the minimum number of steps to go from the source to target without passing through invalid states.&lt;/p>
&lt;h1 id="intuition">&lt;a href="#intuition" class="header-anchor">&lt;/a>Intuition
&lt;/h1>&lt;p>We can image this as a graph path finding problem, where we need to find a path from the &lt;code>source&lt;/code> to the &lt;code>target&lt;/code> with the minimum number of steps.&lt;/p>
&lt;h1 id="approach">&lt;a href="#approach" class="header-anchor">&lt;/a>Approach
&lt;/h1>&lt;p>We use BFS to solve this problem. The graph is constructed as follows: each possible state is a node of the graph, such as &lt;code>&amp;quot;1234&amp;quot;&lt;/code>, &lt;code>&amp;quot;4567&amp;quot;&lt;/code>, each operation defines an edge between two nodes, for example, we can rotate third digit of &lt;code>&amp;quot;1234&amp;quot;&lt;/code> to obtain &lt;code>&amp;quot;1244&amp;quot;&lt;/code>, since there are two possible directions and four digits, each node has $2^4=16$ adjacent nodes. We keep track of visited nodes and add them to &lt;code>deadends&lt;/code> since there are no difference between them.&lt;/p>
&lt;h1 id="complexity">&lt;a href="#complexity" class="header-anchor">&lt;/a>Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(1)$$&lt;/li>
&lt;li>
$$O(1)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;span class="lnt">49
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">string&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">get_ajacent_nodes&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="n">string&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">string&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">string&lt;/span> &lt;span class="n">s1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">string&lt;/span> &lt;span class="n">s2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="sc">&amp;#39;1&amp;#39;&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="sc">&amp;#39;9&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">s1&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">replace&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">string&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">s2&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">replace&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">string&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="nf">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="sc">&amp;#39;9&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">s1&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">replace&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#34;0&amp;#34;&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">s2&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">replace&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#34;8&amp;#34;&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">s1&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">replace&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#34;1&amp;#34;&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">s2&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">replace&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#34;9&amp;#34;&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">s1&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">s2&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="nf">openLock&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">string&lt;/span>&lt;span class="o">&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">deadends&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">string&lt;/span> &lt;span class="n">target&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">unordered_set&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">string&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">set_deadends&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">deadends&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">begin&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">deadends&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">());&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">queue&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">string&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">q&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#34;0000&amp;#34;&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="o">!&lt;/span>&lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">empty&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">size&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">string&lt;/span> &lt;span class="n">node&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">front&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">pop&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// dead end check
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">set_deadends&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">find&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="n">set_deadends&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// target check
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">target&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// mark as visited
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="n">set_deadends&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">insert&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">const&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">string&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">adjacent_nodes&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">get_ajacent_nodes&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="n">string&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="nl">adjacent_node&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">adjacent_nodes&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">adjacent_node&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">++&lt;/span>&lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>Notes on Llama3</title><link>https://maosong.website/p/notes-on-llama3/</link><pubDate>Mon, 22 Apr 2024 16:22:19 +0800</pubDate><guid>https://maosong.website/p/notes-on-llama3/</guid><description>&lt;p>Meta released Llama3 at April 18, which is evaluated on several benchmarks and achieves the SOTA on open-sourced LLMs.&lt;/p>
&lt;h1 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h1>&lt;h2 id="instruct-model-performance">&lt;a href="#instruct-model-performance" class="header-anchor">&lt;/a>Instruct model performance
&lt;/h2>&lt;p>The performance of Llama3 8B compared with Gemma and Mistral:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Llama3 8B&lt;/th>
&lt;th>Gemma 7B - It&lt;/th>
&lt;th>Mistral &amp;amp;B Instruct&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;em>MMLU&lt;/em> (5 shot)&lt;/td>
&lt;td>&lt;strong>68.4&lt;/strong>&lt;/td>
&lt;td>53.3&lt;/td>
&lt;td>58.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;em>GPQA&lt;/em> (0 shot)&lt;/td>
&lt;td>&lt;strong>34.2&lt;/strong>&lt;/td>
&lt;td>21.4&lt;/td>
&lt;td>26.3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;em>HumanEval&lt;/em> (0 shot)&lt;/td>
&lt;td>&lt;strong>62.2&lt;/strong>&lt;/td>
&lt;td>30.5&lt;/td>
&lt;td>36.6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;em>GSM-8K&lt;/em> (8 shot, CoT)&lt;/td>
&lt;td>&lt;strong>79.6&lt;/strong>&lt;/td>
&lt;td>30.6&lt;/td>
&lt;td>39.9&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;em>MATH&lt;/em> (4 shot, CoT)&lt;/td>
&lt;td>&lt;strong>30.0&lt;/strong>&lt;/td>
&lt;td>12.2&lt;/td>
&lt;td>11.0&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>performance of Llama3 70B compared with Gemini Pro 1.5 and Claude Sonnet:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Llama3 70B&lt;/th>
&lt;th>Gemini Pro 1.5 (Published)&lt;/th>
&lt;th>Claude 3 Sonnet (Published)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;em>MMLU&lt;/em> (5 shot)&lt;/td>
&lt;td>&lt;strong>82.0&lt;/strong>&lt;/td>
&lt;td>81.9&lt;/td>
&lt;td>79.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;em>GPQA&lt;/em> (0 shot)&lt;/td>
&lt;td>39.5&lt;/td>
&lt;td>&lt;strong>41.5&lt;/strong> (CoT)&lt;/td>
&lt;td>38.5 (CoT)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;em>HumanEval&lt;/em> (0 shot)&lt;/td>
&lt;td>&lt;strong>81.7&lt;/strong>&lt;/td>
&lt;td>71.9&lt;/td>
&lt;td>73.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;em>GSM-8K&lt;/em> (8 shot, CoT)&lt;/td>
&lt;td>&lt;strong>93.0&lt;/strong>&lt;/td>
&lt;td>91.7 (11 shot)&lt;/td>
&lt;td>92.3 (0 shot)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;em>MATH&lt;/em> (4 shot, CoT)&lt;/td>
&lt;td>50.4&lt;/td>
&lt;td>&lt;strong>58.5&lt;/strong> (Minerva prompt)&lt;/td>
&lt;td>40.5&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="pre-trained-model-performance">&lt;a href="#pre-trained-model-performance" class="header-anchor">&lt;/a>Pre-trained model performance
&lt;/h2>&lt;p>The performance of Llama3 8B compared with Gemma and Mistral:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Llama3 8B&lt;/th>
&lt;th>Gemma 7B (Published)&lt;/th>
&lt;th>Gemma 7B (Measured)&lt;/th>
&lt;th>Mistral 7B (Published)&lt;/th>
&lt;th>Mistral 7B (Measured)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;em>MMLU&lt;/em> (5 shot)&lt;/td>
&lt;td>&lt;strong>66.6&lt;/strong>&lt;/td>
&lt;td>64.3&lt;/td>
&lt;td>64.4&lt;/td>
&lt;td>62.5&lt;/td>
&lt;td>63.9&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;em>AGIEval English&lt;/em> (3-5 shot)&lt;/td>
&lt;td>&lt;strong>45.9&lt;/strong>&lt;/td>
&lt;td>41.7&lt;/td>
&lt;td>44.9&lt;/td>
&lt;td>-&lt;/td>
&lt;td>44.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;em>BIG-Bench Hard&lt;/em> (3 shot, CoT)&lt;/td>
&lt;td>&lt;strong>61.1&lt;/strong>&lt;/td>
&lt;td>55.1&lt;/td>
&lt;td>59.0&lt;/td>
&lt;td>-&lt;/td>
&lt;td>56.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;em>ARC-Challenge&lt;/em> (25 shot)&lt;/td>
&lt;td>78.6&lt;/td>
&lt;td>53.2(0 shot)&lt;/td>
&lt;td>&lt;strong>79.1&lt;/strong>&lt;/td>
&lt;td>78.1&lt;/td>
&lt;td>78.7&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;em>DROP&lt;/em> (3 shot, F1)&lt;/td>
&lt;td>&lt;strong>58.4&lt;/strong>&lt;/td>
&lt;td>-&lt;/td>
&lt;td>56.3&lt;/td>
&lt;td>-&lt;/td>
&lt;td>54.4&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>performance of Llama3 70B compared with Gemini Pro 1.5 and Claude Sonnet:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Llama3 70B&lt;/th>
&lt;th>Gemini Pro 1.0 (Published)&lt;/th>
&lt;th>Mixtral 8 $\times$ 22B (Measured)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;em>MMLU&lt;/em> (5-shot)&lt;/td>
&lt;td>&lt;strong>79.5&lt;/strong>&lt;/td>
&lt;td>71.8&lt;/td>
&lt;td>77.7&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;em>AGIEval English&lt;/em> (3-5 shot)&lt;/td>
&lt;td>&lt;strong>63.0&lt;/strong>&lt;/td>
&lt;td>-&lt;/td>
&lt;td>61.2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;em>BIG-Bench Hard&lt;/em> (3 shot, CoT)&lt;/td>
&lt;td>&lt;strong>81.3&lt;/strong>&lt;/td>
&lt;td>75.0&lt;/td>
&lt;td>79.2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;em>ARC-Challenge&lt;/em> (25 shot)&lt;/td>
&lt;td>&lt;strong>93.0&lt;/strong>&lt;/td>
&lt;td>-&lt;/td>
&lt;td>90.7&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;em>DROP&lt;/em> (3 shot, F1)&lt;/td>
&lt;td>&lt;strong>79.7&lt;/strong>&lt;/td>
&lt;td>74.1 (variable shot)&lt;/td>
&lt;td>77.6&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h1 id="model-architecture">&lt;a href="#model-architecture" class="header-anchor">&lt;/a>Model Architecture
&lt;/h1>&lt;p>Several improvements are made on Llama3 compared to llama2:&lt;/p>
&lt;ol>
&lt;li>Llama3 uses a tokenizer with a vocabulary of 128K tokens.&lt;/li>
&lt;li>Llama3 adopts &lt;a class="link" href="https://maosong.website/p/notes-on-gqa/" target="_blank" rel="noopener"
>grouped query attention (GQA)&lt;/a> across both the 8B and 70B sizes.&lt;/li>
&lt;li>Llama3 uses to context window of size 8192 tokens&lt;/li>
&lt;/ol>
&lt;h1 id="traning">&lt;a href="#traning" class="header-anchor">&lt;/a>Traning
&lt;/h1>&lt;p>Llama3 uses 15T tokens for pre-training. Compares to Llama2, it is seven times larger and includes four times more code.&lt;/p>
&lt;p>5% data of the training dataset are non-English to support multi-lingual use cases.&lt;/p>
&lt;p>Data processing includes:&lt;/p>
&lt;ol>
&lt;li>Heuristic filters&lt;/li>
&lt;li>NSFW filters&lt;/li>
&lt;li>Semantic deduplication approaches&lt;/li>
&lt;li>Text classifiers to predict data quality. Llama2 is used to generate training data for the text classifiers.&lt;/li>
&lt;/ol>
&lt;p>Data mixing strategy is explored to improve the performance of Llama3.&lt;/p>
&lt;h1 id="scaling-up-pretraining">&lt;a href="#scaling-up-pretraining" class="header-anchor">&lt;/a>Scaling up pretraining
&lt;/h1>&lt;p>Llama3 developed a series of scaling laws for downstream benchmark evaluations.&lt;/p>
&lt;p>Scaling laws help:&lt;/p>
&lt;ol>
&lt;li>Select an optimal data mix and to make informed decisions on how to best use training compute.&lt;/li>
&lt;li>Scaling laws allow Llama3 to predict the performance of the largest models on key tasks without training the models.&lt;/li>
&lt;/ol>
&lt;p>The authors finds our that the performance of the model continues to improve log-linearly as the training tokens increase. It is seen that Larger models can match the performance of these smaller models with less training compute, but smaller models are generally preferred because they are much more efficient during inference.&lt;/p>
&lt;p>The authors combine three types of parallelization:&lt;/p>
&lt;ol>
&lt;li>Data parallelization&lt;/li>
&lt;li>Model parallelization&lt;/li>
&lt;li>Pipeline parallelization&lt;/li>
&lt;/ol>
&lt;h1 id="instruction-fine-tuning">&lt;a href="#instruction-fine-tuning" class="header-anchor">&lt;/a>Instruction fine-tuning
&lt;/h1>&lt;p>The fine-tuning of Llama3 contains:&lt;/p>
&lt;ol>
&lt;li>Supervised fine-tuning&lt;/li>
&lt;li>Rejection sampling&lt;/li>
&lt;li>Proximal Policy Optimization&lt;/li>
&lt;li>Direct Preference Optimization&lt;/li>
&lt;/ol>
&lt;p>Learning from perference rankings via PPO and &lt;a class="link" href="https://maosong.website/p/notes-on-dpo/" target="_blank" rel="noopener"
>DPO&lt;/a> also greatly improved the performance of LLma3 on reasoning and coding tasks. Since perference ranking helps the model to select answer when it is in a dilemma.&lt;/p>
&lt;h1 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://ai.meta.com/blog/meta-llama-3/" target="_blank" rel="noopener"
>Llam3 blog&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/meta-llama/llama3/blob/main/eval_details.md" target="_blank" rel="noopener"
>Evaluation details&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md" target="_blank" rel="noopener"
>Model Card&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>1971. Find if Path Exists in Graph</title><link>https://maosong.website/p/1971.-find-if-path-exists-in-graph/</link><pubDate>Sun, 21 Apr 2024 10:51:46 +0800</pubDate><guid>https://maosong.website/p/1971.-find-if-path-exists-in-graph/</guid><description>&lt;p>Find a available path from a given source to a given destination in a given graph.&lt;/p>
&lt;h1 id="intuition">&lt;a href="#intuition" class="header-anchor">&lt;/a>Intuition
&lt;/h1>&lt;p>Use DFS to find all reachable nodes and check if the destination lie within those nodes.&lt;/p>
&lt;h1 id="approach">&lt;a href="#approach" class="header-anchor">&lt;/a>Approach
&lt;/h1>&lt;p>We first transform the adjacency matrix to adjacency list to make BFS easier, then we use
a queue to maintain the reachable nodes, to prevent from cycling, we also use a set to keep track of visited nodes.
If at any point, we reach the &lt;code>destination&lt;/code>, we return directly.&lt;/p>
&lt;h1 id="complexity">&lt;a href="#complexity" class="header-anchor">&lt;/a>Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n)$$&lt;/li>
&lt;li>
$$O(n)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">bool&lt;/span> &lt;span class="n">validPath&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">edges&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">source&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">destination&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">unordered_map&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">adjs&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="k">auto&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="nl">edge&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">edges&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">adjs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]].&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">adjs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]].&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">edge&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">unordered_set&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">visited&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">queue&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">q&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">source&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="o">!&lt;/span>&lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">empty&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">node&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">front&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">pop&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">destination&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">return&lt;/span> &lt;span class="nb">true&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="nl">next_node&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">adjs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">visited&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">find&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">next_node&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="n">visited&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">next_node&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">visited&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">insert&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">next_node&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="nb">false&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>1992. Find All Groups of Farmland</title><link>https://maosong.website/p/1992.-find-all-groups-of-farmland/</link><pubDate>Sat, 20 Apr 2024 15:31:49 +0800</pubDate><guid>https://maosong.website/p/1992.-find-all-groups-of-farmland/</guid><description>&lt;p>Given a matrix where its grid component representing islands and forests, count the number of farmlands.&lt;/p>
&lt;h1 id="intuition">&lt;a href="#intuition" class="header-anchor">&lt;/a>Intuition
&lt;/h1>&lt;p>Start from the top left coordinate of the farmland, Use DFS to find the bottom right coordinate of the farmland.&lt;/p>
&lt;h1 id="approach">&lt;a href="#approach" class="header-anchor">&lt;/a>Approach
&lt;/h1>&lt;p>We define the possible directions as &lt;code>go_right&lt;/code> and &lt;code>go_down&lt;/code> respectively, we iterate all grids, if it is an grid of the farmland,
then we use DFS to find the bottom right coordinate of the current farmland, and then we mark the farmland as visited and store the coordinates.&lt;/p>
&lt;h1 id="complexity">&lt;a href="#complexity" class="header-anchor">&lt;/a>Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(mn)$$&lt;/li>
&lt;li>
$$O(mn)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">dirs&lt;/span>&lt;span class="p">{{&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">},&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">}};&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">void&lt;/span> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">land&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">bottom_right&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">land&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">bool&lt;/span> &lt;span class="n">reach_end&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">true&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="k">auto&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="nl">dir&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">dirs&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">row&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">col&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">row&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="n">row&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">land&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="mi">0&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">col&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="n">col&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">land&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">].&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">land&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">row&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">col&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">reach_end&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">false&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">land&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">row&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">col&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bottom_right&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">reach_end&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="n">bottom_right&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">empty&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">bottom_right&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">findFarmland&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">land&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">land&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">land&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">].&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">land&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">bottom_right&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">land&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bottom_right&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bottom_right&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">bottom_right&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]});&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>200. Number of Islands</title><link>https://maosong.website/p/200.-number-of-islands/</link><pubDate>Fri, 19 Apr 2024 20:25:03 +0800</pubDate><guid>https://maosong.website/p/200.-number-of-islands/</guid><description>&lt;p>Given a matrix where its grid component representing islands and waters, count the number of Islands.&lt;/p>
&lt;h1 id="intuition">&lt;a href="#intuition" class="header-anchor">&lt;/a>Intuition
&lt;/h1>&lt;p>Use DFS to find all connected components of the island, then count the number of islands.&lt;/p>
&lt;h1 id="approach">&lt;a href="#approach" class="header-anchor">&lt;/a>Approach
&lt;/h1>&lt;p>We iterate all grid component, when we meet the land, we use DFS to find all connected components of the island, and mark those connected components as visited.&lt;/p>
&lt;h1 id="complexity">&lt;a href="#complexity" class="header-anchor">&lt;/a>Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(mn)$$&lt;/li>
&lt;li>
$$O(1)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">dirs&lt;/span>&lt;span class="p">{{&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">},&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">},&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">},&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">}};&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">void&lt;/span> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">char&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="sc">&amp;#39;0&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">return&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="sc">&amp;#39;0&amp;#39;&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="k">auto&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="nl">dir&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">dirs&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="mi">0&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">].&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">grid&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="nf">numIslands&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">char&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">count&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">].&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="sc">&amp;#39;0&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">grid&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">++&lt;/span>&lt;span class="n">count&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">count&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>463. Island Perimeter</title><link>https://maosong.website/p/463.-island-perimeter/</link><pubDate>Thu, 18 Apr 2024 20:49:51 +0800</pubDate><guid>https://maosong.website/p/463.-island-perimeter/</guid><description>&lt;p>Given a matrix, find the perimeter of the connected grid land&lt;/p>
&lt;h1 id="intuition">&lt;a href="#intuition" class="header-anchor">&lt;/a>Intuition
&lt;/h1>&lt;p>Use DFS to find the island, then compute the perimeter.&lt;/p>
&lt;h1 id="approach">&lt;a href="#approach" class="header-anchor">&lt;/a>Approach
&lt;/h1>&lt;p>We use &lt;code>result&lt;/code> to store the result and find all connected components of the island with DFS, to compute the perimeter,
to update &lt;code>result&lt;/code>, we need to compute how many components that are connected the current component.&lt;/p>
&lt;p>Now, note that to prevent infinite recursion, we set &lt;code>grid[i][j] = -1&lt;/code> to mark it visited, then for each component, it may be in three states:&lt;/p>
&lt;ol>
&lt;li>&lt;code>grid[i][j] = 0&lt;/code>, it is water&lt;/li>
&lt;li>&lt;code>grid[i][j] = 1&lt;/code>, it is a component of the island and being unvisited&lt;/li>
&lt;li>&lt;code>grid[i][j] = -1&lt;/code>, it is a component of the island and has been visited.&lt;/li>
&lt;/ol>
&lt;h1 id="complexity">&lt;a href="#complexity" class="header-anchor">&lt;/a>Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(mn)$$&lt;/li>
&lt;li>
$$O(1)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">dirs&lt;/span>&lt;span class="p">{{&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">},&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">},&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">},&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">}};&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">void&lt;/span> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">result&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">return&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">num_edges&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">4&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="k">const&lt;/span> &lt;span class="k">auto&lt;/span>&lt;span class="o">&amp;amp;&lt;/span>&lt;span class="nl">dir&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">dirs&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="mi">0&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">].&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]][&lt;/span>&lt;span class="n">j&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">--&lt;/span>&lt;span class="n">num_edges&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">grid&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">dir&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">num_edges&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="nf">islandPerimeter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="kt">int&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&amp;amp;&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">].&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">grid&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">continue&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">grid&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>Practical advice for analysis of large, complex data sets</title><link>https://maosong.website/p/practical-advice-for-analysis-of-large-complex-data-sets/</link><pubDate>Wed, 17 Apr 2024 22:40:11 +0800</pubDate><guid>https://maosong.website/p/practical-advice-for-analysis-of-large-complex-data-sets/</guid><description>&lt;h1 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h1>&lt;p>These advises are given by Patrick Riley in 2016, though it has been years util now, I think some of them are still useful.&lt;/p>
&lt;p>The advice is organized into three general areas:&lt;/p>
&lt;ul>
&lt;li>Technical: Ideas and techniques for how to manipulate and examine your data.&lt;/li>
&lt;li>Process: Recommendation on how you approach your data, what questions to ask, and what things to check.&lt;/li>
&lt;li>Social: How to work with others and communicate about your data and insights.&lt;/li>
&lt;/ul>
&lt;h1 id="technical">&lt;a href="#technical" class="header-anchor">&lt;/a>Technical
&lt;/h1>&lt;h2 id="look-at-your-distribution">&lt;a href="#look-at-your-distribution" class="header-anchor">&lt;/a>Look at your distribution
&lt;/h2>&lt;p>Besides the typically used summary metrics, we should looking at a much richer representation of the distribution, such as histograms, CDFs, Q-Q plots, etc. This allows us to see some interesting features.&lt;/p>
&lt;h2 id="consider-the-outliers">&lt;a href="#consider-the-outliers" class="header-anchor">&lt;/a>Consider the outliers
&lt;/h2>&lt;p>We should look at the outliers in our data. It&amp;rsquo;s fine to exclude them from our data or to lump them together into an unusual category, but we should make sure we know why.&lt;/p>
&lt;h2 id="report-noise-confidence">&lt;a href="#report-noise-confidence" class="header-anchor">&lt;/a>Report noise/ confidence
&lt;/h2>&lt;p>Every estimator that you produce should have a notion of your confidence in this estimate attached to it.&lt;/p>
&lt;h2 id="look-at-examples">&lt;a href="#look-at-examples" class="header-anchor">&lt;/a>Look at examples
&lt;/h2>&lt;p>Anytime you are producing new analysis code, you need to look at examples of the underlying data and how your code is interpreting those examples.&lt;/p>
&lt;h2 id="slice-your-data">&lt;a href="#slice-your-data" class="header-anchor">&lt;/a>Slice your data
&lt;/h2>&lt;p>slicing help us obtain underlying features of the data subgroups easier. However, when we use slicing, we need to care about the mix shift.&lt;/p>
&lt;h2 id="consider-practical-significance">&lt;a href="#consider-practical-significance" class="header-anchor">&lt;/a>Consider practical significance
&lt;/h2>&lt;p>Don&amp;rsquo;t be blind by statistics, watch out those may have an impact on deploying or ethical problems.&lt;/p>
&lt;h2 id="check-for-consistency-over-time">&lt;a href="#check-for-consistency-over-time" class="header-anchor">&lt;/a>Check for consistency over time
&lt;/h2>&lt;p>One particular slicing you should almost always employ is to slice by units of time.
This is because many disturbances to underlying data happen as our systems evolve over time.&lt;/p>
&lt;h1 id="process">&lt;a href="#process" class="header-anchor">&lt;/a>Process
&lt;/h1>&lt;h2 id="separate-validation-description-and-evaluation">&lt;a href="#separate-validation-description-and-evaluation" class="header-anchor">&lt;/a>Separate Validation, description, and evaluation
&lt;/h2>&lt;ul>
&lt;li>Description should be things that everyone can agree on from the data.&lt;/li>
&lt;li>Evaluation is likely to have much more debate because you imbuing meaning and value to the data.&lt;/li>
&lt;/ul>
&lt;h2 id="confirm-exptdata-collection-setup">&lt;a href="#confirm-exptdata-collection-setup" class="header-anchor">&lt;/a>Confirm expt/data collection setup
&lt;/h2>&lt;p>Before looking at any data, make sure you understand the experiment and data collection setup&lt;/p>
&lt;h2 id="check-vital-signs">&lt;a href="#check-vital-signs" class="header-anchor">&lt;/a>Check vital signs
&lt;/h2>&lt;p>Before actually answering the question you are interested in you need to check for a lot of other things that may not be related to what you are interested in but may be useful in later analysis or indicate problems in the data&lt;/p>
&lt;h2 id="standard-first-custom-second">&lt;a href="#standard-first-custom-second" class="header-anchor">&lt;/a>Standard first, custom second
&lt;/h2>&lt;p>When we use metric, we should always look at standard metrics first, even if we expect them to change.&lt;/p>
&lt;h2 id="measure-twice-or-more">&lt;a href="#measure-twice-or-more" class="header-anchor">&lt;/a>Measure twice, or more
&lt;/h2>&lt;p>If you are trying to capture a new phenomenon, try to measure the same underlying thing in multiple ways.
Then, check to see if these multiple measurements are consistent&lt;/p>
&lt;h2 id="check-for-reproducibility">&lt;a href="#check-for-reproducibility" class="header-anchor">&lt;/a>Check for reproducibility
&lt;/h2>&lt;p>Both slicing and consistency over time are particular examples of checking for reproducibility.
If a phenomenon is important and meaningful, you should see it across different user populations and time.&lt;/p>
&lt;h2 id="check-for-consistency-with-past-measurements">&lt;a href="#check-for-consistency-with-past-measurements" class="header-anchor">&lt;/a>Check for consistency with past measurements
&lt;/h2>&lt;p>You should compare your metrics to metrics reported in the past, even if these measurements are on different user populations.&lt;/p>
&lt;p>New metrics should be applied to old data/features first&lt;/p>
&lt;h2 id="make-hypothesis-and-look-for-evidence">&lt;a href="#make-hypothesis-and-look-for-evidence" class="header-anchor">&lt;/a>Make hypothesis and look for evidence
&lt;/h2>&lt;p>Typically, exploratory data analysis for a complex problem is iterative. You will discover anomalies, trends, or other features of the data. Naturally, you will make hypotheses to explain this data. It’s essential that you don’t just make a hypothesis and proclaim it to be true. Look for evidence (inside or outside the data) to confirm/deny this theory.&lt;/p>
&lt;h2 id="exploratory-analysis-benefits-from-end-to-end-iteration">&lt;a href="#exploratory-analysis-benefits-from-end-to-end-iteration" class="header-anchor">&lt;/a>Exploratory analysis benefits from end to end iteration
&lt;/h2>&lt;p>When doing exploratory analysis, you should strive to get as many iterations of the whole analysis as possible.&lt;/p>
&lt;h1 id="social">&lt;a href="#social" class="header-anchor">&lt;/a>Social
&lt;/h1>&lt;h2 id="data-analysis-starts-with-questions-not-data-or-a-technique">&lt;a href="#data-analysis-starts-with-questions-not-data-or-a-technique" class="header-anchor">&lt;/a>Data analysis starts with questions, not data or a technique
&lt;/h2>&lt;p>Ask question first and use tools to answer the questions.&lt;/p>
&lt;h2 id="acknowledge-and-count-your-filtering">&lt;a href="#acknowledge-and-count-your-filtering" class="header-anchor">&lt;/a>Acknowledge and count your filtering
&lt;/h2>&lt;ul>
&lt;li>Acknowledge and clearly specify what filtering you are doing&lt;/li>
&lt;li>Count how much is being filtered at each of your steps
the best way to do the latter is to actually compute all your metrics even for the population you are excluding&lt;/li>
&lt;/ul>
&lt;h2 id="ratios-should-have-clear-numerator-and-denominators">&lt;a href="#ratios-should-have-clear-numerator-and-denominators" class="header-anchor">&lt;/a>Ratios should have clear numerator and denominators
&lt;/h2>&lt;p>When you communicate results containing ratios, you must be clear about the numerator and denominator.&lt;/p>
&lt;h2 id="educate-your-consumers">&lt;a href="#educate-your-consumers" class="header-anchor">&lt;/a>Educate your consumers
&lt;/h2>&lt;p>You are responsible for providing the context and a full picture of the data and not just the number a consumer asked for.&lt;/p>
&lt;h2 id="be-both-skeptic-and-champion">&lt;a href="#be-both-skeptic-and-champion" class="header-anchor">&lt;/a>Be both skeptic and champion
&lt;/h2>&lt;p>As you work with data, you must be both the champion of the insights you are gaining as well as a skeptic.&lt;/p>
&lt;h2 id="share-with-peers-first-external-consumers-second">&lt;a href="#share-with-peers-first-external-consumers-second" class="header-anchor">&lt;/a>Share with peers first, external consumers second
&lt;/h2>&lt;p>A skilled peer reviewer can provide qualitatively different feedback and sanity-checking than the consumers of your data can, especially since consumers generally have an outcome they want to get&lt;/p>
&lt;h1 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://www.unofficialgoogledatascience.com/2016/10/practical-advice-for-analysis-of-large.html" target="_blank" rel="noopener"
>Practical advice for analysis of large, complex data sets&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>988. Smallest String Starting From Leaf</title><link>https://maosong.website/p/988.-smallest-string-starting-from-leaf/</link><pubDate>Wed, 17 Apr 2024 21:15:03 +0800</pubDate><guid>https://maosong.website/p/988.-smallest-string-starting-from-leaf/</guid><description>&lt;p>Given a binary tree with value on each node representing a lowercase letter, find the lexicographically smallest string starting from the leaf to root node&lt;/p>
&lt;h1 id="intuition">&lt;a href="#intuition" class="header-anchor">&lt;/a>Intuition
&lt;/h1>&lt;p>We use DFS to find all strings from the root node to the leaf nodes, then reverse the string and compare it withe the largest string.&lt;/p>
&lt;h1 id="approach">&lt;a href="#approach" class="header-anchor">&lt;/a>Approach
&lt;/h1>&lt;p>We use &lt;code>current&lt;/code> to represent the string starting from the root node to the current node and we use &lt;code>result&lt;/code> to store the currently best result. When we reach the leaf node, we compare the &lt;code>current&lt;/code> with &lt;code>result&lt;/code> and update &lt;code>result&lt;/code>.&lt;/p>
&lt;h1 id="complexity">&lt;a href="#complexity" class="header-anchor">&lt;/a>Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n)$$&lt;/li>
&lt;li>
$$O(\log n)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * Definition for a binary tree node.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * struct TreeNode {
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * int val;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode *left;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode *right;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode() : val(0), left(nullptr), right(nullptr) {}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode(int x) : val(x), left(nullptr), right(nullptr) {}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right) {}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * };
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">void&lt;/span> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">TreeNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">root&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">string&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">current&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">string&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">result&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">current&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push_back&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sc">&amp;#39;a&amp;#39;&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">root&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">val&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// leaf node
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="o">!&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">left&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="o">!&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">right&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// update result
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="n">reverse&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">current&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">begin&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">current&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">());&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">current&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">current&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">reverse&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">current&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">begin&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">current&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">());&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">left&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">left&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">current&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">right&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">right&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">current&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">current&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">pop_back&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">string&lt;/span> &lt;span class="nf">smallestFromLeaf&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">TreeNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">root&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">string&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">8501&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="sc">&amp;#39;z&amp;#39;&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">string&lt;/span> &lt;span class="n">current&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">current&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>129. Sum Root to Leaf Numbers</title><link>https://maosong.website/p/129.-sum-root-to-leaf-numbers/</link><pubDate>Sun, 14 Apr 2024 21:06:34 +0800</pubDate><guid>https://maosong.website/p/129.-sum-root-to-leaf-numbers/</guid><description>&lt;p>concatenate the digit of path from the root to the leaf, and sum over the concatenated numbers.&lt;/p>
&lt;h1 id="intuition">&lt;a href="#intuition" class="header-anchor">&lt;/a>Intuition
&lt;/h1>&lt;p>Use DFS to find all the paths, use a &lt;code>num&lt;/code> variable to store the number concatenated, then sum over &lt;code>num&lt;/code>.&lt;/p>
&lt;h1 id="approach">&lt;a href="#approach" class="header-anchor">&lt;/a>Approach
&lt;/h1>&lt;p>We use &lt;code>num&lt;/code> to represent the number from the root to the current node, if the current node is a leaf node,
we add &lt;code>num&lt;/code> to &lt;code>sum&lt;/code>. Finally, we return &lt;code>sum&lt;/code> as the result.&lt;/p>
&lt;h1 id="complexity">&lt;a href="#complexity" class="header-anchor">&lt;/a>Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n)$$&lt;/li>
&lt;li>
$$O(1)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * Definition for a binary tree node.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * struct TreeNode {
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * int val;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode *left;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode *right;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode() : val(0), left(nullptr), right(nullptr) {}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode(int x) : val(x), left(nullptr), right(nullptr) {}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right) {}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * };
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">void&lt;/span> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">TreeNode&lt;/span> &lt;span class="o">*&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">sum&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="o">&amp;amp;&lt;/span>&lt;span class="n">num&lt;/span>&lt;span class="p">){&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="o">!&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">num&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">num&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">10&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">root&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">val&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="o">!&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">left&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="o">!&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">right&lt;/span>&lt;span class="p">){&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sum&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">num&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>&lt;span class="k">else&lt;/span>&lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">left&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sum&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">right&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sum&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">num&lt;/span> &lt;span class="o">/=&lt;/span> &lt;span class="mi">10&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="nf">sumNumbers&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">TreeNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">root&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">sum&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">num&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dfs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sum&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">sum&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>404. Sum of Left Leaves</title><link>https://maosong.website/p/404.-sum-of-left-leaves/</link><pubDate>Sun, 14 Apr 2024 21:06:34 +0800</pubDate><guid>https://maosong.website/p/404.-sum-of-left-leaves/</guid><description>&lt;p>Given the &lt;code>root&lt;/code> of a binary tree, return the sum of all left leaves.&lt;/p>
&lt;h2 id="intuition">&lt;a href="#intuition" class="header-anchor">&lt;/a>Intuition
&lt;/h2>&lt;p>Use DFS to iterate over all nodes, if it is a left leaf, sum it to the result.&lt;/p>
&lt;h2 id="approach">&lt;a href="#approach" class="header-anchor">&lt;/a>Approach
&lt;/h2>&lt;p>For every node, we care about one thing: whether its left child is a leaf node or not. If it is, then we add it.&lt;/p>
&lt;h2 id="complexity">&lt;a href="#complexity" class="header-anchor">&lt;/a>Complexity
&lt;/h2>&lt;ul>
&lt;li>
$$ O(n) $$&lt;/li>
&lt;li>
$$ O(1) $$&lt;/li>
&lt;/ul>
&lt;h2 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h2>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * Definition for a binary tree node.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * struct TreeNode {
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * int val;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode *left;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode *right;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode() : val(0), left(nullptr), right(nullptr) {}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode(int x) : val(x), left(nullptr), right(nullptr) {}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right) {}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * };
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">bool&lt;/span> &lt;span class="n">is_leaf_node&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">TreeNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="o">!&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">return&lt;/span> &lt;span class="nb">false&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="o">!&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">left&lt;/span> &lt;span class="o">||&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">right&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="nf">sumOfLeftLeaves&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">TreeNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">root&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="o">!&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">return&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">is_leaf_node&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">left&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">root&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">left&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">val&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">sumOfLeftLeaves&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">right&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">sumOfLeftLeaves&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">left&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>405. Convert a Number to Hexadecimal</title><link>https://maosong.website/p/405.-convert-a-number-to-hexadecimal/</link><pubDate>Sun, 14 Apr 2024 21:06:34 +0800</pubDate><guid>https://maosong.website/p/405.-convert-a-number-to-hexadecimal/</guid><description>&lt;h1 id="intuition">&lt;a href="#intuition" class="header-anchor">&lt;/a>Intuition
&lt;/h1>&lt;p>Just use the transformation algorithm from decimal to hexadecimal&lt;/p>
&lt;h1 id="approach">&lt;a href="#approach" class="header-anchor">&lt;/a>Approach
&lt;/h1>&lt;p>Simulation by doing the following:&lt;/p>
&lt;ol>
&lt;li>compute &lt;code>remain = num % 16&lt;/code>, add &lt;code>remain&lt;/code> to the result (&lt;code>push_back&lt;/code>)&lt;/li>
&lt;li>update &lt;code>num = (num - remainder) / 16&lt;/code>&lt;/li>
&lt;li>repeat step 1 and step 2 until &lt;code>num&lt;/code> is 0&lt;/li>
&lt;/ol>
&lt;p>Notice that when &lt;code>num &amp;lt; 0&lt;/code>, we need use its complement.&lt;/p>
&lt;h1 id="complexity">&lt;a href="#complexity" class="header-anchor">&lt;/a>Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(\log n)$$&lt;/li>
&lt;li>
$$ O(1) $$&lt;/li>
&lt;/ul>
&lt;h1 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">string&lt;/span> &lt;span class="n">toHex&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">long&lt;/span> &lt;span class="n">num&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">num&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">return&lt;/span> &lt;span class="s">&amp;#34;0&amp;#34;&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">num&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="n">num&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">INT_MAX&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">num&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">INT_MAX&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">string&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">num&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">divide&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">num&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="mi">16&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">num&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">num&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">divide&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="mi">16&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">divide&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="mi">10&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="n">result&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="sc">&amp;#39;0&amp;#39;&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">divide&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span> &lt;span class="n">result&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="sc">&amp;#39;a&amp;#39;&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">divide&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">10&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">reverse&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">result&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">begin&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">end&lt;/span>&lt;span class="p">());&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>623. Add One Row to Tree</title><link>https://maosong.website/p/623.-add-one-row-to-tree/</link><pubDate>Sun, 14 Apr 2024 21:06:34 +0800</pubDate><guid>https://maosong.website/p/623.-add-one-row-to-tree/</guid><description>&lt;h1 id="intuition">&lt;a href="#intuition" class="header-anchor">&lt;/a>Intuition
&lt;/h1>&lt;p>Just list all nodes with height &lt;code>height - 1&lt;/code> and insert a new layer with the given rules.&lt;/p>
&lt;h1 id="approach">&lt;a href="#approach" class="header-anchor">&lt;/a>Approach
&lt;/h1>&lt;p>Use a &lt;code>queue&lt;/code> to store all nodes with the same height, and use BFS to update the nodes,
once we reach the height &lt;code>height - 1&lt;/code>, we add a new layer with the given &lt;code>val&lt;/code> for each &lt;code>node&lt;/code>:&lt;/p>
&lt;ol>
&lt;li>Create a new left child with the given &lt;code>(val, node-&amp;gt;left, nullptr)&lt;/code>&lt;/li>
&lt;li>Create a new right child with the given &lt;code>(val, nullptr, node-&amp;gt;right)&lt;/code>&lt;/li>
&lt;/ol>
&lt;h1 id="complexity">&lt;a href="#complexity" class="header-anchor">&lt;/a>Complexity
&lt;/h1>&lt;ul>
&lt;li>
$$O(n)$$&lt;/li>
&lt;li>
$$O(n)$$&lt;/li>
&lt;/ul>
&lt;h1 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h1>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-c++" data-lang="c++">&lt;span class="line">&lt;span class="cl">&lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * Definition for a binary tree node.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * struct TreeNode {
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * int val;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode *left;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode *right;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode() : val(0), left(nullptr), right(nullptr) {}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode(int x) : val(x), left(nullptr), right(nullptr) {}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right) {}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * };
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Solution&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">public&lt;/span>&lt;span class="o">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">TreeNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">addOneRow&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">TreeNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">root&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">val&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">depth&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">depth&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">TreeNode&lt;/span>&lt;span class="o">*&lt;/span> &lt;span class="n">new_root&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">TreeNode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">val&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">root&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="k">nullptr&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">new_root&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">queue&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">TreeNode&lt;/span>&lt;span class="o">*&amp;gt;&lt;/span> &lt;span class="n">q&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">root&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">--&lt;/span>&lt;span class="n">depth&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="o">--&lt;/span>&lt;span class="n">depth&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">int&lt;/span> &lt;span class="n">size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">size&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="o">++&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">auto&lt;/span> &lt;span class="n">node&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">front&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">left&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">left&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">right&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">push&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">node&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">right&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">pop&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="o">!&lt;/span>&lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">empty&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">auto&lt;/span> &lt;span class="n">node&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">front&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">node&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">left&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">TreeNode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">val&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">left&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="k">nullptr&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">node&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">right&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">TreeNode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">val&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="k">nullptr&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">node&lt;/span>&lt;span class="o">-&amp;gt;&lt;/span>&lt;span class="n">right&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">pop&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">root&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>Notes on RAG</title><link>https://maosong.website/p/notes-on-rag/</link><pubDate>Sun, 14 Apr 2024 12:38:04 +0800</pubDate><guid>https://maosong.website/p/notes-on-rag/</guid><description>&lt;img src="https://maosong.website/agent_performance.png" alt="Featured image of post Notes on RAG" />&lt;h1 id="problems-of-llm">&lt;a href="#problems-of-llm" class="header-anchor">&lt;/a>Problems of LLM
&lt;/h1>&lt;ul>
&lt;li>Out of date knowledge: the model cannot gain knowledge after training&lt;/li>
&lt;li>Humiliation: the model may generate nonsense output&lt;/li>
&lt;li>Specific domain: the generalized model is difficult to adapt to specific domain&lt;/li>
&lt;li>Enthetic problems: the model may encounter&lt;/li>
&lt;/ul>
&lt;h1 id="fine-tuning">&lt;a href="#fine-tuning" class="header-anchor">&lt;/a>Fine-tuning
&lt;/h1>&lt;p>Fine-tuning is used to improve performance of foundation model on specific tasks with the help with some supervised data&lt;/p>
&lt;p>Fine-tuning methods can be classified into:&lt;/p>
&lt;ol>
&lt;li>Based on range of updated parameters:
&lt;ul>
&lt;li>Full Model fine-tuning: update the parameters of the whole model&lt;/li>
&lt;li>Partial fine-tuning: freeze the top layer; freeze the bottom layer&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Based on special technology:
&lt;ul>
&lt;li>Adapter tuning&lt;/li>
&lt;li>LoRA&lt;/li>
&lt;li>Continual Learning fine-tuning&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Based on input:
&lt;ul>
&lt;li>Instruction tuning&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Based on objective
&lt;ul>
&lt;li>Multi-task fine-tuning&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>Problems of fine-tuning:&lt;/p>
&lt;ol>
&lt;li>Requires task-specific labeled data, may cause overfitting and catastrophic forgetting。&lt;/li>
&lt;li>The generalization ability is limited, and fine-tuning are required when adapting to new tasks&lt;/li>
&lt;li>The performance may be destroyed after fine-tuning, for example, safety.&lt;/li>
&lt;/ol>
&lt;h1 id="rag">&lt;a href="#rag" class="header-anchor">&lt;/a>RAG
&lt;/h1>&lt;p>RAG consists of three major processes of &lt;em>retrieval&lt;/em>, &lt;em>augmentation&lt;/em>, and &lt;em>generation&lt;/em>. The framework of RAG in LLM can be described as follows:&lt;/p>
&lt;p>&lt;img src="https://maosong.website/RAG-framework.png"
loading="lazy"
alt="RAG-framework"
>&lt;/p>
&lt;h2 id="retrieval">&lt;a href="#retrieval" class="header-anchor">&lt;/a>Retrieval
&lt;/h2>&lt;h3 id="retriever-type">&lt;a href="#retriever-type" class="header-anchor">&lt;/a>Retriever type
&lt;/h3>&lt;p>Retrieval methods can be generally categorized into two types: sparse and dense, based on the information encoding methods.&lt;/p>
&lt;ol>
&lt;li>sparse retrieval usually relies on inverted index matching along with the raw data input, for example TF-IDF and BM25. The limitation of sparse retrieval in RAG is
&lt;ol>
&lt;li>its no-training nature, which makes the retrieval performance heavily rely on the quality of database construction and query generation.&lt;/li>
&lt;li>Moreover, such fixed term-based methods only support similarity retrieval, while cannot be adapted for other retrieval considerations demanding in LLM applications, such as the diversity&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>dense retrieval, on the contrary, embeds the query and documents into continuous vector space with certain criteria, for example, semantic similarity. Examples include BERT, Dense Passage Retriever (DPR), etc.&lt;/li>
&lt;/ol>
&lt;h3 id="retrieval-granularity">&lt;a href="#retrieval-granularity" class="header-anchor">&lt;/a>Retrieval Granularity
&lt;/h3>&lt;p>Retrieval granularity denotes the retrieval unit in which the corpus is indexed, e.g., document, passage, token, or other levels like entity.&lt;/p>
&lt;ol>
&lt;li>Chunk retrieval.&lt;/li>
&lt;li>Token retrieval.&lt;/li>
&lt;li>Entity retrieval.&lt;/li>
&lt;/ol>
&lt;h3 id="pre-retrieval-and-post-retrieval-enhancement">&lt;a href="#pre-retrieval-and-post-retrieval-enhancement" class="header-anchor">&lt;/a>Pre-retrieval and Post-retrieval Enhancement
&lt;/h3>&lt;p>Pre-retrieval and post retrieval strategies can be added to improve the quality of the retriever.&lt;/p>
&lt;p>Pre-retrieval methods include:&lt;/p>
&lt;ol>
&lt;li>Query rewrite. This method aims to close the gaps between the input text and the needed knowledge in retrieval, to reformulate the original question into a more conducive version to retrieve.&lt;/li>
&lt;li>Query augmentation. This method aims to combine the original query and the preliminary generated outputs as a new query, which is further used to retrieve relevant information from the external database&lt;/li>
&lt;/ol>
&lt;p>Post-retrieval enhancement denotes the procedure to process the extracted top-k documents from the retriever before feeding them to the generator for the sake of better alignment between the retrieval and generation stages.&lt;/p>
&lt;h3 id="database">&lt;a href="#database" class="header-anchor">&lt;/a>Database
&lt;/h3>&lt;ol>
&lt;li>Wikipedia&lt;/li>
&lt;li>Domain specific database&lt;/li>
&lt;li>search engine&lt;/li>
&lt;/ol>
&lt;h2 id="generation">&lt;a href="#generation" class="header-anchor">&lt;/a>Generation
&lt;/h2>&lt;ol>
&lt;li>Parameter-Accessible Generators (White-box). Allow parameter optimization.&lt;/li>
&lt;li>Parameter-Inaccessible Generators (Black-box). Focus more on retrieval and augmentation processes, trying to enhance the generator by augmenting the input with better knowledge, guidances or examples for the generation.&lt;/li>
&lt;/ol>
&lt;h2 id="augmentation">&lt;a href="#augmentation" class="header-anchor">&lt;/a>Augmentation
&lt;/h2>&lt;ol>
&lt;li>Input layer integration&lt;/li>
&lt;li>Output layer integration&lt;/li>
&lt;li>Intermediate layer integration&lt;/li>
&lt;/ol>
&lt;h2 id="retrieval-frequency">&lt;a href="#retrieval-frequency" class="header-anchor">&lt;/a>Retrieval Frequency
&lt;/h2>&lt;p>If it is necessary to retrieve? Self-RAG&lt;/p>
&lt;p>retrieval frequency:&lt;/p>
&lt;ol>
&lt;li>One-time.&lt;/li>
&lt;li>Every-n-token&lt;/li>
&lt;li>Every token&lt;/li>
&lt;/ol>
&lt;h1 id="rag-training">&lt;a href="#rag-training" class="header-anchor">&lt;/a>RAG training
&lt;/h1>&lt;p>&lt;img src="https://maosong.website/p/notes-on-rag/RAG-training.png"
width="1585"
height="957"
loading="lazy"
alt="RAG-training"
class="gallery-image"
data-flex-grow="165"
data-flex-basis="397px"
>&lt;/p>
&lt;ol>
&lt;li>Training Free&lt;/li>
&lt;li>Independent training&lt;/li>
&lt;li>Sequential training&lt;/li>
&lt;li>Joint training&lt;/li>
&lt;/ol>
&lt;h1 id="advance-rag">&lt;a href="#advance-rag" class="header-anchor">&lt;/a>Advance RAG
&lt;/h1>&lt;h1 id="module-rag">&lt;a href="#module-rag" class="header-anchor">&lt;/a>Module RAG
&lt;/h1>&lt;h1 id="applications">&lt;a href="#applications" class="header-anchor">&lt;/a>Applications
&lt;/h1>&lt;ol>
&lt;li>NLP applications
&lt;ul>
&lt;li>QA systems: REALM&lt;/li>
&lt;li>Chatbot:&lt;/li>
&lt;li>Fact Verification: self-RAG&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Downstream tasks:
&lt;ul>
&lt;li>Recommendations&lt;/li>
&lt;li>Software engineering&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Domain-specific Applications
&lt;ul>
&lt;li>AI for science&lt;/li>
&lt;li>Finance: ChatDOC&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h2 id="limitations-of-rag">&lt;a href="#limitations-of-rag" class="header-anchor">&lt;/a>Limitations of RAG
&lt;/h2>&lt;h1 id="long-context-window">&lt;a href="#long-context-window" class="header-anchor">&lt;/a>Long Context Window
&lt;/h1>&lt;p>Advantages of Long Context Window:&lt;/p>
&lt;ol>
&lt;li>Improve the understanding and relativity: long context window allows model to refer to more context information when generating answers.&lt;/li>
&lt;li>Handling complex tasks: long context window makes handling complex tasks such as writing a long article, coding&lt;/li>
&lt;li>Improve users&amp;rsquo; experience: the user expects the model remember the chat history and use them to interact with the user.&lt;/li>
&lt;/ol>
&lt;p>Disadvantages of long context window:&lt;/p>
&lt;ol>
&lt;li>Only uses context once, Requires refeeding the data to use long context window.&lt;/li>
&lt;li>Cost expensive due to input price.&lt;/li>
&lt;li>Time expensive due to limit of tokens per second.&lt;/li>
&lt;li>Needle in HayStack experiment show that there are problems with long context window.&lt;/li>
&lt;/ol>
&lt;p>Advantages of RAG:&lt;/p>
&lt;ol>
&lt;li>Privacy projection.&lt;/li>
&lt;li>Allow chunking the texts and retrieve the related information more accurately&lt;/li>
&lt;li>Adaptive to the size of data.&lt;/li>
&lt;li>Accepts multiple type of data source (multimodality).&lt;/li>
&lt;li>Only uses a small part of the total data, which is cheaper compared with long context window .&lt;/li>
&lt;/ol>
&lt;p>problems of RAG&lt;/p>
&lt;ol>
&lt;li>The quality of retrieval
&lt;ul>
&lt;li>The retrieved text cannot be aligned with the queried text.&lt;/li>
&lt;li>The queried text are not retrieved all.&lt;/li>
&lt;li>Redundancy or out-dated data may cause inaccuracy.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>the quality of response generation
&lt;ul>
&lt;li>Model Humiliation&lt;/li>
&lt;li>Irrelevance&lt;/li>
&lt;li>Organize the output to make it reasonable&lt;/li>
&lt;li>Depends on the external information&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>Futures:&lt;/p>
&lt;ol>
&lt;li>Trustworthy RA-LLMs&lt;/li>
&lt;li>Multi-lingual RA-LLMs&lt;/li>
&lt;li>Multi-modal RA-LLMs&lt;/li>
&lt;li>Quality of External Knowledge&lt;/li>
&lt;/ol>
&lt;h1 id="other-technologies">&lt;a href="#other-technologies" class="header-anchor">&lt;/a>Other technologies
&lt;/h1>&lt;ol>
&lt;li>Query transformations&lt;/li>
&lt;li>Sentence window retrieval&lt;/li>
&lt;li>Fusion retrieval/ hybrid search&lt;/li>
&lt;li>multi-document agents&lt;/li>
&lt;/ol>
&lt;h1 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://www.semanticscholar.org/paper/A-Survey-on-RAG-Meets-LLMs%3A-Towards-Large-Language-Ding-Fan/0576e9ab604ba4bf20cd5947f3c4a2c609ac2705" target="_blank" rel="noopener"
>A Survey on RAG Meets LLMs: Towards Retrieval-Augmented Large Language Models&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>What's next for AI agentic workflows</title><link>https://maosong.website/p/whats-next-for-ai-agentic-workflows/</link><pubDate>Sun, 14 Apr 2024 12:38:04 +0800</pubDate><guid>https://maosong.website/p/whats-next-for-ai-agentic-workflows/</guid><description>&lt;img src="https://maosong.website/p/whats-next-for-ai-agentic-workflows/agent_performance.png" alt="Featured image of post What's next for AI agentic workflows" />&lt;p>In AI Ascent, Andrew Ng shared his opinion about the agentic workflow, specifically, his talk focused on designs and future works on agentic workflows.&lt;/p>
&lt;h1 id="what-is-agent">&lt;a href="#what-is-agent" class="header-anchor">&lt;/a>What is Agent
&lt;/h1>&lt;p>LLM based agents can be classified into two categories: non-agentic and agentic.&lt;/p>
&lt;ul>
&lt;li>Non-agentic workflow requires LLM do something in a sequence of steps, usually this requires the &amp;ldquo;zero-shot&amp;rdquo; learning ability of LLM.&lt;/li>
&lt;li>Agentic workflow requires LLM do something step by step, and revise its action by thinking or reflection.&lt;/li>
&lt;/ul>
&lt;p>Andrew then posts an image shows the agentic workflow can improve the performance of LLM on HumanEval benchmark.&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/whats-next-for-ai-agentic-workflows/agent_performance.png"
width="3706"
height="1730"
loading="lazy"
alt="Comparisons of agent performance"
class="gallery-image"
data-flex-grow="214"
data-flex-basis="514px"
>&lt;/p>
&lt;h1 id="agentic-design-reasoning-design-pattern">&lt;a href="#agentic-design-reasoning-design-pattern" class="header-anchor">&lt;/a>Agentic Design Reasoning Design Pattern
&lt;/h1>&lt;p>Agentic design pattern can be divided into following kinds. Where the first two design patterns are robust and the latter two are emerging.&lt;/p>
&lt;h3 id="reflection">&lt;a href="#reflection" class="header-anchor">&lt;/a>Reflection
&lt;/h3>&lt;ul>
&lt;li>Self-Refine: Iterative Refinement with Self-Feedback&lt;/li>
&lt;li>Reflexion: Language Agents with Verbal Reinforcement Learning&lt;/li>
&lt;/ul>
&lt;h2 id="tool-use">&lt;a href="#tool-use" class="header-anchor">&lt;/a>Tool Use
&lt;/h2>&lt;ul>
&lt;li>Gorilla: Large Language Model Connected with Massive APIs&lt;/li>
&lt;li>MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action&lt;/li>
&lt;/ul>
&lt;h2 id="planning">&lt;a href="#planning" class="header-anchor">&lt;/a>Planning
&lt;/h2>&lt;ul>
&lt;li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models&lt;/li>
&lt;li>HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face&lt;/li>
&lt;/ul>
&lt;h2 id="multi-agent-collaboration">&lt;a href="#multi-agent-collaboration" class="header-anchor">&lt;/a>Multi-agent collaboration
&lt;/h2>&lt;ul>
&lt;li>Communicative Agents for Software Development&lt;/li>
&lt;li>AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation&lt;/li>
&lt;/ul>
&lt;h1 id="future-perspective">&lt;a href="#future-perspective" class="header-anchor">&lt;/a>Future Perspective
&lt;/h1>&lt;ul>
&lt;li>The set of tasks that AI can do expand dramatically because of agentic workflows.&lt;/li>
&lt;li>We have to get used to delegating tasks to AI agent and wait patiently for response.&lt;/li>
&lt;li>Fast token generation is important. Generating more tokens even from a a lower quality LLM can achieve a good result.&lt;/li>
&lt;/ul>
&lt;h1 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://www.youtube.com/watch?v=sal78ACtGTc&amp;amp;list=PLOhHNjZItNnOoPxOF3dmq30UxYqFuxXKn" target="_blank" rel="noopener"
>What&amp;rsquo;s next for AI agentic workflows ft. Andrew Ng of AI Fund&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Rules of Machine Learning</title><link>https://maosong.website/p/rules-of-machine-learning/</link><pubDate>Sat, 13 Apr 2024 19:57:47 +0800</pubDate><guid>https://maosong.website/p/rules-of-machine-learning/</guid><description>&lt;p>Best practice for machine learning.&lt;/p>
&lt;h1 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h1>&lt;p>Google posts a guide on how to uses machine learning in practice.
It represents a style for machine learning, similar to Google C++ Style Guide.&lt;/p>
&lt;p>In overview, to make great products:&lt;/p>
&lt;blockquote>
&lt;p>Do machine learning like the great engineer your are, not like the great machine learning expert you are.&lt;/p>
&lt;/blockquote>
&lt;p>Most algorithms we are facing are engineering problems instead of machine learning algorithms. A basic approach Google recommends is:&lt;/p>
&lt;ol>
&lt;li>Make sure your pipeline is solid end to end&lt;/li>
&lt;li>Start with a reasonable objective&lt;/li>
&lt;li>Add a common-sense features in a simple way&lt;/li>
&lt;li>Make sure that your pipeline stays solid.&lt;/li>
&lt;/ol>
&lt;p>Google separates rules with respect to different stages.&lt;/p>
&lt;h1 id="before-machine-learning">&lt;a href="#before-machine-learning" class="header-anchor">&lt;/a>Before machine learning
&lt;/h1>&lt;p>These rules help us understand whether the time is right for building a machine learning system.&lt;/p>
&lt;blockquote>
&lt;p>Rule #1: Do not be afraid of lunch a product without machine learning.&lt;/p>
&lt;/blockquote>
&lt;p>This rule tells us that is not absolutely necessary, if rule-based methods work well, there is no need to develop a machine learning algorithm.&lt;/p>
&lt;blockquote>
&lt;p>Rule #2: First, design and implement metrics.&lt;/p>
&lt;/blockquote>
&lt;p>This rule tells us that tracking as much as possible before we formalize what our machine learning system will do. This step helps us construct the goal of our system, that is, a metric.&lt;/p>
&lt;blockquote>
&lt;p>Rule #3: Choose machine learning over a complex heuristic.&lt;/p>
&lt;/blockquote>
&lt;p>Considering using machine learning algorithms only if the heuristic algorithm doesn&amp;rsquo;t work well, since a complex heuristic is not maintainable. Meanwhile, machine-learned models are easier to update and maintain.&lt;/p>
&lt;h1 id="ml-phase-1-your-first-pipeline">&lt;a href="#ml-phase-1-your-first-pipeline" class="header-anchor">&lt;/a>ML phase 1: Your First Pipeline
&lt;/h1>&lt;p>When creating our first pipeline, we should focus on our system infrastructure&lt;/p>
&lt;blockquote>
&lt;p>Rule #4: Keep the first model simple and get the infrastructure right.&lt;/p>
&lt;/blockquote>
&lt;p>Remember infrastructure issues are many more than model problems when we creating the first machine learning model. We have to determine:&lt;/p>
&lt;ol>
&lt;li>How to obtain data for our model&lt;/li>
&lt;li>How to evaluate the performance of our model&lt;/li>
&lt;li>How to integrate our model into our application&lt;/li>
&lt;/ol>
&lt;p>Moreover, we should choose simple features to ensure that:&lt;/p>
&lt;ul>
&lt;li>The features reach our learning algorithm correctly&lt;/li>
&lt;li>The model learns reasonably weights&lt;/li>
&lt;li>The features reach our model in the sever correctly&lt;/li>
&lt;/ul>
&lt;p>Once we have a system that does above things reliably, we have done most of the work.&lt;/p>
&lt;blockquote>
&lt;p>Rule #5: Test the infrastructure independently from the machine learning.&lt;/p>
&lt;/blockquote>
&lt;p>This rule tells us split the infrastructure and the machine learning model and test them separately to avoid dependency issue occurs. In this way, our model can be developed without worrying about environment. Specifically:&lt;/p>
&lt;ol>
&lt;li>Test getting data into algorithm. Check the data that are feed into the models and do some statistics before using the data&lt;/li>
&lt;li>Testing getting models out of the training algorithm. Make sure our algorithms work in the same way in our serving environment as the training environment.&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>Rule #6: Be careful about dropped data.&lt;/p>
&lt;/blockquote>
&lt;p>Do not drop data without thoroughly test, this may data loss.&lt;/p>
&lt;blockquote>
&lt;p>Rule #7: Turn heuristics into features, or handle them externally.&lt;/p>
&lt;/blockquote>
&lt;p>Before using machine learning models, if we have tried some heuristic algorithms, then these algorithms may help us to improve the overall performance. Some ways we can use an existing heuristic algorithm:&lt;/p>
&lt;ol>
&lt;li>Preprocessing using the heuristic. If the feature is incredibly awesome, then do not try to relearn the feature. Just use the heuristic way to pre-process the data.&lt;/li>
&lt;li>Create a feature. We can use the heuristic way to create a new feature to help improve the machine learning performance.&lt;/li>
&lt;li>Mine the raw inputs of the heuristic. We can use the inputs of heuristic as features to learn the heuristic implicitly.&lt;/li>
&lt;li>Modify the label.&lt;/li>
&lt;/ol>
&lt;h2 id="monitoring">&lt;a href="#monitoring" class="header-anchor">&lt;/a>Monitoring
&lt;/h2>&lt;p>In general, such as making alerts and having a dashboard page.&lt;/p>
&lt;blockquote>
&lt;p>Rule #8: Know the freshness requirements of our system.&lt;/p>
&lt;/blockquote>
&lt;p>It is important for us to know the freshness of our model, for example, how much does performance degrade if we have a model that is a day old. The freshness helps us monitor and improve the performance.&lt;/p>
&lt;blockquote>
&lt;p>Rule #9: Detect problems before exporting models.&lt;/p>
&lt;/blockquote>
&lt;p>This rule tells us that evaluating the performance of the model before serving. The evaluation includes the testing on hold-out data, check AUC metric.&lt;/p>
&lt;blockquote>
&lt;p>Rule #10: Watch for silent failures.&lt;/p>
&lt;/blockquote>
&lt;p>Since the continuos change of data, silent failures may occur, so keep tracking statistics of the data as well as manually inspect the data on occasion help us reduce these kind of issues.&lt;/p>
&lt;blockquote>
&lt;p>Rule #11: Give feature owners and documentation.&lt;/p>
&lt;/blockquote>
&lt;p>Knowing who created the feature helps us gain information about data. A detailed documentation helps user understand how it works.&lt;/p>
&lt;h2 id="your-first-objective">&lt;a href="#your-first-objective" class="header-anchor">&lt;/a>Your first objective
&lt;/h2>&lt;blockquote>
&lt;p>Rule #12: Don&amp;rsquo;t overthink which objective you choose to optimize.&lt;/p>
&lt;/blockquote>
&lt;p>There are many metrics to optimize according to Rule #2. However, it turns out in early stage, some metrics are optimized even though we not directly optimizing them.&lt;/p>
&lt;blockquote>
&lt;p>Rule #13: Choose simple, observable and attributable metric for your first objective.&lt;/p>
&lt;/blockquote>
&lt;p>This rule tells us the strategy of choosing metric in the beginning. In principal, The ML objective should be something that is easy to measure and is a proxy for the &amp;ldquo;true&amp;rdquo; objective. In fact however, there is no such &amp;ldquo;true&amp;rdquo; objective, so we should keep the objective as simple as possible, it&amp;rsquo;s better if the objective is observable. Then, we can modify the objective based on the performance of the model.&lt;/p>
&lt;blockquote>
&lt;p>Rule #14: Starts with an interpretable model makes debugging easier.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Rule #15: Separate spam filtering and quality ranking in a policy layer.&lt;/p>
&lt;/blockquote>
&lt;p>Sometimes, spam filtering confuses quality ranking, when we do quality ranking, we should clean the data.&lt;/p>
&lt;h1 id="ml-phase-2-feature-engineering">&lt;a href="#ml-phase-2-feature-engineering" class="header-anchor">&lt;/a>ML phase 2: Feature engineering
&lt;/h1>&lt;p>After we have a working end to end system with unit and system tests instrumented, Phase II begins.
In this phase, we should make use of features.&lt;/p>
&lt;blockquote>
&lt;p>Rule #16: Plan to lunch and iterate.&lt;/p>
&lt;/blockquote>
&lt;p>There are three reasons to lunch new models:&lt;/p>
&lt;ol>
&lt;li>You are coming up with new features&lt;/li>
&lt;li>You are tuning regularization and combining old features in new ways&lt;/li>
&lt;li>You are tuning the objectives&lt;/li>
&lt;/ol>
&lt;p>When lunch a new model, we should think about:&lt;/p>
&lt;ol>
&lt;li>How easy is it to add or remove or recombine features&lt;/li>
&lt;li>How easy is it to create a fresh copy of the pipeline and verify its correctness.&lt;/li>
&lt;li>Is it possible to have two or three copies running in parallel.&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>Rule #17: Start with directly observed and reported features as opposed to learned features.&lt;/p>
&lt;/blockquote>
&lt;p>a learned feature is a feature generated by an external system or by the learner itself.
If the learned feature comes from an external system, then bias or being out-of-date may affect the model. If the learned feature comes from the learner itself, then it is hard to tell the impact of the feature.&lt;/p>
&lt;blockquote>
&lt;p>Rule #18: Explore with features of content that generalize across contexts.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Rule #19: Use very specific features when you can.&lt;/p>
&lt;/blockquote>
&lt;p>It is simpler to learn millions of simple features than a few complex features.&lt;/p>
&lt;blockquote>
&lt;p>Rule #20: Combine and modify existing features to create new features in human-understanding ways.&lt;/p>
&lt;/blockquote>
&lt;p>Combine features may causing overfitting problems&lt;/p>
&lt;blockquote>
&lt;p>Rule #21: The number of feature weights we can learn in a linear model is roughly proportional to the number of data you have.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Rule #22: Clean up features you are no longer using.&lt;/p>
&lt;/blockquote>
&lt;p>If you find that you are not using a feature, and that combining it with other features is not working, then drop it out of your infrastructure.&lt;/p>
&lt;h2 id="human-analysis-of-the-system">&lt;a href="#human-analysis-of-the-system" class="header-anchor">&lt;/a>Human analysis of the system
&lt;/h2>&lt;p>This subsection teaches us how to look at an existing model and improve it.&lt;/p>
&lt;blockquote>
&lt;p>Rule #23: You are not a typical end user.&lt;/p>
&lt;/blockquote>
&lt;p>Check carefully before we deploying the model.&lt;/p>
&lt;blockquote>
&lt;p>Rule #24: Measure the delta between models&lt;/p>
&lt;/blockquote>
&lt;p>Make sure the system is stable when making small changes. Make sure that a model when compared with itself has a low (ideally zero) symmetric difference.&lt;/p>
&lt;blockquote>
&lt;p>Rule #25: When choosing models, utilitarian performance trumps predictive power.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Rule #26: Look for patterns in the measured errors, and create new features.&lt;/p>
&lt;/blockquote>
&lt;p>Once you have examples that the model got wrong, look for trends that are outside your current feature set.&lt;/p>
&lt;blockquote>
&lt;p>Rule #27: Try to quantify observed undesired behavior&lt;/p>
&lt;/blockquote>
&lt;p>If your issues are measurable, then you can start using them as features, objectives, or metrics. The general rule is &amp;ldquo;&lt;strong>measure first, optimize second&lt;/strong>&amp;rdquo;.&lt;/p>
&lt;blockquote>
&lt;p>Rule #28: Be aware that identical short-term behavior does not imply identical long-term behavior.&lt;/p>
&lt;/blockquote>
&lt;h2 id="training-serving-skew">&lt;a href="#training-serving-skew" class="header-anchor">&lt;/a>Training-Serving skew
&lt;/h2>&lt;p>Training-serving skew is a difference between performance during training and performance during serving.
This skew can be caused by:&lt;/p>
&lt;ol>
&lt;li>A discrepancy between how you handle data in the training and serving pipelines&lt;/li>
&lt;li>A change in the data between when you train and when you serve&lt;/li>
&lt;li>A feedback loop between your model and your algorithm.&lt;/li>
&lt;/ol>
&lt;p>The best solution is to explicitly monitor it so that system and data changes don&amp;rsquo;t introduce skew unnoticed.&lt;/p>
&lt;blockquote>
&lt;p>Rule #29: The best way to make sure that you train like you serve is to save the set of features used at the serving time, and then pipe those features to a log to use them at training time.&lt;/p>
&lt;/blockquote>
&lt;p>This can help verify the consistency between the training and serving.&lt;/p>
&lt;blockquote>
&lt;p>Rule #30: Importance-weight sampled data, don&amp;rsquo;t arbitrarily drop it.&lt;/p>
&lt;/blockquote>
&lt;p>Importance weighting means that if you decide that you are going to sample example X with a 30% probability, then give it a weight of 10/3. With importance weighting, all of the calibration properties discussed in Rule #14 still hold.&lt;/p>
&lt;blockquote>
&lt;p>Rule #31: Beware that if your join data from a table at training and serving time, the data in the table may change.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Rule #32: Re-use code between your training pipeline and your serving pipeline whenever possible.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Rule #33: If you produce a model based on the data until January 5th, test the model on the data from January 6th and after.&lt;/p>
&lt;/blockquote>
&lt;p>In general, measure performance of a model on the data gathered after the data you trained the model on, as this better reflects what your system will do in production&lt;/p>
&lt;blockquote>
&lt;p>Rule #37: Measure Training-Serving Skew.&lt;/p>
&lt;/blockquote>
&lt;p>We can divide causes of Training-Serving Skew into several parts:&lt;/p>
&lt;ol>
&lt;li>The difference between the performance on the training data and the holdout data. In general, this will always exist, and it is not always bad.&lt;/li>
&lt;li>The difference between the performance on the holdout data and the &amp;ldquo;next­day&amp;rdquo; data. Again, this will always exist&lt;/li>
&lt;li>The difference between the performance on the &amp;ldquo;next-day&amp;rdquo; data and the live data.&lt;/li>
&lt;/ol>
&lt;h1 id="ml-phase-3-slowed-growth-optimization-refinement-and-complex-models">&lt;a href="#ml-phase-3-slowed-growth-optimization-refinement-and-complex-models" class="header-anchor">&lt;/a>ML phase 3: Slowed growth, Optimization refinement, and complex models
&lt;/h1>&lt;blockquote>
&lt;p>Rule #38: Don’t waste time on new features if unaligned objectives have become the issue.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Rule #39: Launch decisions are a proxy for long-term product goals.&lt;/p>
&lt;/blockquote>
&lt;p>The only easy launch decisions are when all metrics get better (or at least do not get worse).&lt;/p>
&lt;p>Individuals, on the other hand, tend to favor one objective that they can directly optimize.&lt;/p>
&lt;blockquote>
&lt;p>Rule #40: Keep ensembles simple.&lt;/p>
&lt;/blockquote>
&lt;p>To keep things simple, each model should either be an ensemble only taking the input of other models, or a base model taking many features, but not both.&lt;/p>
&lt;blockquote>
&lt;p>Rule #41: When performance plateaus, look for qualitatively new sources of information to add rather than refining existing signals.&lt;/p>
&lt;/blockquote>
&lt;p>As in any engineering project, you have to weigh the benefit of adding new features against the cost of increased complexity.&lt;/p>
&lt;blockquote>
&lt;p>Rule #42: Don’t expect diversity, personalization, or relevance to be as correlated with popularity as you think they are.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Rule #43: Your friends tend to be the same across different products. Your interests tend not to be.&lt;/p>
&lt;/blockquote>
&lt;h1 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h1>&lt;p>In early stage, make sure that the infrastructure is well constructed, the used model can be simple.&lt;/p>
&lt;p>In main stage, focusing on the utilitarian performance and the gap between training data and test data.&lt;/p>
&lt;p>When utilizing features, use simple, observable features.&lt;/p>
&lt;p>When deploying models, watch out training-serving skew.&lt;/p>
&lt;h1 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://developers.google.com/machine-learning/guides/rules-of-ml" target="_blank" rel="noopener"
>Rules of Machine Learning&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>How to fix http error when creating a new environment.</title><link>https://maosong.website/p/how-to-fix-http-error-when-creating-a-new-environment./</link><pubDate>Fri, 25 Aug 2023 00:00:00 +0000</pubDate><guid>https://maosong.website/p/how-to-fix-http-error-when-creating-a-new-environment./</guid><description>&lt;h1 id="problem-description">&lt;a href="#problem-description" class="header-anchor">&lt;/a>Problem description
&lt;/h1>&lt;p>When we use &lt;code>conda create --env myenv&lt;/code> to create a new environment, it may throw an exception&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">CondaHTTPError: HTTP &lt;span class="m">000&lt;/span> CONNECTION FAILED &lt;span class="k">for&lt;/span> url &amp;lt;https://repo.anaconda.com/pkgs/main/osx-64/repodata.json&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>usually, this occurs when the source channels are not domestic. In this case, we need the change the configuration.&lt;/p>
&lt;h1 id="solution">&lt;a href="#solution" class="header-anchor">&lt;/a>Solution
&lt;/h1>&lt;ol>
&lt;li>use &lt;code>conda info&lt;/code> to view the configuration file:&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">conda info
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>focus on the &lt;code>user config file&lt;/code> and open it.
2. use the following configuration to overwrite the file:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">ssl_verify: &lt;span class="nb">false&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">channels:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> - defaults
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">show_channel_urls: &lt;span class="nb">true&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">default_channels:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">custom_channels:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> pytorch-lts: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> deepmodeling: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>Archives</title><link>https://maosong.website/archives/</link><pubDate>Sun, 06 Mar 2022 00:00:00 +0000</pubDate><guid>https://maosong.website/archives/</guid><description/></item><item><title>Tags</title><link>https://maosong.website/tags/</link><pubDate>Sun, 06 Mar 2022 00:00:00 +0000</pubDate><guid>https://maosong.website/tags/</guid><description>&lt;p>This page displays a tag cloud of all tags used across the blog posts. The size of each tag indicates its frequency. Click on any tag to see all posts associated with that tag.&lt;/p></description></item><item><title>About</title><link>https://maosong.website/about/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://maosong.website/about/</guid><description>&lt;div class="about-page">
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;div class="intro-section">
&lt;p>&lt;strong>Mao Song (毛松)&lt;/strong> is a researcher at &lt;a class="link" href="https://www.shlab.org.cn/" target="_blank" rel="noopener"
>&lt;strong>Shanghai Artificial Intelligence Laboratory (Shanghai AI Lab, 上海人工智能实验室)&lt;/strong>&lt;/a>, where he focuses on pushing the boundaries of artificial intelligence through cutting-edge research in &lt;strong>Large Language Models (LLMs)&lt;/strong> and &lt;strong>Multimodal Large Language Models (MLLMs)&lt;/strong>. He also goes by the name &lt;strong>&amp;ldquo;以往的月&amp;rdquo;&lt;/strong> on platforms such as Zhihu, Bilibili, and WeChat Official Account.&lt;/p>
&lt;/div>
&lt;hr>
&lt;h2 id="research-interests">&lt;a href="#research-interests" class="header-anchor">&lt;/a>Research Interests
&lt;/h2>&lt;div class="research-section">
&lt;p>My research focuses on the intersection of language and vision, with particular emphasis on:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Large Language Models (LLMs)&lt;/strong> - Advancing the capabilities of language understanding and generation&lt;/li>
&lt;li>&lt;strong>Multimodal Large Language Models (MLLMs)&lt;/strong> - Bridging vision and language for unified AI systems&lt;/li>
&lt;li>&lt;strong>Reasoning Models&lt;/strong> - Developing systems capable of complex logical reasoning&lt;/li>
&lt;li>&lt;strong>Unified Understanding &amp;amp; Generation&lt;/strong> - Creating models that seamlessly integrate comprehension and creation&lt;/li>
&lt;/ul>
&lt;/div>
&lt;hr>
&lt;h2 id="academic-background">&lt;a href="#academic-background" class="header-anchor">&lt;/a>Academic Background
&lt;/h2>&lt;div class="education-section">
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Master&amp;rsquo;s Degree&lt;/strong> - &lt;a class="link" href="https://www.shanghaitech.edu.cn/" target="_blank" rel="noopener"
>ShanghaiTech University&lt;/a>&lt;/p>
&lt;ul>
&lt;li>Supervised by &lt;a class="link" href="https://sist.shanghaitech.edu.cn/wanghao1/main.htm" target="_blank" rel="noopener"
>Prof. Hao Wang&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Bachelor&amp;rsquo;s Degree&lt;/strong> - &lt;a class="link" href="https://www.bit.edu.cn/" target="_blank" rel="noopener"
>Beijing Institute of Technology&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;hr>
&lt;h2 id="get-in-touch">&lt;a href="#get-in-touch" class="header-anchor">&lt;/a>Get in Touch
&lt;/h2>&lt;div class="contact-section">
&lt;p>I&amp;rsquo;m always excited to discuss research collaborations, innovative ideas, or potential opportunities. Feel free to reach out!&lt;/p>
&lt;p>&lt;strong>Email:&lt;/strong> &lt;a class="link" href="mailto:maosong@pjlab.org.cn" >maosong@pjlab.org.cn&lt;/a>&lt;/p>
&lt;p>&lt;strong>Resume/CV:&lt;/strong> &lt;a class="link" href="https://github.com/MaoSong2022/MaoSong2022.github.io/blob/master/assets/%E6%AF%9B%E6%9D%BE_%E7%AE%80%E5%8E%86.pdf" target="_blank" rel="noopener"
>Download PDF&lt;/a>&lt;/p>
&lt;/div>
&lt;hr>
&lt;h2 id="find-me-online">&lt;a href="#find-me-online" class="header-anchor">&lt;/a>Find Me Online
&lt;/h2>&lt;div class="social-links-section">
&lt;div class="social-grid">
&lt;a href="https://scholar.google.com/citations?user=BaqGkQQAAAAJ" class="social-card scholar-card" target="_blank" rel="noopener">
&lt;div class="social-icon">
&lt;svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24" fill="currentColor">
&lt;path d="M12 24a7 7 0 1 1 0-14 7 7 0 0 1 0 14zm0-24L0 9.5l4.838 3.94A8 8 0 0 1 12 9a8 8 0 0 1 7.162 4.44L24 9.5z"/>
&lt;/svg>
&lt;/div>
&lt;div class="social-info">
&lt;h3>Google Scholar&lt;/h3>
&lt;p>Publications &amp; citations&lt;/p>
&lt;/div>
&lt;/a>
&lt;a href="https://github.com/MaoSong2022" class="social-card github-card" target="_blank" rel="noopener">
&lt;div class="social-icon">
&lt;svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24" fill="currentColor">
&lt;path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/>
&lt;/svg>
&lt;/div>
&lt;div class="social-info">
&lt;h3>GitHub&lt;/h3>
&lt;p>Open source projects &amp; code&lt;/p>
&lt;/div>
&lt;/a>
&lt;a href="https://b23.tv/a0Bb9Z1" class="social-card bilibili-card" target="_blank" rel="noopener">
&lt;div class="social-icon">
&lt;svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24" fill="currentColor">
&lt;path d="M17.813 4.653h.854c1.51.054 2.769.578 3.773 1.574 1.004.995 1.524 2.249 1.56 3.76v7.36c-.036 1.51-.556 2.769-1.56 3.773s-2.262 1.524-3.773 1.56H5.333c-1.51-.036-2.769-.556-3.773-1.56S.036 18.858 0 17.347v-7.36c.036-1.511.556-2.765 1.56-3.76 1.004-.996 2.262-1.52 3.773-1.574h.774l-1.174-1.12a1.234 1.234 0 0 1-.373-.906c0-.356.124-.658.373-.907l.027-.027c.267-.249.573-.373.92-.373.347 0 .653.124.92.373L9.653 4.44c.071.071.134.142.187.213h4.267a.836.836 0 0 1 .16-.213l2.853-2.747c.267-.249.573-.373.92-.373.347 0 .662.151.929.4.267.249.391.551.391.907 0 .355-.124.657-.373.906zM5.333 7.24c-.746.018-1.373.276-1.88.773-.506.498-.769 1.13-.786 1.894v7.52c.017.764.28 1.395.786 1.893.507.498 1.134.756 1.88.773h13.334c.746-.017 1.373-.275 1.88-.773.506-.498.769-1.129.786-1.893v-7.52c-.017-.765-.28-1.396-.786-1.894-.507-.497-1.134-.755-1.88-.773zM8 11.107c.373 0 .684.124.933.373.25.249.383.569.4.96v1.173c-.017.391-.15.711-.4.96-.249.25-.56.374-.933.374s-.684-.125-.933-.374c-.25-.249-.383-.569-.4-.96V12.44c0-.373.129-.689.386-.947.258-.257.574-.386.947-.386zm8 0c.373 0 .684.124.933.373.25.249.383.569.4.96v1.173c-.017.391-.15.711-.4.96-.249.25-.56.374-.933.374s-.684-.125-.933-.374c-.25-.249-.383-.569-.4-.96V12.44c.017-.391.15-.711.4-.96.249-.249.56-.373.933-.373Z"/>
&lt;/svg>
&lt;/div>
&lt;div class="social-info">
&lt;h3>Bilibili&lt;/h3>
&lt;p>Learning videos &amp; tutorials&lt;/p>
&lt;/div>
&lt;/a>
&lt;div class="social-card wechat-card">
&lt;div class="social-icon">
&lt;svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24" fill="currentColor">
&lt;path d="M8.691 2.188C3.891 2.188 0 5.476 0 9.53c0 2.212 1.17 4.203 3.002 5.55a.59.59 0 0 1 .213.665l-.39 1.48c-.019.07-.048.141-.048.213 0 .163.13.295.29.295a.326.326 0 0 0 .167-.054l1.903-1.114a.864.864 0 0 1 .717-.098 10.16 10.16 0 0 0 2.837.403c.276 0 .543-.027.811-.05-.857-2.578.157-4.972 1.932-6.446 1.703-1.415 3.882-1.98 5.853-1.838-.576-3.583-4.196-6.348-8.596-6.348zM5.785 5.991c.642 0 1.162.529 1.162 1.18a1.17 1.17 0 0 1-1.162 1.178A1.17 1.17 0 0 1 4.623 7.17c0-.651.52-1.18 1.162-1.18zm5.813 0c.642 0 1.162.529 1.162 1.18a1.17 1.17 0 0 1-1.162 1.178 1.17 1.17 0 0 1-1.162-1.178c0-.651.52-1.18 1.162-1.18zm5.34 2.867c-1.797-.052-3.746.512-5.28 1.786-1.72 1.428-2.687 3.72-1.78 6.22.942 2.453 3.666 4.229 6.884 4.229.826 0 1.622-.12 2.361-.336a.722.722 0 0 1 .598.082l1.584.926a.272.272 0 0 0 .14.047c.134 0 .24-.111.24-.247 0-.06-.023-.12-.038-.177l-.327-1.233a.582.582 0 0 1 .166-.555c1.548-1.18 2.485-2.87 2.485-4.716 0-3.503-3.161-6.026-6.799-6.026h-.234zm-.24 1.66c.567 0 1.027.467 1.027 1.042 0 .573-.46 1.04-1.027 1.04-.566 0-1.027-.467-1.027-1.04 0-.575.461-1.041 1.027-1.041zm4.81 0c.567 0 1.027.467 1.027 1.042 0 .573-.46 1.04-1.027 1.04-.565 0-1.027-.467-1.027-1.04 0-.575.462-1.041 1.027-1.041z"/>
&lt;/svg>
&lt;/div>
&lt;div class="social-info">
&lt;h3>WeChat Official Account&lt;/h3>
&lt;p>Scan QR code to follow&lt;/p>
&lt;/div>
&lt;div class="wechat-qr-popup">
&lt;img src="https://maosong.website/wechat_avatar.jpg" alt="WeChat QR Code" />
&lt;p>Scan with WeChat&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;hr>
&lt;h2 id="about-this-blog">&lt;a href="#about-this-blog" class="header-anchor">&lt;/a>About This Blog
&lt;/h2>&lt;div class="blog-info-section">
&lt;p>This blog serves as my digital garden where I share insights, research notes, and explorations in the field of artificial intelligence. Here you&amp;rsquo;ll find:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Technical Deep Dives&lt;/strong> - In-depth analyses of recent papers and models&lt;/li>
&lt;li>&lt;strong>Research Notes&lt;/strong> - Thoughts and observations from my research journey&lt;/li>
&lt;li>&lt;strong>Practical Guides&lt;/strong> - Tutorials and implementation details&lt;/li>
&lt;li>&lt;strong>Reflections&lt;/strong> - Perspectives on AI trends and future directions&lt;/li>
&lt;/ul>
&lt;p>I occasionally create educational content on &lt;a class="link" href="https://b23.tv/a0Bb9Z1" target="_blank" rel="noopener"
>Bilibili&lt;/a> to make complex AI concepts more accessible. Feel free to check it out and share your thoughts!&lt;/p>
&lt;/div>
&lt;/div></description></item><item><title>Search</title><link>https://maosong.website/search/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://maosong.website/search/</guid><description/></item></channel></rss>