---
title: 压缩即智能
description: 从压缩即智能的角度理解大模型
date: 2025-03-06 17:57:51+0800
math: true
tags: 
    - math
categories:
    - Large Language Model 
---

# 介绍

我们知道，基于decoder-only transformer的LLM的训练目标是最小化next-token-prediction loss，即给定sequence $x=(x_1,\dots, x_n)\in D$，我们的目标为求解以下优化问题

$$
\min_{\theta} -\sum_{x\in D}\log P_{\theta}(x_i|x_1,\dots,x_{i-1})
$$

这里 $\theta$ 就是我们的模型参数，$D$ 是我们的训练数据集。

无数模型通过实际效果告诉我们，这个优化目标可以很好地训练出具有良好泛化能力的大语言模型。但是，我们的问题是，为什么这个优化目标可以训练出智能的模型？ 本文将从压缩即智能的角度来理解这个问题。

# 压缩即智能

## 一个例子

我们首先来看一个简单的例子。给定如下三个0-1字符串：

```
01010101010101010101
01001000100001000001
01101000101010100101
```

我们该如何描述这三个字符串的规律？显然，第一个字符串最简单，它是`01`字符串重复得到的结果；第二个字符串稍微复杂一些，它在每个`1`之前插入重复次数的`0`；第三个字符串则最复杂，它是我随手写的一个字符串，基本没有任何规律，因此，我们只能直接存储这个字符串。

这个例子告诉我们，一个字符串的规律越简单，我们越容易描述它，因此，我们越容易压缩它。实际上，大语言模型做的也是类似的事情。它们的核心思想是，压缩即智能。

##

# 结论

本文中，我们从压缩即智能的角度来理解大语言模型的原理。我们发现，大语言模型的next-token-prediction其实就是压缩。我们通过压缩让大语言模型学习到了语言中的规律，从而让模型具有了智能。

# 参考文献
