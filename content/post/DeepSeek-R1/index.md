---
title: Notes on DeepSeek-R1
description: DeepSeek 在 2024 年 5 月提出了 DeepSeek-V2，一个基于 MoE 架构的大语言模型，参数量为 236B-A21B. 作者使用了 MLA 来压缩 KV cache, 使用 DeepSeekMoE 架构来提高模型训练效率和表现。
date: 2025-12-02 18:21:54+0800
lastmod: 2025-12-02 18:21:54+0800
math: true
tags: 
    - deepseek
    - MoE
categories:
    - LLM 
---