<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>VFM on Mao Song(毛松)'s Homepage</title><link>https://maosong.website/tags/vfm/</link><description>Recent content in VFM on Mao Song(毛松)'s Homepage</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sat, 24 Jan 2026 17:06:04 +0800</lastBuildDate><atom:link href="https://maosong.website/tags/vfm/index.xml" rel="self" type="application/rss+xml"/><item><title>Notes on ViT</title><link>https://maosong.website/p/notes-on-vit/</link><pubDate>Thu, 04 Dec 2025 11:00:44 +0800</pubDate><guid>https://maosong.website/p/notes-on-vit/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>Transformer 架构在 NLP 领域已经成为了事实上的标准。而在 CV 领域，目前还是 CNN 架构占据了主导。&lt;/p>
&lt;p>在本文中，作者就探究是否可以应用 Transformer 架构来完成图像识别的任务。&lt;/p>
&lt;p>作者发现，当在小规模数据集上进行训练时，Transformer 架构的表现是不如 CNN 架构的，作者认为原因是 Transformer 架构缺乏如 translation equivariance 和 locality 等 inductive biases. 但是当数据集规模上去之后，作者发现这种 inductive bias 可以通过大规模训练抵消掉。其中，最好的模型在 ImageNet 上达到了 $88.55\%$ 的准确率。&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;p>ViT 的架构如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-vit/ViT-architecture.png"
width="949"
height="483"
loading="lazy"
alt="Architecture of ViT"
class="gallery-image"
data-flex-grow="196"
data-flex-basis="471px"
>&lt;/p>
&lt;p>为了能够处理图片，对于输入的图片 $x\in\mathbb{R}^{H\times W\times C}$, 其中 $(H,W)$ 是图片大小，$C$ 是 channel 的个数。作者将其展开为一个 2D patch 序列, $x_p\in\mathbb{R}^{N\times (P^2\times C)}$, 其中 $(P, P)$ 是 image patch 的大小，$N=HW/P^2$ 是 image patch 的的个数，也是 Transformer 输入 token 的个数，Transformer 的 hidden size 为 $D$, 作者使用了一个 linear layer 来将 image patch 转换为 Transformer 的输入。&lt;/p>
&lt;p>与 BERT 一致，作者使用了一个 &lt;code>[class]&lt;/code> token 来作为输入 patch 序列的 embedding, 即 $z_0^0=x_{class}$, 其对应的输出 $z_L^0$ 作为该图片的表示。作者还使用了 1D 的 position encoding. 最终，ViT 表达式如下所示&lt;/p>
$$
\begin{aligned}
z_0 &amp;= [x_{class};x_p^1\mathbf{E};x_p^2\mathbf{E};\cdots;x_p^N\mathbf{E};]+\mathbf{E}_{pos}, &amp;\mathbf{E}\in\mathbb{R}^{(P^2\cdot C)\times D},\mathbf{E}_{pos}\in\mathbb{R}^{(N+1)\times D}\\
z_{\ell}'&amp;=\mathrm{MultiHeadAttention}(\mathrm{LayerNorm}(z_{\ell-1}))+z_{\ell-1},&amp;\ell=1,\dots,L\\
z_{\ell} &amp;= \mathrm{MLP}(\mathrm{LayerNorm}(z_{\ell}'))+z_{\ell}',&amp;\ell=1,\dots,L\\
y&amp;=\mathrm{LayerNorm}(z_L^0)
\end{aligned}
$$&lt;p>作者分析认为，相比于 CNN 架构，Transformer 架构缺乏 inductive bias, 这是因为每个 patch 对其周围的 2D 临近信息用的非常少，ViT 架构必须从零开始学习这些位置以及空间关系。&lt;/p>
&lt;p>作者还介绍了混合架构，即 patch embedding 由一个 linear layer 替换为一个 CNN 架构&lt;/p>
&lt;p>在 finetune 时，作者移除了 pre-trained 的 prediction head, 然后加入了一个 $D\times K$ 的 feed forward layer, 其中 $K$ 是分类的类别数。&lt;/p>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>作者使用的数据集如下所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Dataset&lt;/th>
&lt;th>classes&lt;/th>
&lt;th>images&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>ImageNet&lt;/td>
&lt;td>1K&lt;/td>
&lt;td>1.3M&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ImageNet-21K&lt;/td>
&lt;td>21K&lt;/td>
&lt;td>14M&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>JFT&lt;/td>
&lt;td>18K&lt;/td>
&lt;td>303M&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>作者在 ImageNet, CIFAR-10/100, Oxford-IIIT Pets, Oxford Flowers-102 这些数据集上进行评测。&lt;/p>
&lt;p>模型配置如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Layers&lt;/th>
&lt;th>$D$&lt;/th>
&lt;th>$D_{FFN}$&lt;/th>
&lt;th># heads&lt;/th>
&lt;th># params&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>ViT-Base&lt;/td>
&lt;td>12&lt;/td>
&lt;td>768&lt;/td>
&lt;td>3072&lt;/td>
&lt;td>12&lt;/td>
&lt;td>86M&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ViT-Large&lt;/td>
&lt;td>24&lt;/td>
&lt;td>1024&lt;/td>
&lt;td>4096&lt;/td>
&lt;td>16&lt;/td>
&lt;td>307M&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ViT-Huge&lt;/td>
&lt;td>32&lt;/td>
&lt;td>1280&lt;/td>
&lt;td>5120&lt;/td>
&lt;td>16&lt;/td>
&lt;td>632M&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>为了简便，作者在后续实验过程中，对模型名字进行了改写，比如 ViT-L/16 就代表了 ViT-Large model, 其对应的 patch size 为 16.&lt;/p>
&lt;p>ViT 的实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-vit/ViT-performance.png"
width="1159"
height="357"
loading="lazy"
alt="Performance of ViT"
class="gallery-image"
data-flex-grow="324"
data-flex-basis="779px"
>&lt;/p>
&lt;p>可以看到相比于 CNN 架构，ViT 所需要的训练时间更少，且效果更好。&lt;/p>
&lt;p>为了验证 ViT 模型具有更高的训练效率，作者在不同的数据集上进行的实验，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-vit/ViT-transfer-performance.png"
width="545"
height="387"
loading="lazy"
alt="Transfer to ImageNet"
class="gallery-image"
data-flex-grow="140"
data-flex-basis="337px"
>&lt;/p>
&lt;p>可以看到，当数据集比较小时，大部分 ViT 模型表现都比 ResNet 差，但是当数据集规模增加之后，ViT 的表现逐渐超过了 ResNet. 并且，随着数据集规模的增加，大模型的表现也比小模型好。&lt;/p>
&lt;p>作者进一步在四个不同大小的随机数据集上进行了验证，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-vit/ViT-linear-few-shot-performance.png"
width="551"
height="384"
loading="lazy"
alt="Linear few-shot evaluation on ImageNet v.s. pre-training size"
class="gallery-image"
data-flex-grow="143"
data-flex-basis="344px"
>&lt;/p>
&lt;p>实验结果显示，inductive bias 在小规模数据集上比较有效，但是当数据集规模上去之后，直接从数据集中学习相关的 pattern 更加高效和有效。&lt;/p>
&lt;p>由于不同的模型参数可能不太一致，为了更加公平地对比，作者使用算力来进行对比，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-vit/ViT-cost-vs-performance.png"
width="1050"
height="457"
loading="lazy"
alt="Performance v.s. cost for different architectures"
class="gallery-image"
data-flex-grow="229"
data-flex-basis="551px"
>&lt;/p>
&lt;p>实验结果显示，ViT 的训练效率比 ResNet 更高，其次，hybrid model 在算力比较小的时候，其表现比 ViT 更好，但是当算力提升之后，两者表现差不多。最后，ViT 随着算力的提升，还有进一步提升的空间。&lt;/p>
&lt;p>作者还对 ViT 的 attention 进行了进一步的分析，结果有以下几点：&lt;/p>
&lt;ol>
&lt;li>row-column structure 很明显，同一行同一列的 patches 有比较相似的 embedding&lt;/li>
&lt;li>ViT 学习到的 position embedding 对于比较近的 patches 其相似度也更高&lt;/li>
&lt;li>对于 attention 来说，某些 heads 在 early layer 就已经开始关注全局信息，并且随着 layer 的提升，attention distance 也在提升，说明模型关注全局信息的能力逐渐增强&lt;/li>
&lt;/ol>
&lt;p>作者尝试了对 Transformer 进行 scaling up, 结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-vit/ViT-scaling.png"
width="1117"
height="385"
loading="lazy"
alt="Scaling of ViT"
class="gallery-image"
data-flex-grow="290"
data-flex-basis="696px"
>&lt;/p>
&lt;p>实验结果显示，使用更多的 layer 可以有效提高模型的表现，但是当 layer 数大于 16 之后，其提升有限。另一方面，提高宽度对模型表现影响不大。作者发现，降低 patch size 也可以提高模型的表现。&lt;/p>
&lt;p>作者还对比了不同类型的 position encoding, 结果发现，使用/不使用 position encoding 对模型表现影响非常大，但是不同的 position encoding 总体来说表现差别不是很大。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 ViT, 一个基于 Transformer 架构的图像识别模型，作者通过在大规模数据集上进行训练验证了 ViT 架构的有效性。&lt;/p>
&lt;p>作者发现目前还存在如下挑战：&lt;/p>
&lt;ol>
&lt;li>如何使用自监督的方式进行训练，比如类似 BERT 的 masked patch prediction&lt;/li>
&lt;li>将 ViT 应用于其他的视觉任务，比如检测和分割&lt;/li>
&lt;li>进一步 scaling ViT&lt;/li>
&lt;/ol>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://openreview.net/forum?id=YicbFdNTTy" target="_blank" rel="noopener"
>openreview&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on CoMP</title><link>https://maosong.website/p/notes-on-comp/</link><pubDate>Thu, 04 Dec 2025 10:58:30 +0800</pubDate><guid>https://maosong.website/p/notes-on-comp/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者首先提到已有的 vision foundation model (VFM) 存在两个问题：&lt;/p>
&lt;ol>
&lt;li>不能处理动态分辨率图片输入，尽管我们可以使用 Bilinear interpolation 和 multi-resolution training 等方法，但是模型对于动态分辨率图片输入处理能力仍然不足&lt;/li>
&lt;li>VFM 和 LLM 之间存在 representation gap&lt;/li>
&lt;/ol>
&lt;p>针对这两个问题，作者提出了 CoMP, 一个 continual pre-training pipeline, CoMP 包含两个模块：&lt;/p>
&lt;ol>
&lt;li>C-RoPE, 一个针对 VFM 的 continual RoPE, 用于帮助 VFM 处理动态分辨率图片输入&lt;/li>
&lt;li>Alignment Loss, 用于对齐 VFM 和 LLM 的 representation&lt;/li>
&lt;/ol>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;p>CoMP 整体的架构图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-comp/CoMP-overall-architecture.png"
width="868"
height="454"
loading="lazy"
alt="Overview of CoMP"
class="gallery-image"
data-flex-grow="191"
data-flex-basis="458px"
>&lt;/p>
&lt;p>可以看到，CoMP 本质上就是一个多模态大模型，知识我们训练的目标为 VFM&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-comp/CoMP-modules-architecture.png"
width="977"
height="461"
loading="lazy"
alt="C-RoPE and Alignment Loss"
class="gallery-image"
data-flex-grow="211"
data-flex-basis="508px"
>&lt;/p>
&lt;h3 id="c-rope">&lt;a href="#c-rope" class="header-anchor">&lt;/a>C-RoPE
&lt;/h3>&lt;p>C-RopE 的核心思想是结合绝对位置编码以及相对位置编码来使得 pre-trained ViT 可以接受任意精度图片输入，对于输入图片 $X_V\in\mathbb{R}^{H\times W}$, 首先经过 patchify 得到 $N=HW/P^2$ 个 patch, 这里 $P$ 是 patch size, 每个 patch 大小为 $x_p\in\mathbb{R}^{N\times (P^2\cdot C)}$, $C$ 是 channels. 然后 VFM 每一个 layer 的计算过程为&lt;/p>
$$
\begin{aligned}
z_0 &amp;= [x_p^1E;\dots;x_p^NE] + \mathrm{Int}(E_{pos})\\
q_i,k_i,v_i &amp;= \mathrm{Proj}_q(z_i), \mathrm{Proj}_k(z_i), \mathrm{Proj}_v(z_i)\\
y_i &amp;= z_i + \mathrm{Proj}_o(\mathrm{Softmax}\left((Rq_i)^T(Rk_i) / D_v\right)v_i)\\
z_{i+1} &amp;= y_i + \mathrm{FFN}(y_i)
\end{aligned}
$$&lt;p>这里 $E\in\mathbb{E}^{P^2\cdot C\times D_v}$, $E_{pos}\in\mathbb{R}^{N\times D_V}$ 分别是 patch embedding 和 learnable position embedding, $\mathrm{Int}(\cdot)$ 是 bilinear interpolation.&lt;/p>
&lt;p>相比于 &lt;a class="link" href="https://maosong.website/p/notes-on-vit/" target="_blank" rel="noopener"
>ViT&lt;/a>, C-RoPE 做出了两点改动：&lt;/p>
&lt;ol>
&lt;li>使用了 Interpolation 来支持动态分辨率图片输入&lt;/li>
&lt;li>使用了 RoPE 来高效利用位置信息&lt;/li>
&lt;/ol>
&lt;h3 id="text-supervised-generative-pre-training">&lt;a href="#text-supervised-generative-pre-training" class="header-anchor">&lt;/a>Text-supervised Generative Pre-training
&lt;/h3>&lt;p>作者还是用了 LLM 的 cross-entropy loss 来进行对齐，text decoding loss 定义为&lt;/p>
$$
\mathcal{L}_{dec} = -\frac1T\sum_{i=V+1}^{V+T}\log P(X_i\mid X_{&lt;i}, H_v)
$$&lt;p>其中 $H_v=\mathcal{F}_{\psi}(\mathcal{V}(X_V))$ 是视觉特征，$T, V$ 分别代表了文本 token 和视觉 token 个数&lt;/p>
&lt;h3 id="vision-language-representation-alignment">&lt;a href="#vision-language-representation-alignment" class="header-anchor">&lt;/a>Vision-language Representation Alignment
&lt;/h3>&lt;p>text-decoding loss 可以对其视觉特征和文本特征，但是对于使用自监督训练方式的 VLM, 比如 DINOv2, 其预训练目标与 text-decoding loss 之间存在较大 gap, 为了解决这个问题，作者提出了 alignment loss.&lt;/p>
&lt;p>具体做法就是先计算出文本和视觉特征：&lt;/p>
$$
F_v = \mathrm{Pool}(H_v), F_t = \mathrm{Pool}(P_{\theta}(X_t))
$$&lt;p>然后作者将 $F_v, F_t$ 映射到语言空间，&lt;/p>
$$
C_v = W^TF_v, C_t=W^TF_t
$$&lt;p>这里 $W\in\mathbb{R}^{D_t\times K}$, $D_t, K$ 分别为 LLM 的 hidden size 以及 vocabulary size.&lt;/p>
&lt;p>接下来作者使用了 iterative Sinkhorn-Knopp 算法来归一化 $C_t$&lt;/p>
$$
p_t=\mathrm{Diag}(u_W)\exp(\frac{C_t}{\epsilon})\mathrm{Diag}(v)
$$&lt;p>这里 $u_W\in\mathbb{R}^K$ 是 words 的 prior marginal distribution,$v\in\mathbb{R}^B$ 是 renormalization vector&lt;/p>
&lt;p>最终 alignment loss 定义为&lt;/p>
$$
\mathcal{L}_{align} = -p_t\log p_v
$$&lt;p>这里 $p_v=\mathrm{softmax}(C_v)$, 作者对 LLM 使用了 stop gradient 操作，仅训练 VFM&lt;/p>
&lt;h3 id="training">&lt;a href="#training" class="header-anchor">&lt;/a>Training
&lt;/h3>&lt;p>CoMP 训练分为三个阶段：&lt;/p>
&lt;ul>
&lt;li>Stage 1: warming up, 仅训练 adapter&lt;/li>
&lt;li>Stage 2: 所有模型参数参与训练，使用了 RoPE 2D 和高精度图片输入&lt;/li>
&lt;li>Stage 3: instruction tuning, 使用 RoPE 2D 和动态分辨率图片输入&lt;/li>
&lt;/ul>
&lt;p>训练的损失为&lt;/p>
$$
\mathcal{L} = \begin{cases}
\mathcal{L}_{dec} + \alpha \mathcal{L}_{align}, &amp;\text{Stage 1 and Stage 2}\\
\mathcal{L}_{dec}, &amp;\text{Stage 3}
\end{cases}
$$&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>作者基于 CLIP, SigLIP, SigLip-2, DINOv2, AIMv2 等 VFM 进行了实验，主要结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-comp/CoMP-performance.png"
width="1153"
height="499"
loading="lazy"
alt="Performance of CoMP on multimodal understanding tasks"
class="gallery-image"
data-flex-grow="231"
data-flex-basis="554px"
>&lt;/p>
&lt;p>作者还对 training recipe 进行的消融实验，结果如下，可以看到，RoPE-2D 对模型表现提升最大，数据量和动态分辨率图片输入次之&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-comp/CoMP-trianing-ablation.png"
width="1124"
height="400"
loading="lazy"
class="gallery-image"
data-flex-grow="281"
data-flex-basis="674px"
>&lt;/p>
&lt;p>作者还对 C-RoPE 以及 alignment loss 进行的消融实验，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-comp/CoMP-ablation-performance.png"
width="1164"
height="365"
loading="lazy"
alt="Ablation on C-RoPE and aligment loss"
class="gallery-image"
data-flex-grow="318"
data-flex-basis="765px"
>&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 CoMP，一个 continual pre-training VFM 的方法，通过 C-RoPE 以及 alignment loss, 作者提高了 VFM 在 MLLM 中的表现&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2503.18931" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>