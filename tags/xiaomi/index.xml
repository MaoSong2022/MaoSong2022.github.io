<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Xiaomi on Mao Song(毛松)'s Homepage</title><link>https://maosong2022.github.io/tags/xiaomi/</link><description>Recent content in Xiaomi on Mao Song(毛松)'s Homepage</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 05 Jun 2025 10:51:43 +0800</lastBuildDate><atom:link href="https://maosong2022.github.io/tags/xiaomi/index.xml" rel="self" type="application/rss+xml"/><item><title>Notes on MiMo-VL</title><link>https://maosong2022.github.io/p/notes-on-mimo-vl/</link><pubDate>Thu, 05 Jun 2025 10:51:43 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-mimo-vl/</guid><description>&lt;h1 id="介绍">介绍
&lt;/h1>&lt;p>小米在5月30号发布了MiMo-VL 7B多模态推理大模型，模型包括SFT和RL两个版本，作者在技术报告里讲解了MiMo-VL的架构，预训练和后训练过程，最后对MiMo-VL的性能进行了评测&lt;/p>
&lt;h1 id="架构">架构
&lt;/h1>&lt;p>MiMo-VL 7B的架构是一个标准的 &amp;ldquo;ViT-MLP-LLM&amp;quot;的架构，其中：&lt;/p>
&lt;ul>
&lt;li>ViT使用了Qwen2.5-VL的Qwen2.5-ViT&lt;/li>
&lt;li>MLP是一个两层的SwiGLU MLP Layer&lt;/li>
&lt;li>LLM是之前发布的MiMo-7B&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-mimo-vl/architecture.png"
width="1237"
height="798"
srcset="https://maosong2022.github.io/p/notes-on-mimo-vl/architecture_hu14459510433823787530.png 480w, https://maosong2022.github.io/p/notes-on-mimo-vl/architecture_hu17770933799094781156.png 1024w"
loading="lazy"
alt="Architecture of MiMo-VL 7B"
class="gallery-image"
data-flex-grow="155"
data-flex-basis="372px"
>&lt;/p>
&lt;h1 id="预训练">预训练
&lt;/h1>&lt;h2 id="数据">数据
&lt;/h2>&lt;p>预训练阶段一共使用了2.4T的token&lt;/p>
&lt;ul>
&lt;li>Image caption data: 使用了rewrite caption等方法提升和过滤数据，作者还基于MetaCLIP来降低数据的不平衡性&lt;/li>
&lt;li>Interleaved data: 主要从webpage, books, papers中提取，最后基于relevance, complementarity, balance of information density来保证数据的质量&lt;/li>
&lt;li>OCR and grounding data: OCR数据包括文档，表格，数学公式等。grounding data包括单物体和多物体&lt;/li>
&lt;li>video data: rewrite caption, caption有对应的时间戳，加入了video analysis数据&lt;/li>
&lt;li>GUI data: 覆盖mobile, web, desktop三种场景, 加入了GUI grounding和GUI action数据&lt;/li>
&lt;li>Synthetic reasoning data:使用一个reasoning model来生成带有思考过程的reasoning数据&lt;/li>
&lt;/ul>
&lt;h2 id="训练">训练
&lt;/h2>&lt;p>训练有四个阶段，基本上和之前的模型一致，先训练MLP，然后解冻ViT，再训练全部参数，最后扩展模型的上下文。训练过程如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-mimo-vl/pretraining.png"
width="1339"
height="570"
srcset="https://maosong2022.github.io/p/notes-on-mimo-vl/pretraining_hu9951247225304577295.png 480w, https://maosong2022.github.io/p/notes-on-mimo-vl/pretraining_hu7419219620267078791.png 1024w"
loading="lazy"
alt="Pretraining of MiMo-VL 7B"
class="gallery-image"
data-flex-grow="234"
data-flex-basis="563px"
>&lt;/p>
&lt;h1 id="后训练">后训练
&lt;/h1>&lt;p>作者提出了Mixed On-policy Reinforcement Learning (MORL)框架来把RLVR和RLHF结合起来&lt;/p>
&lt;h2 id="数据-1">数据
&lt;/h2>&lt;p>RLVR的数据包括:&lt;/p>
&lt;ol>
&lt;li>visual reasoning, 80K数据，使用rule-based math-verify library来评估&lt;/li>
&lt;li>text reasoning, 使用了MiMo的数学推理数据，评估方式与visual reasoning一致&lt;/li>
&lt;li>Image grounding, 包括general and GUI grounding任务，使用GIoU来计算奖励&lt;/li>
&lt;li>Visual Counting, 使用准确率来打分&lt;/li>
&lt;li>Temporal Video Grounding, temporal video grounding任务，使用IoU来计算奖励&lt;/li>
&lt;/ol>
&lt;p>RLHF的数据包括中英文的query，作者使用MiMo-VL-7B和其他模型来采样，然后使用一个advanced VLM来排序.
RLHF的reward model使用Breadley-Terry model作为训练目标，text-only reward由MiMo-7B初始化得到，多模态reward model由MiMo-VL-7B初始化得到&lt;/p>
&lt;h2 id="训练-1">训练
&lt;/h2>&lt;p>训练过程就是将RLVR和RLHF放在一起进行训练，作者使用了MiMo的seamless rollout engine.&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-mimo-vl/RL_training.png"
width="1328"
height="466"
srcset="https://maosong2022.github.io/p/notes-on-mimo-vl/RL_training_hu16717293030641612071.png 480w, https://maosong2022.github.io/p/notes-on-mimo-vl/RL_training_hu14101128257135095160.png 1024w"
loading="lazy"
alt="RL Training"
class="gallery-image"
data-flex-grow="284"
data-flex-basis="683px"
>&lt;/p>
&lt;p>训练目标与GRPO一致，但是本文中作者使用了on-policy的训练方式。&lt;/p>
&lt;h1 id="评估">评估
&lt;/h1>&lt;p>作者评估了MiMo-VL-7B的通用能力，reasoning能力，GUI交互理解能力，最后还给出了MiMo-VL-7B在ChatbotArena上的排名&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-mimo-vl/Performance.png"
width="1367"
height="688"
srcset="https://maosong2022.github.io/p/notes-on-mimo-vl/Performance_hu14893086811555912207.png 480w, https://maosong2022.github.io/p/notes-on-mimo-vl/Performance_hu14858348037266394856.png 1024w"
loading="lazy"
alt="Performance of MiMo-VL-7B"
class="gallery-image"
data-flex-grow="198"
data-flex-basis="476px"
>&lt;/p>
&lt;h2 id="ablation-study">Ablation study
&lt;/h2>&lt;p>作者首先探究了在预训练阶段加入Long CoT数据对模型表现的影响，结果发现模型正在MMMU, OlympiadBench等benchmark上都有了提升&lt;/p>
&lt;p>作者还比较了原始的GRPO和本文中使用的on-policy版本的GRPO，结果发现on-policy版本的GRPO效果更好&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-mimo-vl/on-policy-GRPO-ablation.png"
width="1368"
height="843"
srcset="https://maosong2022.github.io/p/notes-on-mimo-vl/on-policy-GRPO-ablation_hu2940336130466218128.png 480w, https://maosong2022.github.io/p/notes-on-mimo-vl/on-policy-GRPO-ablation_hu1129411055183592852.png 1024w"
loading="lazy"
alt="Ablation of on-policy GRPO"
class="gallery-image"
data-flex-grow="162"
data-flex-basis="389px"
>&lt;/p>
&lt;p>最后，作者探讨了一下将不同任务放在一起训练导致的互相干扰问题，这个在MiMo里也提到过，核心问题就是有的任务是short CoT的，有的任务是Long CoT的，如何让模型根据任务来选择不同的reasoning length是一个需要探讨的问题，这个问题在&lt;a class="link" href="https://maosong.website/p/notes-on-vc-ppo/" target="_blank" rel="noopener"
>VC-PPO&lt;/a>里进行了初步的探讨。&lt;/p>
&lt;h1 id="结论">结论
&lt;/h1>&lt;p>与MiMo一样，MiMo-VL-7B通过在预训练阶段加入reasoning data，来提高模型的reasoning能力，并指出模型可以通过这个方式来媲美其他模型的表现&lt;/p>
&lt;h1 id="参考文献">参考文献
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://github.com/XiaomiMiMo/MiMo-VL" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>