<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Xiaomi on Mao Song(毛松)'s Homepage</title><link>https://maosong.website/tags/xiaomi/</link><description>Recent content in Xiaomi on Mao Song(毛松)'s Homepage</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Tue, 15 Jul 2025 17:59:50 +0800</lastBuildDate><atom:link href="https://maosong.website/tags/xiaomi/index.xml" rel="self" type="application/rss+xml"/><item><title>Notes on MiMo-VL</title><link>https://maosong.website/p/notes-on-mimo-vl/</link><pubDate>Thu, 05 Jun 2025 10:51:43 +0800</pubDate><guid>https://maosong.website/p/notes-on-mimo-vl/</guid><description>&lt;h1 id="介绍">&lt;a href="#%e4%bb%8b%e7%bb%8d" class="header-anchor">&lt;/a>介绍
&lt;/h1>&lt;p>小米在5月30号发布了MiMo-VL 7B多模态推理大模型，模型包括SFT和RL两个版本，作者在技术报告里讲解了MiMo-VL的架构，预训练和后训练过程，最后对MiMo-VL的性能进行了评测&lt;/p>
&lt;h1 id="架构">&lt;a href="#%e6%9e%b6%e6%9e%84" class="header-anchor">&lt;/a>架构
&lt;/h1>&lt;p>MiMo-VL 7B的架构是一个标准的 &amp;ldquo;ViT-MLP-LLM&amp;quot;的架构，其中：&lt;/p>
&lt;ul>
&lt;li>ViT使用了Qwen2.5-VL的Qwen2.5-ViT&lt;/li>
&lt;li>MLP是一个两层的SwiGLU MLP Layer&lt;/li>
&lt;li>LLM是之前发布的MiMo-7B&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-mimo-vl/architecture.png"
width="1237"
height="798"
loading="lazy"
alt="Architecture of MiMo-VL 7B"
class="gallery-image"
data-flex-grow="155"
data-flex-basis="372px"
>&lt;/p>
&lt;h1 id="预训练">&lt;a href="#%e9%a2%84%e8%ae%ad%e7%bb%83" class="header-anchor">&lt;/a>预训练
&lt;/h1>&lt;h2 id="数据">&lt;a href="#%e6%95%b0%e6%8d%ae" class="header-anchor">&lt;/a>数据
&lt;/h2>&lt;p>预训练阶段一共使用了2.4T的token&lt;/p>
&lt;ul>
&lt;li>Image caption data: 使用了rewrite caption等方法提升和过滤数据，作者还基于MetaCLIP来降低数据的不平衡性&lt;/li>
&lt;li>Interleaved data: 主要从webpage, books, papers中提取，最后基于relevance, complementarity, balance of information density来保证数据的质量&lt;/li>
&lt;li>OCR and grounding data: OCR数据包括文档，表格，数学公式等。grounding data包括单物体和多物体&lt;/li>
&lt;li>video data: rewrite caption, caption有对应的时间戳，加入了video analysis数据&lt;/li>
&lt;li>GUI data: 覆盖mobile, web, desktop三种场景, 加入了GUI grounding和GUI action数据&lt;/li>
&lt;li>Synthetic reasoning data:使用一个reasoning model来生成带有思考过程的reasoning数据&lt;/li>
&lt;/ul>
&lt;h2 id="训练">&lt;a href="#%e8%ae%ad%e7%bb%83" class="header-anchor">&lt;/a>训练
&lt;/h2>&lt;p>训练有四个阶段，基本上和之前的模型一致，先训练MLP，然后解冻ViT，再训练全部参数，最后扩展模型的上下文。训练过程如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-mimo-vl/pretraining.png"
width="1339"
height="570"
loading="lazy"
alt="Pretraining of MiMo-VL 7B"
class="gallery-image"
data-flex-grow="234"
data-flex-basis="563px"
>&lt;/p>
&lt;h1 id="后训练">&lt;a href="#%e5%90%8e%e8%ae%ad%e7%bb%83" class="header-anchor">&lt;/a>后训练
&lt;/h1>&lt;p>作者提出了Mixed On-policy Reinforcement Learning (MORL)框架来把RLVR和RLHF结合起来&lt;/p>
&lt;h2 id="数据-1">&lt;a href="#%e6%95%b0%e6%8d%ae-1" class="header-anchor">&lt;/a>数据
&lt;/h2>&lt;p>RLVR的数据包括:&lt;/p>
&lt;ol>
&lt;li>visual reasoning, 80K数据，使用rule-based math-verify library来评估&lt;/li>
&lt;li>text reasoning, 使用了MiMo的数学推理数据，评估方式与visual reasoning一致&lt;/li>
&lt;li>Image grounding, 包括general and GUI grounding任务，使用GIoU来计算奖励&lt;/li>
&lt;li>Visual Counting, 使用准确率来打分&lt;/li>
&lt;li>Temporal Video Grounding, temporal video grounding任务，使用IoU来计算奖励&lt;/li>
&lt;/ol>
&lt;p>RLHF的数据包括中英文的query，作者使用MiMo-VL-7B和其他模型来采样，然后使用一个advanced VLM来排序.
RLHF的reward model使用Breadley-Terry model作为训练目标，text-only reward由MiMo-7B初始化得到，多模态reward model由MiMo-VL-7B初始化得到&lt;/p>
&lt;h2 id="训练-1">&lt;a href="#%e8%ae%ad%e7%bb%83-1" class="header-anchor">&lt;/a>训练
&lt;/h2>&lt;p>训练过程就是将RLVR和RLHF放在一起进行训练，作者使用了MiMo的seamless rollout engine.&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-mimo-vl/RL_training.png"
width="1328"
height="466"
loading="lazy"
alt="RL Training"
class="gallery-image"
data-flex-grow="284"
data-flex-basis="683px"
>&lt;/p>
&lt;p>训练目标与GRPO一致，但是本文中作者使用了on-policy的训练方式。&lt;/p>
&lt;h1 id="评估">&lt;a href="#%e8%af%84%e4%bc%b0" class="header-anchor">&lt;/a>评估
&lt;/h1>&lt;p>作者评估了MiMo-VL-7B的通用能力，reasoning能力，GUI交互理解能力，最后还给出了MiMo-VL-7B在ChatbotArena上的排名&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-mimo-vl/Performance.png"
width="1367"
height="688"
loading="lazy"
alt="Performance of MiMo-VL-7B"
class="gallery-image"
data-flex-grow="198"
data-flex-basis="476px"
>&lt;/p>
&lt;h2 id="ablation-study">&lt;a href="#ablation-study" class="header-anchor">&lt;/a>Ablation study
&lt;/h2>&lt;p>作者首先探究了在预训练阶段加入Long CoT数据对模型表现的影响，结果发现模型正在MMMU, OlympiadBench等benchmark上都有了提升&lt;/p>
&lt;p>作者还比较了原始的GRPO和本文中使用的on-policy版本的GRPO，结果发现on-policy版本的GRPO效果更好&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-mimo-vl/on-policy-GRPO-ablation.png"
width="1368"
height="843"
loading="lazy"
alt="Ablation of on-policy GRPO"
class="gallery-image"
data-flex-grow="162"
data-flex-basis="389px"
>&lt;/p>
&lt;p>最后，作者探讨了一下将不同任务放在一起训练导致的互相干扰问题，这个在MiMo里也提到过，核心问题就是有的任务是short CoT的，有的任务是Long CoT的，如何让模型根据任务来选择不同的reasoning length是一个需要探讨的问题，这个问题在&lt;a class="link" href="https://maosong.website/p/notes-on-vc-ppo/" target="_blank" rel="noopener"
>VC-PPO&lt;/a>里进行了初步的探讨。&lt;/p>
&lt;h1 id="结论">&lt;a href="#%e7%bb%93%e8%ae%ba" class="header-anchor">&lt;/a>结论
&lt;/h1>&lt;p>与MiMo一样，MiMo-VL-7B通过在预训练阶段加入reasoning data，来提高模型的reasoning能力，并指出模型可以通过这个方式来媲美其他模型的表现&lt;/p>
&lt;h1 id="参考文献">&lt;a href="#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae" class="header-anchor">&lt;/a>参考文献
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://github.com/XiaomiMiMo/MiMo-VL" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>