<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>NeurIPS2024 on Mao Song(毛松)'s Homepage</title><link>https://maosong.website/tags/neurips2024/</link><description>Recent content in NeurIPS2024 on Mao Song(毛松)'s Homepage</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Thu, 04 Dec 2025 17:34:07 +0800</lastBuildDate><atom:link href="https://maosong.website/tags/neurips2024/index.xml" rel="self" type="application/rss+xml"/><item><title>Notes on DeepStack</title><link>https://maosong.website/p/notes-on-deepstack/</link><pubDate>Thu, 04 Dec 2025 17:32:41 +0800</pubDate><guid>https://maosong.website/p/notes-on-deepstack/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>已有的 MLLM 将视觉 token 作为一个 1d sequence, 输入给 LLM. 在本文中，作者将 visual token 注入到 LLM 的不同 layer 中来提高视觉信息的利用率&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepstack/DeepStack-architecture.png"
width="1149"
height="427"
loading="lazy"
alt="Architecture of DeepStack"
class="gallery-image"
data-flex-grow="269"
data-flex-basis="645px"
>&lt;/p>
&lt;p>首先，对于输入的图片 $I$, 我们将其分为高精度图片版本 $I_{high}$ 和低精度图片版本 $I_{low}$, $I_{low}$ 通过 vision encoder 和 MLP 得到对应的视觉 token $X_v$ 作为 LLM 的输入，然后在 LLM transformer block 的第 $i$ 层，其对应的视觉 token $X_{i,v}$ 会与 stack feature $X_{v}^i$ 相加，这里 $X_v^i$ 是对高精度图片输入的一个采样，即&lt;/p>
$$
X_v^i = \mathrm{Sampling2D}(\mathrm{MLP}(\mathrm{ViT}(I_{high})))
$$&lt;p>算法伪代码如下所示&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># H0: Input embeddings for LLM (Original inputs args for traditional LMM); # vis_pos: the location of visual tokens; &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># X, Xstack: Original visual tokens, Extra high-resolution visual token list; &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># lstart, n: Index of starting layer, and layer interval for stacking.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">H0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Xstack&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">lstart&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vis_pos&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">H&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">H0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">idx&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">TransformerLayer&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">enumerate&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">layers&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># DeepStack: &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">idx&lt;/span> &lt;span class="o">&amp;gt;=&lt;/span> &lt;span class="n">lstart&lt;/span> &lt;span class="o">&amp;amp;&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">idx&lt;/span> &lt;span class="err">−&lt;/span> &lt;span class="n">lstart&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">H&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">vis_pos&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">Xstack&lt;/span>&lt;span class="p">[(&lt;/span>&lt;span class="n">idx&lt;/span> &lt;span class="err">−&lt;/span> &lt;span class="n">lstart&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">//&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Original Transformer: &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">H&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">TransformerLayer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">H&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;h3 id="ablation-study">&lt;a href="#ablation-study" class="header-anchor">&lt;/a>Ablation Study
&lt;/h3>&lt;p>作者进一步验证了不同实验配置，结果发现在 early layer 进行 deepstack 效果最好，越往后效果越差&lt;/p>
&lt;p>作者还在 ViT 上应用了 DeepStack 策略，结果发现 ViT 的效果也有所提升&lt;/p>
&lt;p>作者还发现，模型表现提升是因为加入了 high-reoslution image token 信息&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 DeepStack, 一个提高 MLLM 中视觉信息利用率的方法，作者验证了这个方法的有效性。&lt;/p>
&lt;h2 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://openreview.net/forum?id=fXDpDzHTDV" target="_blank" rel="noopener"
>paper&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>