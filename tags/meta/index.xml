<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Meta on Mao Song(毛松)'s Homepage</title><link>https://maosong2022.github.io/tags/meta/</link><description>Recent content in Meta on Mao Song(毛松)'s Homepage</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 20 Aug 2025 10:57:03 +0800</lastBuildDate><atom:link href="https://maosong2022.github.io/tags/meta/index.xml" rel="self" type="application/rss+xml"/><item><title>Notes on StreamingLLM</title><link>https://maosong2022.github.io/p/notes-on-streamingllm/</link><pubDate>Wed, 20 Aug 2025 10:16:35 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-streamingllm/</guid><description>&lt;p>作者提出了 StreamingLLM, 一个基于 attention sink 来提高 sliding window attention 在超长上下文场景下表现的方法。&lt;/p>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>已有的基于 softmax attention 的架构的问题在于很难扩展到长上下文的场景，主要原因有两点：&lt;/p>
&lt;ol>
&lt;li>KV cache 会随着序列长度增加而商城，从而提高 decoding 的 latency&lt;/li>
&lt;li>序列长度超过预训练的 context length 之后，模型表现会急剧下降&lt;/li>
&lt;/ol>
&lt;p>为了解决这个问题，已有的方法可以分为三类：&lt;/p>
&lt;ol>
&lt;li>length extrapolation: 使用 RoPE 或者 AliBi 等方法来扩展 LLM 的 context length, 这类方法的问题是扩展的上下文长度仍然有限，对于 streaming 的场景作用有限&lt;/li>
&lt;li>context window attention: 扩展 LLM 的上下文长度，如 &lt;a class="link" href="https://maosong.website/p/notes-on-flashattention/" target="_blank" rel="noopener"
>flash attention&lt;/a> 等来降低 attention 的计算和内存开销。这类方法也是只在有限的上下文场景下 work&lt;/li>
&lt;li>Improving LLMs’ Utilization of Long Text: 更好利用长上下文的数据&lt;/li>
&lt;/ol>
&lt;p>基于已有的工作的发现，作者提出了本文研究的核心问题：&lt;/p>
&lt;blockquote>
&lt;p>如何在不损失模型表现和效率的情况下，提高模型在无限长上下文场景下的表现。&lt;/p>
&lt;/blockquote>
&lt;p>为了解决这个问题，作者首先分析了 sliding window attention 的不足，作者发现，sliding window attention 在超过 KV cache size 之后，表现也会急剧下降。作者通过实验发现，sliding window attention 表现急剧下降的原因在于 &lt;strong>attention sink&lt;/strong>, 也就是模型损失了对于初始 token 的关注，从而导致模型表现下降。&lt;/p>
&lt;p>基于 attention sink, 作者设计了 StreamingLLM, 用于提高 sliding window attention 在长上下文场景下的表现，结果发现，模型的表现有了大幅度的提升。&lt;/p>
&lt;p>作者还进一步在预训练阶段加入了 sink token 充当初始 token, 进一步提高模型的表现。&lt;/p>
&lt;h2 id="method">Method
&lt;/h2>&lt;h3 id="attention-sink">Attention Sink
&lt;/h3>&lt;p>作者首先探究了一下 softmax attention 以及 sliding window attention 性能下降的节点，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink_perplexity.png"
width="1155"
height="216"
srcset="https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink_perplexity_hu17107578125132293124.png 480w, https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink_perplexity_hu13497598808075063968.png 1024w"
loading="lazy"
alt="Perplexity of different attention with 20K tokens"
class="gallery-image"
data-flex-grow="534"
data-flex-basis="1283px"
>&lt;/p>
&lt;p>可以看到，softmax attention 性能急剧下降的节点为 pre-training 的 context length; 而 sliding window attention 性能急剧下降的节点为 KV cache size.&lt;/p>
&lt;p>接下来，作者分析了一下不同 layer 的 attention 分布情况，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-attention-logits-visualization.png"
width="1099"
height="256"
srcset="https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-attention-logits-visualization_hu10873929585851139529.png 480w, https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-attention-logits-visualization_hu18075330575893701465.png 1024w"
loading="lazy"
alt="Visualization of attention logits"
class="gallery-image"
data-flex-grow="429"
data-flex-basis="1030px"
>&lt;/p>
&lt;p>可以看到，初始的 2 层 layer 里 attention logits 的分布比较均匀。但是在后续的 layer 里，第一个 token 的权重都大幅度上升。&lt;/p>
&lt;p>作者分析原因认为，sliding window attention 在超过 KV cache size 之后性能急剧下降的主要原因是初始 token 不再参与 softmax 的计算，这导致了 softmax 的计算出现了比较大的变化，从而模型的表现开始下降。&lt;/p>
&lt;p>为了探究初始 token 对最终模型表现的影响因素是语义层面还是位置层面的，作者将初始的 token 替换为 &lt;code>\n&lt;/code>, 并比较了模型的表现，结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Llama-2-13B&lt;/th>
&lt;th>PPL (↓)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>0 + 1024(Window)&lt;/td>
&lt;td>5158.07&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4 + 1020&lt;/td>
&lt;td>5.40&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4&amp;quot;\n&amp;quot;+1020&lt;/td>
&lt;td>5.60&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，把初始的四个 token 替换为 &lt;code>\n&lt;/code>, 并不影响模型最终的表现，这说明是初始 token 的位置信息在发挥作用。&lt;/p>
&lt;p>作者接下来探究了一下模型架构的影响，实验结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Cache Config&lt;/th>
&lt;th>0+2048&lt;/th>
&lt;th>1+2047&lt;/th>
&lt;th>2+2046&lt;/th>
&lt;th>4+2044&lt;/th>
&lt;th>8+2040&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Falcon-7B&lt;/td>
&lt;td>17.90&lt;/td>
&lt;td>12.12&lt;/td>
&lt;td>12.12&lt;/td>
&lt;td>12.12&lt;/td>
&lt;td>12.12&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MPT-7B&lt;/td>
&lt;td>460.29&lt;/td>
&lt;td>14.99&lt;/td>
&lt;td>15.00&lt;/td>
&lt;td>14.99&lt;/td>
&lt;td>14.98&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Pythia-12B&lt;/td>
&lt;td>21.62&lt;/td>
&lt;td>11.95&lt;/td>
&lt;td>12.09&lt;/td>
&lt;td>12.09&lt;/td>
&lt;td>12.02&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Cache Config&lt;/td>
&lt;td>0+4096&lt;/td>
&lt;td>1+4095&lt;/td>
&lt;td>2+4094&lt;/td>
&lt;td>4+4092&lt;/td>
&lt;td>8+4088&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Llama-2-7B&lt;/td>
&lt;td>3359.95&lt;/td>
&lt;td>11.88&lt;/td>
&lt;td>10.51&lt;/td>
&lt;td>9.59&lt;/td>
&lt;td>9.54&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，不同的模型架构都存在这个问题，这说明 sliding window attention 的影响与架构无关。并且，作者认为，使用初始 4 个 token 就可以有效的避免模型的性能下降，进一步增加初始 token 的数量不会有进一步提升。&lt;/p>
&lt;p>作者分析 attention sink 出现的原因在于，&lt;/p>
&lt;ol>
&lt;li>初始的 token 对于后续所有的 token 都是可见的，因此其会携带一些信息&lt;/li>
&lt;li>在预训练阶段，模型并没有一个一致的初始 token 来标注起始信息，这导致模型会默认使用第一个 token 来储存一些信息。&lt;/li>
&lt;/ol>
&lt;p>为了解决这个问题，作者就提出了缓存初始 token 的方法，具体做法就是，在 sliding window attention 的基础上，我们还会加上初始 token 的信息，作者展示示意图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-KV_cache-rolling.png"
width="481"
height="190"
srcset="https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-KV_cache-rolling_hu14539682788052812184.png 480w, https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-KV_cache-rolling_hu8473910721752708950.png 1024w"
loading="lazy"
alt="Rolling KV cache of StramingLLM"
class="gallery-image"
data-flex-grow="253"
data-flex-basis="607px"
>&lt;/p>
&lt;p>也就是说，我们初始 token 始终会参与计算（论文中初始 token 数量为 4），然后我们会维持一个大小为 3 的 KV cache 队列来进行最终 sliding window attention 的计算，这样，每次计算 attention 的时候，我们就会使用 $# \text{iniital token} + # \text{sliding window token}$ 这么多的 token 来计算 attention. 作者对比了不同 attention 的计算方式，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-illustration-streamingLLM.png"
width="1148"
height="389"
srcset="https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-illustration-streamingLLM_hu10367217091090354661.png 480w, https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-illustration-streamingLLM_hu8009958211234850987.png 1024w"
loading="lazy"
alt="Illustration of StreamingLLM"
class="gallery-image"
data-flex-grow="295"
data-flex-basis="708px"
>&lt;/p>
&lt;p>前面是在 inference 阶段进行优化的，作者现在进一步探究在 pre-training 阶段加入 attention sink 参与训练对模型表现的影响。&lt;/p>
&lt;p>[[softmax-off-by-one]] 提出了我们应该加入一个 zero sink token, 其计算公式如下&lt;/p>
$$
\mathrm{softmax}_1(x)_i = \frac{\exp(x_i)}{1 + \sum_{j=1}^N \exp(x_j)}
$$&lt;p>这里 $x\in\mathbb{R}^N$ 是输入的序列。我们可以将 sink token 视为一个 key 以及 value 都是 0 向量的特殊 token.&lt;/p>
&lt;p>在本文中，作者使用了一个可学习的 sink token. 作者对比了原始 softmax attention, 使用 zero sink attention, learnable sink attention 三种方法的表现，结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Cache Config&lt;/th>
&lt;th>0+1024&lt;/th>
&lt;th>1+1023&lt;/th>
&lt;th>2+1022&lt;/th>
&lt;th>4+1020&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Vanilla&lt;/td>
&lt;td>27.87&lt;/td>
&lt;td>18.49&lt;/td>
&lt;td>18.05&lt;/td>
&lt;td>18.05&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Zero Sink&lt;/td>
&lt;td>29214&lt;/td>
&lt;td>19.90&lt;/td>
&lt;td>18.27&lt;/td>
&lt;td>18.01&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Learnable Sink&lt;/td>
&lt;td>1235&lt;/td>
&lt;td>18.01&lt;/td>
&lt;td>18.01&lt;/td>
&lt;td>18.02&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到 zero sink 仍然需要一部分初始 token 来维持模型的表现。作者在论文中推荐使用 learnable sink.&lt;/p>
&lt;h2 id="experiments">Experiments
&lt;/h2>&lt;p>作者首先验证了 StreamlingLLM 在不同架构上的表现，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-preplexsity-4M-tokens.png"
width="1151"
height="172"
srcset="https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-preplexsity-4M-tokens_hu10222675238153024805.png 480w, https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-preplexsity-4M-tokens_hu4302749122445536560.png 1024w"
loading="lazy"
alt="Perplexity of StreamingLLM on 4M tokens"
class="gallery-image"
data-flex-grow="669"
data-flex-basis="1606px"
>&lt;/p>
&lt;p>实验结果显示，StreamingLLM 可以扩展到 4M 的上下文&lt;/p>
&lt;p>接下来，作者探究了以下在 Pretraining 阶段加入 learnable sink token 对模型表现的影响，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-pre-training-sink-token.png"
width="358"
height="208"
srcset="https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-pre-training-sink-token_hu10575160091401072951.png 480w, https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-pre-training-sink-token_hu1459256266174936863.png 1024w"
loading="lazy"
alt="Pre-training loss curves of models w/ sink tokens"
class="gallery-image"
data-flex-grow="172"
data-flex-basis="413px"
>&lt;/p>
&lt;p>可以看到，加入 sink token 之后对模型的表现没有显著影响。并且，模型在下游任务上的表现与标准的 softmax attention 表现差不多。&lt;/p>
&lt;p>作者还对 StreamlingLLM 进行了可视化，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-visualization-attention-logits.png"
width="1153"
height="214"
srcset="https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-visualization-attention-logits_hu17917977003625206939.png 480w, https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-visualization-attention-logits_hu10512833175537702855.png 1024w"
loading="lazy"
alt="Visualization of attention logits with StreamingLLM"
class="gallery-image"
data-flex-grow="538"
data-flex-basis="1293px"
>&lt;/p>
&lt;p>作者进一步评估了 StreamingLLM 在下游任务上的表现，我们主要关注一下 ARC 上的表现，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-ARC-performance.png"
width="808"
height="255"
srcset="https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-ARC-performance_hu21210541812592330.png 480w, https://maosong2022.github.io/p/notes-on-streamingllm/attention_sink-ARC-performance_hu7689106914913301718.png 1024w"
loading="lazy"
alt="Accuracy on the ARC"
class="gallery-image"
data-flex-grow="316"
data-flex-basis="760px"
>&lt;/p>
&lt;p>可以看到，full attention 出现了 OOM error, 而 sliding window attention 虽然避免了 OOM 的问题，但是其表现非常差。而 StreamingLLM 则进一步提高了 Sliding Window attention 的表现。&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 StreamingLLM, 一个在 Sliding window attention 中加入 sink token 来避免超过 cache size 之后模型表现急剧下降的问题。作者详细介绍了 attention sink 现象以及解决方法。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://openreview.net/forum?id=NG7sS51zVF" target="_blank" rel="noopener"
>Efficient Streaming Language Models with Attention Sinks&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>