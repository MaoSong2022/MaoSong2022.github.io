<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Mixtral on Mao Song(毛松)'s Homepage</title><link>https://maosong2022.github.io/tags/mixtral/</link><description>Recent content in Mixtral on Mao Song(毛松)'s Homepage</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 01 Nov 2025 15:32:30 +0800</lastBuildDate><atom:link href="https://maosong2022.github.io/tags/mixtral/index.xml" rel="self" type="application/rss+xml"/><item><title>Mixstral 8x7B</title><link>https://maosong2022.github.io/p/mixstral-8x7b/</link><pubDate>Sat, 01 Nov 2025 15:32:30 +0800</pubDate><guid>https://maosong2022.github.io/p/mixstral-8x7b/</guid><description>&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>作者在本文中提出了 Mixtral 8x7B, 一个 MoE 模型，模型上下文为 32K. 作者还对模型进行 finetune 得到了 Mixtral 8x7B-Instruct, finetuning 包含 SFT 和 DPO 两个阶段。&lt;/p>
&lt;h2 id="method">Method
&lt;/h2>&lt;p>模型架构与 &lt;a class="link" href="https://maosong.website/p/mixstral-7b/" target="_blank" rel="noopener"
>Mistral-7B&lt;/a> 基本相同，参数如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Parameter&lt;/th>
&lt;th>Value&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>dim&lt;/code>&lt;/td>
&lt;td>4096&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>n_layers&lt;/code>&lt;/td>
&lt;td>32&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>head_dim&lt;/code>&lt;/td>
&lt;td>128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>hidden_dim&lt;/code>&lt;/td>
&lt;td>14336&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>n_heads&lt;/code>&lt;/td>
&lt;td>32&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>n_kv_heads&lt;/code>&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>window_size&lt;/code>&lt;/td>
&lt;td>4096&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>context_len&lt;/code>&lt;/td>
&lt;td>32768&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>vocab_size&lt;/code>&lt;/td>
&lt;td>32000&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>num_experts&lt;/code>&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>top_k_experts&lt;/code>&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>MoE 的架构与 &lt;a class="link" href="https://maosong.website/p/gshard/" target="_blank" rel="noopener"
>GShard&lt;/a> 基本一致&lt;/p>
&lt;h2 id="results">Results
&lt;/h2>&lt;p>作者探究了专家的 specialization, 结果有三点发现：&lt;/p>
&lt;ol>
&lt;li>不同专家对于不同 domain 的数据并没有出现 specialization&lt;/li>
&lt;li>在 math domain 上，专家的分布有一个明显的区别。&lt;/li>
&lt;li>连续的 token 往往会被分配到同一个专家上&lt;/li>
&lt;/ol>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者在本文提出了 Mistral 8x7B, 一个 MoE 大语言模型&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2401.04088" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Mixstral 7B</title><link>https://maosong2022.github.io/p/mixstral-7b/</link><pubDate>Sat, 01 Nov 2025 15:28:19 +0800</pubDate><guid>https://maosong2022.github.io/p/mixstral-7b/</guid><description>&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>作者主要强调 Mistral 7B 的表现超过了 LLaMA 2 7B 和 LLaMA 34B 的表现。&lt;/p>
&lt;p>Mistral 7B 主要使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-gqa/" target="_blank" rel="noopener"
>GQA&lt;/a> 以及 SWA 两个方法来加速推理和减少内存占用，进而提高 batch size 和 throughput.&lt;/p>
&lt;h2 id="method">Method
&lt;/h2>&lt;p>Mistral 7B 的模型配置如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Parameter&lt;/th>
&lt;th>Value&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>dim&lt;/code>&lt;/td>
&lt;td>4096&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>n_layers&lt;/code>&lt;/td>
&lt;td>32&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>head_dim&lt;/code>&lt;/td>
&lt;td>128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>hidden_dim&lt;/code>&lt;/td>
&lt;td>14336&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>n_heads&lt;/code>&lt;/td>
&lt;td>32&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>n_kv_heads&lt;/code>&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>window_size&lt;/code>&lt;/td>
&lt;td>4096&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>context_len&lt;/code>&lt;/td>
&lt;td>8192&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>vocab_size&lt;/code>&lt;/td>
&lt;td>32000&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>SWA 的示意图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/mixstral-7b/Mistral-7B-SWA.png"
width="1047"
height="387"
srcset="https://maosong2022.github.io/p/mixstral-7b/Mistral-7B-SWA_hu6372088832831988140.png 480w, https://maosong2022.github.io/p/mixstral-7b/Mistral-7B-SWA_hu8868497716526501398.png 1024w"
loading="lazy"
alt="Sliding Window Attention"
class="gallery-image"
data-flex-grow="270"
data-flex-basis="649px"
>&lt;/p>
&lt;p>对于第 $k$ 层，模型可以感知到 $W\times k$ 的 tokens, 进而可以在提高训练效率的同时保持模型的表现。作者发现，在 &lt;a class="link" href="https://maosong.website/p/notes-on-flashattention/" target="_blank" rel="noopener"
>flash attention&lt;/a> 中使用 SWA 之后，模型的效率提升了 2 倍左右。&lt;/p>
&lt;p>并且使用了 SWA 之后，我们的 kv cache 也就随之固定了，因此我们可以使用一个 rolling buff cache, 其大小为 $W$, 对于第 $i$ 个 token, 我们将其保存在 cache 中的第 $i % M$ 个位置。&lt;/p>
&lt;p>作者还进一步将 sequence 分割为多个 chunk, 每个 chunk 的大小都是 window size $M$, 这样在计算 attention 的时候，对于当前的 chunk, 我们使用 self-attention, 对于 cache 中的 attention, 我们使用 SWA, 然后对于 past token, 由于这部分不在 sliding window 内因此不参与计算&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/mixstral-7b/Mixtral-7B-prefill-and-chunking.png"
width="748"
height="307"
srcset="https://maosong2022.github.io/p/mixstral-7b/Mixtral-7B-prefill-and-chunking_hu1455448342047297388.png 480w, https://maosong2022.github.io/p/mixstral-7b/Mixtral-7B-prefill-and-chunking_hu8665872249214920433.png 1024w"
loading="lazy"
alt="Prefilling and Chunking"
class="gallery-image"
data-flex-grow="243"
data-flex-basis="584px"
>&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 Mistral 7B, 一个基于 GQA 和 SWA 的大语言模型。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2310.06825v1" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>