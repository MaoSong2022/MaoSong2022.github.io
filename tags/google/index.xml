<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Google on Mao Song(毛松)'s Homepage</title><link>https://maosong2022.github.io/tags/google/</link><description>Recent content in Google on Mao Song(毛松)'s Homepage</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 29 Oct 2025 11:22:39 +0800</lastBuildDate><atom:link href="https://maosong2022.github.io/tags/google/index.xml" rel="self" type="application/rss+xml"/><item><title>GShard</title><link>https://maosong2022.github.io/p/gshard/</link><pubDate>Wed, 29 Oct 2025 11:22:39 +0800</pubDate><guid>https://maosong2022.github.io/p/gshard/</guid><description>&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>作者认为，训练大规模的模型存在如下问题：&lt;/p>
&lt;ol>
&lt;li>缺乏有效的 Model parallelism 算法&lt;/li>
&lt;li>随着设备数的增加，训练时间与 model size 呈现超线性增长的关系&lt;/li>
&lt;li>(tensorflow) 在 nodes 数增多时，构建所需要的时间也大幅度增长&lt;/li>
&lt;li>在多个设备上 partition model 比较困难&lt;/li>
&lt;/ol>
&lt;p>作者在本文中构建了一个基于 sparse MoE 架构的 600B 模型。为了解决这些问题，作者作出了如下贡献：&lt;/p>
&lt;ol>
&lt;li>作者提出了基于 MoE 架构的模型，来减少计算和通信开销&lt;/li>
&lt;li>作者提出了 Gshard, 来自动化实现并行&lt;/li>
&lt;li>作者提出了 SPMD (single program multiple data) 来减少计算表示和编译的难度&lt;/li>
&lt;/ol>
&lt;h2 id="method">Method
&lt;/h2>&lt;p>架构如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/gshard/GShard-model-architecture.png"
width="1118"
height="713"
srcset="https://maosong2022.github.io/p/gshard/GShard-model-architecture_hu18193360422981598966.png 480w, https://maosong2022.github.io/p/gshard/GShard-model-architecture_hu1150641733643218422.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="156"
data-flex-basis="376px"
>&lt;/p>
&lt;p>其中激活专家个数为 2 个，MoE layer 和 FFN layer 交替出现。&lt;/p>
&lt;p>作者对 GATE 函数进行了如下优化：&lt;/p>
&lt;ol>
&lt;li>expert capacity. 每个 expert 都有一个处理 token 的上限值，超过该上限值之后 GATE 直接输出 0 进入下一层，假设 batch size 为 $N$, 专家个数为 $E$, 则该阈值定义为 $N/E$&lt;/li>
&lt;li>group dispatching. 将 token 拆分为 $G$ 个 group, 每个 group 并行处理，每个 group 里每个专家处理的 token 个数上限为 $N/(GE)$.&lt;/li>
&lt;li>Auxiliary loss. 作者使用了 &lt;a class="link" href="Load%20Balancing%20loss.md" >Load Balancing loss&lt;/a> 来实现负载均衡&lt;/li>
&lt;li>Random routing. 作者选取了 top-2 的专家，当第二个专家的权重太小是，作者直接忽略第二个专家，简化为选取 top-1 的专家&lt;/li>
&lt;/ol>
&lt;p>算法运行如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/gshard/GShard-expert-computation.png"
width="1124"
height="917"
srcset="https://maosong2022.github.io/p/gshard/GShard-expert-computation_hu10173219915397521593.png 480w, https://maosong2022.github.io/p/gshard/GShard-expert-computation_hu2905974338988159778.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="122"
data-flex-basis="294px"
>&lt;/p>
&lt;h2 id="parallel-implementation">Parallel Implementation
&lt;/h2>&lt;p>第一步是将算法转化为线性代数的方式，算法的代码如下所示&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">gates&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">softmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">einsum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;GSM, ME-&amp;gt;GSE&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">inputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">wg&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">combine_weights&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dispatch_mask&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Top2Gating&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">gates&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">dispatched_expert_inputs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">einsum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;GSEC, GSM-&amp;gt;EGCM&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dispatch_mask&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">reshaped_inputs&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">h&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">einsum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;EGCM, EMH-&amp;gt;EGCH&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dispatched_expert_inputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">wi&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">h&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">relu&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">h&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">expert_outputs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">einsum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;EGCH, EHM-&amp;gt;GECM&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">h&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">wo&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">outputs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">einsum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;GSEC, GECM-&amp;gt;GSM&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">combine_weights&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">expert_outputs&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>第二步是通过 API 来实现并行执行&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Partition inputs along group (G) dim. &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">inputs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">split&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">inputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">D&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Replicate the gating weights&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">wg&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">replicate&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">wg&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">gates&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">softmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">einsum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;GSM, ME-&amp;gt;GSE&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">inputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">wg&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">combine_weights&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dispatch_mask&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Top2Gating&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">gating_logits&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">dispatched_expert_inputs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">einsum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;GSEC, GSM-&amp;gt;EGCM&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dispatch_mask&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">reshaped_inputs&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Partition dispatched inputs along expert (E) dim.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">dispatched_expert_inputs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">split&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dispatched_expert_inputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">D&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">h&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">einsum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;EGCM, EMH-&amp;gt;EGCH&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dispatched_expert_inputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">wi&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>第三部就是基于 compiler infra 来基于 sharding annotation 自动化分割一个 computation graph.&lt;/p>
&lt;h2 id="experiments">Experiments
&lt;/h2>&lt;p>作者在机器翻译的任务上训练了若干模型，结果发现：&lt;/p>
&lt;ol>
&lt;li>层数更多的模型表现更好&lt;/li>
&lt;li>提高 expert capacity 有效提高模型的表现&lt;/li>
&lt;li>使用更多的 expert 可以在 high-resourced 任务上提高表现&lt;/li>
&lt;li>dense 模型相比于 MoE 模型拥有更强的迁移能力&lt;/li>
&lt;/ol>
&lt;p>从训练效率上来看&lt;/p>
&lt;ol>
&lt;li>层数更多的模型的 sample efficiency 也更高&lt;/li>
&lt;li>600B 的模型也可以在 4 天之内训练完毕&lt;/li>
&lt;/ol>
&lt;p>从内存使用效率上来看&lt;/p>
&lt;ol>
&lt;li>层数相同时，weight memory 以及 activation memory 不随专家个数增加而增加&lt;/li>
&lt;li>专家个数比较少（128）时，模型可以达到 roofline performance 的 $70%$, 专家个数比较多（2048）时，模型依然可以达到 roofline performance 的 $48%$.&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://maosong2022.github.io/p/gshard/GShard-execution-time-breakdown.png"
width="1064"
height="521"
srcset="https://maosong2022.github.io/p/gshard/GShard-execution-time-breakdown_hu4516039562048509286.png 480w, https://maosong2022.github.io/p/gshard/GShard-execution-time-breakdown_hu5230886350146491299.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="204"
data-flex-basis="490px"
>&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者探究了如何提高大规模模型的训练效率，作者提出了基于 MoE 架构的模型，然后作者还设计了 GShard, 一个自动化分割大规模模型的深度学习模块。实现结果发现，增加模型 size 可以提高模型的表现。作者还发现，SPMD 是一个更好的提高计算效率的方法。&lt;/p>
&lt;h2 id="reference">Reference
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2006.16668" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>ST-MoE</title><link>https://maosong2022.github.io/p/st-moe/</link><pubDate>Wed, 29 Oct 2025 11:19:37 +0800</pubDate><guid>https://maosong2022.github.io/p/st-moe/</guid><description>&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>已有的工作如 &lt;a class="link" href="https://maosong.website/p/GShard.md" target="_blank" rel="noopener"
>GShard&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/switch-transformer/" target="_blank" rel="noopener"
>Switch Transformer&lt;/a> 均验证了 MoE 模型的有效性，但是他们的问题在于训练不稳定。&lt;/p>
&lt;p>因此，在本文中，作者就提出了 ST-MoE 来解决这个问题。作者的主要贡献如下：&lt;/p>
&lt;ol>
&lt;li>探究了如何平衡模型的表现与训练稳定性&lt;/li>
&lt;li>提出了 router Z-loss 来解决训练的不稳定性&lt;/li>
&lt;li>探究了如何设定 MoE 模型 fine tuning 时的超参数&lt;/li>
&lt;li>对 MoE 的性质进行了分析&lt;/li>
&lt;/ol>
&lt;h2 id="training-stability">Training Stability
&lt;/h2>&lt;p>作者发现，直接训练 MoE 模型会面临训练不稳定性的问题，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/st-moe/ST-MoE-training-instabilities.png"
width="1040"
height="420"
srcset="https://maosong2022.github.io/p/st-moe/ST-MoE-training-instabilities_hu7326905665646279246.png 480w, https://maosong2022.github.io/p/st-moe/ST-MoE-training-instabilities_hu5976367835112795528.png 1024w"
loading="lazy"
alt="Training instabilities for sparse models."
class="gallery-image"
data-flex-grow="247"
data-flex-basis="594px"
>&lt;/p>
&lt;p>可以看到不同的实验，一次训练崩溃，一次训练正常。&lt;/p>
&lt;p>作者接下来探究了如何提高训练的稳定性，作者有三点发现：&lt;/p>
&lt;ol>
&lt;li>大多数方法都可以提高训练稳定性，但是也会是的模型表现更差&lt;/li>
&lt;li>router Z-loss 可以在不损失模型表现的情况下提高训练的稳定性&lt;/li>
&lt;li>multiplicative components 如 RMSNorm 等可以提高模型表现，但是训练稳定性更差&lt;/li>
&lt;/ol>
&lt;p>作者构建了一个 baseline 的 MoE 模型，总专家个数为 $N=32$, 激活专家个数为 $K=2$, Transformer block 按照 4 个 block 为一组，每组里包含一个 MoE layer. 为了避免初始化带来的误差，作者使用 6 个随机数种子进行训练。&lt;/p>
&lt;h3 id="multiplicative-components">Multiplicative Components
&lt;/h3>&lt;p>multiplicative components 指的是 GEGLU 和 RMSNorm 这些操作，其实验结果如下所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Method&lt;/th>
&lt;th>Fraction Stable&lt;/th>
&lt;th>Quality&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Baseline&lt;/td>
&lt;td>4/6&lt;/td>
&lt;td>$-1.755 \pm0.02$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Remove GEGLU&lt;/td>
&lt;td>3/3&lt;/td>
&lt;td>$-1.849 \pm0.02$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Remove RMS Norm. Scale Param&lt;/td>
&lt;td>3/3&lt;/td>
&lt;td>$-2.020 \pm0.06$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>结果发现，移除掉 multiplicative components 之后，模型训练变得稳定，但是效果也变差了。&lt;/p>
&lt;h3 id="adding-noise">Adding Noise
&lt;/h3>&lt;p>接下来作者尝试了 &lt;a class="link" href="https://maosong.website/p/switch-transformer/" target="_blank" rel="noopener"
>Switch Transformer&lt;/a> 中的 jitter noise 和 dropoout 方法，实验结果如下图所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Method&lt;/th>
&lt;th>Fraction Stable&lt;/th>
&lt;th>Quality&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Baseline&lt;/td>
&lt;td>4/6&lt;/td>
&lt;td>$-1.755 ± 0.02$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Input jitter ($10^{-2}$)&lt;/td>
&lt;td>3/3&lt;/td>
&lt;td>$-1.777 \pm 0.03$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Dropout (0.1)&lt;/td>
&lt;td>3/3&lt;/td>
&lt;td>$-1.822 \pm0.11$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>实验结果显示，加入噪声对模型的表现存在负面影响。&lt;/p>
&lt;h3 id="constraining-activations">Constraining Activations
&lt;/h3>&lt;p>作者接下来分析了以下 &lt;a class="link" href="https://maosong.website/p/switch-transformer/" target="_blank" rel="noopener"
>Switch Transformer&lt;/a> 中 router 存在的问题，作者发现尽管在 router 中使用 &lt;code>float32&lt;/code> 可以提高训练稳定性，但是这还不够，因此作者使用了如下的 router Z-loss:&lt;/p>
$$
\mathcal{L}_z(x) = \frac1B\sum_{i=1}^B\left(\log\sum_{j=1}^N e^{x_j^{(i)}}\right)^2
$$&lt;p>其中 $B, N$ 分别是 batch size 和专家个数，$x_j^{(i)}$ 代表了第 $j$ 个专家对 $i$ 个 token 的激活 logits. 通过增加这个 loss, 我们可以让 routing layer 的 logits 都在一个合理的范围内。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Method&lt;/th>
&lt;th>Fraction Stable&lt;/th>
&lt;th>Quality ($\uparrow$)&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Baseline&lt;/td>
&lt;td>$4/6$&lt;/td>
&lt;td>$-1.755 \pm 0.02$&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Update clipping (clip = 0.1)&lt;/td>
&lt;td>$3/3$&lt;/td>
&lt;td>$-4.206 \pm 0.17$&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Router Z-Loss&lt;/td>
&lt;td>$3/3$&lt;/td>
&lt;td>$-1.741 \pm 0.02$&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，加入 router Z-loss 之后，模型的稳定性和表现都有了提升。&lt;/p>
&lt;p>因此，在本文中，作者使用的损失函数为&lt;/p>
$$
\mathcal{L} = \mathcal{L}_{CE} + \alpha \mathcal{L}_B + \beta \mathcal{L}_z
$$&lt;p>其中 $\mathcal{L}_{CE}, \mathcal{L}_B$ 分别是 cross-entropy loss 和 Load Balancing loss, $\alpha,\beta$ 是对应 loss 的权重。&lt;/p>
&lt;h3 id="numerical-precision">Numerical Precision
&lt;/h3>&lt;p>接下来作者探究了数值精度对训练效率和稳定性的影响，作者发现使用低精度进行训练的优势有：&lt;/p>
&lt;ol>
&lt;li>通信开销更小&lt;/li>
&lt;li>计算消耗更小&lt;/li>
&lt;li>内存需求更小&lt;/li>
&lt;/ol>
&lt;p>但是低精度进行训练的问题是存在严重的 roundoff error, 其示意图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/st-moe/ST-MoE-numerical-precision-and-roundoff.png"
width="925"
height="249"
srcset="https://maosong2022.github.io/p/st-moe/ST-MoE-numerical-precision-and-roundoff_hu1402581372307421845.png 480w, https://maosong2022.github.io/p/st-moe/ST-MoE-numerical-precision-and-roundoff_hu14172820709588307084.png 1024w"
loading="lazy"
alt="Numerical precision formats and roundoff errors."
class="gallery-image"
data-flex-grow="371"
data-flex-basis="891px"
>&lt;/p>
&lt;h2 id="fine-tuning">Fine-tuning
&lt;/h2>&lt;p>作者在本节探究了如何 fine-tune 一个 MoE 模型。&lt;/p>
&lt;p>作者首先通过实验给出了 MoE 模型容易过拟合的结论，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/st-moe/ST-MoE-overfitting-of-MoE.png"
width="1138"
height="456"
srcset="https://maosong2022.github.io/p/st-moe/ST-MoE-overfitting-of-MoE_hu15053576913876449027.png 480w, https://maosong2022.github.io/p/st-moe/ST-MoE-overfitting-of-MoE_hu11759343686158783167.png 1024w"
loading="lazy"
alt="Sparse models are prone to overfit."
class="gallery-image"
data-flex-grow="249"
data-flex-basis="598px"
>&lt;/p>
&lt;p>可以看到，在两个人呢误伤，MoE 模型相比于 dense 模型都更容易过拟合。&lt;/p>
&lt;p>接下来作者探究了超参数特别是 batch size 和 learning rate 对模型表现的影响，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/st-moe/ST-MoE-hyperparameter-sensitivity.png"
width="1036"
height="416"
srcset="https://maosong2022.github.io/p/st-moe/ST-MoE-hyperparameter-sensitivity_hu2550488555205211608.png 480w, https://maosong2022.github.io/p/st-moe/ST-MoE-hyperparameter-sensitivity_hu9603778225196677401.png 1024w"
loading="lazy"
alt="Batch size and learning rate sensitivity"
class="gallery-image"
data-flex-grow="249"
data-flex-basis="597px"
>&lt;/p>
&lt;p>可以看到，与 dense 模型相反，MoE 模型需要更大的 batch size 以及更小的 learning rate.&lt;/p>
&lt;h2 id="design-sparse-models">Design Sparse Models
&lt;/h2>&lt;p>首先作者分析了专家个数 $N$ 的设置，作者认为当专家个数超过 $256$ 之后，带来的收益就微乎其微了。并且，当专家个数增加之后，虽然算力没有变化，但是通信开销以及计算优化会变得更加困难。作者还认为，在 TPU 上，每个 core 应该进放置一个 expert&lt;/p>
&lt;p>作者还对 capacity factor 进行了实验，实验结果显示，提升 capacity factor 可以提高模型的表现。但是同时也会带来计算开销，从而让模型训练更慢，实验结果如下图所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Train CF&lt;/th>
&lt;th>Step Time (s) ($\downarrow$)&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>ST-MoE-L&lt;/td>
&lt;td>1.25&lt;/td>
&lt;td>$2.397$&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ST-MoE-L&lt;/td>
&lt;td>2.0&lt;/td>
&lt;td>$2.447 (+7%)$&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ST-MoE-32B&lt;/td>
&lt;td>1.25&lt;/td>
&lt;td>$4.244$&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ST-MoE-32B&lt;/td>
&lt;td>2.0&lt;/td>
&lt;td>$4.819 (+14%)$&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>最终结论如下：&lt;/p>
&lt;blockquote>
&lt;ol>
&lt;li>使用 top-2 routing, 即激活 $K=2$ 个专家，capacity factor 设置为 $1.25$, 每个 core 上最多放置一个专家&lt;/li>
&lt;li>在评估时可以动态调整 capacity factor&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;h2 id="tracing-tokens-through-the-model">Tracing Tokens through the Model
&lt;/h2>&lt;p>在这一节里，作者分析了 MoE 模型专家的性质。&lt;/p>
&lt;p>首先，作者发现，每一层里，至少有一个专家对于 sentinel token 比较敏感。并且，encoder 里的专家 specialization 更强，比如某些专家会负责 punctuation, verbs, proper names 等&lt;/p>
&lt;p>接下来，作者发现，专家的 specialization 在 decoder 里几乎没有，作者认为这是因为 target token distribution 不同导致的，即：&lt;/p>
&lt;ol>
&lt;li>decoding 是只有一小部分 token 被一个专家处理&lt;/li>
&lt;li>decoding 过程中大部分 token 都是 sentinel token&lt;/li>
&lt;/ol>
&lt;p>因此，每个 group 相比于 encoder 来说通常只 cover 一小部分的 semantic space&lt;/p>
&lt;p>encoder 和 decoder 专家的 specialization 实验结果如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Layer 1&lt;/th>
&lt;th>Layer 2&lt;/th>
&lt;th>Layer 3&lt;/th>
&lt;th>Layer 4&lt;/th>
&lt;th>Layer 5&lt;/th>
&lt;th>Layer 6&lt;/th>
&lt;th>Uniform (32-experts)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Encoder&lt;/td>
&lt;td>2.2&lt;/td>
&lt;td>1.8&lt;/td>
&lt;td>1.6&lt;/td>
&lt;td>1.7&lt;/td>
&lt;td>1.7&lt;/td>
&lt;td>1.2&lt;/td>
&lt;td>3.5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Decoder&lt;/td>
&lt;td>3.4&lt;/td>
&lt;td>3.4&lt;/td>
&lt;td>3.4&lt;/td>
&lt;td>3.4&lt;/td>
&lt;td>3.4&lt;/td>
&lt;td>3.4&lt;/td>
&lt;td>3.5&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，decoder 的专家分布和均匀分布差不多，这说明 decoder 里的专家 specialization 更弱。&lt;/p>
&lt;p>作者还探究了不同专家对于不同语言的敏感程度，结果发现，专家对于不同语言并没有出现明显的 specialization 现象，作者分析原因认为，输入一般只含有一种语言，因此各个专家均需要去处理对应语言的 token, 这就导致了专家对于不同语种的数据处理基本上没有太大差别。&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>在本文中，作者提出了 ST-MoE 系列 MoE 大语言模型，作者通过实验优化了 MoE 模型训练的不稳定性问题。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2202.08906" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Switch Transformer</title><link>https://maosong2022.github.io/p/switch-transformer/</link><pubDate>Tue, 28 Oct 2025 09:38:12 +0800</pubDate><guid>https://maosong2022.github.io/p/switch-transformer/</guid><description>&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>&lt;a class="link" href="https://maosong.website/p/kaplan-scaling-law/" target="_blank" rel="noopener"
>Kaplan scaling law&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/chinchilla-scaling-law/" target="_blank" rel="noopener"
>Chinchilla scaling law&lt;/a> 已经给出了针对 dense transformer model 的 scaling law, 即在给定算力下如何找到最优的 model size 和 dataset size. 已有的工作如 T5 主要是扩展 dense model 来达到更高的表现, 但是他们的问题是对算力要求比较高。&lt;/p>
&lt;p>为了解决这个问题，作者尝试在不增加算力的情况下提升模型的参数量，为了达到这个目的，作者构建了基于 MoE 架构的模型 Switch Transformer.&lt;/p>
&lt;p>作者的主要贡献如下：&lt;/p>
&lt;ol>
&lt;li>提出了基于 MoE 架构的 Switch Transformer model&lt;/li>
&lt;li>探究了针对 MoE 架构的 scaling law&lt;/li>
&lt;li>将 MoE model 的能力蒸馏到 small dense model 里去&lt;/li>
&lt;li>若干提升训练效率和稳定性的技巧&lt;/li>
&lt;/ol>
&lt;h2 id="architecture">Architecture
&lt;/h2>&lt;p>Switch Transformer 的架构如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-Architecture.png"
width="1130"
height="571"
srcset="https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-Architecture_hu15565212143753599310.png 480w, https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-Architecture_hu6275654350693265782.png 1024w"
loading="lazy"
alt="Architecture of Switch Transformer"
class="gallery-image"
data-flex-grow="197"
data-flex-basis="474px"
>&lt;/p>
&lt;h3 id="moe">MoE
&lt;/h3>&lt;p>MoE 的定义见 &lt;a class="link" href="MoE%20tutorial.md" >MoE tutorial&lt;/a>, 我们假设有 $N$ 个专家，其中激活 $K$ 个专家。&lt;/p>
&lt;p>之前的工作认为我们只有在激活 $&amp;gt;2$ 个专家时，模型才能够比较好的训练，但是在本文中，作者决定只使用 1 个 expert, 也就是 $K=1$. 作者将激活一个专家的 layer 称为 &lt;strong>Switch layer&lt;/strong>.&lt;/p>
&lt;p>作者认为 Switch Layer 有三个优势：&lt;/p>
&lt;ol>
&lt;li>router computation 现在只需要将每个 token route 到 1 个 expert&lt;/li>
&lt;li>每个专家的 capacity 更小，负载更加均衡&lt;/li>
&lt;li>routing 的实现更简单，且通信开销也降低了&lt;/li>
&lt;/ol>
&lt;h3 id="efficient-sparse-routing">Efficient Sparse Routing
&lt;/h3>&lt;p>作者首先定义了&lt;strong>expert capacity&lt;/strong>, 也就是每个 expert 处理的 token 数量，其定义如下&lt;/p>
$$
\text{expert capacity} = \left(\frac{\text{tokens per batch}}{\text{number of experts}}\right) * \text{capacity factor}
$$&lt;p>其示意图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-expert-capacity.png"
width="1269"
height="479"
srcset="https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-expert-capacity_hu15802103571420683711.png 480w, https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-expert-capacity_hu1304176036435719856.png 1024w"
loading="lazy"
alt="Token routing dynamics"
class="gallery-image"
data-flex-grow="264"
data-flex-basis="635px"
>&lt;/p>
&lt;p>提升 capacity factor 可以减少 token overflow 的概率，但同时也会导致计算和内存的浪费。作者通过实验发现应该尽可能降低 dropped token 比例。&lt;/p>
&lt;p>为了平衡每个 expert 处理 token 的个数，作者设计了 load balancing loss, 见 &lt;a class="link" href="Load%20Balancing%20loss.md" >Load Balancing loss&lt;/a> 来要求每个 expert 处理的 token 数基本一致。&lt;/p>
&lt;h2 id="parallelism">Parallelism
&lt;/h2>&lt;p>作者在本节介绍了针对 MoE 模型的并行策略，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-Parallelism.png"
width="1268"
height="746"
srcset="https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-Parallelism_hu5410496410627554406.png 480w, https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-Parallelism_hu10984838773348886305.png 1024w"
loading="lazy"
alt="Data and weight partitioning strategies"
class="gallery-image"
data-flex-grow="169"
data-flex-basis="407px"
>&lt;/p>
&lt;p>这里，我们给定 notation 如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Term&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$B$&lt;/td>
&lt;td>Number of tokens in the batch.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$N$&lt;/td>
&lt;td>Number of total cores.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$n$&lt;/td>
&lt;td>Number of ways for data-parallelism sharding.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$m$&lt;/td>
&lt;td>Number of ways for model-parallelism sharding.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$E$&lt;/td>
&lt;td>Number of experts in Switch layers.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$C$&lt;/td>
&lt;td>Expert capacity, the batch size of each expert.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>作者将一个 Logical mesh 分为两个维度，一个是 data-parallel sharding, 用 $n$ 表示，另一个是 model-parallel sharding, 用 $m$ 表示，从而总的 cores 数为 $N=n\times m$.&lt;/p>
&lt;h3 id="data-parallelism">Data Parallelism
&lt;/h3>&lt;p>对于 data prallelism 来说，每个 core 上的模型是一样的，而一个 batch 的数据被切分为了 $B/n$ 份，此时 $n=N$, $m=1$. 这种并行策略的好处是仅在 forward 和 backward 完成之后才会进行一次通信。&lt;/p>
&lt;h3 id="model-parallelism">Model Parallelism
&lt;/h3>&lt;p>对于 model parallelism, 每个 core 上的数据都是全部数据（通过拷贝得到），模型被 shard 到所有 core 上，此时 $n=1,m=N$, 这种情况下，每次 forward 或者 backward，都需要进行 $N$ 次通信（假设我们使用 pipeline parallelism）， 这种并行策略的好处是每个 core 上的计算压力小了，但是缺点是通信开销多了。&lt;/p>
&lt;h3 id="model-and-data-parallelism">Model and Data Parallelism
&lt;/h3>&lt;p>第三种总策略是同时进行 model parallelism 和 data parallelism, 这种情况下，每个 core 上保存 $B/n$ 的数据以及 shard 为 $m$ 份的 sharding weight.&lt;/p>
&lt;h3 id="expert-and-data-parallelism">Expert and Data Parallelism
&lt;/h3>&lt;p>第四种策略是同时进行 expert parallelism 和 data parallelism, 作者在这里假设 $n=N$ 且 $E=n=N$, 也就是每个 core 上保留 $B/n$ 的数据, 然后还有一个对应的专家。&lt;/p>
&lt;p>首先，对于输入大小为 $[B,d]$ 的 tensor, 我们会进行拆分得到大小为 $[n, B/n, d]$ 的 tensor, 代表 $n$ 个设备上分别存储 $[B/n, d]$ 的数据，首先我们计算路由权重，得到&lt;/p>
$$
[n,B/n, d]\times [d, E] \to [n, B/n, E]
$$&lt;p>权重的每个值 $[i,j,k]$ 代表了第 $i$ 个 core 上,第 $j$ 个 token 分配一个第 $k$ 个专家的概率，我们对其进行 softmax 与 top-K 操作之后，就得到了对应的专家（注意 Switch Transformer 中的 $K=1$, 我们通过 one-hot 编码将其输出转化为一个二进制矩阵，大小为 $[n,B/n, E, C]$, 这里的每个值 $[i,j,k,l]$ 代表了第 $i$ 个 core 上,第 $j$ 个 token 分配给第 $k$ 个专家第 $l$ 个 token, 接下来我们再基于输入 &lt;code>[n,B/n,d]&lt;/code> 计算 core 到 expert 的数据，其大小为 &lt;code>[n,E,C,d]&lt;/code>, 计算方式为&lt;/p>
$$
\mathrm{einsum}([n,B/n, d], [n,B/n,E,C], \mathrm{dim}=[B/n])
$$&lt;p>里面的元素 &lt;code>[i,j,k,:]&lt;/code> 代表了第 $i$ 个设备路由到第 $j$ 个专家的第 $k$ ($k&amp;lt;C$) 个 token.&lt;/p>
&lt;p>然后我们就可以执行 &lt;code>all-to-all&lt;/code> 通信来把对应的 token 传输给对应专家所在的设备上了，通信完成之后，每个设备上的专家对收集到的输入 token 进行计算，计算完之后，再通过 &lt;code>all-to-all&lt;/code> 通信来把输出传输给输入的设备。这样就完成了 MoE 模块的计算与通信&lt;/p>
&lt;h3 id="expert-model-and-data-parallelism">Expert, Model and Data Parallelism
&lt;/h3>&lt;p>目前我们只是通过增加专家个数来提高模型的表现，但是 FLOPs 并没有增加，如果我们希望通过增加 FLOPs 来提高模型表现的话，我们需要增加 expert layer 的 model size, 这就涉及到了 Model parallelism, 此时的做法和前面提到的 Expert and Data Parallelism 一样，我们在收集到对应的设备的输入之后，再执行 model parallelism 就可以了。&lt;/p>
&lt;h2 id="results">Results
&lt;/h2>&lt;p>基于以上改进，作者构建了 Switch Transformer, 模型训练的数据集为 C4, 训练的目标为 masked language modeling, 训练时，作者将 $15%$ 的 token 替换为 &lt;code>[mask]&lt;/code> token.&lt;/p>
&lt;p>模型的配置如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-model-configuration.png"
width="1256"
height="252"
srcset="https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-model-configuration_hu4357538381573383876.png 480w, https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-model-configuration_hu3038916745392518968.png 1024w"
loading="lazy"
alt="Model configuration"
class="gallery-image"
data-flex-grow="498"
data-flex-basis="1196px"
>&lt;/p>
&lt;p>实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-performance.png"
width="1206"
height="526"
srcset="https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-performance_hu13682634193827039586.png 480w, https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-performance_hu4383386043902142845.png 1024w"
loading="lazy"
alt="Performance of Switch Transformer"
class="gallery-image"
data-flex-grow="229"
data-flex-basis="550px"
>&lt;/p>
&lt;p>实验结果发现&lt;/p>
&lt;ol>
&lt;li>Switch Transformer 的表现和训练效率都超过了 Dense model&lt;/li>
&lt;li>Switch transformer 的训练效率稍微优于使用 2 个 expert 的 MoE-Base 模型&lt;/li>
&lt;li>Switch transformer 在 low capacity factor 的场景下效果更好&lt;/li>
&lt;/ol>
&lt;h3 id="scaling">Scaling
&lt;/h3>&lt;p>作者对比了 MoE 模型的 scaling law, 结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-scaling-law.png"
width="1244"
height="497"
srcset="https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-scaling-law_hu8777599275504326134.png 480w, https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-scaling-law_hu16075252639264525468.png 1024w"
loading="lazy"
alt="Scaling law of MoE model"
class="gallery-image"
data-flex-grow="250"
data-flex-basis="600px"
>&lt;/p>
&lt;p>可以看到，当我们增加专家个数的时候，模型的表现是持续提升的。并且当我们增加专家个数之后，模型的训练效率也有所提升。&lt;/p>
&lt;p>作者接下来在训练时间上进行了对比，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-speed-comparison.png"
width="839"
height="672"
srcset="https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-speed-comparison_hu9106963091176582103.png 480w, https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-speed-comparison_hu6038297728123734119.png 1024w"
loading="lazy"
alt="Speed comparison of MoE model"
class="gallery-image"
data-flex-grow="124"
data-flex-basis="299px"
>&lt;/p>
&lt;p>实验结果说明，switch transformer 的训练效率比 dense model 快 7 倍左右&lt;/p>
&lt;p>作者进一步对比 switch transformer 和更大的 dense 模型 (T5 large, $3.5\times$ FLOPs), 实验结果证明 Switch transformer 的训练效率仍然更高。&lt;/p>
&lt;h3 id="switch-for-attention">Switch for Attention
&lt;/h3>&lt;p>作者还探究了在 attention layer 加入 MoE 的表现，结果发现尽管效果有提升，但是训练不稳定。&lt;/p>
&lt;h3 id="no-token-left-behind">No Token Left behind
&lt;/h3>&lt;p>由于 tensorflow 为一个静态计算框架，tensor 的形状必须预先定义好，因此必须为每个 expert 设定 capacity, 但是这样就会导致有些 token 被 drop 掉。&lt;/p>
&lt;p>因此，作者提出了 &amp;ldquo;No token left behind&amp;rdquo; 这种方法，来避免出现 token overflow 的情况。具体做法就是先按照正常的 router 逻辑进行计算，如果 router 选取出来的 top-K expert （本文中 $K=1$） 都已经满载了，则选取 &lt;code>top-(K+1)&lt;/code> expert 进行计算，作者发现这种方式可以保证大部分 token 都不会被 drop. 作者通过尝试之后发现，这种方法并没有带来提升。&lt;/p>
&lt;h3 id="encouraging-exploration-across-experts">Encouraging Exploration Across Experts
&lt;/h3>&lt;p>作者还探究了不同选取 top-Kexpert 的方式，作者对比了以下三种方法：&lt;/p>
&lt;ol>
&lt;li>argmax&lt;/li>
&lt;li>sampling from the softmax distribution&lt;/li>
&lt;li>input dropout on the incoming representation&lt;/li>
&lt;li>multiplicative jitter noise on the incoming representation&lt;/li>
&lt;/ol>
&lt;p>实验结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model Quality&lt;/th>
&lt;th>(Neg. Log Perp.) (↑)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Argmax&lt;/td>
&lt;td>-1.471&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Sample softmax&lt;/td>
&lt;td>-1.570&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Input dropout&lt;/td>
&lt;td>-1.480&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Input jitter&lt;/td>
&lt;td>-1.468&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>作者发现，jitter 的效果最好，因此在本文中作者使用 jitter 来加入 noise.&lt;/p>
&lt;h3 id="ablation-on-few-experts">Ablation on Few Experts
&lt;/h3>&lt;p>作者还使用了更少的专家进行实验，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-few-experts.png"
width="846"
height="666"
srcset="https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-few-experts_hu12167909709105131407.png 480w, https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-few-experts_hu9734617699320083614.png 1024w"
loading="lazy"
alt="Switch Transformer with few experts"
class="gallery-image"
data-flex-grow="127"
data-flex-basis="304px"
>&lt;/p>
&lt;p>实验结果显示，即使我们只使用少数 experts, 其模型表现仍然超过了 dense model 的表现，说明了 MoE 架构的有效性。&lt;/p>
&lt;h3 id="downstream-model-performance">Downstream Model Performance
&lt;/h3>&lt;p>作者还进一步探究了模型的预训练表现与 downstream task 任务上表现的关系，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-downstream-performance-scaling.png"
width="1257"
height="499"
srcset="https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-downstream-performance-scaling_hu1602937172753393933.png 480w, https://maosong2022.github.io/p/switch-transformer/Switch-Transformer-downstream-performance-scaling_hu14741610769347968667.png 1024w"
loading="lazy"
alt="Upstream pre-trained quality to downstream model quality."
class="gallery-image"
data-flex-grow="251"
data-flex-basis="604px"
>&lt;/p>
&lt;p>实验结果显示，不管是 baseline 还是 Switch Transformer 其预训练的表现与下游任务上的表现都是正相关的。但是，作者也发现，MoE 模型在微调之后，其表现并不总是与预训练的表现正相关。因此，作者认为进一步探究这个机制是有必要的。&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 Switch Transformer，一个基于 MoE 架构的 Transformer 模型。作者通过改进 MoE 算法，大幅度提高了计算和通信效率，结果发现模型比 dense model 有更高的训练效率。&lt;/p>
&lt;p>作者认为，未来的工作有：&lt;/p>
&lt;ol>
&lt;li>提升大规模模型的训练稳定性&lt;/li>
&lt;li>解决 MoE 模型微调之后效果不如预期的问题&lt;/li>
&lt;li>探究针对 MoE 模型的 scaling law&lt;/li>
&lt;li>支持异构架构的 MoE 模型&lt;/li>
&lt;li>在 FFN 模块意外应用 MoE 架构&lt;/li>
&lt;li>将 Switch Transformer 扩展到其他的模态&lt;/li>
&lt;/ol>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2101.03961" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>