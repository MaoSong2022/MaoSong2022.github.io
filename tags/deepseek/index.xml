<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Deepseek on Mao Song(毛松)'s Homepage</title><link>https://maosong2022.github.io/tags/deepseek/</link><description>Recent content in Deepseek on Mao Song(毛松)'s Homepage</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 28 Oct 2025 09:46:09 +0800</lastBuildDate><atom:link href="https://maosong2022.github.io/tags/deepseek/index.xml" rel="self" type="application/rss+xml"/><item><title>Notes on DeepSeekMoE</title><link>https://maosong2022.github.io/p/notes-on-deepseekmoe/</link><pubDate>Fri, 29 Aug 2025 11:03:12 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-deepseekmoe/</guid><description>&lt;p>DeepSeek 在 2024 年 1 月发布了 DeepSeekMoE, 一个解决 MoE 模型 specialization 不足以及 redundancy 问题的大模型系列。&lt;/p>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>作者首先回顾了已有 MoE 模型的不足，主要有两点：&lt;/p>
&lt;ol>
&lt;li>knowledge hybridity: 已有 MoE 模型的专家个数比较少，这就导致每个专家需要掌握更多样化的知识，提高了训练难度&lt;/li>
&lt;li>knowledge redundancy: 不同专家掌握的知识可能有重叠，从而导致了模型参数的 redundancy&lt;/li>
&lt;/ol>
&lt;p>为了解决这两个问题，作者提出了 DeepSeek-MoE, DeepSeek-MoE 主要做出了两点改变：&lt;/p>
&lt;ol>
&lt;li>Fine-Grained Expert Segmentation: 作者使用了更多的专家，来提高每个专家的 specialization, 降低训练成本&lt;/li>
&lt;li>Shared Expert Isolation: 作者在 Routing expert 的基础上，加入了 shared expert 来学习 common knowledge.&lt;/li>
&lt;/ol>
&lt;p>作者在 2B-A0.6B 的模型进行了实验，结果显示模型表现超过了 &lt;a class="link" href="https://maosong.website/p/GShard.md" target="_blank" rel="noopener"
>GShard&lt;/a>, 说明了 DeepSeekMoE 模型架构的有效性。&lt;/p>
&lt;p>作者还进一步将模型 scale 到了 16B-A2.8B 和 145B-A22B, 实验结果均验证了模型的 scaling 效果。&lt;/p>
&lt;h2 id="method">Method
&lt;/h2>&lt;h3 id="preliminary">Preliminary
&lt;/h3>&lt;p>作者首先回顾了 transformer 架构，transformer 的第 $\ell$ 层 decode layer 可以表示为&lt;/p>
$$
\begin{aligned}
u_{1:T}^{\ell} &amp;= \mathrm{self\_attention}(h_{1:T}^{\ell-1}) + h_{1:T}^{\ell}\\
h_t^\ell &amp;= \mathrm{FFN}(u_t^{\ell}) + u_t^{\ell}
\end{aligned}
$$&lt;p>其中 $T$ 是 sequence length, $h_{1:T}^{\ell-1}$ 是第 $\ell-1$ 层 decoder layer 输出的 hidden states.&lt;/p>
&lt;p>接下来，我们可以将 dense 架构转换为 MoE 架构，MoE 架构与 dense 架构不同的地方在与 $\mathrm{FFN}$ 不再是 MLP, 而是一个 MoE 模块，其表达式如下&lt;/p>
$$
\begin{aligned}
h_t^\ell &amp;= \sum_{i=1}^N\left(g_{i,t}\mathrm{FFN}_i(u_t^{\ell})\right) + u_t^{\ell}\\
g_{i,t} &amp;= \begin{cases}
s_{i,t,}, &amp;s_{i,t}\in\mathrm{Topk}(\{s_{j,t}\mid 1\leq j \leq N\},K)\\
0, &amp;\text{otherwise}
\end{cases}\\
s_{i,t,} &amp;= \mathrm{softmax}_i({u_t^{\ell}}^Te_i^{\ell})
\end{aligned}
$$&lt;p>这里 $N$ 是专家的总个数， $K$ 是激活专家个数， $e_{i}^{\ell}$ 是 routing layer 的权重矩阵，$\mathrm{FFN}_i$ 是每个专家对应的 FFN.&lt;/p>
&lt;h3 id="deepseekmoe-architecutre">DeepSeekMoE Architecutre
&lt;/h3>&lt;p>DeepSeekMoE 架构如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-deepseekmoe/DeepSeekMoE-architecture.png"
width="1200"
height="586"
srcset="https://maosong2022.github.io/p/notes-on-deepseekmoe/DeepSeekMoE-architecture_hu4792723050699036467.png 480w, https://maosong2022.github.io/p/notes-on-deepseekmoe/DeepSeekMoE-architecture_hu1111125453942995081.png 1024w"
loading="lazy"
alt="Architecture of DeepSeekMoE"
class="gallery-image"
data-flex-grow="204"
data-flex-basis="491px"
>&lt;/p>
&lt;p>相比于其他 MoE 架构，DeepSeekMoE 主要做了以下几点改变。&lt;/p>
&lt;h4 id="fine-grained-expert-segmentation">Fine-Grained Expert Segmentation
&lt;/h4>&lt;p>作者首先解决了每个专家学习内容过多的问题。作者的做法就是将每个 expert FFN 分割为 $m$ 个更小的专家，具体做法就是将 FFN intermediate hidden size 降低为原来的 $1/m$. 这样的话就可以在不增加模型参数量的情况下提高模型的表现。修正后的 MoE 模块为&lt;/p>
$$
\begin{aligned}
h_t^\ell &amp;= \sum_{i=1}^{mN}\left(g_{i,t}\mathrm{FFN}_i(u_t^{\ell})\right) + u_t^{\ell}\\
g_{i,t} &amp;= \begin{cases}
s_{i,t,}, &amp;s_{i,t}\in\mathrm{Topk}(\{s_{j,t}\mid 1\leq j \leq mN\},mK)\\
0, &amp;\text{otherwise}
\end{cases}\\
s_{i,t,} &amp;= \mathrm{softmax}_i({u_t^{\ell}}^Te_i^{\ell})
\end{aligned}
$$&lt;p>可以看到，现在我们一共有 $mN$ 个专家，激活专家个数为 $mK$ 个。作者认为，通过提高专家的粒度，我们可以有效增加专家组合的可能性，这就提高了最终的组合多样性。&lt;/p>
&lt;h4 id="shared-expert-isolation">Shared Expert Isolation
&lt;/h4>&lt;p>接下来，作者介绍了解决不同专家学习到重复知识的问题，作者的做法是在 routing Expert 的基础上加入 Shared expert. 也就是说，有固定几个专家始终都会被激活，这部分专家复杂学习通用知识，从而减少知识冗余。增加 shared expert 之后的 MoE 模块为&lt;/p>
$$
\begin{aligned}
h_t^\ell &amp;= \sum_{i=1}^{K_s}\mathrm{FFN}_i(u_t^{\ell})+\sum_{i=K_s+1}^{mN}\left(g_{i,t}\mathrm{FFN}_i(u_t^{\ell})\right) + u_t^{\ell}\\
g_{i,t} &amp;= \begin{cases}
s_{i,t,}, &amp;s_{i,t}\in\mathrm{Topk}(\{s_{j,t}\mid K_s+1\leq j \leq mN\},mK-K_s)\\
0, &amp;\text{otherwise}
\end{cases}\\
s_{i,t,} &amp;= \mathrm{softmax}_i({u_t^{\ell}}^Te_i^{\ell})
\end{aligned}
$$&lt;p>此时，模型中一共包含 $K_s$ 个共享专家，$mN-K_s$ 个 routing expert, 其中激活专家个数为 $mK-K_s$.&lt;/p>
&lt;h4 id="load-balancing-loss">Load Balancing Loss
&lt;/h4>&lt;p>接下来，作者解决了训练时的 load imbalance 问题，作者提出了两个 loss 来分别解决不同层面的 load imbalance 问题。&lt;/p>
&lt;p>首先，在 expert 层面，作者使用了如下的 load balancing loss:&lt;/p>
$$
\mathcal{L} = \alpha_1\sum_{i=1}^{N'}f_iP_i
$$&lt;p>其中 $\alpha_1$ 是超参数，&lt;/p>
$$
f_i = \frac{N'}{K'T}\sum_{i=1}^{N'}\mathbb{1}(\text{Token }i \text{ selects Expert }i),\quad P_i = \frac{1}{T}\sum_{t=1}^Ts_{i,t}
$$&lt;p>分别为以及分配给第 $i$ 个专家的 token 比例以及概率之和。$N&amp;rsquo;=mN-K_s$, $K&amp;rsquo;=mK-K_s$. $\mathbb{1}(\cdot)$ 是 indicator function.&lt;/p>
&lt;p>其次，在 device 层面，作者也是用了 load balancing loss 来减少不同设备之间不必要的通信。作者将 routed experts 分为 $D$ 个 group $\mathcal{E}_1,\dots,\mathcal{E}_D$, 然后每个设备部署一个 group, group level 的 load balancing loss 定义如下：&lt;/p>
$$
\mathcal{L} = \alpha_2\sum_{i=1}^D f_i' P_i'
$$&lt;p>其中 $\alpha_2$ 是超参数，&lt;/p>
$$
f_i' = \frac{1}{|\mathcal{E}_i|}\sum_{j\in\mathcal{E}_i}f_i,\quad P_i' = \sum_{j\in\mathcal{E}_i}P_i
$$&lt;p>实际中，作者使用了一个较小的 $\alpha_1$ 来避免 routing collapse, 使用了一个较大的 $\alpha_2$ 来提高 Device 层面的负载均衡。&lt;/p>
&lt;h3 id="training">Training
&lt;/h3>&lt;p>作者使用了中英文数据进行训练，tokenizer 基于 BPE 算法，vocab size 为 8K. 模型训练基于 HAI-LLM.训练时使用了 TP, DP, PP, EP 等并行策略。&lt;/p>
&lt;p>2B, 16B, 145B 模型的参数如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>2B&lt;/th>
&lt;th>16B&lt;/th>
&lt;th>145B&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>total params&lt;/td>
&lt;td>2B&lt;/td>
&lt;td>16.4B&lt;/td>
&lt;td>144.6B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>activated params&lt;/td>
&lt;td>0.3B&lt;/td>
&lt;td>2.8B&lt;/td>
&lt;td>22.2B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>hidden size&lt;/td>
&lt;td>1280&lt;/td>
&lt;td>2048&lt;/td>
&lt;td>4096&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>layers&lt;/td>
&lt;td>9&lt;/td>
&lt;td>28&lt;/td>
&lt;td>62&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>attention heads&lt;/td>
&lt;td>10&lt;/td>
&lt;td>16&lt;/td>
&lt;td>32&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>head dimension&lt;/td>
&lt;td>128&lt;/td>
&lt;td>128&lt;/td>
&lt;td>128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>routed experts&lt;/td>
&lt;td>63&lt;/td>
&lt;td>64&lt;/td>
&lt;td>128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>activated experts&lt;/td>
&lt;td>7&lt;/td>
&lt;td>6&lt;/td>
&lt;td>12&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>shared experts&lt;/td>
&lt;td>1&lt;/td>
&lt;td>2&lt;/td>
&lt;td>4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>training tokens&lt;/td>
&lt;td>100B&lt;/td>
&lt;td>2T&lt;/td>
&lt;td>245B&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="experiments">Experiments
&lt;/h2>&lt;h3 id="alignment">Alignment
&lt;/h3>&lt;p>作者针对 DeepseekMoE 16B 进行了微调，微调使用了 &lt;strong>1.4M&lt;/strong> 的训练样本，覆盖了 math, code, QA, reasoning 等任务。&lt;/p>
&lt;h3 id="ablation-study">Ablation Study
&lt;/h3>&lt;p>作者在 2B 的模型上进行了 ablation study.&lt;/p>
&lt;p>首先，作者探究了细粒度专家和共享专家的有效性，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-deepseekmoe/DeepSeekMoE-ablation-experts.png"
width="1197"
height="552"
srcset="https://maosong2022.github.io/p/notes-on-deepseekmoe/DeepSeekMoE-ablation-experts_hu13488373378225854754.png 480w, https://maosong2022.github.io/p/notes-on-deepseekmoe/DeepSeekMoE-ablation-experts_hu6581578019190079093.png 1024w"
loading="lazy"
alt="Ablation on experts"
class="gallery-image"
data-flex-grow="216"
data-flex-basis="520px"
>&lt;/p>
&lt;p>实验结果显示，与 &lt;a class="link" href="https://maosong.website/p/GShard.md" target="_blank" rel="noopener"
>GShard&lt;/a> 相比，&lt;strong>使用共享专家可以有效提高模型的表现&lt;/strong>。并且，&lt;strong>使用更细粒度的专家也可以进一步提高模型的表现&lt;/strong>&lt;/p>
&lt;p>作者还探究了共享专家与路由专家的比例，作者分别使用不同的比例进行实验，结果发现共享专家：路由专家个数为 1：3 的时候模型效果最好。&lt;/p>
&lt;p>作者还探究了模型的泛化性，作者 mask 掉一部分概率最高的 routing expert, 然后从剩下的专家里进行 topK 的挑选，然后作者比较模型和 GShard 的表现，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-deepseekmoe/DeepSeekMoE-ablation-expert-specialization.png"
width="737"
height="526"
srcset="https://maosong2022.github.io/p/notes-on-deepseekmoe/DeepSeekMoE-ablation-expert-specialization_hu1048559941586353742.png 480w, https://maosong2022.github.io/p/notes-on-deepseekmoe/DeepSeekMoE-ablation-expert-specialization_hu16111375923387789940.png 1024w"
loading="lazy"
alt="Ablation study on expert specialization"
class="gallery-image"
data-flex-grow="140"
data-flex-basis="336px"
>&lt;/p>
&lt;p>实验结果显示，&lt;strong>DeepSeekMoE 对于 mask 操作更敏感，这说明 DeepSeekMoE 模型中专家的 specialization 更强。&lt;/strong>&lt;/p>
&lt;p>作者还探究了 mask 掉共享专家对模型表现的影响，结果显示&lt;strong>共享专家与路由专家之间的 overlap 很小，去掉共享专家之后，模型表现会变差。&lt;/strong>&lt;/p>
&lt;p>作者进一步分析了共享专家与路由专家组合的有效性。作者探究 DeepSeekMoE 是否可以使用更少的路由专家来获取知识。作者通过使用不同的 activated routed experts 来进行实验，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-deepseekmoe/DeepSeekMoE-ablation-number-activated-experts.png"
width="730"
height="524"
srcset="https://maosong2022.github.io/p/notes-on-deepseekmoe/DeepSeekMoE-ablation-number-activated-experts_hu3482294733356635794.png 480w, https://maosong2022.github.io/p/notes-on-deepseekmoe/DeepSeekMoE-ablation-number-activated-experts_hu10658320898924064401.png 1024w"
loading="lazy"
alt="Ablation study on activated routed experts"
class="gallery-image"
data-flex-grow="139"
data-flex-basis="334px"
>&lt;/p>
&lt;p>实验结果显示，DeepSeekMoE 仅需激活 4 个路由专家，就可以达到与 GShard 相同的表现。&lt;strong>这说明了 DeepSeekMoE 模型中每个专家可以学习到更准确的知识。&lt;/strong>&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 DeepSeekMoE 架构，来提高 MoE 模型中专家的利用效率。为了达到这一点，作者首先使用了更细粒度的专家，降低每个专家的学习成本，然后，作者使用了共享专家，来降低不同专家之间的知识冗余。作者先在 2B 的模型上验证了方法的有效性，然后作者将模型 scale 到了 16B 和 125B。结果显示模型的效果均超过了以前的工作。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2401.06066" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on DeepSeek-LLM</title><link>https://maosong2022.github.io/p/notes-on-deepseek-llm/</link><pubDate>Tue, 26 Aug 2025 10:53:10 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-deepseek-llm/</guid><description>&lt;p>DeepSeek 在 2024 年 1 月 5 日发布了 DeepSeek LLM, 包括 7B 和 67B 两个 size, 作者主要强调了对于 scaling law 的探究&lt;/p>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>已有的 scaling law 如 &lt;a class="link" href="https://maosong.website/p/kaplan-scaling-law/" target="_blank" rel="noopener"
>Kaplan&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/chinchilla-scaling-law/" target="_blank" rel="noopener"
>Chinchilla&lt;/a> 介绍了 model size, dataset size, compute budget 与模型表现之间的关系。在本文中，作者进一步探究了 learning rate 和 batch size 等超参数与模型表现之间的关系。基于发现的 scaling law, 作者为不同大小的模型设置了最优的超参数。并且，作者还发现不同数据集与模型表现之间的关系。&lt;/p>
&lt;p>最终，基于这些实验结果，作者提出了 DeepSeek LLM, 模型使用 &lt;strong>2T token&lt;/strong> 进行预训练，使用 1M samples 进行后训练，后训练包括 SFT 以及 DPO.&lt;/p>
&lt;h2 id="pre-training">Pre-training
&lt;/h2>&lt;h3 id="architecture">Architecture
&lt;/h3>&lt;p>DeepSeek-LLM 的架构与 LLaMA 基本相同，作者在 67B 的模型上使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-gqa/" target="_blank" rel="noopener"
>GQA&lt;/a> 来提高 inference 效率。最终模型的配置如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Params&lt;/th>
&lt;th>7B&lt;/th>
&lt;th>67B&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$n_{\text{layers}}$&lt;/td>
&lt;td>$30$&lt;/td>
&lt;td>$95$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$d_{\text{model}}$&lt;/td>
&lt;td>$4096$&lt;/td>
&lt;td>$8192$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$n_{\text{heads}}$&lt;/td>
&lt;td>$32$&lt;/td>
&lt;td>$64$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$n_{\text{kv_heads}}$&lt;/td>
&lt;td>$32$&lt;/td>
&lt;td>$8$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Context Length&lt;/td>
&lt;td>$4096$&lt;/td>
&lt;td>$4096$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Sequence Batch Size&lt;/td>
&lt;td>$2304$&lt;/td>
&lt;td>$4608$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Learning Rate&lt;/td>
&lt;td>$4.2e-4$&lt;/td>
&lt;td>$3.2e-4$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Tokens&lt;/td>
&lt;td>2T&lt;/td>
&lt;td>2T&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="data">Data
&lt;/h3>&lt;p>作者主要从 Common Crawl 构建预训练数据，数据处理过程包括：去重，过滤以及 remixing 三个步骤。&lt;/p>
&lt;p>对于 tokenizer, 作者使用了 &lt;a class="link" href="https://maosong.website/p/hands-on-llm1-tokenizer/" target="_blank" rel="noopener"
>BBPE&lt;/a> 算法，tokenizer 的大小设置为 100,000, 最终的 tokenizer 大小为 102400.&lt;/p>
&lt;h3 id="hyper-parameters">Hyper Parameters
&lt;/h3>&lt;p>作者主要对比了一下不同 learning rate schedule 的表现：&lt;/p>
&lt;ol>
&lt;li>cosine learning schedule&lt;/li>
&lt;li>multi-step learning rate schedule: 包含三个 Stage, 第一个 stage 保持最大学习率，第二个 stage 将学习率降低为最大学习率的 $31.6%$, 第三个 stage 降低为最大学习率的 $10%$.&lt;/li>
&lt;/ol>
&lt;p>对比的实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-deepseek-llm/DeepSeek-LLM-learning-rate-ablation.png"
width="1077"
height="381"
srcset="https://maosong2022.github.io/p/notes-on-deepseek-llm/DeepSeek-LLM-learning-rate-ablation_hu7127490498946913523.png 480w, https://maosong2022.github.io/p/notes-on-deepseek-llm/DeepSeek-LLM-learning-rate-ablation_hu3030451669268181506.png 1024w"
loading="lazy"
alt="Comparison of different learning schedulers"
class="gallery-image"
data-flex-grow="282"
data-flex-basis="678px"
>&lt;/p>
&lt;p>实验结果显示，multi-step learning rate scheduler 的表现与 cosine learning rate 表现差不多。并且，multi-step learning rate scheduler 对于 continue pretraining 支持更好。因此在本文中作者使用了 multi-step learning rate scheduler.&lt;/p>
&lt;h3 id="infra">Infra
&lt;/h3>&lt;p>作者使用了数据并行，张量并行，序列并行以及 1F1B pipeline 并行。作者还使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-flashattention/" target="_blank" rel="noopener"
>flash attention&lt;/a> 来提高硬件利用率。&lt;/p>
&lt;h2 id="scaling-law">Scaling Law
&lt;/h2>&lt;p>本节中，作者分析了 scaling law, 主要有以下三点：&lt;/p>
&lt;ol>
&lt;li>构建了针对 learning rate 和 batch size 的 scaling law&lt;/li>
&lt;li>作者使用 non-embedding FLOPs/token $M$ 来表示 model scale&lt;/li>
&lt;li>预训练数据的质量对最后中的 scaling 影响很大&lt;/li>
&lt;/ol>
&lt;p>作者首先构建了针对 batch size 和 learning rate 的 scaling law, 结果显示最优的 learning rate 和 batch size 范围都比较广，这个结论与 &lt;a class="link" href="https://maosong.website/p/kaplan-scaling-law/" target="_blank" rel="noopener"
>Kaplan&lt;/a> 一致。&lt;/p>
&lt;p>接下来，作者构建了 batch size $B$, learning rate $\eta$ 与 compute budget $C 之间的关系，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-deepseek-llm/DeepSeek-LLM-scaling-curve-bsz-lr.png"
width="1066"
height="379"
srcset="https://maosong2022.github.io/p/notes-on-deepseek-llm/DeepSeek-LLM-scaling-curve-bsz-lr_hu9017218681376297400.png 480w, https://maosong2022.github.io/p/notes-on-deepseek-llm/DeepSeek-LLM-scaling-curve-bsz-lr_hu13578224903628364776.png 1024w"
loading="lazy"
alt="Scaling curves of batch size and learning rate"
class="gallery-image"
data-flex-grow="281"
data-flex-basis="675px"
>&lt;/p>
&lt;p>拟合得到的曲线为&lt;/p>
$$
\begin{aligned}
\eta_{opt} &amp;= 0.3118* C^{-0.1250}\\
B_{opt} &amp;= 0.2920 * C^{0.3271}
\end{aligned}
$$&lt;p>可以看到，随着 compute budget 增加，$B_{opt}$ 也逐渐增加，而 $\eta_{opt}$ 逐渐减小。并且，最优参数的范围都比较广。&lt;/p>
&lt;p>接下来，作者进一步探究了 batch size 与 generalization error $L$ 之间的关系。作者希望找到 model scale $N$, data scale $D$ 与 compute budget $C$ 之间的关系，即&lt;/p>
$$
N_{opt} \varpropto C^a,D_{opt} \varpropto C^b
$$&lt;p>compute budget 与 model scale, data scale 之间的关系可以近似表示为 $C=6ND$, 这个公式的推导见 &lt;a class="link" href="https://maosong.website/p/llm-flops-computation/" target="_blank" rel="noopener"
>LLM FLOPs computation&lt;/a>。我们用 $N_1,N_2$ 分别表示模型的 non-embedding parameter 以及 complete parameters, 则我们可以用 $6N_1$ 或者 $6N_2$ 来近似 model scale, 但是 $6N_1$ 和 $6N_2$ 均没有考虑 attention 的计算开销，因此这两种近似的误差都比较大。&lt;/p>
&lt;p>为了解决这个问题，作者提出了一个新的 model scale 表示形式，即 non-embedding FLOPS/token $M$, 其中 $M$ 包含 attention 的计算开销但是不包含 vocabulary computation. 基于这种表示，compute budget 可以近似表示为 $C=MD$. $M$ 与 $6N_1,6N_2$ 的区别表示如下所示&lt;/p>
$$
\begin{aligned}
6N_1 &amp;= 72nd^2\\
6N_2 &amp;= 72nd^2 + 6Vd\\
M &amp;= 72nd^2+12ndl
\end{aligned}
$$&lt;p>其中, $d$ 是 hidden size, $n$ 是 layers 个数, $V$ 是 vocabulary size, $l$ 是 sequence length. 作者在不同 scale 的模型上比较了三种表示方式，结果发现 $6N_1$ 和 $6N_2$ 要么低估，要么高估了模型的参数量。&lt;/p>
&lt;p>基于 model scale 的表示方式，作者构建了如下的优化问题&lt;/p>
$$
M_{opt}(C), D_{opt}(C) = {\arg\min}_{M,D\ s.t.\ C=MD} L(N,D)
$$&lt;p>作者使用了 &lt;a class="link" href="https://maosong.website/p/chinchilla-scaling-law/" target="_blank" rel="noopener"
>Chinchilla&lt;/a> 提出来的 IsoFLOP 曲线进行拟合，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-deepseek-llm/DeepSeek-LLM-IsoFLOP-curve.png"
width="1070"
height="398"
srcset="https://maosong2022.github.io/p/notes-on-deepseek-llm/DeepSeek-LLM-IsoFLOP-curve_hu9939380833919642003.png 480w, https://maosong2022.github.io/p/notes-on-deepseek-llm/DeepSeek-LLM-IsoFLOP-curve_hu3158434771816979870.png 1024w"
loading="lazy"
alt="IsoFLOP curve and optimal model/data allocation"
class="gallery-image"
data-flex-grow="268"
data-flex-basis="645px"
>&lt;/p>
&lt;p>拟合的曲线为&lt;/p>
$$
M_{opt}(C) = 0.1715*C^{0.5243}, D_{opt}(C) = 5.8316*C^{0.4757}
$$&lt;p>作者还进一步拟合了 compute budget 与 optimal generalization error 之间的关系，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-deepseek-llm/DeepSeek-LLM-performance-scaling-curve.png"
width="662"
height="414"
srcset="https://maosong2022.github.io/p/notes-on-deepseek-llm/DeepSeek-LLM-performance-scaling-curve_hu4460428567910772515.png 480w, https://maosong2022.github.io/p/notes-on-deepseek-llm/DeepSeek-LLM-performance-scaling-curve_hu2255234009678671508.png 1024w"
loading="lazy"
alt="Performance Scaling curve"
class="gallery-image"
data-flex-grow="159"
data-flex-basis="383px"
>&lt;/p>
&lt;p>实验结果显示，作者提出的 scaling law 可以很好预测模型的表现。&lt;/p>
&lt;p>最后，作者探究了以下不同数据集的 scaling law, 作者使用 early in-house data, current in-house data 以及 OpenWebText2 来将进行实验，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-deepseek-llm/DeepSeek-LLM-dataset-ablation.png"
width="746"
height="285"
srcset="https://maosong2022.github.io/p/notes-on-deepseek-llm/DeepSeek-LLM-dataset-ablation_hu5807511465671654580.png 480w, https://maosong2022.github.io/p/notes-on-deepseek-llm/DeepSeek-LLM-dataset-ablation_hu6482466502783940283.png 1024w"
loading="lazy"
alt="Comparison of different dataset scaling"
class="gallery-image"
data-flex-grow="261"
data-flex-basis="628px"
>&lt;/p>
&lt;p>结果显示，scaling law 与数据质量高度相关。当数据质量提升时，model scaling exponent $a$ 逐步提升，data scaling exponent $b$ 逐步下降，说明 compute budget 更多由模型参数量决定。因此，作者认为提升 compute budget 之后，我们应该优先提高模型的 model size.&lt;/p>
&lt;h2 id="post-training">Post-training
&lt;/h2>&lt;p>作者构建了 1.5M 的中英文指令数据。其中安全性的数据有 300K, 有帮助性的数据有 1.2M, 其中包括 $31.2%$ 的通用数据，$46.6%$ 的数学相关数据，$22.2%$ 的代码数据。&lt;/p>
&lt;p>post-training 包含两个阶段：&lt;/p>
&lt;ol>
&lt;li>SFT:7B 的模型训练了 4 个 epoch, 67B 的模型训练了 2 个 epoch, 作者发信进一步训练 67B 的模型会导致过拟合。作者发现，模型在训练过程中会出现重复输出的情况，特别是数学 SFT 数据，为了解决这个问题，作者使用了一个两阶段的 SFT 以及 DPO.&lt;/li>
&lt;li>DPO: 提高模型的能力，作者发现 DPO 可以提高模型 open-ended generation skill.&lt;/li>
&lt;/ol>
&lt;h2 id="evaluation">Evaluation
&lt;/h2>&lt;p>我们主要关注一下消融实验。&lt;/p>
&lt;p>首先作者探究了分阶段 SFT 对模型表现的影响。作者发现，小模型在 math 和 code 数据集上需要训练更长时间，但是这也损害了模型的对话能力。为了解决这个问题，作者使用两阶段的训练模式，第一个阶段使用所有的数据进行训练，第二个阶段仅使用对话数据进行训练，实验结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>HumanEval&lt;/th>
&lt;th>GSM8K&lt;/th>
&lt;th>Repetition&lt;/th>
&lt;th>IFEval&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>DeepSeek LLM 7B Chat Stage1&lt;/td>
&lt;td>48.2&lt;/td>
&lt;td>63.9&lt;/td>
&lt;td>0.020&lt;/td>
&lt;td>38.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DeepSeek LLM 7B Chat Stage2&lt;/td>
&lt;td>48.2&lt;/td>
&lt;td>63.0&lt;/td>
&lt;td>0.014&lt;/td>
&lt;td>41.2&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，经过第二阶段训练之后，模型的表现有所提升&lt;/p>
&lt;p>接下来，作者探究了 Multi-choice question 对模型表现的影响，MCQ 要求模型不仅需要有相关的知识，还要理解每个选项的含义。作者使用 20M 中文 MCQ 来进行消融实验，结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>MMLU&lt;/th>
&lt;th>CEval&lt;/th>
&lt;th>CMMLU&lt;/th>
&lt;th>TriviaQA&lt;/th>
&lt;th>ChineseQA&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>DeepSeek LLM 7B Chat&lt;/td>
&lt;td>49.4&lt;/td>
&lt;td>47.0&lt;/td>
&lt;td>49.7&lt;/td>
&lt;td>57.9&lt;/td>
&lt;td>75.0&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DeepSeek LLM 7B Chat + MC&lt;/td>
&lt;td>60.9&lt;/td>
&lt;td>71.3&lt;/td>
&lt;td>73.8&lt;/td>
&lt;td>57.9&lt;/td>
&lt;td>74.4&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>实验结果显示，MCQ 确实可以提高模型在上述几个 benchmark 上的表现，但是其泛化性会下降。因此，作者在 pre-training 和 fine-tuning 阶段并没有使用 MCQ 数据进行训练。&lt;/p>
&lt;p>作者还探究了在 pre-training 阶段加入 instruction data, 来提高 base model 在下游 benchmark 上的表现。结果发现，base model 的表现提升优先。作者认为，尽管 instruction data 可以提高 base model 表现，但是如果 Instruction data 数量过少，则模型表现不太可能学习到有用的知识。因此，作者的做法是不在 pretraining 阶段加入 Instruction data.&lt;/p>
&lt;p>最后，作者探究了 system prompt 对模型表现的影响。受 LLaMA 2 启发，作者也尝试在输入中加入 system prompt. 实验结果如下所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>MT Bench&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>DeepSeek LLM 7B Chat&lt;/td>
&lt;td>7.15&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DeepSeek LLM 7B Chat + System Prompt&lt;/td>
&lt;td>7.11&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DeepSeek LLM 67B Chat&lt;/td>
&lt;td>8.35&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DeepSeek LLM 67B Chat + System Prompt&lt;/td>
&lt;td>8.58&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，7B 的模型加入 system prompt 之后，模型表现有所下降；67B 的模型加入 system prompt 之后，模型表现有所提升。作者认为，大模型更容易理解 system prompt 的意图，而小模型的指令跟随能力则较差，因此 system prompt 反而会影响模型表现。&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 DeepSeek LLM 系列大语言模型，作者详细介绍了超参数的选择以及 scaling law 等。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2401.02954" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>