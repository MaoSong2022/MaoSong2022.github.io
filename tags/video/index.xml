<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Video on Mao Song(毛松)'s Homepage</title><link>https://maosong2022.github.io/tags/video/</link><description>Recent content in Video on Mao Song(毛松)'s Homepage</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 11 Sep 2025 11:33:31 +0800</lastBuildDate><atom:link href="https://maosong2022.github.io/tags/video/index.xml" rel="self" type="application/rss+xml"/><item><title>Notes on Keye-VL 1.5</title><link>https://maosong2022.github.io/p/notes-on-keye-vl-1.5/</link><pubDate>Thu, 11 Sep 2025 11:33:31 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-keye-vl-1.5/</guid><description>&lt;p>快手提出了 Keye-VL 1.5, 一个强调 reasoning, video understanding 的 8B 多模态大模型。作者提出了 slow-fast video encoding strategy 来提高模型的视频理解能力，作者通过在预训练和后训练提高了模型的长上下文能力和 reasoning 能力&lt;/p>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>作者回顾了多模态大模型的进展，然后提到视频理解能力仍然是一个挑战。为了解决这个问题，作者提出了 Keye-VL-1.5, 一个 8B 的视频理解多模态大模型。&lt;/p>
&lt;p>Keye-VL-1.5 主要做了三点改进：&lt;/p>
&lt;ol>
&lt;li>在架构上，使用了 Slow-Fast Video Encoding&lt;/li>
&lt;li>在预训练阶段，使用多个 stage 来提升模型的长上下文能力&lt;/li>
&lt;li>在 post-training 阶段，提高模型的 reasoning 能力和 alignment 表现&lt;/li>
&lt;/ol>
&lt;h2 id="method">Method
&lt;/h2>&lt;h3 id="architecture">Architecture
&lt;/h3>&lt;p>Keye-VL 1.5 的架构与 &lt;a class="link" href="https://maosong.website/p/notes-on-keye-vl/" target="_blank" rel="noopener"
>Keye-VL&lt;/a> 一致。&lt;/p>
&lt;p>Keye-VL 1.5 主要做出的改进点为针对视频的 encoding 方式&lt;/p>
&lt;p>作者回顾了已有 MLLM 处理视频的方式，比如 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5-vl/" target="_blank" rel="noopener"
>Qwen2.5-VL&lt;/a> 使用 3D convolution 来 merge 相邻的两帧，&lt;a class="link" href="https://maosong.website/p/notes-on-seed1.5-vl/" target="_blank" rel="noopener"
>Seed1.5-VL&lt;/a> 采用了 Dynamic Frame-Resolution Sampling 技巧，来根据 budget 和处理的任务来动态调整采样率 (frame) 和每一帧的图片精度 (resolution)。&lt;/p>
&lt;p>但是这些方法很难进行泛化。因此，作者在本文中就提出了 &lt;strong>SlowFast video encoding stratrgy&lt;/strong>:&lt;/p>
&lt;ol>
&lt;li>Slow Pathway: 空间信息丰富（high resolution），时间信息简略 (low number of frames)&lt;/li>
&lt;li>Fast Pathway: 时间信息丰富（high number of frames），空间信息简略 (low resolution)&lt;/li>
&lt;/ol>
&lt;p>为了区分 slow/fast frames, 作者提出了一个基于 patch similarity 的 metric:&lt;/p>
&lt;ol>
&lt;li>第一帧始终定义为 slow frame&lt;/li>
&lt;li>接下来的每一帧，如果其和上一帧的相似度超过 $95%$, 则定义为 fast frame; 反之则定义为 slow frame.&lt;/li>
&lt;/ol>
&lt;p>得到 slow/fast frames 之后，作者将 fast-frame 的 token budget 限制为 slow frame token budget 的 $30%$ 来平衡时间信息以及空间信息。接下来，作者使用二分搜索来决定 slow frame 的 token budget. 为了区分 slow frame 和 fast frame 的 token, 作者使用了特殊的 token 来进行分离。&lt;/p>
&lt;p>最终的处理结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-video-encoding.png"
width="1364"
height="335"
srcset="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-video-encoding_hu3751451477354070023.png 480w, https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-video-encoding_hu15836414842667305534.png 1024w"
loading="lazy"
alt="SlowFast Video Encoding"
class="gallery-image"
data-flex-grow="407"
data-flex-basis="977px"
>&lt;/p>
&lt;h3 id="pre-training">Pre-training
&lt;/h3>&lt;p>预训练的数据和 Keye-VL 基本一致，我们主要介绍改进的点&lt;/p>
&lt;p>对于 Image caption 数据，作者认为这批数据可能会损害模型的指令跟随和 reasoning 能力，因此作者对数据进行了增广，主要是调整了数据的格式：&lt;/p>
&lt;ol>
&lt;li>QA, 数据格式为 &lt;code>&amp;lt;image, caption, [eos], question, answer&amp;gt;&lt;/code>&lt;/li>
&lt;li>reverse QA, 数据格式为 &lt;code>&amp;lt;image, question, answer, [eos], caption&amp;gt;&lt;/code>&lt;/li>
&lt;li>instruction following: 随机给一批数据作为输入，然后让模型基于特定 image 输出 caption&lt;/li>
&lt;/ol>
&lt;p>作者还构建了一个 trap question 来提高模型的 robustness 以及 faithfulness.&lt;/p>
&lt;p>OCR 数据在 Keye-VL 的基础上加入了两点：&lt;/p>
&lt;ol>
&lt;li>Structured Document and Code Understanding: 基于 markdown 和 HTML 等数据来获取 code OCR 数据&lt;/li>
&lt;li>Instruction Following OCR: 基于特定指令进行 OCR&lt;/li>
&lt;/ol>
&lt;p>对于 grounding 数据，作者进一步加入了 temporal grounding 数据，作者首先使用 TEMPURA&lt;/p>
&lt;p>来将短视频分割成若干个 video clips. 然后作者使用 SOTA MLLM 来过滤数据，最后作者基于 Gemini2.5 来生成对应的 QA.&lt;/p>
&lt;p>预训练和 Keye-VL 一样，包含 3 个 stage&lt;/p>
&lt;p>前两个 stage，作者将模型的上下文限制为 8K, 使用了 DP 和 Zero-2 来减少内存开销。在 stage 3, 作者将模型的上下文从 8K 扩展到 128K, 对应的 base frequency 从 1M 提升到 8M. 训练数据包括长视频，长文本和大规模图片。作者将优化策略调整为 Zero-1, CP 和 PP 来支持 long-context 的训练。训练时数据分布为 video:images:text=24:50:26.&lt;/p>
&lt;h3 id="post-training">Post-training
&lt;/h3>&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-post-training-pipeline.png"
width="1360"
height="449"
srcset="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-post-training-pipeline_hu4088885002935100923.png 480w, https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-post-training-pipeline_hu14194521321835783259.png 1024w"
loading="lazy"
alt="Post-Training Pipeline"
class="gallery-image"
data-flex-grow="302"
data-flex-basis="726px"
>&lt;/p>
&lt;p>SFT 阶段使用了 &lt;strong>7.5M&lt;/strong> 多模态 QA 样本进行训练。&lt;/p>
&lt;p>MPO 阶段的数据相比 Keye-VL 有所减少，包含：&lt;/p>
&lt;ol>
&lt;li>250K 开源样本&lt;/li>
&lt;li>150K 纯文本数据&lt;/li>
&lt;li>26K 人类标注数据&lt;/li>
&lt;/ol>
&lt;p>对于 reward model 的训练，作者使用了 SFT 和 RL 两个阶段。SFT 阶段的数据包括 R1-Reward 和 MMPR, 训练之后作者还是用比较短的 good response 来避免产生较长的回答&lt;/p>
&lt;p>在 SFT 和 MPO 阶段之后，作者使用 LongCoT code-start 初步激活模型的 reasoning 能力。&lt;/p>
&lt;p>作者构建了一个 5 部的自动化数据生成 pipeline, 如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-LongCoT-data-generation-pipeline.png"
width="1362"
height="389"
srcset="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-LongCoT-data-generation-pipeline_hu10802193350758216375.png 480w, https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-LongCoT-data-generation-pipeline_hu15923562071597966044.png 1024w"
loading="lazy"
alt="LongCoT data generation pipeline"
class="gallery-image"
data-flex-grow="350"
data-flex-basis="840px"
>&lt;/p>
&lt;p>步骤如下：&lt;/p>
&lt;ol>
&lt;li>Multi-Source Data Collection and Enhancement：收集数据&lt;/li>
&lt;li>Multi-Path Reasoning Generation with Confidence Quantification: 基于 confidence 来挑选数据&lt;/li>
&lt;li>Comprehensive Two-Level Quality Assessment: 基于答案和过程的正确性来提高数据质量&lt;/li>
&lt;li>Human-in-the-Loop Quality Enhancement: 对于中等质量的数据请人类进一步进行标注&lt;/li>
&lt;li>Dynamic Quality Scoring and Data Utilization Strategy: 对数据进行打分，高质量数据进行上采样&lt;/li>
&lt;/ol>
&lt;p>接下来就是 General RL 过程。&lt;/p>
&lt;p>对于通用的 RLVR 训练，作者使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-gspo/" target="_blank" rel="noopener"
>GSPO&lt;/a> 算法来进行训练。&lt;/p>
&lt;p>在训练过程中，作者采取了 progressive hint sampling 方式，也就是提供不同程度的 hint 来提高模型的训练效率。作者将 hint 分为五个等级：&lt;/p>
&lt;ol>
&lt;li>Level 1 (Concept / Observation)&lt;/li>
&lt;li>Level 2 (Strategy / Method)&lt;/li>
&lt;li>Level 3 (Tools / Formula)&lt;/li>
&lt;li>Level 4 (Steps / Calculation)&lt;/li>
&lt;li>Level 5 (Complete Solution)&lt;/li>
&lt;/ol>
&lt;p>来提供不同程度的辅助，作者使用 Keye-VL 1.5 来确定最小的 hint level, 然后基于这个 level 提供信息进行 RL 的训练&lt;/p>
&lt;p>为了进一步提高模型的表现，作者采用了一个和 &lt;a class="link" href="https://maosong.website/p/notes-on-seed1.5-vl/" target="_blank" rel="noopener"
>Seed1.5-VL&lt;/a> 一样的迭代式训练策略，即反复进行 SFT 和 RL 来降低训练成本，提高训练效率。&lt;/p>
&lt;p>最后，再通用 RL 阶段之后，作者加入了 alignment RL 的训练来进行对齐，reward 包括三个方面：&lt;/p>
&lt;ol>
&lt;li>rule-based reward&lt;/li>
&lt;li>generative reward&lt;/li>
&lt;li>model-based reward&lt;/li>
&lt;/ol>
&lt;p>任务主要包括三个方面：&lt;/p>
&lt;ol>
&lt;li>instruction following&lt;/li>
&lt;li>format adherence&lt;/li>
&lt;li>preference alignment&lt;/li>
&lt;/ol>
&lt;p>数据介绍如下：&lt;/p>
&lt;ol>
&lt;li>instruction following:25 类硬约束，20 类软约束，数据包括 17K 多模态数据和 23K 纯文本数据，奖励包括 rule-based reward 和 generative reward&lt;/li>
&lt;li>reasoning: 12K 数学和逻辑推理数据&lt;/li>
&lt;li>RAG: 提高模型的搜索能力，作者使用 GSPO 算法进行训练&lt;/li>
&lt;/ol>
&lt;h2 id="experiments">Experiments
&lt;/h2>&lt;p>模型表现如下&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-Performance.png"
width="1366"
height="988"
srcset="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-Performance_hu12685109521298202886.png 480w, https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-Performance_hu1121153072311157701.png 1024w"
loading="lazy"
alt="Performance of Keye-VL 1.5"
class="gallery-image"
data-flex-grow="138"
data-flex-basis="331px"
>&lt;/p>
&lt;p>接下来作者进行了消融实验。&lt;/p>
&lt;p>首先是不同训练阶段对模型表现的影响，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-training-stage.png"
width="1369"
height="458"
srcset="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-training-stage_hu14045805214300333895.png 480w, https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-training-stage_hu10663261281529444295.png 1024w"
loading="lazy"
alt="Ablation study on training strategy"
class="gallery-image"
data-flex-grow="298"
data-flex-basis="717px"
>&lt;/p>
&lt;p>实验结果显示，提高 SFT 训练数据可以有效提高模型在数学推理，逻辑推理和 OCR 任务上的表现。MPO 可以进一步提高模型的表现，Long CoT cold start 可以有效提高模型的 reasoning 表现。&lt;/p>
&lt;p>作者还探究了 model merging 对模型表现的影响，作者首先基于 base model 和 OCR 数据训练得到 OCR expert, 然后进行 merge, 实验结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-model-merging.png"
width="1244"
height="340"
srcset="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-model-merging_hu5485713103224105634.png 480w, https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-model-merging_hu4736239008410953171.png 1024w"
loading="lazy"
alt="Ablation study on model merging"
class="gallery-image"
data-flex-grow="365"
data-flex-basis="878px"
>&lt;/p>
&lt;p>实验结果显示，model merging 可以有效提高模型在 special domain 上的表现，并且还可以维持模型的通用能力&lt;/p>
&lt;p>作者还发现：&lt;/p>
&lt;ol>
&lt;li>expert model 训练时间过长会影响最终 merge model 的表现&lt;/li>
&lt;li>expert mode 训练的学习率应该要设置比较小&lt;/li>
&lt;/ol>
&lt;p>接下来，作者探究了 alignment RL 对模型表现的影响，&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-alignment-RL.png"
width="1385"
height="275"
srcset="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-alignment-RL_hu16960946704897994390.png 480w, https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-alignment-RL_hu16484448956614414637.png 1024w"
loading="lazy"
alt="Ablation on alignment RL"
class="gallery-image"
data-flex-grow="503"
data-flex-basis="1208px"
>&lt;/p>
&lt;p>实验结果说明，alignment RL 可以在保持模型 reasoning 能力的同时提高模型的指令跟随能力&lt;/p>
&lt;p>作者还探究了 hint 对模型表现的影响，使用不同 level hint 进行训练对模型表现影响的结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-hint.png"
width="1209"
height="310"
srcset="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-hint_hu5531435168214557685.png 480w, https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-hint_hu1703494621381694086.png 1024w"
loading="lazy"
alt="Ablation on hint level"
class="gallery-image"
data-flex-grow="390"
data-flex-basis="936px"
>&lt;/p>
&lt;p>实验结果显示，使用 high level 的 hint 可以有效提高模型输出的正确率。&lt;/p>
&lt;p>最后作者探究了 rejection sampling 对模型表现的影响，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-rejection-sampling.png"
width="1384"
height="340"
srcset="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-rejection-sampling_hu12066184384347437079.png 480w, https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-rejection-sampling_hu1521405363756042548.png 1024w"
loading="lazy"
alt="Ablation on rejection sampling"
class="gallery-image"
data-flex-grow="407"
data-flex-basis="976px"
>&lt;/p>
&lt;p>结果发现，通过 rejection sampling，模型的表现有了进一步的提升。因此作者采用了 ST-RL-(RFT-SFT)-(RFT-RL) 的训练方式来进行训练&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 Keye-VL1.5, 一个强调 video understanding 和 reasoning 的 MLLM. Keye-VL 1.5 使用了 SlowFast video encoding strategy 来提高模型的效率和视频理解能力。作者详细介绍了模型的数据，训练和评估。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2509.01563" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on ARC-Hunyuan-Video-7B</title><link>https://maosong2022.github.io/p/notes-on-arc-hunyuan-video-7b/</link><pubDate>Tue, 12 Aug 2025 10:57:57 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-arc-hunyuan-video-7b/</guid><description>&lt;p>腾讯 ARC LAB 提出了 ARC-Hunyuan-Video-7B, 一个针对短视频理解和推理的视频多模态大模型。&lt;/p>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>作者首先提出了 Structured video comprehension 的概念&lt;/p>
&lt;blockquote>
&lt;p>the ability to decompose a video into its constituent events and narrative elements with temporal precision.&lt;/p>
&lt;/blockquote>
&lt;p>视频信息包含了 dense visual elements, 比如文字等信息，还有 rich audio streams, 比如音效，音乐，再就是 rapid narrative pacing, 用于强调情感。&lt;/p>
&lt;p>已有的多模态大模型对于视频的细粒度理解和推理能力存在不足。尽管 &lt;a class="link" href="https://maosong.website/p/notes-on-keye-vl/" target="_blank" rel="noopener"
>Keye-VL&lt;/a> 也是一个短视频理解多模态大模型，但是其并没有利用 audio 模态的信息。其他的 audio-visual LLM 虽然可以处理 audio 模态的信息，但是他们主要集中于通用场景的视频，这类视频的特点是：叙述节奏慢，信息密度低。因此，对于叙述节奏快，信息丰富的短视频，这些模型表现就比较差。&lt;/p>
&lt;p>基于以上这些问题，作者提出了 ARC-Hunyuan-Video-7B, 一个基于 Hunyuan-7B vision-language model 的短视频多模态大模型，其主要做了两点改进：&lt;/p>
&lt;ol>
&lt;li>加入了一个 audio encoder 用于提取 audio 信息，然后对 audio 信息与视觉信息进行同步&lt;/li>
&lt;li>使用了一个 timestamp overlap 机制，将时间戳印在视频的帧上，帮助模型理解事件的时序信息&lt;/li>
&lt;/ol>
&lt;p>作者还涉及了多阶段的训练策略，来提高模型的表现。最后在测试时，对于一个 1 分钟的视频，模型在 H20 GPU 上进需要 10 秒就可以处理完毕。&lt;/p>
&lt;h2 id="method">Method
&lt;/h2>&lt;h3 id="architecture">Architecture
&lt;/h3>&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-arc-hunyuan-video-7b/ARC-Hunyuan-Video-architecture.png"
width="682"
height="467"
srcset="https://maosong2022.github.io/p/notes-on-arc-hunyuan-video-7b/ARC-Hunyuan-Video-architecture_hu7049036770913836449.png 480w, https://maosong2022.github.io/p/notes-on-arc-hunyuan-video-7b/ARC-Hunyuan-Video-architecture_hu10155841825363121766.png 1024w"
loading="lazy"
alt="Architecture of ARC-Hunyuan-Video-7B"
class="gallery-image"
data-flex-grow="146"
data-flex-basis="350px"
>&lt;/p>
&lt;p>模型基于 Hunyuan-7B-VLM 开发得到，&lt;/p>
&lt;ul>
&lt;li>Visual Encoding: 作者将时间戳以 &lt;code>HH:MM:SS&lt;/code> 的形式印在视频的帧上。视频的采样率为 1FPS, 超过 150s 的视频会进行均匀采样降低至 150s. 每一帧会 resize 到 $640\times 640$, 最后每一帧输出 112 token&lt;/li>
&lt;li>Audio Encoding: 使用 Whisper 作为编码器。小于 300s 的视频，会按照 30s 为单位切分为不同的 chunk, 大于 300s 的视频，会分割为 150 个 chunk, 然后每个 chunk 只保留开头 2s 的内容。最后，使用 Whisper 进行特征提取，最后再通过 MLP 与 LLM 的特征空间进行对齐&lt;/li>
&lt;li>Visual-audio Synchronization: 先对 audio token 进行 zero padding 与 Visual token 的个数进行对齐，然后将两者相加&lt;/li>
&lt;/ul>
&lt;h3 id="training">Training
&lt;/h3>&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-arc-hunyuan-video-7b/ARC-Hunyuan-Video-training-overall.png"
width="682"
height="467"
srcset="https://maosong2022.github.io/p/notes-on-arc-hunyuan-video-7b/ARC-Hunyuan-Video-training-overall_hu11903493230483800583.png 480w, https://maosong2022.github.io/p/notes-on-arc-hunyuan-video-7b/ARC-Hunyuan-Video-training-overall_hu12166673853554469610.png 1024w"
loading="lazy"
alt="Training pipeline of ARC-Hunyuan-Video-7B"
class="gallery-image"
data-flex-grow="146"
data-flex-basis="350px"
>&lt;/p>
&lt;h4 id="pre-training">Pre-training
&lt;/h4>&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-arc-hunyuan-video-7b/ARC-Hunyuan-Video-annotation.png"
width="1362"
height="289"
srcset="https://maosong2022.github.io/p/notes-on-arc-hunyuan-video-7b/ARC-Hunyuan-Video-annotation_hu16957196791930419461.png 480w, https://maosong2022.github.io/p/notes-on-arc-hunyuan-video-7b/ARC-Hunyuan-Video-annotation_hu5574767225043336818.png 1024w"
loading="lazy"
alt="Boostrapped annotation pipeline of ARC-Hunyuan-Video-7B"
class="gallery-image"
data-flex-grow="471"
data-flex-basis="1131px"
>&lt;/p>
&lt;p>作者首先保住了一些数据。具体流程就是，使用 Whisper-v3 来转录带时间戳的音频，然后使用 InternVL-2.5-8B [[InternVL-2.5]] 来生成细粒度的 caption. 作者还使用 CoT prompt 策略，让模型一步步生成时间的描述，创作者的想法以及如何吸引用户的 tag 等。&lt;/p>
&lt;p>预训练数据如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Category&lt;/th>
&lt;th>data&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Video description and summary&lt;/td>
&lt;td>4.5M short-form video&lt;br>0.2M public video&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Image caption and OCR&lt;/td>
&lt;td>4.7M image-text pairs&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ASR&lt;/td>
&lt;td>3.2M audio-text pairs&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Video temporal grounding&lt;/td>
&lt;td>0.5M temporally grounding instances&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Video multi-granular caption&lt;/td>
&lt;td>50K high-quality samples&lt;br>80K in-house videos&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>训练包含两个 stage:&lt;/p>
&lt;ol>
&lt;li>Stage 1: 使用 ASR 数据训练模型接受音频输入的能力，为了避免视觉模态能力下降，作者还加入了 Image-text pair data 来训练，缺失的模态用 zero padding 来补齐&lt;/li>
&lt;li>Stage 2: 使用全部数据进行训练，仅训练 MLP 和 LLM&lt;/li>
&lt;/ol>
&lt;h4 id="post-training">Post-training
&lt;/h4>&lt;p>作者首先进行了消融实验，探究 human-annotated 数据对模型表现的影响。作者收集了 140 条人类标注的数据，然后基于这些数据进行消融实验：&lt;/p>
&lt;ol>
&lt;li>直接使用人类标注数据进行 SFT, 模型表现变化不大&lt;/li>
&lt;li>直接使用人类标注数据作为正样本，合成数据作为负样本进行 DPO, 模型表现变化也不大&lt;/li>
&lt;/ol>
&lt;p>作者分析原因认为，&lt;strong>人类标注数据和合成数据之间存在 distribution shift&lt;/strong>.&lt;/p>
&lt;p>受 DeepSeek-R1 启发，作者决定使用 GRPO 算法来提高模型的表现，训练任务包含两个：&lt;/p>
&lt;ol>
&lt;li>multi-dimensional Multi-choide QA: 提高模型的视频理解能力&lt;/li>
&lt;li>Temporal video grounding: 提高模型的时序感知能力&lt;/li>
&lt;/ol>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Stage&lt;/th>
&lt;th>Data&lt;/th>
&lt;th>Module&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>SFT&lt;/td>
&lt;td>MCQ:&lt;br> - 460K open-ended QA&lt;br> - 70K MCQA&lt;br> - 20K QA&lt;br>Grounding:&lt;br> - 10K academic&lt;br> - 5K real-world&lt;br>General:&lt;br> - 45K description&lt;br> - 12K caption&lt;/td>
&lt;td>MLP+LLM&lt;/td>
&lt;td>提高指令跟随能力&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Cold Start SFT&lt;/td>
&lt;td>- 90K MCQA&lt;br>- 18K temporal grounding&lt;br>- 20K open-ended QA&lt;br>- 15K summarization&lt;br>- 3K chapter-level captioning&lt;/td>
&lt;td>MLP+LLM&lt;/td>
&lt;td>初步激活模型的 reas 能力&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RL&lt;/td>
&lt;td>- 100K MCQ&lt;br>- 35K temporal grounding&lt;/td>
&lt;td>LLM&lt;/td>
&lt;td>提升模型的 reasoning 能力&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SFT&lt;/td>
&lt;td>- 25K human-annotated subjective question&lt;br>- 100K MCQ with CoT&lt;br>- 50K temporal grounding with reasoning traces&lt;/td>
&lt;td>-&lt;/td>
&lt;td>使用人类标注数据进一步提高模型的能力&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="experiments">Experiments
&lt;/h2>&lt;p>作者构建了一个评估短视频理解能力的 benchmark: ShortVid-Bench, 评估以下方面：&lt;/p>
&lt;ol>
&lt;li>Temporal Reasoning and Localization&lt;/li>
&lt;li>Affective Intent Classification&lt;/li>
&lt;li>Creator Intent Taxonomy&lt;/li>
&lt;li>Narrative Comprehension&lt;/li>
&lt;li>Humor &amp;amp; Meme Deconstruction&lt;/li>
&lt;li>Creative Innovation Analysis&lt;/li>
&lt;/ol>
&lt;p>对比的模型包括 Qwen2.5-VL-7B, Qwen2.5-omni-7B 和 Keye-VL-8B&lt;/p>
&lt;p>评估结果如下：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-arc-hunyuan-video-7b/ARC-Hunyuan-Video-performance.png"
width="1367"
height="244"
srcset="https://maosong2022.github.io/p/notes-on-arc-hunyuan-video-7b/ARC-Hunyuan-Video-performance_hu14059164927016574712.png 480w, https://maosong2022.github.io/p/notes-on-arc-hunyuan-video-7b/ARC-Hunyuan-Video-performance_hu131946593681889770.png 1024w"
loading="lazy"
alt="Performance of ARC-Hunyuan-Video-7B"
class="gallery-image"
data-flex-grow="560"
data-flex-basis="1344px"
>&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者提出了 ARC-Hunyuan-Video-7B, 一个针对短视频的视频理解多模态大模型。作者详细介绍了模型的架构，训练数据以及评估。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2507.20939" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/TencentARC/ARC-Hunyuan-Video-7B" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on VITA</title><link>https://maosong2022.github.io/p/notes-on-vita/</link><pubDate>Tue, 13 Aug 2024 14:36:45 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-vita/</guid><description>&lt;h1 id="tldr">TLDR
&lt;/h1>&lt;p>This paper proposes a Multimodal Large Language Model VITA (Video, Image, Text, Audio).
VITA supports non-awakening interaction and audio interruption for better interactive experience.
VITA aims to be an open-sourced version of GPT-4o.&lt;/p>
&lt;h1 id="introduction">Introduction
&lt;/h1>&lt;p>Features of GPT-4o:&lt;/p>
&lt;ol>
&lt;li>a unified framework that processes text, vision, and audio signals in an end-to-end manner,&lt;/li>
&lt;li>the capability to enable natural multimodal human-computer interaction.&lt;/li>
&lt;/ol>
&lt;p>Similar to Mini-GPT4, this paper tries to proposed an open-sourced version of GPT-4o.&lt;/p>
&lt;h1 id="method">Method
&lt;/h1>&lt;h2 id="model">Model
&lt;/h2>&lt;p>The architecture of VITA is shown as follows:
&lt;img src="https://maosong2022.github.io/p/notes-on-vita/VITA_architecture.png"
width="1282"
height="1069"
srcset="https://maosong2022.github.io/p/notes-on-vita/VITA_architecture_hu13166811980198188055.png 480w, https://maosong2022.github.io/p/notes-on-vita/VITA_architecture_hu6710414439966249089.png 1024w"
loading="lazy"
alt="Architecture of VITA"
class="gallery-image"
data-flex-grow="119"
data-flex-basis="287px"
>&lt;/p>
&lt;ul>
&lt;li>LLM: Mixtral $8\times 7$ B&lt;/li>
&lt;li>Visual Encoder: InternViT-300M-448px&lt;/li>
&lt;li>Audio Encoder: Mel Filter Bank block&lt;/li>
&lt;/ul>
&lt;p>To support audio interruption, the author uses two model at the same time, where the generation model is responsible for handling user queries and the other model monitors the environment. The other models starts to work is there is an audio interruption.&lt;/p>
&lt;h2 id="data">Data
&lt;/h2>&lt;h3 id="multimodal-instruction-tuning">multimodal instruction tuning
&lt;/h3>&lt;p>Training data of multimodal instruction tuning is given as follows:&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-vita/VITA_SFT_data.png"
width="1286"
height="672"
srcset="https://maosong2022.github.io/p/notes-on-vita/VITA_SFT_data_hu12706316391905990779.png 480w, https://maosong2022.github.io/p/notes-on-vita/VITA_SFT_data_hu6833967022569500187.png 1024w"
loading="lazy"
alt="Training data of multimodal instruction tuning"
class="gallery-image"
data-flex-grow="191"
data-flex-basis="459px"
>&lt;/p>
&lt;p>Improvements are made:&lt;/p>
&lt;ol>
&lt;li>The questions are randomly (about half) replaced with their audio versions, using TTS technique such as GPT-SoVITS&lt;/li>
&lt;li>Different system prompts are set to avoid conflicts between different types of data&lt;/li>
&lt;/ol>
&lt;p>To support human-AI interaction, the noisy audio data are also constructed. Noisy audio samples are generated from existed QA data. These negative sample texts aim to improve the ability of VITA to not respond to non-query-related content.&lt;/p>
&lt;p>To distinguish three types of queries, the author uses three state tokens:&lt;/p>
&lt;ul>
&lt;li>Token &lt;code>&amp;lt;1&amp;gt;&lt;/code> denotes that the question input is the query audio&lt;/li>
&lt;li>Token &lt;code>&amp;lt;2&amp;gt;&lt;/code> denotes that the question input is the noisy audio.&lt;/li>
&lt;li>Token &lt;code>&amp;lt;3&amp;gt;&lt;/code> signifies the question of pure text.&lt;/li>
&lt;/ul>
&lt;h2 id="training-pipeline">Training pipeline
&lt;/h2>&lt;p>Training pipeline of VITA consists of three stages:&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-vita/VITA_training_pipeline.png"
width="1301"
height="852"
srcset="https://maosong2022.github.io/p/notes-on-vita/VITA_training_pipeline_hu12778043419258304094.png 480w, https://maosong2022.github.io/p/notes-on-vita/VITA_training_pipeline_hu1848643356938254411.png 1024w"
loading="lazy"
alt="Training pipeline of VITA"
class="gallery-image"
data-flex-grow="152"
data-flex-basis="366px"
>&lt;/p>
&lt;h3 id="non-awakening-interaction">Non-awakening Interaction
&lt;/h3>&lt;p>There are following requirements and solutions:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Real-time Tracking of Environmental Sounds. This paper uses SileroVAD to complete the Voice Activity Detection (VAD) task.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Filtering out noisy audio. This is done by making use of token &lt;code>&amp;lt;2&amp;gt;&lt;/code>.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="audio-interrupt-interaction">Audio Interrupt Interaction
&lt;/h3>&lt;p>There are following requirements and solutions:&lt;/p>
&lt;ol>
&lt;li>Real-time Tracking and Filtering of External Queries. This is done by use another VITA model as stated in Model section.&lt;/li>
&lt;/ol>
&lt;h1 id="evaluation">Evaluation
&lt;/h1>&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-vita/VITA_evaluation.png"
width="1297"
height="536"
srcset="https://maosong2022.github.io/p/notes-on-vita/VITA_evaluation_hu3461940985657118198.png 480w, https://maosong2022.github.io/p/notes-on-vita/VITA_evaluation_hu13137771961429217061.png 1024w"
loading="lazy"
alt="Evaluation on image and video understanding"
class="gallery-image"
data-flex-grow="241"
data-flex-basis="580px"
>&lt;/p>
&lt;h1 id="conclusion">Conclusion
&lt;/h1>&lt;p>The paper points out three limitations of VITA:&lt;/p>
&lt;ol>
&lt;li>Enhancement of Foundational Capabilities.&lt;/li>
&lt;li>Refinement of Noisy Audio Construction.&lt;/li>
&lt;li>Building end-to-end TTS in conjunction with LLM.&lt;/li>
&lt;/ol>
&lt;h1 id="reference">Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2408.05211" target="_blank" rel="noopener"
>Arxiv paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://vita-home.github.io" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>