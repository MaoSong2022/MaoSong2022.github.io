<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Reasoning on Mao Song(毛松)'s Homepage</title><link>https://maosong.website/tags/reasoning/</link><description>Recent content in Reasoning on Mao Song(毛松)'s Homepage</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sat, 14 Feb 2026 10:00:05 +0800</lastBuildDate><atom:link href="https://maosong.website/tags/reasoning/index.xml" rel="self" type="application/rss+xml"/><item><title>Notes on Kimi-k2.5</title><link>https://maosong.website/p/notes-on-kimi-k2.5/</link><pubDate>Thu, 12 Feb 2026 11:13:13 +0800</pubDate><guid>https://maosong.website/p/notes-on-kimi-k2.5/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>Kimi-K2.5 的核心有亮点：&lt;/p>
&lt;ol>
&lt;li>native multi-modal: 通过在预训练，SFT, RL 阶段使用多模态数据来提高模型的多模态能力&lt;/li>
&lt;li>agent: 通过并行 multi-agent 的方式来提高模型解决复杂问题的效率和能力&lt;/li>
&lt;/ol>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;p>Kimi K2.5 是一个标准的 ViT-MLP-LLM 架构，其中&lt;/p>
&lt;ol>
&lt;li>ViT, 基于 &lt;a class="link" href="Kimi-VL.md" >Kimi-VL&lt;/a> 提出的 MoonViT, 并进行了改进, 参数量为 400M&lt;/li>
&lt;li>MLP, 基于 patch merger,&lt;/li>
&lt;li>LLM, 基于 &lt;a class="link" href="https://maosong.website/p/notes-on-kimi-k2/" target="_blank" rel="noopener"
>Kimi-k2&lt;/a>, 参数量为 1.02T-A32B&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>ViT&lt;/strong>
作者使用了 &lt;a class="link" href="Kimi-VL.md" >Kimi-VL&lt;/a> 提出的 MoonViT, MoonViT 基于 &lt;a class="link" href="SigLIP.md" >SigLIP&lt;/a> 提出的 SigLIP-SO-400M 开发得到，MoonViT 使用了 &lt;a class="link" href="NaViT.md" >NaViT&lt;/a> 来避免切分图片和使用不同精度图片进行训练。&lt;/p>
&lt;p>在 MoonViT 的基础上，Kimi-K2.5 还进一步提出了 MoonViT-3D, 将 &lt;a class="link" href="NaViT.md" >NaViT&lt;/a> 的思想扩展到了 3D 用于提高模型的视频理解能力，具体做法为将连续 4 帧的视频展开为 1D sequence, 这样在图像上的注意力机制就可以无缝衔接到视频上了。并且，通过这种方式，我们可以让模型关注跨帧的信息（注意力在 4 帧的 token 之间进行），简化代码如下所示&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># config.json temporal_merge_kernel_size&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># kimi_k25_vision_processing.py split_video_chunks&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">video_chunk&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">frames&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mi">4&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">patches&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">frame&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">video_chunk&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">patches&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">extend&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">split_into_patches&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">frame&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">tokens&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">patches&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># modeling_kimi_k25.py Learnable2DInterpPosEmbDivided_fixed&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">positions&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">spatial_embedding&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">temporal_embedding&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># modeling_kimi_k25.py MoonViT3dEncoder&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">transformer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tokens&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">positions&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>最后，在进入 MLP 之前，作者还对每个 temporal chunk 内的特征进行 pooling 操作，将时序长度压缩到了原来的 1/4, 进而提高模型可处理的视频长度。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># modeling_kimi_k25.py tpool_patch_merger&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">tpool_patch_merger&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">grid_thws&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">merge_kernel_size&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">tuple&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">list&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">d_model&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">outputs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pre_sum&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">t&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">h&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">w&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">grid_thws&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tolist&lt;/span>&lt;span class="p">():&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Get the current sequence&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">seq&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">pre_sum&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">pre_sum&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">t&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">h&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">w&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Reshape along self.merge_kernel_size and concat to the last dimension&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">kernel_height&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">kernel_width&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">merge_kernel_size&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">new_height&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">new_width&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">h&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="n">kernel_height&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">w&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="n">kernel_width&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">reshaped_seq&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">seq&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">new_height&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">kernel_height&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">new_width&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">kernel_width&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d_model&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">reshaped_seq&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">reshaped_seq&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">permute&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">4&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">contiguous&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mean&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># temporal pooling&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">padded_seq&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">reshaped_seq&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">new_height&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">new_width&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">kernel_height&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">kernel_width&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">outputs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">padded_seq&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pre_sum&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">t&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">h&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">w&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">outputs&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;strong>MLP&lt;/strong>
MLP 使用了 PatchMerger, 用于减少视觉 token 个数，这个方案在之前的 Qwen-VL 系列里已经得到了应用。&lt;/p>
&lt;p>&lt;strong>LLM&lt;/strong>
LLM 基于 &lt;a class="link" href="https://maosong.website/p/notes-on-kimi-k2/" target="_blank" rel="noopener"
>Kimi-k2&lt;/a> 的 MoE 模型，总参数为 1T, 激活参数为 32B&lt;/p>
&lt;h3 id="data">&lt;a href="#data" class="header-anchor">&lt;/a>Data
&lt;/h3>&lt;h3 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h3>&lt;p>预训练阶段一共使用了 15T token, 分为了三个阶段：&lt;/p>
&lt;ol>
&lt;li>ViT-training: 单独训练 ViT, 实际用了 image caption, grounding, ocr, video 等数据进行训练，训练方式采用了类似 InternVL 的方式，即通过 cross entropy loss 来与一个清凉话的 LLM 进行对齐，这个阶段训练使用了 1T token, 然后作者使用了一个非常短的 stage 来更新 MLP 用于对齐 ViT 和 Kimi-K2&lt;/li>
&lt;li>Joint pre-training: 训练所有参数，长下文长度为 4K, 使用了 15T token. 这里主要强调了提升代码数据的比例&lt;/li>
&lt;li>Long context mid-training: 使用 &lt;a class="link" href="https://maosong.website/p/notes-on-yarn/" target="_blank" rel="noopener"
>YARN&lt;/a> 来提高模型的上下文长度&lt;/li>
&lt;/ol>
&lt;p>最终预训练阶段 recipe 如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2.5/Kimi-k2.5-pre-training.png"
width="1430"
height="415"
loading="lazy"
alt="Pre-training recipe of Kimi-K2.5"
class="gallery-image"
data-flex-grow="344"
data-flex-basis="826px"
>&lt;/p>
&lt;p>&lt;strong>Native Multimodal pre-training&lt;/strong>
在 Joint pre-training stage, Kimi-K2.5 还采用了一个与 &lt;a class="link" href="InternVL3.md" >InternVL3&lt;/a> 类似的策略，即在预训练一开始直接使用多模态数据进行预训练。&lt;/p>
&lt;p>传统的多模态大模型往往基于一个比较成熟的 LLM backbone 来完成多模态大模型的训练，但是其问题在于成熟的 LLM 其表示空间会收敛到语言模态上，多模态信息的迁移能力比较差。&lt;a class="link" href="InternVL3.md" >InternVL3&lt;/a> 虽然也是 native multimodal pre-training, 但是其仍然依赖于成熟的 LLM. Kimi K2.5 则是使用预训练阶段的 Kimi K2 作为 backbone 来避免表示空间的塌缩，在训练一开始即直接加入少量多模态数据来保持模型的多模态能力。&lt;/p>
&lt;p>作者探究了预训练阶段不同的数据对比，试验结果如下图所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Vision Injection Timing&lt;/th>
&lt;th>Vision-Text Ratio&lt;/th>
&lt;th>Vision Knowledge&lt;/th>
&lt;th>Vision Reasoning&lt;/th>
&lt;th>OCR&lt;/th>
&lt;th>Text Knowledge&lt;/th>
&lt;th>Text Reasoning&lt;/th>
&lt;th>Code&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Early&lt;/td>
&lt;td>0%&lt;/td>
&lt;td>10%:90%&lt;/td>
&lt;td>25.8&lt;/td>
&lt;td>43.8&lt;/td>
&lt;td>65.7&lt;/td>
&lt;td>45.5&lt;/td>
&lt;td>58.5&lt;/td>
&lt;td>24.8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Mid&lt;/td>
&lt;td>50%&lt;/td>
&lt;td>20%:80%&lt;/td>
&lt;td>25.0&lt;/td>
&lt;td>40.7&lt;/td>
&lt;td>64.1&lt;/td>
&lt;td>43.9&lt;/td>
&lt;td>58.6&lt;/td>
&lt;td>24.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Late&lt;/td>
&lt;td>80%&lt;/td>
&lt;td>50%:50%&lt;/td>
&lt;td>24.2&lt;/td>
&lt;td>39.0&lt;/td>
&lt;td>61.5&lt;/td>
&lt;td>43.1&lt;/td>
&lt;td>57.8&lt;/td>
&lt;td>24.0&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>结果显示，在训练早期加入少部分的多模态数据可以有效提高模型的表现。&lt;/p>
&lt;h3 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h3>&lt;p>Post-training 分为了 SFT 和 RL, SFT 阶段作者使用了合成的高质量数据，主要提升模型的交互式推理能力以及工具调用能力。为了解决传统 VLM 工具调用能力比较差且扩展性差的问题，Kimi-k2.5 提出了 Zero-Vision SFT, 其核心思想模型在预训练阶段已经完成了多模态对齐，因此我们可以仅使用纯文本 SFT 数据来激活 VLM 的视觉 agent 能力，具体做法就是将所有图像操作通过 IPython 的代码进行代理操作，这样视觉工具的调用就编程了程序化的图像处理指令。&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2.5/Kimi-k2.5-zero-vision-sft.png"
width="1340"
height="796"
loading="lazy"
alt="Performance of Vision RL on zero-vision SFT"
class="gallery-image"
data-flex-grow="168"
data-flex-basis="404px"
>&lt;/p>
&lt;p>在 RL 阶段，作者基于 &lt;a class="link" href="https://maosong.website/p/notes-on-kimi-k1.5/" target="_blank" rel="noopener"
>Kimi-k1.5&lt;/a> 提出的策略优化算法加入了一个 token-level clipping 机制来减少 off-policy divergence, 目标函数如下所示&lt;/p>
$$
\mathcal{L}(\theta)=\mathbb{E}_{x\sim\mathcal{D}}\left[\frac{1}{N}\sum_{j=1}^k\sum_{i=1}^{|y_i|}\mathrm{clip}\left(\log\frac{\pi_{\theta}(y_j^i\mid x, y_{j}^{0:i})}{\pi_{\mathrm{old}}(y_j^i\mid x, y_{j}^{0:i})},\alpha,\beta\right)(r(x, y_j) - \bar{r}(x)) - \tau\left(\log\frac{\pi_{\theta}(y_j^i\mid x, y_{j}^{0:i})}{\pi_{\mathrm{old}}(y_j^i\mid x, y_{j}^{0:i})}\right)^2\right]
$$&lt;p>其中 $k$ 是针对每个回答 $x$ 的采样次数，$N=\sum_{j=1}^k|y_j|$ 是一个 batch 里总的 token 个数， $\alpha,\beta,\tau$ 为超参数，$\bar{r}(x)$ 是对 normalization 的估计，这里采用了 Kimi-K1.5 的 mean reward, 即 $\bar{r}(x)=1/k\sum_{j=1}^Kr(x,y_j)$. 这里的 clipping 机制与 PPO 不同的地方在于针对 log-ratio 进行 clipping, 而不依赖于 advantage 的计算。最终训练时使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-moonlight/" target="_blank" rel="noopener"
>Moonlight&lt;/a> 的 MuonClip 算法&lt;/p>
&lt;p>对于 reward 的设计，Kimi-k2.5 也使用了基于规则和基于 reward model 的方式，前者针对答案可验证的任务，后者针对开放式的任务。&lt;/p>
&lt;p>作者还构建了 length penalty 来提高模型的推理效率，作者发现 &lt;a class="link" href="https://maosong.website/p/notes-on-kimi-k1.5/" target="_blank" rel="noopener"
>Kimi-k1.5&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/notes-on-kimi-k2/" target="_blank" rel="noopener"
>Kimi-k2&lt;/a> 中的 length penalty 虽然可以生成更准确的 reasoning chain, 但是其很难泛化到更高的算力. 为了解决这个问题，作者提出了 &lt;strong>Toggle&lt;/strong> 策略，即在 inference-time scaling 和 budget-constrained optimization 两种模式之间进行切换优化，对应的 reward 定义为&lt;/p>
$$
\tilde{r}(x,y) = \begin{cases}
r(x,y)*\mathbb{I}\{1/k\sum_{j=1}^kr(x,y_i&lt;\lambda \text{ or }|y_j|\leq \text{budget}(x)\},&amp;\text{if }\lfloor t/m\rfloor\mod 2 == 0\ (\text{Phase }0)\\
r(x,y),&amp;\text{otherwise } (\text{Phase }1)
\end{cases}
$$&lt;p>其中 $\lambda, m$ 都是超参数。budget 基于正确回答的长度的 p 分位得到：&lt;/p>
$$
\text{budget}(x) = \text{Percentile}(\{|y_j| \mid r(x,y_j)=1, j\in[k]\},\rho)
$$&lt;p>两种模式每隔 $m$ 个 iteration 切换一次：&lt;/p>
&lt;ul>
&lt;li>phase 0: budget limited phase, 训练模型在给定 token budget 下解决问题，减少 reasoning chain 长度&lt;/li>
&lt;li>phase 1: scaling phase, 训练模型使用更多的算力来解决更复杂的问题，提高模型的智能程度&lt;/li>
&lt;/ul>
&lt;p>作者评估 Toogle 策略得到的结果如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2.5/Kimi-k2.5-toggle.png"
width="1200"
height="689"
loading="lazy"
alt="Performance of toggle strategy"
class="gallery-image"
data-flex-grow="174"
data-flex-basis="417px"
>&lt;/p>
&lt;p>结果现实，使用 toggle 策略之后，模型的输出长度减少了 30% 左右，且模型的表现并没有明显下降。作者还发现，一些重复的 pattern 也随之降低，且 toggle 策略的泛化程度更高。&lt;/p>
&lt;p>在 Zero-Vision SFT 的基础上，Kimi-k2.5 使用了 Joint multimodal RL 训练策略。现有的多模态 RL 存在的问题为：模型很容易忽略视觉输入而过度依赖于纯文本进行推理。为了解决这个问题，作者构建了需要视觉理解才能得到答案的任务来提高模型对于视觉信息的利用程度，这些任务覆盖三个 domain:&lt;/p>
&lt;ol>
&lt;li>visual grounding and counting: 定位和计数&lt;/li>
&lt;li>chart and document understanding: 图表文档理解&lt;/li>
&lt;li>vision-critical STEM problems: 需要图片来完成求解的数学物理问题&lt;/li>
&lt;/ol>
&lt;p>作者在 visual RL 之后评估了模型的表现，发现模型在 MMLU-Pro, GPQA-Diamond 等任务上的表现都有了提升，作者认为 visual RL 可以在不损害模型纯文本能力的情况下提高模型跨模态的泛化性&lt;/p>
&lt;h3 id="agent-swarm">&lt;a href="#agent-swarm" class="header-anchor">&lt;/a>Agent Swarm
&lt;/h3>&lt;p>Kimi-k2.5 的另一个重大改进为使用并行机制来提高模型的 agent 能力。传统的 agent 往往序列执行 reasoning, tool-use, 这限制了模型处理复杂任务的能力，Kimi-k2.5 通过 Agent Swarm 和 Parallel Agent Reinforcement Learning (PARL) 来解决这个问题，其核心思想就是并行，框架图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2.5/Kimi-k2.5-agent-swarm.png"
width="1354"
height="729"
loading="lazy"
alt="Framework of Agent Swarm"
class="gallery-image"
data-flex-grow="185"
data-flex-basis="445px"
>&lt;/p>
&lt;p>agent swarm 架构包含了一个 orchestrator 和若干个 subagent, 为了解决 agent swarm 的 reward 比较难以设置的问题，PARL 构建了三个不同 level 的 reward&lt;/p>
$$
r_{PARL}(x,y) = \lambda_1 r_{parallel} + \lambda_2r_{finish} + \lambda_3r_{perf}(x,y)
$$&lt;p>其中 $r_{perf}$ 评估了 solution $y$ 的质量， $r_{parallel}$ 则是避免并行模式崩塌，从 multi-agent 崩塌为 single agent, $r_{finish}$ 则是评估模型的完成性。超参数 $\lambda_1,\lambda_2$ 随训练逐渐降为 0 来提高模型整体的表现。&lt;/p>
&lt;p>作者还提出了使用 critical steps 来评估 parallel agent 的计算时间消耗，其计算公式如下&lt;/p>
$$
CriticalSteps = \sum_{i=1}^T\left(S_{main}^{(t)}+\max_iS_{sub,i}^{(t)}\right)
$$&lt;p>其中 $T$ 为一个 episode 的时间，$S_{main}^{(t)}$ 为 orchestrator 在第 $t$ 步的运行时间， $S_{sub,i}^{(t)}$ 为第 i 个 subagent 的运行时间。&lt;/p>
&lt;p>为了提高模型的并行能力，作者构建了一批广度优先搜索和深度优先搜索的数据，通过这些数据的构建，我们可以提高 orchestrator 的并行调用能力。&lt;/p>
&lt;p>最终，PARL 的表现如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2.5/Kimi-k2.5-PARL.png"
width="1501"
height="441"
loading="lazy"
alt="Performance of PARL"
class="gallery-image"
data-flex-grow="340"
data-flex-basis="816px"
>&lt;/p>
&lt;h3 id="infra">&lt;a href="#infra" class="header-anchor">&lt;/a>Infra
&lt;/h3>&lt;p>Kimi-k2.5 的 infra 基于 &lt;a class="link" href="https://maosong.website/p/notes-on-kimi-k2/" target="_blank" rel="noopener"
>Kimi-k2&lt;/a>, 作者主要强调了 decouple encoder process (DEP) 这一改进。之前的工作将 vision encoder 和 text embedding 都做为 PP 的第一个 stage, 但是由于 vision encoder 对不同输入的处理时间不同，这个 stage 的算历和内存分配随输入变化比较大。为了解决这个问题，作者提出了 DEP, 包含三个 stage 来提高训练效率:&lt;/p>
&lt;ol>
&lt;li>balanced vision forward: 由于 vision encoder 比较小 (400M), 因此作者将 vision encoder 复制到所有 GPU 上，然后根据负载来将 visual data 分配到不同的 GPU 上进行处理，这个阶段不保存中间激活值，处理完毕之后所有的结果作为 PP Stage0 的输入&lt;/li>
&lt;li>backbone training: 正常进行训练，与 LLM 的训练优化一致&lt;/li>
&lt;li>vision recomputation &amp;amp; backward: 这个阶段，我们重新计算 vision encoder 的 forward pass, 然后再对 vision encoder 进行反向传播&lt;/li>
&lt;/ol>
&lt;p>通过 DEP, Kimi-k2.5 的训练效率达到了 Kimi-k2 的 90%.&lt;/p>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>首先是 Kimi-k2.5 在 general &amp;amp; reasoning 类任务上的表现，可以看到，Kimi-k2.5 超过了 DeeoSeek-V3.2 的表现，&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Benchmark&lt;/th>
&lt;th>Kimi K2.5&lt;/th>
&lt;th>Claude Opus 4.5&lt;/th>
&lt;th>GPT-5.2 (xhigh)&lt;/th>
&lt;th>Gemini 3 Pro&lt;/th>
&lt;th>DeepSeek-V3.2&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>HLE-Full&lt;/td>
&lt;td>30.1&lt;/td>
&lt;td>30.8&lt;/td>
&lt;td>34.5&lt;/td>
&lt;td>37.5&lt;/td>
&lt;td>25.1†&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>HLE-Full w/ tools&lt;/td>
&lt;td>50.2&lt;/td>
&lt;td>43.2&lt;/td>
&lt;td>45.5&lt;/td>
&lt;td>45.8&lt;/td>
&lt;td>40.8†&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>AIME 2025&lt;/td>
&lt;td>96.1&lt;/td>
&lt;td>92.8&lt;/td>
&lt;td>100&lt;/td>
&lt;td>95.0&lt;/td>
&lt;td>93.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>HMMT 2025 (Feb)&lt;/td>
&lt;td>95.4&lt;/td>
&lt;td>92.9*&lt;/td>
&lt;td>99.4&lt;/td>
&lt;td>97.3*&lt;/td>
&lt;td>92.5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>IMO-AnswerBench&lt;/td>
&lt;td>81.8&lt;/td>
&lt;td>78.5*&lt;/td>
&lt;td>86.3&lt;/td>
&lt;td>83.1*&lt;/td>
&lt;td>78.3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GPQA-Diamond&lt;/td>
&lt;td>87.6&lt;/td>
&lt;td>87.0&lt;/td>
&lt;td>92.4&lt;/td>
&lt;td>91.9&lt;/td>
&lt;td>82.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MMLU-Pro&lt;/td>
&lt;td>87.1&lt;/td>
&lt;td>89.3*&lt;/td>
&lt;td>86.7*&lt;/td>
&lt;td>90.1&lt;/td>
&lt;td>85.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SimpleQA Verified&lt;/td>
&lt;td>36.9&lt;/td>
&lt;td>44.1&lt;/td>
&lt;td>38.9&lt;/td>
&lt;td>72.1&lt;/td>
&lt;td>27.5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>AdvancedIF&lt;/td>
&lt;td>75.6&lt;/td>
&lt;td>63.1&lt;/td>
&lt;td>81.1&lt;/td>
&lt;td>74.7&lt;/td>
&lt;td>58.8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LongBench v2&lt;/td>
&lt;td>61.0&lt;/td>
&lt;td>64.4*&lt;/td>
&lt;td>54.5*&lt;/td>
&lt;td>68.2*&lt;/td>
&lt;td>59.8*&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>接下来是模型在 coding 任务上的表现&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Benchmark&lt;/th>
&lt;th>Kimi K2.5&lt;/th>
&lt;th>Claude Opus 4.5&lt;/th>
&lt;th>GPT-5.2 (xhigh)&lt;/th>
&lt;th>Gemini 3 Pro&lt;/th>
&lt;th>DeepSeek-V3.2&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>SWE-Bench Verified&lt;/td>
&lt;td>76.8&lt;/td>
&lt;td>80.9&lt;/td>
&lt;td>80.0&lt;/td>
&lt;td>76.2&lt;/td>
&lt;td>73.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SWE-Bench Pro (public)&lt;/td>
&lt;td>50.7&lt;/td>
&lt;td>55.4*&lt;/td>
&lt;td>55.6&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SWE-Bench Multilingual&lt;/td>
&lt;td>73.0&lt;/td>
&lt;td>77.5&lt;/td>
&lt;td>72.0&lt;/td>
&lt;td>65.0&lt;/td>
&lt;td>70.2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Terminal Bench 2.0&lt;/td>
&lt;td>50.8&lt;/td>
&lt;td>59.3&lt;/td>
&lt;td>54.0&lt;/td>
&lt;td>54.2&lt;/td>
&lt;td>46.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>PaperBench (CodeDev)&lt;/td>
&lt;td>63.5&lt;/td>
&lt;td>72.9*&lt;/td>
&lt;td>63.7*&lt;/td>
&lt;td>-&lt;/td>
&lt;td>47.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CyberGym&lt;/td>
&lt;td>41.3&lt;/td>
&lt;td>50.6&lt;/td>
&lt;td>-&lt;/td>
&lt;td>39.9*&lt;/td>
&lt;td>17.3*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SciCode&lt;/td>
&lt;td>48.7&lt;/td>
&lt;td>49.5&lt;/td>
&lt;td>52.1&lt;/td>
&lt;td>56.1&lt;/td>
&lt;td>38.9&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OIBench (cpp)&lt;/td>
&lt;td>57.4&lt;/td>
&lt;td>54.6*&lt;/td>
&lt;td>-&lt;/td>
&lt;td>68.5*&lt;/td>
&lt;td>54.7*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LiveCodeBench (v6)&lt;/td>
&lt;td>85.0&lt;/td>
&lt;td>82.2*&lt;/td>
&lt;td>-&lt;/td>
&lt;td>87.4*&lt;/td>
&lt;td>83.3&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>在 agent 任务上的表现&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Benchmark&lt;/th>
&lt;th>Kimi K2.5&lt;/th>
&lt;th>Claude Opus 4.5&lt;/th>
&lt;th>GPT-5.2 (xhigh)&lt;/th>
&lt;th>Gemini 3 Pro&lt;/th>
&lt;th>DeepSeek-V3.2&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>BrowseComp&lt;/td>
&lt;td>60.6&lt;/td>
&lt;td>37.0&lt;/td>
&lt;td>65.8&lt;/td>
&lt;td>37.8&lt;/td>
&lt;td>51.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>BrowseComp (w/ ctx manage)&lt;/td>
&lt;td>74.9&lt;/td>
&lt;td>57.8&lt;/td>
&lt;td>-&lt;/td>
&lt;td>59.2&lt;/td>
&lt;td>67.6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>BrowseComp (Agent Swarm)&lt;/td>
&lt;td>78.4&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>WideSearch&lt;/td>
&lt;td>72.7&lt;/td>
&lt;td>76.2*&lt;/td>
&lt;td>-&lt;/td>
&lt;td>57.0&lt;/td>
&lt;td>32.5*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>WideSearch (Agent Swarm)&lt;/td>
&lt;td>79.0&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DeepSearchQA&lt;/td>
&lt;td>77.1&lt;/td>
&lt;td>76.1*&lt;/td>
&lt;td>71.3*&lt;/td>
&lt;td>63.2*&lt;/td>
&lt;td>60.9*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FinSearchCompT2&amp;amp;T3&lt;/td>
&lt;td>67.8&lt;/td>
&lt;td>66.2*&lt;/td>
&lt;td>-&lt;/td>
&lt;td>49.9&lt;/td>
&lt;td>59.1*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Seal-0&lt;/td>
&lt;td>57.4&lt;/td>
&lt;td>47.7*&lt;/td>
&lt;td>45.0&lt;/td>
&lt;td>45.5*&lt;/td>
&lt;td>49.5*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GDPVal-AA&lt;/td>
&lt;td>41.0&lt;/td>
&lt;td>45.0&lt;/td>
&lt;td>48.0&lt;/td>
&lt;td>35.0&lt;/td>
&lt;td>34.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OSWorld-Verified&lt;/td>
&lt;td>63.3&lt;/td>
&lt;td>66.3&lt;/td>
&lt;td>8.6&lt;/td>
&lt;td>20.7&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>WebArena&lt;/td>
&lt;td>58.9&lt;/td>
&lt;td>63.4&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>多模态表现&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Benchmark&lt;/th>
&lt;th>Kimi K2.5&lt;/th>
&lt;th>Claude Opus 4.5&lt;/th>
&lt;th>GPT-5.2 (xhigh)&lt;/th>
&lt;th>Gemini 3 Pro&lt;/th>
&lt;th>Qwen3-VL-235B-A22B&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;strong>Image&lt;/strong>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MMMU-Pro&lt;/td>
&lt;td>78.5&lt;/td>
&lt;td>74.0&lt;/td>
&lt;td>79.5*&lt;/td>
&lt;td>81.0&lt;/td>
&lt;td>69.3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MMMU (val)&lt;/td>
&lt;td>84.3&lt;/td>
&lt;td>80.7&lt;/td>
&lt;td>86.7*&lt;/td>
&lt;td>87.5*&lt;/td>
&lt;td>80.6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CharXiv (RQ)&lt;/td>
&lt;td>77.5&lt;/td>
&lt;td>67.2*&lt;/td>
&lt;td>82.1&lt;/td>
&lt;td>81.4&lt;/td>
&lt;td>66.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MathVision&lt;/td>
&lt;td>84.2&lt;/td>
&lt;td>77.1*&lt;/td>
&lt;td>83.0&lt;/td>
&lt;td>86.1*&lt;/td>
&lt;td>74.6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MathVista (mini)&lt;/td>
&lt;td>90.1&lt;/td>
&lt;td>80.2*&lt;/td>
&lt;td>82.8*&lt;/td>
&lt;td>89.8*&lt;/td>
&lt;td>85.8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SimpleVQA&lt;/td>
&lt;td>71.2&lt;/td>
&lt;td>69.7*&lt;/td>
&lt;td>55.8*&lt;/td>
&lt;td>69.7*&lt;/td>
&lt;td>56.8*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>WorldVQA&lt;/td>
&lt;td>46.3&lt;/td>
&lt;td>36.8&lt;/td>
&lt;td>28.0&lt;/td>
&lt;td>47.4&lt;/td>
&lt;td>23.5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ZeroBench&lt;/td>
&lt;td>9&lt;/td>
&lt;td>3*&lt;/td>
&lt;td>9*&lt;/td>
&lt;td>8*&lt;/td>
&lt;td>4*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ZeroBench w/ tools&lt;/td>
&lt;td>11&lt;/td>
&lt;td>9*&lt;/td>
&lt;td>7*&lt;/td>
&lt;td>12*&lt;/td>
&lt;td>3*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>BabyVision&lt;/td>
&lt;td>36.5&lt;/td>
&lt;td>14.2&lt;/td>
&lt;td>34.4&lt;/td>
&lt;td>49.7&lt;/td>
&lt;td>22.2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>BLINK&lt;/td>
&lt;td>78.9&lt;/td>
&lt;td>68.8*&lt;/td>
&lt;td>-&lt;/td>
&lt;td>78.7*&lt;/td>
&lt;td>68.9&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MMVP&lt;/td>
&lt;td>87.0&lt;/td>
&lt;td>80.0*&lt;/td>
&lt;td>83.0*&lt;/td>
&lt;td>90.0*&lt;/td>
&lt;td>84.3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OmniDocBench 1.5&lt;/td>
&lt;td>88.8&lt;/td>
&lt;td>87.7*&lt;/td>
&lt;td>85.7&lt;/td>
&lt;td>88.5&lt;/td>
&lt;td>82.0*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OCRBench&lt;/td>
&lt;td>92.3&lt;/td>
&lt;td>86.5*&lt;/td>
&lt;td>80.7*&lt;/td>
&lt;td>90.3*&lt;/td>
&lt;td>87.5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>InfoVQA (test)&lt;/td>
&lt;td>92.6&lt;/td>
&lt;td>76.9*&lt;/td>
&lt;td>84*&lt;/td>
&lt;td>57.2*&lt;/td>
&lt;td>89.5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>Video&lt;/strong>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>VideoMMMU&lt;/td>
&lt;td>86.6&lt;/td>
&lt;td>84.4*&lt;/td>
&lt;td>85.9&lt;/td>
&lt;td>87.6&lt;/td>
&lt;td>80.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MMVU&lt;/td>
&lt;td>80.4&lt;/td>
&lt;td>77.3*&lt;/td>
&lt;td>80.8*&lt;/td>
&lt;td>77.5*&lt;/td>
&lt;td>71.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MotionBench&lt;/td>
&lt;td>70.4&lt;/td>
&lt;td>60.3&lt;/td>
&lt;td>64.8&lt;/td>
&lt;td>70.3&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Video-MME&lt;/td>
&lt;td>87.4&lt;/td>
&lt;td>66.0*&lt;/td>
&lt;td>86.0*&lt;/td>
&lt;td>88.4*&lt;/td>
&lt;td>79.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LongVideoBench&lt;/td>
&lt;td>79.8&lt;/td>
&lt;td>67.2*&lt;/td>
&lt;td>76.5*&lt;/td>
&lt;td>77.7*&lt;/td>
&lt;td>65.6*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LVBench&lt;/td>
&lt;td>75.9&lt;/td>
&lt;td>57.3&lt;/td>
&lt;td>-&lt;/td>
&lt;td>73.5*&lt;/td>
&lt;td>63.6&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>我们这里基于模型在不同类别任务上的排名来进行可视化，结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2.5/Kimi-k2.5-rank-frequency.png"
width="1590"
height="790"
loading="lazy"
alt="Rank performance of difference models"
class="gallery-image"
data-flex-grow="201"
data-flex-basis="483px"
>&lt;/p>
&lt;p>从结果可以看出，Kimi-K2.5 的 agent 能力达到了 SOTA 级别，其多模态能力也比较强。&lt;/p>
&lt;p>与 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v3.2/" target="_blank" rel="noopener"
>DeepSeek-V3.2&lt;/a> 一样，作者也对比了不同模型的推理效率，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2.5/Kimi-k2.5-reasoning-efficiency.png"
width="783"
height="262"
loading="lazy"
alt="Reasoning efficiency of Kimi K2.5"
class="gallery-image"
data-flex-grow="298"
data-flex-basis="717px"
>&lt;/p>
&lt;p>可以看到，相比与 Kimi-K2, Kimi-K2.5 通过在 RL 层面进行优化，降低了输出长度，但是相比与 DeepSeel-V3.2 和 Gemini3.0 Pro 之间还存在一定差距。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>在本文中，作者提出了 Kimi-k2.5， 一个多模态的 agent model, Kimi-k2.5 集成了 Kimi-k2 和 Kimi-VL 的能力，扩展了模型的 agent 能力。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://huggingface.co/moonshotai/Kimi-K2.5" target="_blank" rel="noopener"
>huggingface&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2602.02276" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on DeepSeek-V3.2</title><link>https://maosong.website/p/notes-on-deepseek-v3.2/</link><pubDate>Tue, 06 Jan 2026 17:30:40 +0800</pubDate><guid>https://maosong.website/p/notes-on-deepseek-v3.2/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者首先回顾了开源模型如 &lt;a class="link" href="https://maosong.website/p/notes-on-minimax-01/" target="_blank" rel="noopener"
>MiniMax-01&lt;/a>, &lt;a class="link" href="https://maosong.website/p/notes-on-kimi-k2/" target="_blank" rel="noopener"
>Kimi-k2&lt;/a>, &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a>, &lt;a class="link" href="https://maosong.website/p/notes-on-glm-4.5/" target="_blank" rel="noopener"
>GLM-4.5&lt;/a> 和闭源模型的进展，作者指出，现在的开源模型和闭源模型在表现上仍然存在较大差距。作者认为这种差距主要是由于三个原因：&lt;/p>
&lt;ol>
&lt;li>Transformer 提出的 softmax attention 在处理长文本时效率非常低&lt;/li>
&lt;li>已有的开源模型在 post-training 阶段使用的算力不够&lt;/li>
&lt;li>开原模型的泛化和指令跟随能力不如闭源模型&lt;/li>
&lt;/ol>
&lt;p>基于这三个问题，DeepSeek-V3.2 分别进行了改进：&lt;/p>
&lt;ol>
&lt;li>在架构上，作者提出了 DSA，一个高效的稀疏注意力机制，用于降低计算复杂度&lt;/li>
&lt;li>在 post-training 阶段，作者使用了比 pre-training 阶段高 $10\%$ 的算力，用于提高模型的能力&lt;/li>
&lt;li>作者提出了一个 pipeline 用于提高模型在工具调用场景下的 reasoning 能力&lt;/li>
&lt;/ol>
&lt;p>通过实验作者发现，模型达到了和 &lt;a class="link" href="https://maosong.website/p/notes-on-kimi-k2/" target="_blank" rel="noopener"
>Kimi-k2&lt;/a> 以及 GPT-5 差不多的 reasoning 表现。&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;p>DeepSeek-V3.2 与 DeepSeek-V3.1 不同之处在于使用了 DeepSeek Sparse Attention (DSA). 架构如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3.2/DeepSeek-V3-2-architecture.png"
width="1285"
height="672"
loading="lazy"
alt="Attention architecture of DeepSeek-V3.2-Exp"
class="gallery-image"
data-flex-grow="191"
data-flex-basis="458px"
>&lt;/p>
&lt;p>DSA 包含两个模块：&lt;/p>
&lt;ol>
&lt;li>lightning indexer&lt;/li>
&lt;li>fine-grained token selection mechanism&lt;/li>
&lt;/ol>
&lt;p>其中，lightning indexer 负责计算 query token $h_t\in\mathbb{R}^d$ 和一个 preceding token $h_s\in\mathbb{R}^d$ 之间的 index score $I_{t,s}$ 来决定 query token 选择的 token:&lt;/p>
$$
I_{t,s} = \sum_{j=1}^{H_I}w_{t,j}^I \mathrm{ReLU}(q_{t,j}^I\cdot k_s^I)
$$&lt;p>其中， $H^I$ 代表 indexer heads 的个数，$q_{t,j}^I\in\mathbb{R}^{d^I}$ 和 $w_{t,j}^I$ 由 query token $h_t$ 得到，$k_s^I\in\mathbb{R}^{d^I}$ 由 preceding token $h_s$ 得到&lt;/p>
&lt;p>给定 query token $h_t$ 对应的 index score $\{I_{t,s}\}$, fine-grained token selection mechanism 负责选取 top-K index score 对应的 key-value entries $\{c_s\}$, 然后 attention 的输出由 query token 个选取的 key value entries 得到：&lt;/p>
$$
u_t = \mathrm{Attn}(h_t,\{c_s\mid I_{t,s}\in \mathrm{TopK}(I_{t,:})\})
$$&lt;blockquote>
&lt;p>[!Recall]
MoBA 也提出了类似的方法，但是 MoBA 是一个无需训练的策略&lt;/p>
&lt;/blockquote>
&lt;p>受 &lt;a class="link" href="https://maosong.website/p/notes-on-nsa/" target="_blank" rel="noopener"
>NSA&lt;/a> 启发，作者实现了基于 &lt;a class="link" href="https://maosong.website/p/notes-on-mqa/" target="_blank" rel="noopener"
>MQA&lt;/a> 模式的 &lt;a class="link" href="https://maosong.website/p/notes-on-mla/" target="_blank" rel="noopener"
>MLA&lt;/a>, 其中 latent vector 对于 query token 所有的 query heads 都是共享的。示意图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3.2/DeepSeek-V3-2-modes-of-MLA.png"
width="1344"
height="533"
loading="lazy"
alt="Different modes of MLA"
class="gallery-image"
data-flex-grow="252"
data-flex-basis="605px"
>&lt;/p>
&lt;h3 id="continue-pre-training">&lt;a href="#continue-pre-training" class="header-anchor">&lt;/a>Continue Pre-training
&lt;/h3>&lt;p>作者在 DeepSeek-V3.1 的基础上进行了 continue pre-training. Continue pre-training 包含两个阶段：&lt;/p>
&lt;p>&lt;strong>Dense Warm-up stage&lt;/strong>
这个阶段用于训练 lightning indexer, 作者冻结除 lightning indexer 之外的参数，为了对齐 indexer output 和 main attention distribution, 对于第 $t$ 个 query token, 作者首先计算所有 attention heads 的 main attention score 之和，然后在 sequence 层面进行 L1-normalization 得到 $p_{t,:}\in\mathbb{R}^t$, 最后计算 lightning indexer 输出与 $p_{t,:}$ 之间的 KL divergence:&lt;/p>
$$
\mathcal{L}^I = \sum_{t}\mathcal{D}_{KL}(p_{t,:}\ \Vert\ \mathrm{Softmax}(I_{t,:}))
$$&lt;p>这个阶段训练一共使用了 &lt;strong>2.1B&lt;/strong> 的 token, lr 为 1e-3, 训练的步数为 1000 steps, batch size 为 16.&lt;/p>
&lt;p>&lt;strong>Sparse training stage&lt;/strong>
这个阶段模型所有的参数都参与训练，该阶段的目的是让模型学习到 DSA 的 sparse pattern. 训练时，作者让 lightning indexer 的输出与 $p_{t,S_t}$ 之间的输出进行对齐，其中 $S_t=\{s\mid I_{t,s}\in\mathrm{TopK}(I_{t,:})\}$:&lt;/p>
$$
\mathcal{L}^I = \sum_{t}\mathcal{D}_{KL}(p_{t,S_t}\ \Vert\ \mathrm{Softmax}(I_{t,:}))
$$&lt;p>实际训练时，lightning indexer 仅接受 $\mathcal{L}^I$ 的反向传播，而 LLM 则仅接受 next-token prediction loss. 这个阶段模型一共使用了&lt;strong>943.7B&lt;/strong> token, 其中 $K$ 设置为 $2048$. 学习率为 $7.3\times 1e-6$, 训练步数为 15,000 steps, batch size 为 480.&lt;/p>
&lt;h3 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h3>&lt;p>post-training 与 DeepSeek-V3.1 一致：&lt;/p>
&lt;p>&lt;strong>Specialist Distillation&lt;/strong>
作者基于 DeepSeek-V3.2 base 构建了不同领域的 specialized model, 这些领域包括：&lt;/p>
&lt;ol>
&lt;li>math&lt;/li>
&lt;li>competitive programming&lt;/li>
&lt;li>general logical reasoning&lt;/li>
&lt;li>agentic coding&lt;/li>
&lt;li>agentic search&lt;/li>
&lt;/ol>
&lt;p>每个 specialized model 都使用 RL 进行训练，训练数据包括 long CoT reasoning 数据以及 direct response generation 数据，specialized model 训练完毕之后，就被用于生产 domain-specific data, 作者通过实验发现，基于这种蒸馏方法，模型的表现仅比 specialized model 低一点，并且这个 gap 可以被后续的 RL 训练所抵消。&lt;a class="link" href="https://maosong.website/p/notes-on-glm-4.5/" target="_blank" rel="noopener"
>GLM-4.5&lt;/a> 也采取了类似的做法&lt;/p>
&lt;p>&lt;strong>Mixed RL Training&lt;/strong>
作者使用了 GRPO 算法进行训练，与 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v3/" target="_blank" rel="noopener"
>DeepSeek-V3&lt;/a> 不同，作者将 reasoning, agent 以及 human alignment 的 RL 训练合并为了一个阶段，作者认为这种方法可以平衡模型在多个 domain 上的表现，并且可以防止 multi-stage training 带来的灾难性遗忘问题。对于 reasoning 和 agent 任务，作者使用了 rule-basd outcome reward, length penalty 以及 language consistency reward. 对于通用任务，作者使用了 generative reward model, 每个 prompt 都有对应的 rubris 用于 evaluation. 作者构建 reward 时主要考虑了:&lt;/p>
&lt;ol>
&lt;li>length versus accuracy&lt;/li>
&lt;li>language consistency versus accuracy&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>DeepSeek-V3.2-Speciale&lt;/strong>
除了 DeepSeek-V3.2 之外，作者还训练了 DeepSeek-V3.2-Speciale 模型，该模型仅使用 reasoning 数据进行训练，reasoning 数据包含了 DeepSeek-Math-V2 的训练数据以及 reward 方法。训练时，作者降低了 length penalty 的惩罚系数，最终 DeepSeek-V3.2-Speciale 模型拥有更强的 reasoning 能力&lt;/p>
&lt;h4 id="scaling-grpo">&lt;a href="#scaling-grpo" class="header-anchor">&lt;/a>Scaling GRPO
&lt;/h4>&lt;p>作者在 GRPO 的基础上对 KL estimate 进行了改进（见 &lt;a class="link" href="https://maosong.website/p/notes-on-kl-divergence/" target="_blank" rel="noopener"
>KL divergence&lt;/a>），使用了 importance sampling 对 K3 estimator 进行修正:&lt;/p>
$$
\mathcal{D}_{\mathrm{KL}}(\pi_\theta(o_{i,t})\Vert\pi_{\mathrm{ref}}(o_{i,t})) = \frac{\pi_\theta(o_{i,t}\mid q, o_{i,&lt;t})}{\pi_{\mathrm{old}}(o_{i,t}\mid q, o_{i,&lt;t})}\left(\frac{\pi_{\mathrm{ref}}(o_{i,t}\mid q, o_{i,&lt;t})}{\pi_\theta(o_{i,t}\mid q, o_{i,&lt;t})}-\frac{\pi_{\mathrm{ref}}(o_{i,t}\mid q, o_{i,&lt;t})}{\pi_\theta(o_{i,t}\mid q, o_{i,&lt;t})}-1\right)
$$&lt;p>使用 K3 estimator 之后，现在 KL estimator 的梯度估计就变成无偏估计了，从而提高了整体训练的稳定性。作者还发现不同任务对 KL regularization 的需求不一致，对数学等 domain, 我们应该采取较小的 KL penalty 或者不使用 KL penalty 反而能提升性能&lt;/p>
&lt;p>&lt;strong>Off-Policy Sequence Masking&lt;/strong>
作者还使用了 sequence masking 来提高 off-policy data 的数据使用效率，由于不同 rollout 的完成时间不一致，训练过程中会出现 off-policy 现象，即某些 mini-batch 不是由当前 policy 产生，这个现象在 &lt;a class="link" href="https://maosong.website/p/notes-on-magistral/" target="_blank" rel="noopener"
>Magistral&lt;/a> 中也有提到，这种训练 - 推理不一致性会进一步加剧 off-policy 程度，为了提高训练稳定性，作者将 policy divergence 程度比较高的 sequence 给 mask 掉，更新后的损失函数如下所示&lt;/p>
$$
\mathcal{J}_{\mathrm{GRPO}}(\theta) = \mathbb{E}_{(q,a)\sim\mathcal{D},\{o_i\}_{i=1}^G\sim \pi_{\theta_{old}}(\cdot\mid q)}\left[ \frac{1}{G}\sum_{i=1}^G\frac{1}{|o_i|}\sum_{t=1}^{|o_i|}\min\left(r_{i,t}(\theta)\hat{A}_{i,t},\mathrm{clip}\left(r_{i,t}(\theta), 1-\epsilon, 1+\epsilon\right)\hat{A}_{i,t}\right)M_{i,t}-\beta \mathcal{D}_{\mathrm{KL}}(\pi_\theta(o_{i,t})\Vert\pi_{\mathrm{ref}}(o_{i,t}))\right]
$$&lt;p>其中，&lt;/p>
$$
M_{i,t} = \begin{cases}
0, &amp;\hat{A}_{i,t}&lt;0, \frac{1}{|o_i|}\sum_{t=1}^{|o_i|}\log r_{i,t}(\theta)&lt;\delta\\
1, &amp;\text{otherwise}
\end{cases}
$$&lt;p>用于决定是否对当前 sequence 进行 mask, $\delta$ 是一个超参数控制 policy divergence 程度。作者认为，模型主要从自身的错误进行学习，而 off-policy 的负样本模型学习提升有限甚至有害。作者发现加入这个 masking 策略之后，模型的训练稳定性有所提升。&lt;/p>
&lt;p>&lt;strong>Keep Routing&lt;/strong>
MoE 模型在进行 On-policy RL 训练是，由于 policy 的更新，新 policy 和旧 policy 专家的 routing 可能会不一致，这种不一致性会降低训练的稳定性以及 off-policy 现象。为了解决这个问题，作者在采样室，保存了训练阶段所使用的 expert routing path, 来保证训练推理的一致性。这种策略可以有效提高针对 MoE 模型的 RL 训练稳定性&lt;/p>
&lt;p>&lt;strong>Keep Sampling Mask&lt;/strong>
作者发现，使用 top-p 和 top-K 可以提高 LLM 输出的质量，但是这种采样擦略也会导致 $\pi_{\mathrm{old}}$ 和 $\pi_{\theta}$ action space 的不匹配，因此，作者记录了 $\pi_{\mathrm{old}}$ 采样过程中的 truncation mask, 然后再训练的时候将其应用到 $\pi_{\theta}$ 上，作者发现通过这种方式可以有效提高 RL 训练过程中的 language consistency&lt;/p>
&lt;h4 id="thinking-in-tool-use">&lt;a href="#thinking-in-tool-use" class="header-anchor">&lt;/a>Thinking in Tool-Use
&lt;/h4>&lt;p>本节的目的是希望能够将 reasoning 能力应用到工具调用的场景下。&lt;/p>
&lt;p>作者发现，如果使用 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-r1/" target="_blank" rel="noopener"
>DeepSeek-R1&lt;/a> 的策略，在下一轮对话到来时，丢弃到当前的 reasoning content 会让模型重新生成针对问题的 CoT, 从而产生 token inefficiency. 为了解决这个问题，作者构建了一个上下文管理策略，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3.2/DeepSeek-V3-2-context-management-tool-calling.png"
width="1281"
height="677"
loading="lazy"
alt="Context Management of DeepSeek V3.2 in tool-calling senarios"
class="gallery-image"
data-flex-grow="189"
data-flex-basis="454px"
>&lt;/p>
&lt;p>具体做法为：&lt;/p>
&lt;ul>
&lt;li>reasoning content 只有当有新的 user message 进入时才会丢弃；如果只有工具调用相关的 message, 则 reasoning content 会保留&lt;/li>
&lt;li>移除 reasoning content 时，会保留对应的工具调用及其结果&lt;/li>
&lt;/ul>
&lt;p>在训练时，作者认为模型已经掌握了比较好的指令跟随能力，我们仅需要将 reasoning data (non-agentic) 和 non-reasoning data(agentic) 以不同的 prompt 输入给模型就能够得到比较好的结果。&lt;/p>
&lt;p>对于训练的数据，作者认为 RL 任务的多样性可以有效提高模型的 robustness, 因此作者构建了不同的环境及其对应的 prompt, 生成的任务如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>number of tasks&lt;/th>
&lt;th>environment&lt;/th>
&lt;th>prompt&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>code agent&lt;/td>
&lt;td>24667&lt;/td>
&lt;td>real&lt;/td>
&lt;td>extracted&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>search agent&lt;/td>
&lt;td>50275&lt;/td>
&lt;td>real&lt;/td>
&lt;td>synthesized&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>general agent&lt;/td>
&lt;td>4417&lt;/td>
&lt;td>synthesized&lt;/td>
&lt;td>synthesized&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>code interpreter&lt;/td>
&lt;td>5908&lt;/td>
&lt;td>real&lt;/td>
&lt;td>extracted&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;ul>
&lt;li>search agent: 作者使用了 multi-agent 的策略，包括 question-construction agent 用于构建 QA pair, multiple answer-generation agent 用于构建不同的 response, 一个 verification agent 用于评估生成的 response. 最后作者使用 generative reward model 来评分&lt;/li>
&lt;li>code agent: 作者爬取了 Github 上的 pull request, 然后进行过滤，接下来作者通过一个 environment-setup agent 来构建对应的环境&lt;/li>
&lt;li>code interpreter agent: 作者使用 jupyter notebook 作为代码解释器来解决复杂的 reasoning tasks, 包括 math, logic, data science 等&lt;/li>
&lt;li>general agent: 作者构建了验证简单解决困难的任务。首先作者基于 agent 和 task category 来生成或检索相关数据；接下来作者合成一个任务相关的工具集合；最后，作者让一个 agent 来提出任务以及对应的解法，并不断提高任务的难度。最后得到 &lt;code>&amp;lt;environment, tools, task, verifier&amp;gt;&lt;/code> 的 tuple 格式&lt;/li>
&lt;/ul>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>作者对比了 DeepSeek-V3.2-Exp 和 Claude-4.5 Sonnet, GPT-5, Gemini 3.0 Pro, Kimi-K2 thinking, MiniMax M2 的表现，评测结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3.2/DeepSeek-V3-2-performance.png"
width="1345"
height="817"
loading="lazy"
alt="Performance of DeepSeek V3.2"
class="gallery-image"
data-flex-grow="164"
data-flex-basis="395px"
>&lt;/p>
&lt;p>结果显示，DeepSeek V 3.2 和 GPT-high 在 reasoning 任务上的表现差不多。作者认为，进一步提高 RL 阶段的算力可以有效提高模型的表现&lt;/p>
&lt;p>作者还对比了 DeepSeek-V 3.2-Speciale 的表现，结果显示，提高 token budget 之后，模型的表现显著提高，与 Gemini 3.0 Pro 可以相比，但是其 token efficiency 仍然弱于 Gemini 3.0 Pro, 结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3.2/DeepSeek-V3-2-speciale-performance.png"
width="1299"
height="421"
loading="lazy"
alt="Performance of DeepSeek V 3.2 speciale"
class="gallery-image"
data-flex-grow="308"
data-flex-basis="740px"
>&lt;/p>
&lt;p>接下来作者验证了 synthesis agentic tasks 对模型表现的影响。首先，作者随机采样一批样本使用闭源 LLM 进行测试，发现闭源模型表现最好为 $62\%$, 这说明了合成数据对于 DeepSeek V3.2 和闭源模型都是有挑战的。&lt;/p>
&lt;p>其次，作者探究了合成数据是否能够提高 RL 的泛化性，作者构建了两个额外模型：&lt;/p>
&lt;ul>
&lt;li>SFT: 在 SFT checkpoint 上进行 RL&lt;/li>
&lt;li>Exp: 仅在 search 以及 code environment 上进行 RL&lt;/li>
&lt;/ul>
&lt;p>对比结果发现，合成数据缺失可以有效提高模型的表现&lt;/p>
&lt;p>作者还对比了 DSA 的效率，DSA 可以将 attention 的计算复杂度由 $\mathcal{O}(L^2)$ 降低到 $\mathcal{O}(LK)$, 其中 $K$ 是选取的 top-K tokens. 尽管 lightning indexer 的复杂度仍然是 $\mathcal{O}(L^2)$, 但是其计算量远小于 MLA, 作者对比了两者的效率，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3.2/DeepSeek-V3-2-efficiency.png"
width="1323"
height="547"
loading="lazy"
alt="Efficiency of DeepSeek-V3.2"
class="gallery-image"
data-flex-grow="241"
data-flex-basis="580px"
>&lt;/p>
&lt;p>可以看到，DeepSeek-V3.2 的 prefilling 和 decoding 效率都远高于 DeepSeek-V3.1&lt;/p>
&lt;p>最后，作者对比了不同上下文管理策略对模型表现的影响，对比策略有：&lt;/p>
&lt;ol>
&lt;li>Summary: 对轨迹进行总结然后重新初始化 rollout&lt;/li>
&lt;li>Discard 75%: 丢弃到初始 75% 的工具调用历史&lt;/li>
&lt;li>Discard-all: 丢弃掉所有的工具调用历史&lt;/li>
&lt;li>Parallel-fewest step: 多次采样然后保留步数最少得轨迹&lt;/li>
&lt;/ol>
&lt;p>实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3.2/DeepSeek-V3-2-context-management-ablation.png"
width="1286"
height="643"
loading="lazy"
alt="Ablation study on context management"
class="gallery-image"
data-flex-grow="200"
data-flex-basis="480px"
>&lt;/p>
&lt;p>结果显示，使用上下文管理策略之后，模型的表现有了显著提升。并且，discard-all 策略虽然很简单，但是其表现非常好。作者认为如何根据不同场景来选取合适的策略是一个待解决的问题。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 DeepSeek-V 3.2, DeepSeek-V 3.2 使用了一个稀疏注意力机制来提高模型在长上下文场景下的计算效率。作者还通过提升 RL 阶段的算力来提高模型在下游任务上的表现。最后，作者合成了大规模的 agentic task 来提升模型的 agent 能力。&lt;/p>
&lt;p>作者认为，相比于 Gemini 3.0 Pro, 模型的知识广度仍然有限。并且，目前模型的 token efficiency 仍然是一个问题，模型需要更长的轨迹输出才能达到 Gemini 3.0 Pro 的表现。最后，模型解决复杂问题的能力仍然弱于闭源模型。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2512.02556" target="_blank" rel="noopener"
>DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on DeepSeek-V3</title><link>https://maosong.website/p/notes-on-deepseek-v3/</link><pubDate>Mon, 08 Dec 2025 11:14:45 +0800</pubDate><guid>https://maosong.website/p/notes-on-deepseek-v3/</guid><description>&lt;p>DeepSeek 在 24 年 11 月发布了 DeepSeek-V3, 一个仅花费 2.8M H800 hours 的大语言模型，且在各个 benchmark 上达到了 SOTA 表现&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者在本文中提出了 DeepSeek-V3, 一个 671B-A37B 的 MoE 大语言模型。&lt;/p>
&lt;p>在训练目标和架构上，作者做了如下改进：&lt;/p>
&lt;ol>
&lt;li>efficiency inference: 采用了 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v2/" target="_blank" rel="noopener"
>DeepSeek-V2&lt;/a> 提出的 &lt;a class="link" href="https://maosong.website/p/notes-on-mla/" target="_blank" rel="noopener"
>MLA&lt;/a>&lt;/li>
&lt;li>cost-effective training: 采用了 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseekmoe/" target="_blank" rel="noopener"
>DeepSeekMoE&lt;/a> 提出了 MoE 架构&lt;/li>
&lt;li>auxiliary-loss-free strategy: 采用了 &lt;a class="link" href="https://maosong.website/p/notes-on-loss-free-balancing/" target="_blank" rel="noopener"
>Loss-Free Balancing&lt;/a> 提出的 loss balancing 策略&lt;/li>
&lt;li>multi-token prediction: 采用了 MTP 的训练目标来提升模型的表现&lt;/li>
&lt;/ol>
&lt;p>在训练上，作者做了如下改进：&lt;/p>
&lt;ol>
&lt;li>使用了 FP8 混合精度进行训练并验证了其在大规模模型上的有效性&lt;/li>
&lt;li>作者构建了 DualPipe 算法用于高效的 pipeline parallelism&lt;/li>
&lt;li>构建了 cross-node all-to-all communication kernel 来高效使用 InfiniBand 以及 NVLink bandwidth&lt;/li>
&lt;li>优化了 memory footprint, 来避免使用 tensor parallelism&lt;/li>
&lt;/ol>
&lt;p>预训练阶段，DeepSeek-V3 使用了&lt;strong>14.8T&lt;/strong> token. 在 mid-training 阶段，作者将模型的上下文长度由 8K 扩展到 32K, 再扩展到 128K.&lt;/p>
&lt;p>后训练阶段，作者使用了 SFT 和 RL 两个阶段来提高模型的表现，作者还对 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-r1/" target="_blank" rel="noopener"
>DeepSeek-R1&lt;/a> 进行蒸馏来提高模型的 reasoning 能力&lt;/p>
&lt;h2 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h2>&lt;h3 id="basic-architecture">&lt;a href="#basic-architecture" class="header-anchor">&lt;/a>Basic Architecture
&lt;/h3>&lt;p>DeepSeek-V3 的架构与 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v2/" target="_blank" rel="noopener"
>DeepSeek-V2&lt;/a> 的架构一致，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3/DeepSeek-V2-architecture.png"
width="1324"
height="1064"
loading="lazy"
alt="Architecture of DeepSeek-V2"
class="gallery-image"
data-flex-grow="124"
data-flex-basis="298px"
>&lt;/p>
&lt;p>MLA 的介绍见 &lt;a class="link" href="https://maosong.website/p/notes-on-mla/" target="_blank" rel="noopener"
>MLA&lt;/a>, MoE 架构的介绍见 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseekmoe/" target="_blank" rel="noopener"
>DeepSeekMoE&lt;/a>. DeepSeek-V3 在 DeepSeekMoE 的基础上做了两点改变：&lt;/p>
&lt;ol>
&lt;li>受 &lt;a class="link" href="https://maosong.website/p/notes-on-loss-free-balancing/" target="_blank" rel="noopener"
>Loss-Free Balancing&lt;/a> 启发，作者使用了 sigmoid fu nction 来计算 affinity score&lt;/li>
&lt;li>对于 selected affinity score 应用了 normalization&lt;/li>
&lt;/ol>
&lt;p>在 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseekmoe/" target="_blank" rel="noopener"
>DeepSeekMoE&lt;/a> 的基础上，作者使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-loss-free-balancing/" target="_blank" rel="noopener"
>Loss-Free Balancing&lt;/a>. 其表达式如下&lt;/p>
$$
g_{i,t} = \begin{cases}
s_{i,t}, &amp; s_{i,t}+b_i\in\mathrm{Topk}(\{s_{i,j}+b_j\mid 1\leq j\leq N\}, K)\\
0, &amp;\text{otherwise}
\end{cases}
$$&lt;p>其中 $b_i$ 仅影响 routing, 训练时，如果对应的 expert 负载不均衡，则对 $b_i$ 进行更新，更新方式为增加/减少 $\gamma$, 这里 $\gamma$ 是一个超参数&lt;/p>
&lt;p>为了提高 routing 在 sequence 层面的负载均衡，作者还是用了一个 complementary sequence-wise balance loss:&lt;/p>
$$
\begin{aligned}
\mathcal{L}_{\mathrm{Bal}} &amp;= \alpha\sum_{i=1}^{N_r} f_iP_i\\
f_i &amp;= \frac{}{}\sum_{t=1}^T\mathbb{1}(s_{i,t}\in\mathrm{TopK}(\{s_{j,t}\mid 1\leq j \leq N_r\}, K_r))\\
s_{i,y}' &amp;= \frac{s_{i,t}}{\sum_{j=1}^{N_r}s_{j,t}}\\
P_i &amp;= \frac1T\sum_{t=1}^T s_{i,t}'
\end{aligned}
$$&lt;p>其中 $\alpha$ 是 balance factor, 为超参数，$T$ 是 sequence length, 加入这个损失后，每个 sequence 上的负载会变得更加均衡&lt;/p>
&lt;p>与 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseekmoe/" target="_blank" rel="noopener"
>DeepSeekMoE&lt;/a> 一样，作者也在 node 层面实现负载均衡，具体做法就是，根据 node 中所包含 expert 的 affinity score 之和来选取最高的 $M$ 个 nodes, 这样就可以进一步提高 computation 和 communication 之间的 overlap.&lt;/p>
&lt;p>由于 DeepSeek-V3 负载均衡比较好，因此作者使用了 no token-dropping strategy.&lt;/p>
&lt;h3 id="mtp">&lt;a href="#mtp" class="header-anchor">&lt;/a>MTP
&lt;/h3>&lt;p>受 MTP 启发，DeepSeek-V3 也构建了 MTP 模块，作者认为 MTP 模块有两个优势：&lt;/p>
&lt;ol>
&lt;li>MTP objective 提供了更多的学习信号，进而提高了数据使用效率&lt;/li>
&lt;li>MTP 可以让模型更好预测未来的 token&lt;/li>
&lt;/ol>
&lt;p>与 MTP 不同，DeepSeek-V3 【todo】, DeepSeek-V3 所使用的 MTP 模块架构图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3/DeepSeek-V3-MTP.png"
width="1203"
height="536"
loading="lazy"
alt="Illustration of MTP"
class="gallery-image"
data-flex-grow="224"
data-flex-basis="538px"
>&lt;/p>
&lt;p>MTP 模块使用了 $D$ sequential modules 来预测未来的 $D$ 个 token. 其中，第 $k$ 个 MTP 模块包含一个共享的 embedding layer $\mathrm{Emb}(\cdot)$ , 一个共享的 output head $\mathrm{OutHead}(\cdot)$, 一个 transformer block $\mathrm{TRM}_k(\cdot)$ 和一个 projection matrix $M_k\in\mathbb{R}^{d\times 2d}$.&lt;/p>
&lt;p>对于第 $i$ 个 token 以及第 $k$ 个 MTP 模块，作者首先将第 $k-1$ 个 MTP 模块的第 $i$ 个 token 的 hidden states $h_i^{k-1}\in\mathbb{R}^d$ 和第 $i+k$ 个 token 的 embedding $\mathrm{Emb}(t_{i+1})\in\mathbb{R}^d$ 联合在一起&lt;/p>
$$
h_i'^{k}=M_k[\mathrm{RMSNorm(h_{i}^{k-1});\mathrm{RMSNorm(\mathrm{Emb}(t_{i+k}))}}]
$$&lt;p>其中 $[\cdot;\cdot]$ 代表 concatenation 操作。当 $k=1$ 时，$h_{i}^{k-1}$ 就代表了 main model 的输出。每个 MTP 模块的 embedding layer 和 main model 的 embedding layer 是共享的，接下来， $h_i'^k$ 作为第 $k$ 个 MTP 模块 transformer block 的输入，得到&lt;/p>
$$
h_{i}^k = \mathrm{TRM}_k(h_{i}'^l)
$$&lt;p>最后，共享的 output head 输出对应的概率分布：&lt;/p>
$$
P_{i+k+1}^k = \mathrm{OutHead}(h_i^k)
$$&lt;p>这里的 $\mathrm{OutHead}(\cdot)$ 的权重与 main model 也是共享的。作者这里提到，所使用的思想与 EAGLE 是类似的，但是不同的地方在于，EAGLE 主要是用于 speculative decoding, 而 MTP 主要用于提升训练。&lt;/p>
&lt;p>MTP 的训练目标为未来 $T-k-1$ 个 token 的 cross-entropy loss:&lt;/p>
$$
\mathcal{L}_{\mathrm{MTP}}^k = \mathrm{CrossEntropy}(P_{2+k:T+1}^k,t_{2+k:T+1}) = -\frac1T\sum_{t=k+2}^{T+1}\log P_i^k[t_i],
$$&lt;p>其中 $T$ 是 sequence length, $t_i$ 是第 $i$ 个位置对应的 ground truth token, $P_i^k[t_i]$ 代表低 $k$ 个 MTP 模块给出的 $t_i$ 的预测概率。最后，作者对所有的 MTP loss 进行求和，得到&lt;/p>
$$
\mathcal{L}_{\mathrm{MTP}} = \frac{\lambda}{D}\sum_{k=1}^D \mathcal{L}_{\mathrm{MTP}}^k.
$$&lt;h2 id="infra">&lt;a href="#infra" class="header-anchor">&lt;/a>Infra
&lt;/h2>&lt;p>DeepSeek-V3 训练使用了 2048 张 H800, 每个 node 包含 8 张 GPU, node 内部使用 NVLink 和 NVSwitch 进行连接，node 之间使用 InfiniBand 进行连接&lt;/p>
&lt;p>与之前的 DeepSeek 系列相同，DeepSeek-V3 也是用了 HAI-LLM 框架来支持训练。训练时，DeepSeek-V3 使用了 16-way PP, 64-way EP (spanning 8 nodes, &lt;a class="link" href="https://maosong.website/p/gshard/" target="_blank" rel="noopener"
>GShard&lt;/a>) 以及 ZeRO-1 DP.&lt;/p>
&lt;p>作者主要进行了三点优化：&lt;/p>
&lt;ol>
&lt;li>构建了 DualPipe 用于高效 pipeline parallelism&lt;/li>
&lt;li>构建了 cross-node all-to-all communication kernels 来高效利用 IB 以及 NVLink bandwidth&lt;/li>
&lt;li>优化了训练时的 memory footprint, 使得训练时不再依赖 TP&lt;/li>
&lt;/ol>
&lt;h3 id="training-framework">&lt;a href="#training-framework" class="header-anchor">&lt;/a>Training Framework
&lt;/h3>&lt;h4 id="dualpipe">&lt;a href="#dualpipe" class="header-anchor">&lt;/a>DualPipe
&lt;/h4>&lt;p>DeepSeek-V3 中，由于 cross-node EP, computation-to-communication ratio 近似为 1:1, 为了解决这个问题，作者提出了 DualPipe. DualPipe 的核心思想是将 forward 和 backward 过程中的 computation 以及 communication 进行重叠。与 ZeroBubble 类似，作者将每个 chunk 分为四个部分：attention, all-to-all dispatch, MLP 以及 all-to-all combine. 对于 attention 和 MLP, 作者还进一步将 backward 拆分为 针对权重和输入的 backward. 其示意图如下所示，这里橙色部分代表 forward, 绿色代表了针对输入的 backward, 蓝色代表了针对权重的 backward&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3/DeepSeek-V3-overlapping-strategy.png"
width="1101"
height="125"
loading="lazy"
alt="Overlapping stategy of DeepSeek-V3"
class="gallery-image"
data-flex-grow="880"
data-flex-basis="2113px"
>&lt;/p>
&lt;p>示意图里包含两个 block, 我们记为 block1, block2, 前向计算过程为&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">dispatch(F, block1) -&amp;gt; MLP(F, block1) -&amp;gt; combine(F, block1) -&amp;gt; attention(F, block2)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>其中，&lt;code>dispatch(F, block1)&lt;/code> 与 MLP 的反向传播 &lt;code>MLP(B, block1)&lt;/code> 计算重叠, &lt;code>MLP(F, block1)&lt;/code> 与 MLP 反向传播的 dispatch &lt;code>dispatch(B, block1)&lt;/code> 通信重叠，&lt;code>combine(F, block1)&lt;/code> 与 attention 反向传播的 &lt;code>attention(B, block2)&lt;/code> 重叠，&lt;code>attention(F, block2)&lt;/code> 与反向传播的 combine &lt;code>combine(block2)&lt;/code> 重叠。下面是一个具体的例子&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3/DeepSeek-V3-DualPipe-scheduling.png"
width="1212"
height="210"
loading="lazy"
alt="DualPipe scheduling"
class="gallery-image"
data-flex-grow="577"
data-flex-basis="1385px"
>&lt;/p>
&lt;p>作者进一步对比了 DualPipe, 1F1B 和 ZeroBubble, 结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Method&lt;/th>
&lt;th>Bubble&lt;/th>
&lt;th>Parameter&lt;/th>
&lt;th>Activation&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>1F1B&lt;/td>
&lt;td>$(PP-1)(F+B)$&lt;/td>
&lt;td>$1\times$&lt;/td>
&lt;td>$PP$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ZB1P&lt;/td>
&lt;td>$(PP-1)(F+B-2W)$&lt;/td>
&lt;td>$1\times$&lt;/td>
&lt;td>$PP$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DualPipe (Ours)&lt;/td>
&lt;td>$(\frac{PP}{2}-1)(F\&amp;B+B-3W)$&lt;/td>
&lt;td>$2\times$&lt;/td>
&lt;td>$PP+1$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>这里 $F$ 是 forward chunk 的执行时间，$B$ 是 backward chunk 执行的时间，$W$ 是一个 chunk &amp;ldquo;backward for weights&amp;rdquo; 的执行时间，$F\&amp;B$ 是一个 chunk 前向反向传播重叠的时间。可以看到，DualPipe 只使用了额外的 $1/PP$ 倍的 peak activation memory, 就大幅度降低了 bubble 时间&lt;/p>
&lt;h4 id="cross-node-all-to-all-communication">&lt;a href="#cross-node-all-to-all-communication" class="header-anchor">&lt;/a>Cross-node All-to-all Communication
&lt;/h4>&lt;p>作者针对 DualPipe 构建了 cross-node all-to-all communication kernels 来提高通信效率。&lt;/p>
&lt;p>作者提到，跨节点通信使用的是 IB, 节点内部通信使用的是 NVLink， 通信方式如下图所示。&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3/GPU-communication-pattern.png"
width="1360"
height="749"
loading="lazy"
alt="Communication of GPU"
class="gallery-image"
data-flex-grow="181"
data-flex-basis="435px"
>&lt;/p>
&lt;p>对于 H800 来说,NVLink 的带宽为 160GB/s, IB 的带宽为 50GB/s, 因此 NVLInk 的通信效率是 IB 的 3.2 倍，即节点内部通信效率高于节点之间的通信效率。为了提高通信效率，作者限制每个 token 只能被分发到至多 4 个节点上（&lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v2/" target="_blank" rel="noopener"
>DeepSeek-V2&lt;/a> 里提到的 device limited routing）。其具体通信方式为想传输到目标 node index 相同的 rank 上，然后再通过节点内部通信传输到目标的 rank 上。通过这种方式，我们可以让节点间通信与节点内部通信进行重叠，从而每个 token 可以从一个节点上选取 3.2 个专家，这样 DeepSeek-V3 可以在不损失效率的情况下最高选取 $13$ 个专家。&lt;/p>
&lt;p>作者进一步采用了 warp specialization 技巧来将 20 个 SM 划分为 10 个通信 channel. 作者分别针对 dispatching 和 combing 阶段使用了不同数量的 warps.&lt;/p>
&lt;h4 id="memory-saving">&lt;a href="#memory-saving" class="header-anchor">&lt;/a>Memory saving
&lt;/h4>&lt;p>作者使用了如下技巧来减少内存访问：&lt;/p>
&lt;ol>
&lt;li>Recomputation of RMSNorm and MLA Up-Projection. 在 backward 过程中重新计算 RMSNorm operation 以及 MLA up projection 来避免存储器对应的输出&lt;/li>
&lt;li>Exponential Moving Average in CPU. 作者将 EMA 参数保存在 CPU 中，然后进行异步更新来进一步减少内存访问&lt;/li>
&lt;li>Shared Embedding and Output Head for Multi-Token Prediction. 作者将 embedding layer 和 output head 放在一个 PP rank 上，这样就可以提高 MTP 的内存访问效率&lt;/li>
&lt;/ol>
&lt;h3 id="fp8-training">&lt;a href="#fp8-training" class="header-anchor">&lt;/a>FP8 Training
&lt;/h3>&lt;p>作者提出了一个基于 FP8 的混合精度训练框架。作者提出了两个改进方案：&lt;/p>
&lt;ol>
&lt;li>分组量化，将 tensor 按照 tile 分组或者按照 block 分组来分组量化，这样就避免了全局量化的精度损失&lt;/li>
&lt;li>高精度累加，作者在乘法计算时，使用了 FP8 格式，然后在累加阶段，使用了更高精度的格式&lt;/li>
&lt;/ol>
&lt;p>为了进一步减少内存和通信开销，对于 activation 的 cache 以及 dispatch, 作者使用了 FP8 数据格式，然后对于优化器状态，作者使用了 BF16 数据格式。下面是对上面改进的具体说明。&lt;/p>
&lt;h4 id="mixed-precision-training">&lt;a href="#mixed-precision-training" class="header-anchor">&lt;/a>Mixed Precision Training
&lt;/h4>&lt;p>作者在本文中提出了使用 FP8 混合精度进行预训练，作者参考了 low precision training 构建 FP8 训练框架，即计算量高的使用 FP8 精度，计算量低的使用原本的数据精度, 框架如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3/DeepSeek-V3-mixed-precision.png"
width="1210"
height="365"
loading="lazy"
alt="Mix-precision training of DeepSeek-V3"
class="gallery-image"
data-flex-grow="331"
data-flex-basis="795px"
>&lt;/p>
&lt;p>其中各个模块使用的精度如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Precision&lt;/th>
&lt;th>Modules&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>FP8&lt;/td>
&lt;td>Linear (Fprop, Dgrad, Wgrad)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>higher precision&lt;/td>
&lt;td>- embedding&lt;br>- output head&lt;br>- Moe gating&lt;br>- normalization&lt;br>- attention operator&lt;br>- master weights&lt;br>- weight gradients&lt;br>- optimizer states&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="enhancing-low-precision-training-accuracy">&lt;a href="#enhancing-low-precision-training-accuracy" class="header-anchor">&lt;/a>Enhancing Low-precision Training Accuracy
&lt;/h4>&lt;p>作者介绍了几个策略用于提高 FP8 混合精度训练的表现：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3/DeepSeek-V3-quantization.png"
width="1162"
height="588"
loading="lazy"
alt="Quantization of DeepSeek-V3"
class="gallery-image"
data-flex-grow="197"
data-flex-basis="474px"
>&lt;/p>
&lt;ol>
&lt;li>fine-grained quantization: 对于激活值，作者将 tensor 分割为 $1\times 128$ 大小的 groups, 然后每个 group 内部进行 quantization; 对于权重，作者将其分割为 $128\times 128$ 大小的 groups, 然后进行 quantization, 这样可以降低 quantization error, 如上图左图所示&lt;/li>
&lt;li>increasing accumulation precision: 低精度训练会带来 underflow 的问题，而 FP8 GEMM 累加仅能保留约 14bit 精度，远低于 FP32 的 32bit 精度。为了解决这个问题，作者采用了 Tensor Core 不分累加 +CUDA core 高精度聚合的协同策略。即在 Tensor Core 上执行 MMA 指令时，先按照 14bit 精度对 $N_C$ 个元素进行累加，当达到 $N_C$ 时，作者将结果复制到 CUDA core 的 FP32 寄存器中，在 FP32 精度下完成累加。过程如上图右图所示&lt;/li>
&lt;li>Mantissa over exponents. 与之前的工作不同，作者使用了 E4M3 的数据格式来达到更高的精度&lt;/li>
&lt;li>Online Quantization. 为了降低 quantization 的误差，作者实时计算了 activation block 以及 weight block 的最大绝对值来导出 scaling factor 并量化为 FP8 精度&lt;/li>
&lt;/ol>
&lt;h4 id="low-precision-storage-and-communication">&lt;a href="#low-precision-storage-and-communication" class="header-anchor">&lt;/a>Low Precision Storage and Communication
&lt;/h4>&lt;p>作者通过压缩 cached activation 和 optimizer states 来进一步减少内存访问以及通信访问次数&lt;/p>
&lt;ul>
&lt;li>Low-precision optimizer states: 对于 optimizer states, 作者使用了 BF16 数据格式来保存 &lt;a class="link" href="https://maosong.website/p/notes-on-adamw/" target="_blank" rel="noopener"
>AdamW&lt;/a> 的优化器的一阶和二阶动量，但是对于 master weight 和 gradients 作者仍然使用了 FP32 来保证训练的数值稳定性&lt;/li>
&lt;li>Low-Precision Activation: 对于 linear operator, 作者将其 cache activation 使用 FP8 格式进行存储，对于 attention 的输出，作者使用了 E5M6 数据格式来存储 activations, 这些 activations 的 tile size 为 $1\times 128$, 作者还是用了 2 的幂次作为 scale factor 来减少 quantizationerror; 对于 SwiGLU, 作者将其输入也保存为 FP8 的数据格式来降低内存消耗&lt;/li>
&lt;li>Low-Precision Communication: 在 MoE up-projection 之前，作者将 attention 的输出量化为 FP8 数据格式再执行 dispatch 操作，这样可以降低通信开销。在反向传播的时候同理，先进行 FP8 量化，然后再进行反向 dispatch. 对于 combine 阶段，为了避免精度损失，作者还是使用了 BF16 数据格式&lt;/li>
&lt;/ul>
&lt;p>作者对比了 FP8 和 BF16 精度训练，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3/DeepSeek-V3-FP8-performance.png"
width="1056"
height="305"
loading="lazy"
alt="FP8 v.s. BF16"
class="gallery-image"
data-flex-grow="346"
data-flex-basis="830px"
>&lt;/p>
&lt;p>实验结果显示，FP8 混合精度训练的损失降低不足 $0.25\%$.&lt;/p>
&lt;h3 id="inference-and-deployment">&lt;a href="#inference-and-deployment" class="header-anchor">&lt;/a>Inference and Deployment
&lt;/h3>&lt;p>作者在 H800 集群上部署 DeepSeek-V3, 作者分别针对 prefilling 和 decoding 两个阶段进行了优化&lt;/p>
&lt;h4 id="prefilling">&lt;a href="#prefilling" class="header-anchor">&lt;/a>Prefilling
&lt;/h4>&lt;p>prefilling 阶段在 4 节点 32 GPU 上进行，并行策略如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>模块&lt;/th>
&lt;th>并行策略&lt;/th>
&lt;th>说明&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Dense MLP&lt;/td>
&lt;td>1-wat TP&lt;/td>
&lt;td>减少 TP 通信&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Attention&lt;/td>
&lt;td>4-way TP, SP, 8-way DP&lt;/td>
&lt;td>SP 用于长文本处理&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MoE&lt;/td>
&lt;td>32-way EP&lt;/td>
&lt;td>每个 GPU 包含 8 个专家&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>为了实现负载均衡，作者提出了&lt;strong>redundant experts&lt;/strong>的策略，具体就是将负载比较高的专家进行复制。模型会周期性进行统计，然后计算出负载比较高的专家，接下来每个 GPU 除了原来的 8 个专家之外，还会 host 一个额外的 redundant expert.&lt;/p>
&lt;p>为了提高 throughput, 作者同时处理两个 micro-batch, 来重叠 attention, MoE 和 dispatch, combine 通信，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3/DeepSeek-V3-prefilling-overlap.png"
width="661"
height="316"
loading="lazy"
alt="Prefiling overlap of DeepSeek-V3"
class="gallery-image"
data-flex-grow="209"
data-flex-basis="502px"
>&lt;/p>
&lt;p>作者还探索了 dynamic redundancy 策略，即每个 GPU host 更多的专家（比如 16 个）然后 inference 的每一步仅激活其中的 9 个，在进行 all-to-all operation 时，作者首先进行 global optimal routing 来计算最合适的专家。作者认为 prefilling 计算量很大，因此 routing 的计算量可以忽略不计&lt;/p>
&lt;h4 id="decoding">&lt;a href="#decoding" class="header-anchor">&lt;/a>Decoding
&lt;/h4>&lt;p>在 decoding 阶段，作者在 40 个节点 320 GPU 上进行部署，并行策略如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>模块&lt;/th>
&lt;th>并行策略&lt;/th>
&lt;th>说明&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Attention&lt;/td>
&lt;td>4-way TP, SP, 80-way DP&lt;/td>
&lt;td>SP 用于长文本处理&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MoE&lt;/td>
&lt;td>320-way EP&lt;/td>
&lt;td>每个 GPU 包含 8 个专家&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>这个阶段，每个 GPU 只 host 一个专家，64 个 GPU 负责 host redundant expert 以及 shared expert. 作者通过在 IB 上直接进行 P2P 的传输来降低通信的开销，作者还是用了 IBGDA 来进一步最小化 latency 以及提高通信效率&lt;/p>
&lt;p>在这个阶段，attention 的计算占据了大部分时间，因此，作者采取了如下的 overlap 策略&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3/DeepSeek-V3-decoding-overlap.png"
width="896"
height="308"
loading="lazy"
alt="Decoding overlap of DeepSeek-V3"
class="gallery-image"
data-flex-grow="290"
data-flex-basis="698px"
>&lt;/p>
&lt;p>由于 decoding 阶段一般 batch size 比较小，因此整个 decoding 的瓶颈在于 memory access, 因此为了避免影响 attention 的计算效率，作者仅安排一小部分 SM 用于 dispatch-MoE-combine.&lt;/p>
&lt;h3 id="suggestions-on-hardware-design">&lt;a href="#suggestions-on-hardware-design" class="header-anchor">&lt;/a>Suggestions on Hardware Design
&lt;/h3>&lt;h4 id="communication-hardware">&lt;a href="#communication-hardware" class="header-anchor">&lt;/a>Communication Hardware
&lt;/h4>&lt;p>尽管作者将计算与通信重叠来提高训练效率，但是已有的通信仍然依赖于 SM, 这样限制了计算的效率。作者希望工能够构建专门针对通信的设备，另一方面，作者希望统一 IB 和 NVLink 来降低实现难度&lt;/p>
&lt;h4 id="computation-hardware">&lt;a href="#computation-hardware" class="header-anchor">&lt;/a>Computation Hardware
&lt;/h4>&lt;ol>
&lt;li>提升 FP8 GEMM 的累加精度，当前精度只有 14bit, 作者希望能够在 GPU 设计上进行改进，将这个精度提升到 34-bit&lt;/li>
&lt;li>支持 tile 以及 block 层面的 quantization, 尽管前文作者已经设计出了改进的算法，但是 Tensor Core 以及 CUDA 之前平凡的数据移动降低了计算效率，作者希望未来能够支持细粒度的 quantization&lt;/li>
&lt;li>online quantization, 作者希望将 FP8 cast 以及 TMA 结合在一起，从而 quantization 可以在传输的时候完成计算，减少了内存读写。作者还建议使用 warp-level cast instruction&lt;/li>
&lt;li>Transposed GEMM operations. 本文中，矩阵被切分为不同的 tiles, 在计算的时候需要先加载这些 tiles 然后进行 dequantization, transpose 等操作，作者希望未来能够直接支持 transposed reads of matrices&lt;/li>
&lt;/ol>
&lt;h2 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h2>&lt;h3 id="pre-training-data">&lt;a href="#pre-training-data" class="header-anchor">&lt;/a>Pre-training Data
&lt;/h3>&lt;p>相比于 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v2/" target="_blank" rel="noopener"
>DeepSeek-V2&lt;/a>, DeepSeek-V3 提升了数学和代码数据的比例，以及增加了多语种数据。最终训练数据一共包括 &lt;strong>14.8T&lt;/strong>&lt;/p>
&lt;p>作者还使用了 DeepSeekCoder-V2 里应用的 Fill in the middle 策略来让模型基于上下文越策中间的文本，对应的数据格式如下&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;lt;|fim_begin|&amp;gt;f_pre&amp;lt;|fim_hole|&amp;gt;f_suf&amp;lt;|fim_hole|&amp;gt;f_middle&amp;lt;|fim_end|&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>这个结构与 sequence packing 结合在一起。&lt;/p>
&lt;p>Tokenizer 基于 BBPE, 大小为 128K tokens. 在训练时，作者将随机一部分 combine token 进行切分来减少 token boundary bias 问题&lt;/p>
&lt;h3 id="hyper-parameters">&lt;a href="#hyper-parameters" class="header-anchor">&lt;/a>Hyper-parameters
&lt;/h3>&lt;p>模型参数如下表所示，最终 DeepSeek-V3 拥有 671B 总参数，激活参数为 37B&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>variable&lt;/th>
&lt;th>notation&lt;/th>
&lt;th>value&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>layers&lt;/td>
&lt;td>$\ell$&lt;/td>
&lt;td>61&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>dense layers&lt;/td>
&lt;td>-&lt;/td>
&lt;td>3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>hidden dimension&lt;/td>
&lt;td>$d$&lt;/td>
&lt;td>7168&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>num of attention heads&lt;/td>
&lt;td>$n_h$&lt;/td>
&lt;td>128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>head dimension&lt;/td>
&lt;td>$d_h$&lt;/td>
&lt;td>128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>KV compression dimension&lt;/td>
&lt;td>$d_c$&lt;/td>
&lt;td>512&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>query compression dimension&lt;/td>
&lt;td>$d_c'$&lt;/td>
&lt;td>1536&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>decouple query and key dimension&lt;/td>
&lt;td>$d_h^R$&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>routed expert&lt;/td>
&lt;td>$N_r$&lt;/td>
&lt;td>256&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>shared expert&lt;/td>
&lt;td>$N_s$&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MoE hidden dimension&lt;/td>
&lt;td>$d_{MoE}$&lt;/td>
&lt;td>2048&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>activated experts&lt;/td>
&lt;td>$K$&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>limited node routing&lt;/td>
&lt;td>$M$&lt;/td>
&lt;td>4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MTP depth&lt;/td>
&lt;td>$D$&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>训练时，作者使用了 learning rate scheduling, batch size scheduling 等方法。对于 PP, routed expert 会均匀分布为 8node 对应的 64 个 GPU 上，预训练时模型的上下文长度为 4k&lt;/p>
&lt;h3 id="long-context-extension">&lt;a href="#long-context-extension" class="header-anchor">&lt;/a>Long Context Extension
&lt;/h3>&lt;p>作者使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-yarn/" target="_blank" rel="noopener"
>YARN&lt;/a> 来扩展模型的上下文长度，模型上下文长度经过两个额外的训练阶段从 4K 扩展到 32K 再扩展到 128K, 均训练了 1000 步。&lt;/p>
&lt;p>YARN 配置与 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v2/" target="_blank" rel="noopener"
>DeepSeek-V2&lt;/a> 基本一致，在额外的 decoupled query and key 上作者没有应用这一点。参数配置如下表&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>parameter&lt;/th>
&lt;th>$s$&lt;/th>
&lt;th>$\alpha$&lt;/th>
&lt;th>$\beta$&lt;/th>
&lt;th>$\sqrt{t}$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>value&lt;/td>
&lt;td>40&lt;/td>
&lt;td>1&lt;/td>
&lt;td>32&lt;/td>
&lt;td>$0.1\ln s+1$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>第一阶段的上下文长度为 32K, batch size 为 1920, 第二个阶段的上下文长度为 128K, batch size 为 480.&lt;/p>
&lt;h3 id="performance">&lt;a href="#performance" class="header-anchor">&lt;/a>Performance
&lt;/h3>&lt;p>DeepSeek-V3 base 的表现下图所示，作者对比了 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v2/" target="_blank" rel="noopener"
>DeepSeek-V2&lt;/a>, &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5/" target="_blank" rel="noopener"
>Qwen2.5&lt;/a>, &lt;a class="link" href="LLaMA%203.1" >LLaMA 3.1&lt;/a>&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3/DeepSeek-V3-base-performance.png"
width="1065"
height="1083"
loading="lazy"
alt="Performance of DeepSeek-V3 base"
class="gallery-image"
data-flex-grow="98"
data-flex-basis="236px"
>&lt;/p>
&lt;h3 id="discussion">&lt;a href="#discussion" class="header-anchor">&lt;/a>Discussion
&lt;/h3>&lt;p>作者首先验证了 MTP 的有效性，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3/DeepSeek-V3-ablation-MTP.png"
width="979"
height="451"
loading="lazy"
alt="Ablation study on MTP"
class="gallery-image"
data-flex-grow="217"
data-flex-basis="520px"
>&lt;/p>
&lt;p>可以看到，在模型架构相同，训练数据相同的情况下，1-MTP 的效果超过了 baseline 的表现，说明了 MTP 策略的有效性&lt;/p>
&lt;p>接下来，作者还验证了 &lt;a class="link" href="https://maosong.website/p/notes-on-loss-free-balancing/" target="_blank" rel="noopener"
>Loss-Free Balancing&lt;/a> 的有效性，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3/DeepSeek-V3-ablation-loss-free-balancing.png"
width="987"
height="458"
loading="lazy"
alt="Ablation on loss-free balancing"
class="gallery-image"
data-flex-grow="215"
data-flex-basis="517px"
>&lt;/p>
&lt;p>可以看到，loss-free Balancing 的效果比 loss balancing 的效果更好&lt;/p>
&lt;p>接下来，作者对比了 loss-free balancing 和 sequence-wise auxiliary loss, 即 batch-wise v.s. sequence-wise. 作者认为，前者的约束更灵活，因为其不要求 in-domain balance. 作者在测试集上进行可视化，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3/DeepSeek-V3-ablation-batch-wise.png"
width="1106"
height="522"
loading="lazy"
alt="Ablation study on batch-wise load balancing"
class="gallery-image"
data-flex-grow="211"
data-flex-basis="508px"
>&lt;/p>
&lt;p>实验结果发现，loss-free 策略对应的 expert specialization 更强。作者进一步设计了一个 batch-wise auxiliary loss 来实现 batch-wise load balance, 结果发现，这种策略也能达到和 loss-free balancing 一样的效果，这说明了 batch-wise load balancing 效果更好&lt;/p>
&lt;p>最后，作者提了两点 loss-free 策略的问题：&lt;/p>
&lt;ol>
&lt;li>在特定的 sequence 或者小 batch 里出现负载不均衡。作者通通过增大 batch size 来解决这个问题&lt;/li>
&lt;li>在推理阶段因为 domain-shift 导致的负载不均衡。作者实现了一个基于 redundant expert 的推理框架来解决这个问题&lt;/li>
&lt;/ol>
&lt;h2 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h2>&lt;h3 id="sft">&lt;a href="#sft" class="header-anchor">&lt;/a>SFT
&lt;/h3>&lt;p>post-training 包含 1.5M 样本，数据包括 reasoning 数据以及 non-reasoning 数据，前者由 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-r1/" target="_blank" rel="noopener"
>DeepSeek-R1&lt;/a> 合成，后者由 DeepSeek-V2.5 合成&lt;/p>
&lt;p>SFT 时，作者训练了两个 epoch, 使用了 sequence packing 技巧&lt;/p>
&lt;h3 id="rl">&lt;a href="#rl" class="header-anchor">&lt;/a>RL
&lt;/h3>&lt;p>Reward model 包含 rule-based reward model 和 model-based reward model.&lt;/p>
&lt;p>RL 训练使用的算法为 GRPO&lt;/p>
&lt;h3 id="post-training-performance">&lt;a href="#post-training-performance" class="header-anchor">&lt;/a>Post-training Performance
&lt;/h3>&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepseek-v3/DeepSeek-V3-instruct-performance.png"
width="1112"
height="813"
loading="lazy"
alt="Performance of DeepSeek-V3"
class="gallery-image"
data-flex-grow="136"
data-flex-basis="328px"
>&lt;/p>
&lt;h3 id="post-training-discussion">&lt;a href="#post-training-discussion" class="header-anchor">&lt;/a>Post-training Discussion
&lt;/h3>&lt;p>作者首先探究了 Distillation 对模型表现的影响，作者使用 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-r1/" target="_blank" rel="noopener"
>DeepSeek-R1&lt;/a> 来蒸馏 DeepSeek-V2.5, 结果如下图所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>LiveCodeBench-CoT&lt;/th>
&lt;th>&lt;/th>
&lt;th>MATH-500&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Pass@1&lt;/td>
&lt;td>Length&lt;/td>
&lt;td>Pass@1&lt;/td>
&lt;td>Length&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DeepSeek-V2.5 Baseline&lt;/td>
&lt;td>31.1&lt;/td>
&lt;td>718&lt;/td>
&lt;td>74.6&lt;/td>
&lt;td>769&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DeepSeek-V2.5 +R1 Distill&lt;/td>
&lt;td>37.4&lt;/td>
&lt;td>783&lt;/td>
&lt;td>83.2&lt;/td>
&lt;td>1510&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，distillation 可以显著提高模型的表现。但是其问题在于也会让模型输出的长度增加。作者认为知识蒸馏是 post-training optimization 的一个重要方向。&lt;/p>
&lt;p>接下来，作者讨论了 self-rewarding, 具体做法就是使用 DeepSeek-V3 来对评估结果进行投票，结果发现最终的效果很好，作者认为 LLM 可以很好地将非结构化信息转换为 rewards.&lt;/p>
&lt;p>最后，作者讨论了 MTP. MTP 可以于 speculative decoding 结合，进一步提高 decoding 的速度。通过实验作者发现，second token prediction 的接受率在 $85\%\sim 90\%$ 之间，说明了其有效性。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>在本文中，作者提出了 DeepSeek-V3, 一个 671B-A37B 的 MoE 大语言模型，训练 token 数为 &lt;strong>14.8T&lt;/strong>. 作者使用了 loss-free-balancing strategy 来实现负载均衡。训练时作者使用了 FP8 混合精度。评估发现 DeepSeek-V3 的达到了 SOTA 表现。&lt;/p>
&lt;p>作者认为，DeepSeek-V3 model size 太大，不适合部署。第二，部署策略需要进一步改进。&lt;/p>
&lt;p>最后，作者认为未来工作有以下几点：&lt;/p>
&lt;ol>
&lt;li>改进模型架构，进一步提高训练以及推理效率，扩展模型的上下文&lt;/li>
&lt;li>提升训练数据的数量和质量&lt;/li>
&lt;li>提高模型的 reasoning 能力&lt;/li>
&lt;li>更详尽的评估&lt;/li>
&lt;/ol>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2412.19437" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Keye-VL 1.5</title><link>https://maosong.website/p/notes-on-keye-vl-1.5/</link><pubDate>Thu, 11 Sep 2025 11:33:31 +0800</pubDate><guid>https://maosong.website/p/notes-on-keye-vl-1.5/</guid><description>&lt;p>快手提出了 Keye-VL 1.5, 一个强调 reasoning, video understanding 的 8B 多模态大模型。作者提出了 slow-fast video encoding strategy 来提高模型的视频理解能力，作者通过在预训练和后训练提高了模型的长上下文能力和 reasoning 能力&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者回顾了多模态大模型的进展，然后提到视频理解能力仍然是一个挑战。为了解决这个问题，作者提出了 Keye-VL-1.5, 一个 8B 的视频理解多模态大模型。&lt;/p>
&lt;p>Keye-VL-1.5 主要做了三点改进：&lt;/p>
&lt;ol>
&lt;li>在架构上，使用了 Slow-Fast Video Encoding&lt;/li>
&lt;li>在预训练阶段，使用多个 stage 来提升模型的长上下文能力&lt;/li>
&lt;li>在 post-training 阶段，提高模型的 reasoning 能力和 alignment 表现&lt;/li>
&lt;/ol>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;p>Keye-VL 1.5 的架构与 &lt;a class="link" href="https://maosong.website/p/notes-on-keye-vl/" target="_blank" rel="noopener"
>Keye-VL&lt;/a> 一致。&lt;/p>
&lt;p>Keye-VL 1.5 主要做出的改进点为针对视频的 encoding 方式&lt;/p>
&lt;p>作者回顾了已有 MLLM 处理视频的方式，比如 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5-vl/" target="_blank" rel="noopener"
>Qwen2.5-VL&lt;/a> 使用 3D convolution 来 merge 相邻的两帧，&lt;a class="link" href="https://maosong.website/p/notes-on-seed1.5-vl/" target="_blank" rel="noopener"
>Seed1.5-VL&lt;/a> 采用了 Dynamic Frame-Resolution Sampling 技巧，来根据 budget 和处理的任务来动态调整采样率 (frame) 和每一帧的图片精度 (resolution)。&lt;/p>
&lt;p>但是这些方法很难进行泛化。因此，作者在本文中就提出了 &lt;strong>SlowFast video encoding stratrgy&lt;/strong>:&lt;/p>
&lt;ol>
&lt;li>Slow Pathway: 空间信息丰富（high resolution），时间信息简略 (low number of frames)&lt;/li>
&lt;li>Fast Pathway: 时间信息丰富（high number of frames），空间信息简略 (low resolution)&lt;/li>
&lt;/ol>
&lt;p>为了区分 slow/fast frames, 作者提出了一个基于 patch similarity 的 metric:&lt;/p>
&lt;ol>
&lt;li>第一帧始终定义为 slow frame&lt;/li>
&lt;li>接下来的每一帧，如果其和上一帧的相似度超过 $95\%$, 则定义为 fast frame; 反之则定义为 slow frame.&lt;/li>
&lt;/ol>
&lt;p>得到 slow/fast frames 之后，作者将 fast-frame 的 token budget 限制为 slow frame token budget 的 $30\%$ 来平衡时间信息以及空间信息。接下来，作者使用二分搜索来决定 slow frame 的 token budget. 为了区分 slow frame 和 fast frame 的 token, 作者使用了特殊的 token 来进行分离。&lt;/p>
&lt;p>最终的处理结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl-1.5/Keye-VL-1-5-video-encoding.png"
width="1364"
height="335"
loading="lazy"
alt="SlowFast Video Encoding"
class="gallery-image"
data-flex-grow="407"
data-flex-basis="977px"
>&lt;/p>
&lt;h3 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h3>&lt;p>预训练的数据和 Keye-VL 基本一致，我们主要介绍改进的点&lt;/p>
&lt;p>对于 Image caption 数据，作者认为这批数据可能会损害模型的指令跟随和 reasoning 能力，因此作者对数据进行了增广，主要是调整了数据的格式：&lt;/p>
&lt;ol>
&lt;li>QA, 数据格式为 &lt;code>&amp;lt;image, caption, [eos], question, answer&amp;gt;&lt;/code>&lt;/li>
&lt;li>reverse QA, 数据格式为 &lt;code>&amp;lt;image, question, answer, [eos], caption&amp;gt;&lt;/code>&lt;/li>
&lt;li>instruction following: 随机给一批数据作为输入，然后让模型基于特定 image 输出 caption&lt;/li>
&lt;/ol>
&lt;p>作者还构建了一个 trap question 来提高模型的 robustness 以及 faithfulness.&lt;/p>
&lt;p>OCR 数据在 Keye-VL 的基础上加入了两点：&lt;/p>
&lt;ol>
&lt;li>Structured Document and Code Understanding: 基于 markdown 和 HTML 等数据来获取 code OCR 数据&lt;/li>
&lt;li>Instruction Following OCR: 基于特定指令进行 OCR&lt;/li>
&lt;/ol>
&lt;p>对于 grounding 数据，作者进一步加入了 temporal grounding 数据，作者首先使用 TEMPURA&lt;/p>
&lt;p>来将短视频分割成若干个 video clips. 然后作者使用 SOTA MLLM 来过滤数据，最后作者基于 Gemini2.5 来生成对应的 QA.&lt;/p>
&lt;p>预训练和 Keye-VL 一样，包含 3 个 stage&lt;/p>
&lt;p>前两个 stage，作者将模型的上下文限制为 8K, 使用了 DP 和 Zero-2 来减少内存开销。在 stage 3, 作者将模型的上下文从 8K 扩展到 128K, 对应的 base frequency 从 1M 提升到 8M. 训练数据包括长视频，长文本和大规模图片。作者将优化策略调整为 Zero-1, CP 和 PP 来支持 long-context 的训练。训练时数据分布为 video:images:text=24:50:26.&lt;/p>
&lt;h3 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h3>&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl-1.5/Keye-VL-1-5-post-training-pipeline.png"
width="1360"
height="449"
loading="lazy"
alt="Post-Training Pipeline"
class="gallery-image"
data-flex-grow="302"
data-flex-basis="726px"
>&lt;/p>
&lt;p>SFT 阶段使用了 &lt;strong>7.5M&lt;/strong> 多模态 QA 样本进行训练。&lt;/p>
&lt;p>MPO 阶段的数据相比 Keye-VL 有所减少，包含：&lt;/p>
&lt;ol>
&lt;li>250K 开源样本&lt;/li>
&lt;li>150K 纯文本数据&lt;/li>
&lt;li>26K 人类标注数据&lt;/li>
&lt;/ol>
&lt;p>对于 reward model 的训练，作者使用了 SFT 和 RL 两个阶段。SFT 阶段的数据包括 R1-Reward 和 MMPR, 训练之后作者还是用比较短的 good response 来避免产生较长的回答&lt;/p>
&lt;p>在 SFT 和 MPO 阶段之后，作者使用 LongCoT code-start 初步激活模型的 reasoning 能力。&lt;/p>
&lt;p>作者构建了一个 5 部的自动化数据生成 pipeline, 如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl-1.5/Keye-VL-1-5-LongCoT-data-generation-pipeline.png"
width="1362"
height="389"
loading="lazy"
alt="LongCoT data generation pipeline"
class="gallery-image"
data-flex-grow="350"
data-flex-basis="840px"
>&lt;/p>
&lt;p>步骤如下：&lt;/p>
&lt;ol>
&lt;li>Multi-Source Data Collection and Enhancement：收集数据&lt;/li>
&lt;li>Multi-Path Reasoning Generation with Confidence Quantification: 基于 confidence 来挑选数据&lt;/li>
&lt;li>Comprehensive Two-Level Quality Assessment: 基于答案和过程的正确性来提高数据质量&lt;/li>
&lt;li>Human-in-the-Loop Quality Enhancement: 对于中等质量的数据请人类进一步进行标注&lt;/li>
&lt;li>Dynamic Quality Scoring and Data Utilization Strategy: 对数据进行打分，高质量数据进行上采样&lt;/li>
&lt;/ol>
&lt;p>接下来就是 General RL 过程。&lt;/p>
&lt;p>对于通用的 RLVR 训练，作者使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-gspo/" target="_blank" rel="noopener"
>GSPO&lt;/a> 算法来进行训练。&lt;/p>
&lt;p>在训练过程中，作者采取了 progressive hint sampling 方式，也就是提供不同程度的 hint 来提高模型的训练效率。作者将 hint 分为五个等级：&lt;/p>
&lt;ol>
&lt;li>Level 1 (Concept / Observation)&lt;/li>
&lt;li>Level 2 (Strategy / Method)&lt;/li>
&lt;li>Level 3 (Tools / Formula)&lt;/li>
&lt;li>Level 4 (Steps / Calculation)&lt;/li>
&lt;li>Level 5 (Complete Solution)&lt;/li>
&lt;/ol>
&lt;p>来提供不同程度的辅助，作者使用 Keye-VL 1.5 来确定最小的 hint level, 然后基于这个 level 提供信息进行 RL 的训练&lt;/p>
&lt;p>为了进一步提高模型的表现，作者采用了一个和 &lt;a class="link" href="https://maosong.website/p/notes-on-seed1.5-vl/" target="_blank" rel="noopener"
>Seed1.5-VL&lt;/a> 一样的迭代式训练策略，即反复进行 SFT 和 RL 来降低训练成本，提高训练效率。&lt;/p>
&lt;p>最后，再通用 RL 阶段之后，作者加入了 alignment RL 的训练来进行对齐，reward 包括三个方面：&lt;/p>
&lt;ol>
&lt;li>rule-based reward&lt;/li>
&lt;li>generative reward&lt;/li>
&lt;li>model-based reward&lt;/li>
&lt;/ol>
&lt;p>任务主要包括三个方面：&lt;/p>
&lt;ol>
&lt;li>instruction following&lt;/li>
&lt;li>format adherence&lt;/li>
&lt;li>preference alignment&lt;/li>
&lt;/ol>
&lt;p>数据介绍如下：&lt;/p>
&lt;ol>
&lt;li>instruction following:25 类硬约束，20 类软约束，数据包括 17K 多模态数据和 23K 纯文本数据，奖励包括 rule-based reward 和 generative reward&lt;/li>
&lt;li>reasoning: 12K 数学和逻辑推理数据&lt;/li>
&lt;li>RAG: 提高模型的搜索能力，作者使用 GSPO 算法进行训练&lt;/li>
&lt;/ol>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>模型表现如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl-1.5/Keye-VL-1-5-Performance.png"
width="1366"
height="988"
loading="lazy"
alt="Performance of Keye-VL 1.5"
class="gallery-image"
data-flex-grow="138"
data-flex-basis="331px"
>&lt;/p>
&lt;p>接下来作者进行了消融实验。&lt;/p>
&lt;p>首先是不同训练阶段对模型表现的影响，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-training-stage.png"
width="1369"
height="458"
loading="lazy"
alt="Ablation study on training strategy"
class="gallery-image"
data-flex-grow="298"
data-flex-basis="717px"
>&lt;/p>
&lt;p>实验结果显示，提高 SFT 训练数据可以有效提高模型在数学推理，逻辑推理和 OCR 任务上的表现。MPO 可以进一步提高模型的表现，Long CoT cold start 可以有效提高模型的 reasoning 表现。&lt;/p>
&lt;p>作者还探究了 model merging 对模型表现的影响，作者首先基于 base model 和 OCR 数据训练得到 OCR expert, 然后进行 merge, 实验结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-model-merging.png"
width="1244"
height="340"
loading="lazy"
alt="Ablation study on model merging"
class="gallery-image"
data-flex-grow="365"
data-flex-basis="878px"
>&lt;/p>
&lt;p>实验结果显示，model merging 可以有效提高模型在 special domain 上的表现，并且还可以维持模型的通用能力&lt;/p>
&lt;p>作者还发现：&lt;/p>
&lt;ol>
&lt;li>expert model 训练时间过长会影响最终 merge model 的表现&lt;/li>
&lt;li>expert mode 训练的学习率应该要设置比较小&lt;/li>
&lt;/ol>
&lt;p>接下来，作者探究了 alignment RL 对模型表现的影响，&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-alignment-RL.png"
width="1385"
height="275"
loading="lazy"
alt="Ablation on alignment RL"
class="gallery-image"
data-flex-grow="503"
data-flex-basis="1208px"
>&lt;/p>
&lt;p>实验结果说明，alignment RL 可以在保持模型 reasoning 能力的同时提高模型的指令跟随能力&lt;/p>
&lt;p>作者还探究了 hint 对模型表现的影响，使用不同 level hint 进行训练对模型表现影响的结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-hint.png"
width="1209"
height="310"
loading="lazy"
alt="Ablation on hint level"
class="gallery-image"
data-flex-grow="390"
data-flex-basis="936px"
>&lt;/p>
&lt;p>实验结果显示，使用 high level 的 hint 可以有效提高模型输出的正确率。&lt;/p>
&lt;p>最后作者探究了 rejection sampling 对模型表现的影响，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-rejection-sampling.png"
width="1384"
height="340"
loading="lazy"
alt="Ablation on rejection sampling"
class="gallery-image"
data-flex-grow="407"
data-flex-basis="976px"
>&lt;/p>
&lt;p>结果发现，通过 rejection sampling，模型的表现有了进一步的提升。因此作者采用了 ST-RL-(RFT-SFT)-(RFT-RL) 的训练方式来进行训练&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 Keye-VL1.5, 一个强调 video understanding 和 reasoning 的 MLLM. Keye-VL 1.5 使用了 SlowFast video encoding strategy 来提高模型的效率和视频理解能力。作者详细介绍了模型的数据，训练和评估。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2509.01563" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on InternVL3.5</title><link>https://maosong.website/p/notes-on-internvl3.5/</link><pubDate>Mon, 01 Sep 2025 11:30:50 +0800</pubDate><guid>https://maosong.website/p/notes-on-internvl3.5/</guid><description>&lt;p>上海 AI LAB 提出了 InternVL 3.5 系列多模态大模型，InternVL 3.5 主要强调了模型的 reasoning 能力以及 inference 效率。&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者首先介绍了已有多模态大模型的进展，然后提到有两个未解决的问题：&lt;/p>
&lt;ol>
&lt;li>如何构建一个 stable, effective, scalable 的 RL 框架来训练 MLLM&lt;/li>
&lt;li>如何降低 MLLM 在长上下文场景下的计算开销过高的问题&lt;/li>
&lt;/ol>
&lt;p>为了解决这两个问题，作者提出了 InternVL3.5 多模态大模型系列，相比于 InternVL3, 模型在 reasoning 能力和 efficiency 上均进行了提升。作者主要通过 Cascade RL 框架来提高模型的 reasoning 表现，以及使用 Visual Resolution Router (ViR) 和 Decoupled Vision Language Deployment (DvD) 来提高模型的推理效率。&lt;/p>
&lt;p>总结起来，InternVL3.5 模型的贡献如下：&lt;/p>
&lt;ol>
&lt;li>开源了 InternVL3.5 系列多模态大模型，模型拥有先进的 reasoning 能力以及推理效率&lt;/li>
&lt;li>提出了 Cascade RL, ViR 以及 DvD 等模块来提高模型的表现/推理效率&lt;/li>
&lt;li>系统性评估了模型的表现&lt;/li>
&lt;/ol>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;p>InternVL3.5 的架构与 InternVL3 的架构基本一致，包括一个 ViT, 一个 MLP 和一个 LLM, 模型的架构如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-internvl3.5/InternVL-3-5-architecture.png"
width="1295"
height="419"
loading="lazy"
alt="Architecture of InternVl3.5"
class="gallery-image"
data-flex-grow="309"
data-flex-basis="741px"
>&lt;/p>
&lt;p>模型配置如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-internvl3.5/InternVL-3-5-model-configuration.png"
width="1227"
height="484"
loading="lazy"
alt="Configuration of InternVL3.5"
class="gallery-image"
data-flex-grow="253"
data-flex-basis="608px"
>&lt;/p>
&lt;p>InternVl3.5 延续了 InternVL 系列的做法，即 20B 以下的模型使用 InternViT-300M, 20B 以上的模型使用 InternViT-6B 作为 Visual encoder. 大语言模型基于 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/notes-on-gpt-oss/" target="_blank" rel="noopener"
>gpt-oss&lt;/a>.&lt;/p>
&lt;p>在 InternVL3.5 的基础上，作者还构建了 InternVL3.5-Flash 模型，InternVL3.5-Flash 不同的地方在于，其在 MLP 之前加入了一个 Visual Resolution Router (ViR) 模块，来处理不同分辨率的图片。在 InternVL3.5 中，每个 image patch 由 1024 个视觉 token 来表示，然后通过 pixel shuffle 压缩至 256 个。 在 InternVL3.5-Flash 中，ViR 首先会判断每个 patch 的 semantic richness, 然后基于 semantic richness 来将对应的 token 路由到不同的 pixel shuffle 模块中，最高可以将视觉 token 压缩到 64 个，这样就可以大幅度提高模型的推理效率。&lt;/p>
&lt;h3 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h3>&lt;p>Pre-training 使用 next-token-prediction loss 来更新模型权重，给定多模态 sequence $x=(x_1,\dots,x_L)$, 损失函数定义为&lt;/p>
$$
\mathcal{L}(\theta) = -\sum_{x_i\text{ is text token}} \log p_{\theta}(x_i
\mid x_1,\dots,x_{i-1})
$$&lt;p>与 InternVL2.5 一样，作者还对不同长度的 sequence 进行了加权，避免模型产生 bias, 加权后的 loss 为&lt;/p>
$$
\mathcal{L}(\theta) = -\sum_{x_i\text{ is text token}}\frac{w_i}{\sum_j w_j} \log p_{\theta}(x_i
\mid x_1,\dots,x_{i-1}),\quad w_i = \frac{1}{N^{0.5}}
$$&lt;p>其中 $N$ 是训练样本中需要计算损失的 token 个数。&lt;/p>
&lt;p>训练数据蛀牙包含两部分：&lt;/p>
&lt;ol>
&lt;li>多模态数据，这部分数据基于 InternVL3&lt;/li>
&lt;li>纯文本数据，基于 InternLM 系列和开源的数据集&lt;/li>
&lt;/ol>
&lt;p>最终，预训练数据一共包含&lt;strong>116M&lt;/strong> 样本，&lt;strong>250B&lt;/strong> token. 纯文本数据和多模态数据的比例为 $1:2.5$. 模型上下文长度为 $32K$.&lt;/p>
&lt;h3 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h3>&lt;p>Post-training 包含三个阶段：&lt;/p>
&lt;ol>
&lt;li>SFT： 使用高质量对话数据提高模型表现&lt;/li>
&lt;li>Cascade RL: 提高模型的 reasoning 能力&lt;/li>
&lt;li>Visual Consistency Learning: 训练 ViR 模块，提高模型处理动态分辨率图片的能力&lt;/li>
&lt;/ol>
&lt;p>训练 pipeline 如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-internvl3.5/InternVL-3-5-post-training-pipeline.png"
width="1290"
height="335"
loading="lazy"
alt="Post-training pipeline of InternVL3.5"
class="gallery-image"
data-flex-grow="385"
data-flex-basis="924px"
>&lt;/p>
&lt;p>SFT 阶段的训练数据包括三个方面：&lt;/p>
&lt;ol>
&lt;li>InternVL3 的指令跟随数据&lt;/li>
&lt;li>多模态 reasoning 数据&lt;/li>
&lt;li>能力扩展数据，包括 GUI 交互，具身交互以及 SVG 理解与生成的数据&lt;/li>
&lt;/ol>
&lt;p>Cascade RL 阶段结合了 offline RL 和 online RL 的优点。基本思想是先使用 offline RL 来提高模型的能力，降低训练成本。然后再使用 online RL 进一步提高模型的 reasoning 表现。&lt;/p>
&lt;p>offline RL 阶段使用的是 InternVL-MPO 算法，其损失函数为&lt;/p>
$$
\mathcal{L}_{MPO} = w_p\mathcal{L}_p+w_q\mathcal{L}_q+w_g\mathcal{L}_g
$$&lt;p>其中，$\mathcal{L}_p$ 为 &lt;a class="link" href="https://maosong.website/p/notes-on-dpo/" target="_blank" rel="noopener"
>DPO&lt;/a> 的损失函数，$\mathcal{L}_q$ 为 Quality loss, $\mathcal{L}_g$ 为 SFT 使用的 next-token-prediction loss.&lt;/p>
&lt;p>online RL 阶段使用的是 &lt;a class="link" href="https://maosong.website/p/notes-on-gspo/" target="_blank" rel="noopener"
>GSPO&lt;/a> 算法，核心思想是在 sample 层面做归一化。这个阶段的目的是进一步提高模型的表现。&lt;/p>
&lt;p>Cascade RL 的优势在于：&lt;/p>
&lt;ol>
&lt;li>训练更稳定：offline RL 可以避免模型产生 reward hacking 现象，online RL 阶段可以进一步提高模型的表现&lt;/li>
&lt;li>训练效率更高：offline RL 可以有效提高采样效率&lt;/li>
&lt;li>表现更好：先 MPO 再 RL 可以达到更好的表现&lt;/li>
&lt;/ol>
&lt;p>Visual Consistency Learning (ViCO) 的目的是降低 InternVL3.5 的推理开销。这个阶段主要训练 ViR 模块&lt;/p>
&lt;p>ViCO 包括两个 stage:&lt;/p>
&lt;p>Stage 1: Consistency training&lt;/p>
&lt;p>这一阶段的目的是使得不同压缩率处理的视觉 token 对应的输出与 ground truth 尽可能一致，作者使用另外一个 InternVL3.5 模型作为 reference model, 然后损失函数为&lt;/p>
$$
\mathcal{L}_{ViCO} = \mathbb{E}_{\xi\sim\mathcal{R}}\left[\frac{1}{N}\sum_{i=1}^N\mathrm{KL}\left(\pi_{\mathrm{ref}}(y_i\mid y_{&lt;i}, I)\ \Vert\ \pi_{\theta}(y_i\mid y_{&lt;i}, I_{\xi})\right)\right]
$$&lt;p>$\xi\in\{1/4,1/16\}$ 为 compression rate, 训练是随机采样。$\xi=1/4$ 代表最终会有 256 个视觉 token, $\xi=1/16$ 代表最终会有 64 个 token.&lt;/p>
&lt;p>Stage 2: Router training&lt;/p>
&lt;p>这个阶段的目的是训练 ViR 来选取合适的精度/压缩率。ViR 此时作为一个 binary classifier 来进行训练，训练的损失函数为 cross-entropy loss. 模型其他部分参数冻结，仅训练 ViR 模块，首先，我们将计算压缩 16 倍后的损失与压缩 4 倍的损失的比例&lt;/p>
$$
r_i = \frac{\mathcal{L}_{ViCO}(y_i\mid I_{1/16})}{\mathcal{L}_{ViCO}(y_i\mid I_{1/4})}
$$&lt;p>该比例衡量了压缩 token 之后带来的性能下降程度，接下来，作者设定了一个阈值 $\tau$, 当性能下降超过阈值 $\tau$, 则认为应该使用高精度，也就是 $\xi=1/4$, 反之则说明不需要过度视觉 token, 可以使用低精度，也就是 $\xi=1/16$. 总结得到&lt;/p>
$$
y_i^{router} = \begin{cases}
0, &amp;r_i&lt;\tau\\
1, &amp;r_i>\tau
\end{cases}
$$&lt;p>训练时，作者基于 sliding window 中的 $r_i$ 来动态调整 $\tau$ 的值。&lt;/p>
&lt;p>post-training 的训练数据如下：&lt;/p>
&lt;ol>
&lt;li>SFT 训练数据包括&lt;strong>56M&lt;/strong> 样本，&lt;strong>130B&lt;/strong> token, 纯文本数据与多模态数据的比例为 $1:3.5$.&lt;/li>
&lt;li>Cascade RL 的训练数据主要基于 MMPR, 包含 200K 左右的样本，通过过滤最终得到&lt;strong>70K&lt;/strong>左右的样本。online RL 同样使用这批数据进行训练&lt;/li>
&lt;li>ViCO 的训练数据与 SFT 阶段的训练数据一致。主要是 OCR 以及 VQA 数据&lt;/li>
&lt;/ol>
&lt;h3 id="test-time-scaling">&lt;a href="#test-time-scaling" class="header-anchor">&lt;/a>Test-time Scaling
&lt;/h3>&lt;p>test time scaling 主要基于两个方面：&lt;/p>
&lt;ol>
&lt;li>deep thinking: 就是 reasoning mode&lt;/li>
&lt;li>parallel thinking: 基于 VisualPRM 进行多次采样，然后基于 BoN 策略得到最终的输出。&lt;/li>
&lt;/ol>
&lt;h3 id="infra">&lt;a href="#infra" class="header-anchor">&lt;/a>Infra
&lt;/h3>&lt;p>模型使用 XTuner 框架进行训练，使用了包括 FSDP, data packing, FP8, flashattention3 等策略。&lt;/p>
&lt;p>作者还提出了 DvD 的策略来提高推理效率，核心思想就是将 ViT-MLP 模块放在一个 server 上，然后将 LLM 放在另一个 server 上，这样就也可以提高整体的通信效率。框架示意图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-internvl3.5/InternVL-3-5-DvD.png"
width="1297"
height="580"
loading="lazy"
alt="DvD of InternVL3.5"
class="gallery-image"
data-flex-grow="223"
data-flex-basis="536px"
>&lt;/p>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>模型整体表现如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-internvl3.5/InternVL-3-5-model-performance.png"
width="964"
height="1113"
loading="lazy"
alt="Performance of InternVL3.5"
class="gallery-image"
data-flex-grow="86"
data-flex-basis="207px"
>&lt;/p>
&lt;h3 id="ablation-study">&lt;a href="#ablation-study" class="header-anchor">&lt;/a>Ablation Study
&lt;/h3>&lt;p>作者首先探究了 Cascade RL 对模型表现的影响&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-internvl3.5/InternVL-3-5-ablation-cascade-RL.png"
width="1299"
height="331"
loading="lazy"
alt="Ablation study on Cascade RL"
class="gallery-image"
data-flex-grow="392"
data-flex-basis="941px"
>&lt;/p>
&lt;p>实验结果显示，SFT, MPO 和 offline RL 每个阶段都可以提高模型的多模态 reasoning 表现。&lt;/p>
&lt;p>作者进一步探究了不同阶段的投入产出比，也就是训练时间与最终模型表现的变化情况，如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-internvl3.5/InternVL-3-5-training-efficiency-effectiveness.png"
width="1237"
height="213"
loading="lazy"
alt="Training efficiency and effectiveness"
class="gallery-image"
data-flex-grow="580"
data-flex-basis="1393px"
>&lt;/p>
&lt;p>实验结果显示，尽管 GSPO 的效果提升比较明显，但是训练所需要的时间比较长。如果先 MPO 再进行 GSPO 的话，我们可以在较短的时间里取得较好的表现。&lt;/p>
&lt;p>接下来，作者探究了 ViR 的有效性和效率。作者首先对比了 InternVL3.5 以及 InternVL3.5-flash 的表现。结果显示，大部分情况下，这两个模型的表现没有较大差距。说明 ViR 不会损害模型的性能。&lt;/p>
&lt;p>作者进一步探究了 ViR 和 DvD 对提升模型效率的影响，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-internvl3.5/InternVL-3-5-ablation-DvD.png"
width="877"
height="397"
loading="lazy"
alt="Ablation study on DvD and ViR"
class="gallery-image"
data-flex-grow="220"
data-flex-basis="530px"
>&lt;/p>
&lt;p>实验结果说明，对于高精度图片输入，DvD 和 ViR 均可以提高模型的推理效率。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>在本文中，作者提出了 InternVL3.5 系列多模态大模型，作者提出了 Cascade RL 框架，该框架结合了 offline RL 以及 online RL 来提高模型的 reasoning 表现以及训练效率。作者还提出了 ViR 以及 DvD 模块来提高模型处理高分辨率图片的效率。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2508.18265" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on GLM-4.5</title><link>https://maosong.website/p/notes-on-glm-4.5/</link><pubDate>Wed, 13 Aug 2025 12:27:48 +0800</pubDate><guid>https://maosong.website/p/notes-on-glm-4.5/</guid><description>&lt;p>智谱 AI 提出了 GLM4.5, 包含 GLM4.5 和 GLM-4.5-Air,两个 MoE LLM. 模型大小分别为 355B-A22B 和 106B-A12B, GLM4.5 主要关注 agentic, reasoning 以及 coding 三个领域。&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者认为，通用模型有三个关键能力，即 ARC：&lt;/p>
&lt;ol>
&lt;li>Agent: 与外部工具以及真实世界进行交互&lt;/li>
&lt;li>Reasoning: 解决数学和科学领域的复杂问题&lt;/li>
&lt;li>Coding: 解决真实世界软件工程相关问题&lt;/li>
&lt;/ol>
&lt;p>已有的商业模型如 o1/o3, Claude Sonnet 4 已经在 ARC 上达到了非常好的表现，但是开源模型仍然比较稀缺&lt;/p>
&lt;p>基于这个目标，作者就提出了 GLM4.5 和 GLM-4.5-Air, 来统一完成三个不同的目标。&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h2 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h2>&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;p>GLM-4.5 是一个基于 MoE 架构的 LLM, 架构与 DeepSeek-MoE 相似，作者做了如下几点改变：&lt;/p>
&lt;ol>
&lt;li>在 MoE layer 中，使用了 loss-free balance routing, 然后使用了 sigmoid function 作为 routing score 的 normalization.&lt;/li>
&lt;li>与 &lt;a class="link" href="https://maosong.website/p/notes-on-kimi-k2/" target="_blank" rel="noopener"
>Kimi-k2&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v3/" target="_blank" rel="noopener"
>DeepSeek-V3&lt;/a> 相比，作者降低了 head dimension, 提升了 number of layers. 作者认为更深的模型更有利于提高模型的 Reasoning 表现&lt;/li>
&lt;li>attention 上，作者使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-gqa/" target="_blank" rel="noopener"
>GQA&lt;/a>, 对于 #RoPE, 作者使用了 partial RoPE, 只旋转每个 token 的前半部分， 作者还将 attention heads 的个数增加到了 2.5 倍，作者发现增加 attention heads 可以提高模型的 Reasoning 表现&lt;/li>
&lt;li>作者还使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-qk-norm/" target="_blank" rel="noopener"
>QK-Norm&lt;/a> 来防止 attention logits 爆炸&lt;/li>
&lt;li>作者还使用了一个 MoE layer 作为 MTP layer 来支持 speculative decoding.&lt;/li>
&lt;/ol>
&lt;p>模型与 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v3/" target="_blank" rel="noopener"
>DeepSeek-V3&lt;/a> 和 Kimi-k2 的对比如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>GLM-4.5&lt;/th>
&lt;th>GLM-4.5-Air&lt;/th>
&lt;th>Step 3&lt;/th>
&lt;th>Kimi K2&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Date&lt;/td>
&lt;td>2025/8/8&lt;/td>
&lt;td>2025/8/8&lt;/td>
&lt;td>2025/7/25&lt;/td>
&lt;td>2025/7/28&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Total Parameters&lt;/td>
&lt;td>355B&lt;/td>
&lt;td>106B&lt;/td>
&lt;td>316B&lt;/td>
&lt;td>1043B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Activated Parameters&lt;/td>
&lt;td>32B&lt;/td>
&lt;td>12B&lt;/td>
&lt;td>38B&lt;/td>
&lt;td>32B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Dense Layers&lt;/td>
&lt;td>3&lt;/td>
&lt;td>1&lt;/td>
&lt;td>5&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># MoE Layers&lt;/td>
&lt;td>89&lt;/td>
&lt;td>45&lt;/td>
&lt;td>56&lt;/td>
&lt;td>60&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># MTP Layers&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;td>0&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Hidden Dim&lt;/td>
&lt;td>5120&lt;/td>
&lt;td>4096&lt;/td>
&lt;td>7168&lt;/td>
&lt;td>7168&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Dense Intermediate Dim&lt;/td>
&lt;td>12288&lt;/td>
&lt;td>10944&lt;/td>
&lt;td>18432&lt;/td>
&lt;td>18432&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MoE Intermediate Dim&lt;/td>
&lt;td>1536&lt;/td>
&lt;td>1408&lt;/td>
&lt;td>5120&lt;/td>
&lt;td>2048&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Attention&lt;/td>
&lt;td>GQA&lt;/td>
&lt;td>GQA&lt;/td>
&lt;td>MFA&lt;/td>
&lt;td>MLA&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Attention Head Dim&lt;/td>
&lt;td>128&lt;/td>
&lt;td>128&lt;/td>
&lt;td>256&lt;/td>
&lt;td>192&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Attention Heads&lt;/td>
&lt;td>96&lt;/td>
&lt;td>96&lt;/td>
&lt;td>64&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Key-Value Heads&lt;/td>
&lt;td>8&lt;/td>
&lt;td>8&lt;/td>
&lt;td>1&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>scoring&lt;/td>
&lt;td>sigmoid&lt;/td>
&lt;td>sigmoid&lt;/td>
&lt;td>softmax&lt;/td>
&lt;td>softmax&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Experts (total)&lt;/td>
&lt;td>160&lt;/td>
&lt;td>128&lt;/td>
&lt;td>48&lt;/td>
&lt;td>384&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Experts Active Per Token&lt;/td>
&lt;td>8&lt;/td>
&lt;td>8&lt;/td>
&lt;td>3&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Shared Experts&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>QK-Norm&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>No&lt;/td>
&lt;td>No&lt;/td>
&lt;td>No&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="pre-training-data">&lt;a href="#pre-training-data" class="header-anchor">&lt;/a>Pre-training Data
&lt;/h3>&lt;p>预训练数据包括四个方面&lt;/p>
&lt;ol>
&lt;li>Web: 过滤低质量数据和使用模版产生的数据&lt;/li>
&lt;li>Multilingual: 基于 webpages 和 Fineweb-2&lt;/li>
&lt;li>Code: 基于 GitHub 和其他代码平台，作者使用了 [[Fill in the middle]] 来训练模型。&lt;/li>
&lt;li>Math &amp;amp; Scirence: 训练一个 classifier 来给数据进行打分。&lt;/li>
&lt;/ol>
&lt;p>最终，预训练数据一共包括 &lt;strong>23T token&lt;/strong>.&lt;/p>
&lt;h3 id="pre-training-recipe">&lt;a href="#pre-training-recipe" class="header-anchor">&lt;/a>Pre-training Recipe
&lt;/h3>&lt;p>&lt;img src="https://maosong.website/p/notes-on-glm-4.5/GLM4_5-pre-training.png"
width="1173"
height="406"
loading="lazy"
alt="Pre-training recipe of GLM4.5"
class="gallery-image"
data-flex-grow="288"
data-flex-basis="693px"
>&lt;/p>
&lt;p>预训练包括 2 个阶段:&lt;/p>
&lt;ol>
&lt;li>Pre-training: 使用网页数据进行训练&lt;/li>
&lt;li>Mid-training: 加入 code, math, science 数据进行训练，在这个阶段，作者使用了 repo-level 的 code 数据，合成的 reasoning 数据以及长上下文数据。作者将模型上下文从 4K 扩展到 32K，然后在扩展到 128K.&lt;/li>
&lt;/ol>
&lt;p>作者在 pre-training 的时候使用了 random truncation, 在 mid-training 的时候使用了 best-fit packing 技巧&lt;/p>
&lt;p>训练时，与 Kimi-k2 一样，作者使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-moonlight/" target="_blank" rel="noopener"
>Muon&lt;/a> 作为优化器。作者使用了 cosine decay schedule. batch size 从 16M token 到 64M token.&lt;/p>
&lt;h2 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h2>&lt;p>Post-training 分为两个阶段：&lt;/p>
&lt;ul>
&lt;li>Stage 1, Expert Training. 构建 agent, reasoning, General chat 三个 domain 的专家模型&lt;/li>
&lt;li>Stage 2, Unified Training. 使用 self-distillation 来汇总多个模型的能力&lt;/li>
&lt;/ul>
&lt;p>训练框架如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-glm-4.5/GLM4.5-post-training-framework.png"
width="1337"
height="1001"
loading="lazy"
alt="Post-training of GLM4.5"
class="gallery-image"
data-flex-grow="133"
data-flex-basis="320px"
>&lt;/p>
&lt;h3 id="sft">&lt;a href="#sft" class="header-anchor">&lt;/a>SFT
&lt;/h3>&lt;p>两个 stage 都由 SFT 开始，&lt;/p>
&lt;ul>
&lt;li>在 Stage 1 里，SFT 的目标是让 expert model 掌握初步的 chat, reasoning 以及 tool-use 的能力。作者使用了一小部分包含 CoT 的 SFT 数据进行训练&lt;/li>
&lt;li>在 Stage 2 中，SFT 的目标是将不同的 expert model 蒸馏到一个模型中，作者使用了百万级的数据，包含 reasoning 任务和通用的 chat 数据，来训练模型的 hybrid reasoning 能力&lt;/li>
&lt;/ul>
&lt;p>在训练模型的 tool-use 能力是，作者发现，function call 在 code 场景下会出现混淆，提高了模型的学习成本。因此，作者的解决方法是使用了类似 XML 的 special token tags&lt;/p>
&lt;blockquote>
&lt;p>Recall
与之相反，Kimi-K2 认为模板应该尽可能简洁，因此 Kimi 采取了 TypeScript 作为 function call 的语言&lt;/p>
&lt;/blockquote>
&lt;p>从专家模型进行采样是，作者进行了数据过滤。还对数据进行了分级结果发现，使用难题进行训练可以提升模型 $2\%\sim4\%$ 的表现，多次采样也可以提高模型的表现&lt;/p>
&lt;p>Agentic SFT 数据的构建包括四个步骤：&lt;/p>
&lt;ol>
&lt;li>Agentic Framework and Tool Collection: 收集 MCP 和 tool API&lt;/li>
&lt;li>Task Synthesis: 合成不同的 agentic 任务&lt;/li>
&lt;li>Trajectory Generation: 采样生成的 rollout&lt;/li>
&lt;li>Quality Filtering: 过滤低质量的数据&lt;/li>
&lt;/ol>
&lt;h3 id="rl">&lt;a href="#rl" class="header-anchor">&lt;/a>RL
&lt;/h3>&lt;h4 id="reasoning-rl">&lt;a href="#reasoning-rl" class="header-anchor">&lt;/a>Reasoning RL
&lt;/h4>&lt;p>这个阶段使用了 GRPO 算法进行训练，与 &lt;a class="link" href="https://maosong.website/p/notes-on-dapo/" target="_blank" rel="noopener"
>DAPO&lt;/a> 一样，作者去除了损失函数中的 KL divergence。&lt;/p>
&lt;p>首先，作者探究了课程学习对模型表现的影响，结果发现，课程学习可以有效提高模型的性能。因此，作者构建了一个 2 阶段的课程学习框架。实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-glm-4.5/GLM4.5-curriculum-RL.png"
width="995"
height="504"
loading="lazy"
alt="Performance of curriculum RL"
class="gallery-image"
data-flex-grow="197"
data-flex-basis="473px"
>&lt;/p>
&lt;p>可以看到，在第二个阶段，模型可以进一步通过更难的题目获得提升。&lt;/p>
&lt;p>其次，作者探究了以下渐进式扩展模型上下文对模型表现的影响。DeepScaleR 认为，逐步提高模型的上下文长度，可以有效提高模型的表现。但是，本文确认为这种方法会损害模型的性能，原因在于，模型在 SFT 阶段的上下文长度就是 64K, 如果我们降低模型的上下文长度，这会导致训练数据分布不一致，从而影响模型的长上下文表现。因此作者直接在 64K 的上下文上进行训练。&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-glm-4.5/GLM4.5-multi-stage-context-extension.png"
width="1159"
height="453"
loading="lazy"
alt="Ablation on progressive context extension"
class="gallery-image"
data-flex-grow="255"
data-flex-basis="614px"
>&lt;/p>
&lt;p>接下来，作者探究了以下采样温度对模型表现的影响，温度太低会导致模型探索能力下降，太高的话会导致输出质量下降。因此作者动态调整采样温度来平衡模型的性能以及探索能力。&lt;/p>
&lt;blockquote>
&lt;p>[!tip]
Kimi-K2 认为随着 RL 训练的进行，我们应该逐步降低采样温度来稳定模型的表现&lt;/p>
&lt;/blockquote>
&lt;p>最后，作者分析了以下 code 以及 Science RL 中的一些问题。对于 code RL, 作者发现，我们应该在 sequence 层面而不是 token 层面进行平均。对于 Science RL, 作者强调了高质量数据的重要性。实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-glm-4.5/GLM4.5-code-science-RL.png"
width="1146"
height="450"
loading="lazy"
alt="Ablation on Code and Science RL"
class="gallery-image"
data-flex-grow="254"
data-flex-basis="611px"
>&lt;/p>
&lt;h4 id="agent-rl">&lt;a href="#agent-rl" class="header-anchor">&lt;/a>Agent RL
&lt;/h4>&lt;p>作者主要关注 web-search 以及 code generation 两个任务。对于 web-search, 作者构建了一个数据合成 pipeline, 用于生成 multi-step reasoning 的 QA 数据。构建过程包括基于知识图谱的 multi-hop reasoning 和 human-in-the-loop 的内容提取。对于 code generation, 作者基于 GitHub 的 PR 以及 issues 构建了 benchmark&lt;/p>
&lt;p>RL 的训练目标如下&lt;/p>
$$
\mathcal{L}(\theta) = \mathbb{E}_{x\sim\mathcal{D}}\left[\frac1K\sum_{i=1}^K(r(x,y_i) - \bar{r}(x))\right]
$$&lt;p>其中 $(x,y_i)$ 是基于 $\pi_{\mathrm{old}}$ 采样的 trace, $\bar{r}(x) = 1/k\sum_{i=1}^Kr(x,y_i)$ 是平均的 reward. 计算损失时，只有模型的回答参与计算。&lt;/p>
&lt;p>作者发现，通过训练模型的 web-search 以及 code generation 能分，模型在 tool-use 以及 coding 任务上的表现也有了提升。作者还是用了 format penalty 来保证模型输出格式的正确性。如果格式不对的话，模型获得的奖励是 0&lt;/p>
&lt;blockquote>
&lt;p>Recall
在 &lt;a class="link" href="https://maosong.website/p/notes-on-glm-4.1v-thinking/" target="_blank" rel="noopener"
>GLM-4.V-Thinking&lt;/a> 中，作者认为应该在 cold-start SFT 阶段让模型学会格式要求，而不是在 RL 阶段加入 format reward&lt;/p>
&lt;/blockquote>
&lt;p>由于 agent RL 的训练比较耗时，为了提高训练效率。作者首先基于 SFT 模型进行 agent RL 训练，训练到一定步数之后，作者使用 self-distillation 来将能力蒸馏回 SFT model, 接下来再基于 Self-distillation 后的 SFT 模型来进行 agent RL 训练&lt;/p>
&lt;blockquote>
&lt;p>Recall
&lt;a class="link" href="https://maosong.website/p/notes-on-seed1.5-vl/" target="_blank" rel="noopener"
>Seed1.5-VL&lt;/a> 和 LlaMA3.2 都提到了使用 multi-round SFT-RL 的形式来提高模型的表现&lt;/p>
&lt;/blockquote>
&lt;p>作者还发现，随着交互轮数的提升，模型的表现也有相应提升。实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-glm-4.5/GLM4.5-interaction-turns.png"
width="696"
height="428"
loading="lazy"
alt="Interaction turn scaling"
class="gallery-image"
data-flex-grow="162"
data-flex-basis="390px"
>&lt;/p>
&lt;h4 id="general-rl">&lt;a href="#general-rl" class="header-anchor">&lt;/a>General RL
&lt;/h4>&lt;p>General RL 用于提高模型的整体表现，解决潜在的问题以及提升关键能力，作者主要使用了 RLHF 和 RLAIF 两种方法&lt;/p>
&lt;p>对于 Holistic RL, 作者收集了 5000 条 prompt, reward 基于人类反馈和 AI 反馈。人类反馈用于训练一个 reward model, 对于 AI 反馈，作者构建了 scoring rubrics. 然后作者将两种反馈结合在一起&lt;/p>
&lt;p>对于 Instruction following RL, 作者构建了基于规则的奖励，reward model 的奖励以及 critical model 的奖励。实验结果显示，这种奖励方式可以有效降低模型的 reward hacking&lt;/p>
&lt;p>对于 function calling RL, 作者使用了 step-wise rule-based RL 来提高模型表现。对于 end-to-end multi-turn RL, 作者训练了一个 expert model 来蒸馏专家到模型。&lt;/p>
&lt;p>最后，对于 Pathology RL, 作者希望通过 RL 来解决潜在的问题，比如语言混合输出，重复输出以及格式错误等。作者构建了一批模型容易出错的数据，然后来训练模型。&lt;/p>
&lt;h4 id="infra">&lt;a href="#infra" class="header-anchor">&lt;/a>Infra
&lt;/h4>&lt;p>作者针对不同任务分别构建了不同的 scheduling 模式：&lt;/p>
&lt;ul>
&lt;li>对于通用 RL 任务，作者将 training engine 和 inference engine 放在一个 worker 来提高效率&lt;/li>
&lt;li>对于 agentic RL 任务，作者将 training 和 inference engine 分开，来提高 data throughput&lt;/li>
&lt;/ul>
&lt;p>在训练时，作者使用了 BF16 精度，在推理时，作者使用了 FP8 精度来提高推理效率。&lt;/p>
&lt;p>针对 agentic RL 任务，作者还进行了优化。与 Kimi-k2 类似，作者让 inference engine 持续产出 rollout, 然后让 training engine 来更新模型权重，最后同步到 inference engine 上&lt;/p>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>整体表现如下图所示，GLm4.5 在 ARC benchmark 上的平均表现达到了第三名。&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-glm-4.5/GLM4.5-average_performance.png"
width="1071"
height="776"
loading="lazy"
alt="Average performance on ARC benchmarks"
class="gallery-image"
data-flex-grow="138"
data-flex-basis="331px"
>&lt;/p>
&lt;p>具体来看，&lt;/p>
&lt;ol>
&lt;li>在 agentic benchmark 上, GLM4.5 仅次于 o3 的表现&lt;/li>
&lt;li>在 coding benchmark 上，GLM4.5 次于 Claude Opus 4 和 Claude Sonnet 4, 排第三名&lt;/li>
&lt;li>在通用能力上，GLM&lt;/li>
&lt;/ol>
&lt;p>人工对比 coding agent 能力的结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-glm-4.5/GLM4.5-agent-coding-performance.png"
width="1143"
height="514"
loading="lazy"
alt="Comparison of GLM4.5 aginst other models"
class="gallery-image"
data-flex-grow="222"
data-flex-basis="533px"
>&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者提出了 GLM4.5， 一个基于 MoE 架构的大语言模型系列，包含 GLM4.5(355B-A22B) 和 GLM4.5-Air(106B-A12B) 两个模型，作者详细介绍了模型的架构，训练，数据和评估。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2508.06471" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Kimi-k2</title><link>https://maosong.website/p/notes-on-kimi-k2/</link><pubDate>Thu, 24 Jul 2025 10:56:50 +0800</pubDate><guid>https://maosong.website/p/notes-on-kimi-k2/</guid><description>&lt;p>Kimi-k2 是一个总参数为 1T, 激活参数为 32B 的 MoE 大语言模型，模型使用 15.5T token 进行训练，optimizer 使用了 MuonClip. 作者主要关注模型的 agent 能力&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者首先强调现在 LLM 发展主要是 Agentic Intelligence, 也就是让模型自主感知，规划，思考和与环境交互。&lt;/p>
&lt;p>基于这个目标，作者就提出了 Kimi K2, 一个 1.04T 总参数，32B 激活参数的 MoE 模型，用于解决实现 agent Intelligence 中遇到的问题。作者主要进行了三点改进：&lt;/p>
&lt;ol>
&lt;li>MuonClip, 一个基于 &lt;a class="link" href="https://maosong.website/p/notes-on-moonlight/" target="_blank" rel="noopener"
>Muon&lt;/a> 的优化算法，来提高 Kimi K2 对 token 的利用效率以及提高训练的稳定性&lt;/li>
&lt;li>大规模的 agentic 数据合成 pipeline: 作者构建了一个用于合成工具调用，agent 数据的 pipeline&lt;/li>
&lt;li>通用的 RL 框架，作者将 RLVR 和 self-critic rubric reward mechanism 结合起来，用于提升模型的表现&lt;/li>
&lt;/ol>
&lt;h2 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h2>&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;p>Kimi-K2 的架构与 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v3/" target="_blank" rel="noopener"
>DeepSeek-V3&lt;/a> 相似，配置如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>指标&lt;/th>
&lt;th>DeepSeek-V3&lt;/th>
&lt;th>Kimi K2&lt;/th>
&lt;th>$\Delta$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td># Layers&lt;/td>
&lt;td>61&lt;/td>
&lt;td>61&lt;/td>
&lt;td>=&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Total Parameters&lt;/td>
&lt;td>$671\text{B}$&lt;/td>
&lt;td>$1.04\text{T}$&lt;/td>
&lt;td>$\uparrow 54\%$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Activated Parameters&lt;/td>
&lt;td>$37\text{B}$&lt;/td>
&lt;td>$32.6\text{B}$&lt;/td>
&lt;td>$\downarrow 13\%$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Experts (total)&lt;/td>
&lt;td>256&lt;/td>
&lt;td>384&lt;/td>
&lt;td>$\uparrow 50\%$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Experts Active per Token&lt;/td>
&lt;td>8&lt;/td>
&lt;td>8&lt;/td>
&lt;td>=&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Shared Experts&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;td>=&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Attention Heads&lt;/td>
&lt;td>128&lt;/td>
&lt;td>64&lt;/td>
&lt;td>$\downarrow 50\%$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Number of Dense Layers&lt;/td>
&lt;td>3&lt;/td>
&lt;td>1&lt;/td>
&lt;td>$\downarrow 67\%$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Expert Grouping&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>No&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>与 DeepSeek-V3 相比，模型主要进行了以下改动：&lt;/p>
&lt;ol>
&lt;li>作者认为提高专家的稀疏性，可以有效提高模型表现。因此作者将专家个数提升了 50%.&lt;/li>
&lt;li>为了降低模型在 inference 阶段的算力开销，作者将 attention heads 的个数降低了 50%.&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Sparsity Scaling Law&lt;/strong>
作者首先构建了基于 Muon 的 sparsity law, 这里 sparsity 定义为激活专家个数与总专家个数之比，即：&lt;/p>
$$
\mathrm{sparsity} = \frac{\# \mathrm{activated\ experts}}{\# \mathrm{total\ experts}}
$$&lt;p>作者在小规模上的实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2/Kimi-K2-sparsity-scaling-law.png"
width="673"
height="678"
loading="lazy"
alt="Sparsity Scaling Law"
class="gallery-image"
data-flex-grow="99"
data-flex-basis="238px"
>&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Observation&lt;/strong>
实验结果表明，在相同算力（激活专家个数）下，模型的表现随 sparsity 提高而增加。&lt;/p>
&lt;/blockquote>
&lt;p>但是，进一步提高 sparsity, 会让 infra 变的难以优化，因此在 Kimi-K2 里，将 sparsity 定义为 $48$.&lt;/p>
&lt;p>&lt;strong>Number of attention heads&lt;/strong>
作者还分析了探究了 attention heads 的最优配置。DeepSeek-V3 将 attention heads 的个数设置为 layers 层数的 2 倍，来提高带宽利用率以及提高计算效率。但是，当上下文长度增加之后，attention head 变成了 computational bound. 作者对比了不同配置模型的表现，实验结果如下：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2/Kimi-K2-attention-heads.png"
width="671"
height="675"
loading="lazy"
alt="Scaling curves for attention heads"
class="gallery-image"
data-flex-grow="99"
data-flex-basis="238px"
>&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Observation&lt;/strong>
实验结果表明，使用双倍 attention head 带来的收益是比较小的，只有 $0.5\%$ 到 $1.2\%$ 左右&lt;/p>
&lt;/blockquote>
&lt;p>因此，作者在 Kimi-K2 中，奖 attention head 的个数设置为 $64$.&lt;/p>
&lt;h3 id="data">&lt;a href="#data" class="header-anchor">&lt;/a>Data
&lt;/h3>&lt;p>Kimi-K2 主要强调了 token efficiency, 即每个 token 对模型表现提升的贡献。token efficiency 越高，说明每个 token 对最终模型的贡献也就越高&lt;/p>
&lt;p>相比于 &lt;a class="link" href="https://maosong.website/p/notes-on-kimi-k1.5/" target="_blank" rel="noopener"
>Kimi-k1.5&lt;/a>, Kimi-K2 使用了一个 rephrasing pipeline, 来提高高质量 token 的利用率，作者在 knowledge 和 mathematics domain 上进行了实验。&lt;/p>
&lt;p>最终，Kimi-K2 的预训练数据包括了 &lt;strong>15.5T&lt;/strong> token, 主要覆盖 Web text, code, mathmatics 以及 Knowledge 四个 domain. 大部分的数据处理与 Kimi-k1.5 相同。&lt;/p>
&lt;p>&lt;strong>Knowledge Data Rephrasing&lt;/strong>
作者构建了一个 synthetic rephrasing 框架来提高 knowledge token 的利用率，框架主要包含以下几个模块：&lt;/p>
&lt;ol>
&lt;li>Style and perspective-diverse prompting: 作者构建了一系列 prompt, 来让 LLM 从多个角度和风格来重新表述原始文本&lt;/li>
&lt;li>Chunk-wise auto-regressive generation: 为了保持模型在长文档中的 coherence 以及避免信息损失，作者使用了一个 chunk-based rewriting 策略，也就是对每个 chunk 分别进行改写，然后将它门汇总在一起&lt;/li>
&lt;li>Fidelity verification: 作者对原始文本和改写文本进行了 fidelity 检查来保证语义的一致性。&lt;/li>
&lt;/ol>
&lt;p>作者对比了以下三个设置对模型在 SimpleQA benchmark 上表现的影响，这三个设置分别是：&lt;/p>
&lt;ol>
&lt;li>原始数据集训练 10 epoch&lt;/li>
&lt;li>改写数据一次，然后训练 10 epoch&lt;/li>
&lt;li>改写数据一次，训练 1 epoch&lt;/li>
&lt;/ol>
&lt;p>实验结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th># Rephrasings&lt;/th>
&lt;th># Epochs SimpleQA&lt;/th>
&lt;th>Accuracy&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>0 (raw wiki-text)&lt;/td>
&lt;td>10&lt;/td>
&lt;td>23.76&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>10&lt;/td>
&lt;td>27.39&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>1&lt;/td>
&lt;td>28.94&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，改写的策略可以有效提高模型在 SimpleQA benchmark 上的表现&lt;/p>
&lt;p>&lt;strong>Math Data Rephrasing&lt;/strong>
对于数学相关数据，作者基于 SwallowMath, 将数据改写成了学习笔记 (learning-note) 的风格。然后，作者还将其他语言的高质量文本翻译为英文。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Recall&lt;/strong>
个人认为，数据改写在一定程度上也算是合成数据，对于合成数据这一块，微软提出的 phi 系列是一个比较好的参考&lt;/p>
&lt;/blockquote>
&lt;h3 id="muonclip-optimizer">&lt;a href="#muonclip-optimizer" class="header-anchor">&lt;/a>MuonClip Optimizer
&lt;/h3>&lt;p>Kimi-K2 使用了 Muon optimizer, 之前的实验结果说明了在相同的 compute budget 下，Muon optimizer 的表现超过了 AdamW. 也就是说，Muon optimizer 更加高效。&lt;/p>
&lt;p>但是，Muon optimizer 的问题是训练不稳定。为了解决这个问题，作者提出了 QK-clip, 一个 weight clipping 机制，来显示约束 attention logits. QK-clip 的机制是&lt;strong>当输出的 logits 超过某一个阈值之后，就对齐进行截断&lt;/strong>。&lt;/p>
&lt;p>每个 head 的 attention 的计算公式如下&lt;/p>
$$
O = \mathrm{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V
$$&lt;p>其中, $d$ 是 hidden size, $Q, K,V$ 分别是 query, key, value, 定义如下：&lt;/p>
$$
Q = XW_Q, K=XW_K, V=XW_V
$$&lt;p>这里 $W_Q,W_K,W_V$ 是模型可学习的参数。&lt;/p>
&lt;p>作者定义每个 head 的 max logit 如下：&lt;/p>
$$
S_{\max}^h = \frac{1}{\sqrt{d}} \max_{X\in\mathcal{B}}\max_{i,j} [QK^T]_{ij}
$$&lt;p>最简单的做法就是直接进行截断，也就是&lt;/p>
$$
W_Q\gets \gamma^\alpha W_q, W_K\gets \gamma^{1-\alpha}W_K
$$&lt;p>其中 $\gamma=\min(1, \tau S_{\max})$, 这里 $S_{\max}=\max_h S_{\max}^h$ 是所有 head 对应 $S_{\max}^h$ 的最大值。&lt;/p>
&lt;p>但是，实际中，作者发现只有少部分 head 会出现 logits 爆炸现象。为了提高计算效率，作者针对每个 head 单独进行 scaling, 也就是 $\gamma_{h}=\min(1, \tau S_{\max}^h)$. 对于 MLA 架构，作者仅在 unshared 模块使用 clipping:&lt;/p>
&lt;ul>
&lt;li>$q^c$ 以及 $k^c$, scaling factor 为 $\sqrt{\gamma_h}$&lt;/li>
&lt;li>$q^R$, scaling factor 为 $\gamma_h$&lt;/li>
&lt;/ul>
&lt;p>最后，作者将 Muon, weight decay, RMS matching 和 QK-clip 汇总在一起，得到 Kimi-k2 使用的 MuonClip optimizer, 算法如下所示：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2/Kimi-k2-muonclip.png"
width="1385"
height="668"
loading="lazy"
alt="MuonClip Optimizer"
class="gallery-image"
data-flex-grow="207"
data-flex-basis="497px"
>&lt;/p>
&lt;p>接下来，作者对比了以下 Muon 和 MuonClip 两个优化器的表现，作者分别使用这两个优化器训练 53B-A9B 的 MoE 模型，实验结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2/Kimi-k2-muonclip-performance.png"
width="1365"
height="551"
loading="lazy"
alt="Comparison between Muon and MuonClip"
class="gallery-image"
data-flex-grow="247"
data-flex-basis="594px"
>&lt;/p>
&lt;p>实验结果表明，Muon 优化器在 1000 步左右就出现了 logits 爆炸的现象，但是 MuonClip 通过优化可以避免这个问题。作者还发现，MuonClip 可以减少 loss spike 的产生，整体的 loss 变化情况如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2/Kimi-k2-model-training-dynamics.png"
width="977"
height="561"
loading="lazy"
alt="Training loss curve"
class="gallery-image"
data-flex-grow="174"
data-flex-basis="417px"
>&lt;/p>
&lt;p>作者还在附录中说明 QK-clip 并不影响最终的收敛性，并且，为了尽可能减少对模型训练的干扰，作者发现，仅在训练初期 QK-clip 被激活:&lt;/p>
&lt;ol>
&lt;li>在初始的 70, 000 步里，有 12.7% 的 attention heads 至少触发了一次 QK-clip, 并且将它们的 $S_\max$ 降到了 100 以下&lt;/li>
&lt;li>接下来的 70, 000 步里，QK-clip 就不再被激活&lt;/li>
&lt;/ol>
&lt;h3 id="infra">&lt;a href="#infra" class="header-anchor">&lt;/a>Infra
&lt;/h3>&lt;p>Kimi-k2 使用了 16-way 的 PP, 16-way 的 EP 以及 ZeRO-1 Data Parallelism. 具体流程如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2/Kimi-k2-communication-computation.png"
width="1362"
height="318"
loading="lazy"
alt="Kimi-K2 parallelism"
class="gallery-image"
data-flex-grow="428"
data-flex-basis="1027px"
>&lt;/p>
&lt;p>作者发现通过增加 warm-up mircro-batches, 我们可以有效重叠 EP 的 all-to-all communication 以及 computation. 但是，对于 [[DualPipe]], 其要求双倍的存储来保存参数和梯度，并且提高 PP 粒度会引入更多的 bubble, 因此 Kimi-K2 没有使用 DualPipe.&lt;/p>
&lt;p>为了减少 [[1F1B]] 的 PP 通信开销，作者在反向传播的过程中同时进行 PP 通信，来进一步重合通信与计算。&lt;/p>
&lt;p>作者还发现，使用更小的 EP group 可以提高整体的训练速度，因此作者将 EP 设置为 16.&lt;/p>
&lt;p>作者发现，剩余的 GPU 内存不足以存放 MoE 的 activation, 因此作者采取了三个办法解决这个问题：&lt;/p>
&lt;ol>
&lt;li>Selective recomputation: 作者对 LayerNorm, SwiGLU, MLA 的 up-projection 和 MoE 的 down-projections 等进行重新计算。&lt;/li>
&lt;li>FP8 storage for insentive activations: 作者使用了 FP8 精度来存储 MoE up-projections 以及 SwiGLU. 作者发现使用 FP8 进行计算可能会导致性能下降，因此作者并没有使用 FP8 进行计算。&lt;/li>
&lt;li>Activation CPU offload: 作者将其余的 activation 放在 CPU RAM 上，在 1F1B 的过程中，作者再进行加载&lt;/li>
&lt;/ol>
&lt;h3 id="training-recipe">&lt;a href="#training-recipe" class="header-anchor">&lt;/a>Training Recipe
&lt;/h3>&lt;p>模型上下文长度为 4096, 使用 MuonClip 进行优化，使用 WSD lr Scheduler.weight decay 为 0.1, global batch size 为 67M tokens.&lt;/p>
&lt;p>预训练结束之后，作者加入了一个 long-context activation stage. 这个阶段，作者使用了 400B 的 4K 上下文的 token 和 60B 的 32K 上下文的 60B token. 最后作者使用 &lt;a class="link" href="https://maosong.website/p/notes-on-yarn/" target="_blank" rel="noopener"
>YARN&lt;/a> 将模型上下文长度扩展到 128K.&lt;/p>
&lt;h2 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h2>&lt;h3 id="sft">&lt;a href="#sft" class="header-anchor">&lt;/a>SFT
&lt;/h3>&lt;p>数据的构建主要是基于：&lt;/p>
&lt;ol>
&lt;li>prompt 的多样性&lt;/li>
&lt;li>Response 的质量&lt;/li>
&lt;/ol>
&lt;p>作者构建了一系列的数据生成 pipeline, 然后使用 Kimi-k1.5 来生成多样化的回答，最后再进行质量评估和过滤。这里，作者主要介绍了一下 tool-use 数据的构建&lt;/p>
&lt;p>受 ACEBench 启发，作者构建了一个模仿真实世界 tool-use 的数据合成 pipeline. pipeline 如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2/Kimi-k2-tool-use-synthetic-pipeline.png"
width="1366"
height="439"
loading="lazy"
alt="Data synthesis pipeline for tool use"
class="gallery-image"
data-flex-grow="311"
data-flex-basis="746px"
>&lt;/p>
&lt;p>pipeline 主要包含三个阶段：&lt;/p>
&lt;ul>
&lt;li>tool spec generation: 作者基于 MCP tools 和 self-evolved 的数据合成策略来构建 tool repository. MCP tools 包括 3000+ 的 tools,合成了 20，000 个 tools&lt;/li>
&lt;li>Agent and task generation: 作者还用不同的 system prompt 以及不同的 tools combination 来构建对应的 agent. 然后对不同的 agent, 作者构建了对应的成功标准均，工具调用模式以及评估方式&lt;/li>
&lt;li>Trajectory generation: 作者首先构建不同风格的 LLM, 然后作者使用一个 simulator 来执行 tool call 并给予反馈。&lt;/li>
&lt;/ul>
&lt;p>最后，作者对数据进行了过滤。&lt;/p>
&lt;p>作者还加入了 coding 以及软件工程等任务相关数据来进一步提高模型的 agent 能力&lt;/p>
&lt;h3 id="rl">&lt;a href="#rl" class="header-anchor">&lt;/a>RL
&lt;/h3>&lt;p>RL 与 Kimi-K1.5 差不多。作者进一步提高了 RL 阶段的算力并做出了亮点改进：&lt;/p>
&lt;ol>
&lt;li>作者构建了类似 Gym 的框架，用于扩展 RL 的能力&lt;/li>
&lt;li>作者加入了更多 RLVR 的任务&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Data&lt;/strong>
数据主要包括以下几类：&lt;/p>
&lt;ol>
&lt;li>Math, STEM and logical tasks: 数据构建的原则为多样化和中等难度&lt;/li>
&lt;li>Instruction following: 作者基于 hybrid rule verification 和 multi-source instruction generation 来合成复杂的指令跟随数据&lt;/li>
&lt;li>Faithfulness: 作者训练了一个 judge model 来提供 reward&lt;/li>
&lt;li>Coding &amp;amp; Software Engineering: 作者从开源数据收集并合成了代码相关数据&lt;/li>
&lt;li>Safety. 提高模型的安全性，防止 jailbreak&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Reward&lt;/strong>
作者使用了 self-critique rubric reward 的奖励机制。&lt;/p>
&lt;p>首先，对于 Kimi-k2 的回答，作者会使用另一个 kimi-k2 作为 critic，基于三个方面进行排序：&lt;/p>
&lt;ul>
&lt;li>core rubric: AI 的核心价值观&lt;/li>
&lt;li>prescriptive rubric: 避免 reward hacking&lt;/li>
&lt;li>human-annotated rubric: 特定的上下文&lt;/li>
&lt;/ul>
&lt;p>在训练的过程中，critic 也会基于 verifiable signals 进行 refine&lt;/p>
&lt;p>&lt;strong>RL training&lt;/strong>
RL 的训练目标与 Kimi-k1.5 相同&lt;/p>
$$
\mathcal{L}(\pi_\theta) = \mathbb{E}_{x\sim \mathcal{D}}\left[\frac1K\sum_{i=1}^K \left(r(x,y_i)-\bar{r}(x) -\tau\log\frac{\pi_{\mathrm{\theta}}(y_i\mid x)}{\pi_{\mathrm{old}}(y_i\mid x)}\right)^2\right]
$$&lt;p>其中 $\bar{r}(x)=1/K\sum_{i=1}^Kr(x,y_i)$ 是 sample response 的平均奖励。&lt;/p>
&lt;p>作者做了以下几点改进来提高模型在不同 domain 上的表现：&lt;/p>
&lt;ol>
&lt;li>Budget control: 作者针对不同任务分别设置了最大 token 限制，模型超过这个限制会受到惩罚。猜测应该是 length penalty 或者类似 Qwen3 一样，直接中断思考过程输出最终答案&lt;/li>
&lt;li>PTX loss: 作者使用了 PTX loss 来提高模型对于高质量数据的利用率以及降低模型的过拟合&lt;/li>
&lt;li>Temperature Decay: 作者发现，训练后期保持较高的采样温度会影响模型的表现，因此作者设置了一个 schedule, 来逐步降低采样温度。&lt;/li>
&lt;/ol>
&lt;h3 id="rl-infra">&lt;a href="#rl-infra" class="header-anchor">&lt;/a>RL Infra
&lt;/h3>&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2/Kimi-k2-RL-infra.png"
width="734"
height="477"
loading="lazy"
alt="Kimi-K2 RL infra"
class="gallery-image"
data-flex-grow="153"
data-flex-basis="369px"
>&lt;/p>
&lt;p>RL infra 与 Kimi-k1.5 类似。主要包含三个模块。其中 train engine 和 inference engine 两者互相切换，一个 engine 进行 training 的时候，另一个 engine 就进行 inference, 为了提高 engine 切换的效率，作者使用了 checkpoint engine 来传输更新后的参数。&lt;/p>
&lt;h2 id="evaluation">&lt;a href="#evaluation" class="header-anchor">&lt;/a>Evaluation
&lt;/h2>&lt;p>模型评估结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2/Kimi-k2-performance.png"
width="696"
height="979"
loading="lazy"
alt="Performance of Kimi-k2"
class="gallery-image"
data-flex-grow="71"
data-flex-basis="170px"
>&lt;/p>
&lt;p>评估结果显示，模型在 coding 和通用任务上的表现仅次于 Claude 4, 大部分 benchmark 上都是第二名甚至是 sota.&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 Kimi-K2, 一个总参数为 1B 的 MoE 大语言模型。作者在架构，数据和优化器上进行了创新。并且通过 post-training 显著提高了模型的 agent 以及 tool-use 能力&lt;/p>
&lt;p>作者发现模型主要存在的问题有：&lt;/p>
&lt;ol>
&lt;li>reasoning 任务过难或者 tool 的定义不清晰的时候，模型会使用很多 token&lt;/li>
&lt;li>有时候工具调用可能会降低模型的表现&lt;/li>
&lt;li>模型在 agentic coding 任务上的能力需要进一步提升&lt;/li>
&lt;/ol>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://github.com/MoonshotAI/Kimi-K2" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Keye-VL</title><link>https://maosong.website/p/notes-on-keye-vl/</link><pubDate>Wed, 23 Jul 2025 11:11:43 +0800</pubDate><guid>https://maosong.website/p/notes-on-keye-vl/</guid><description>&lt;p>Keye-VL 是快手在 25 年 7 月份提出的一个 8B 的多模态大模型，其亮点为短视频理解能力。预训练包括 4 个 stage，使用了 600B token，后训练包括 2 个 stage，用于提升模型的 reasoning 和 non-reasoning 能力。&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者首先回顾了已有的 MLLM 工作，然后强调理解短视频仍然是一个很难的任务，特别是要求模型基于 video 和 audio 来理解视频。因此，在本文中，作者提出了 Kwai Keye-VL，一个 8B 的多模态大模型，主要用于短视频理解任务。&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;p>Keye-VL 是一个标准的 ViT-MLP-LLM 的架构，其中，ViT 是 SigLIP-400M-384-14, MLP 是一个基于 SwiGLU 的 2 层 MLP，使用了和 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5-vl/" target="_blank" rel="noopener"
>Qwen2.5-VL&lt;/a> 一样的 patch merge 方法，LLM 使用的是 Qwen3-8B，模型架构示意图如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl/Keye-VL-architecture.png"
width="1371"
height="1000"
loading="lazy"
alt="Keye-VL model architecture"
class="gallery-image"
data-flex-grow="137"
data-flex-basis="329px"
>&lt;/p>
&lt;p>作者针对 ViT 和 visual encoding 分别做了如下改进&lt;/p>
&lt;h4 id="navit">&lt;a href="#navit" class="header-anchor">&lt;/a>NaViT
&lt;/h4>&lt;p>作者实现了 native resolution ViT，来处理不同分辨率的图片。&lt;/p>
&lt;p>具体做法为，作者基于 SigLIP-400M-384-14 来初始化 ViT。&lt;/p>
&lt;p>然后，作者首先采用了 interpolation 来将 ViT 的 position encoding 扩展到不同的图片精度下面去。&lt;/p>
&lt;p>接下来，作者提出了 2D RoPE 来进一步提升 position encoding 的表现。&lt;/p>
&lt;p>最后，作者加入了 NaViT 的 packing 技巧来继续预训练 ViT.&lt;/p>
&lt;p>在 ViT 预训练的过程中，作者使用了 &lt;strong>500B&lt;/strong> 的 token&lt;/p>
&lt;h4 id="visual-encoding">&lt;a href="#visual-encoding" class="header-anchor">&lt;/a>Visual Encoding
&lt;/h4>&lt;p>为了提升模型理解图片和视频的能力，作者针对图片和视频进行了进一步的处理。&lt;/p>
&lt;p>对于不同精度的图片，作者将最大 token 个数设置为 16384。&lt;/p>
&lt;p>对于视频，作者将每帧的 token 数限制在 $[128,768]$, 每个视频的最大 token 个数设置为 24576&lt;/p>
&lt;p>对于提取的 frames，作者重新计算了 FPS, 然后在 3D RoPE 中让时间维度与真实时间严格对齐。&lt;/p>
&lt;h3 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h3>&lt;h4 id="data">&lt;a href="#data" class="header-anchor">&lt;/a>Data
&lt;/h4>&lt;p>预训练数据一共包括 600B token，覆盖了 6 个类别：&lt;/p>
&lt;ul>
&lt;li>Image caption: 包括中英文数据，来源为 LAION, DataComp 以及 Coyo. 作者基于 CLIP 来计算相速度，然后过滤掉相似度比较低的数据，作者还对数据进行了 re-caption，实验发现 re-caption 可以提高模型的细粒度图片理解能力&lt;/li>
&lt;li>OCR &amp;amp; VQA: 数据包括开源数据和合成数据。合成数据包括 Synthesis 和 Rendering 两个方法，第一是基于 text-dense image 构建 OCR 数据以及基于 image-caption pair 数据使用 MLLM 构建 VQA 数据。第二是使用字体渲染工具合成高质量的 OCR 数据&lt;/li>
&lt;li>Grounding &amp;amp; Counting: Grounding 数据主要包括 RefCoCo, VisualGenome, TolokaVQA, Counting 数据包括 PixMo. 作者仍然使用 CLIP 对数据进行过滤&lt;/li>
&lt;li>Interleaved text-image data: 作者发现图文交错数据可以提供通用知识，并且还可以提高模型的视觉语言对齐能力，第三就是提升模型的泛化能力。作者主要从 academic PDF 以及结构化知识中提取对应的数据。作者基于 Garbled character recognition, low-resolution/broken image filtering 以及 text-image similarity validation 来保证数据的质量&lt;/li>
&lt;li>Video understanding: 作者使用 Qwen2.5-omni 从 interleaved video-asr 来将视频数据转化图文交错数据， 然后基于 ASR 的结果进行 recaption，最后对每一帧进行 OCR. 作者还构建了 Frame-level re-ordering 以及 multiple video matching 两个任务来提高模型的上下文理解能力&lt;/li>
&lt;li>Pure Text: 未提及&lt;/li>
&lt;/ul>
&lt;p>对于源数据，作者进行了数据清洗：&lt;/p>
&lt;ol>
&lt;li>使用 CLIP 对数据进行打分，然后过滤掉低质量的数据&lt;/li>
&lt;li>使用开源的 MLLM 作为 discriminator 来选择高质量的数据&lt;/li>
&lt;li>去重&lt;/li>
&lt;/ol>
&lt;h4 id="training-recipe">&lt;a href="#training-recipe" class="header-anchor">&lt;/a>Training Recipe
&lt;/h4>&lt;p>预训练包括 4 个 stage：&lt;/p>
&lt;ul>
&lt;li>Stage 0: 使用 SigLIP 损失函数来继续训练 ViT&lt;/li>
&lt;li>Stage 1: cross-modal Alignment，仅训练 MLP&lt;/li>
&lt;li>Stage 2: multi-task pre-training, 解冻所有参数，使用 Multi-task 数据来训练模型&lt;/li>
&lt;li>Stage 3: annealing, 在高质量数据集上进行 fine-tune，进一步提升模型的能力&lt;/li>
&lt;/ul>
&lt;p>作者发现，预训练后的模型在下游任务上的表现对训练数据配比非常敏感。为了解决这个问题，在最后一个训练阶段，作者使用了一个 merging 的技巧，来保持模型的能力。&lt;/p>
&lt;h3 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h3>&lt;p>post-training 阶段一共包含了 2 个 step, 5 个 stage, 第一个 step 包含 2 个 stage，用于提升模型的 non-reasoning 能力。第二个 step 包含 3 个 stage, 用于提升模型的 reasoning 能力&lt;/p>
&lt;h4 id="no-reasoning-training">&lt;a href="#no-reasoning-training" class="header-anchor">&lt;/a>No-reasoning Training
&lt;/h4>&lt;p>第一个 step 是 non-reasoning training, 包含了 SFT 和 MPO 两个 stage, 训练 pipeline 如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl/Keye-VL-non-reasoning-training.png"
width="1340"
height="472"
loading="lazy"
alt="Non reasoning training pipeline"
class="gallery-image"
data-flex-grow="283"
data-flex-basis="681px"
>&lt;/p>
&lt;p>&lt;strong>SFT&lt;/strong>
SFT 阶段一共使用了 &lt;strong>5M&lt;/strong> 的多模态 QA 样本，为了提升数据的多样性，作者使用 TaskGalaxy 来将数据分类为 70,000 种任务类型，然后作者对每条数据，使用 MLLM 评估问题的难度，来过滤掉过于简单的问题。最后，作者请人类来进行标注，保证数据的可靠性。&lt;/p>
&lt;p>&lt;strong>MPO&lt;/strong>
训练方面，作者使用了 MPO 进行训练。
数据方面，作者使用了：&lt;/p>
&lt;ol>
&lt;li>400,000 开源的数据，作者主要进行了去重以及过滤低质量的数据&lt;/li>
&lt;li>50,000 偏好数据，基于 MM-RLHF 和 MMPR 等数据构建，然后构建高质量的 negative examples&lt;/li>
&lt;li>10,000 条 self-improvement 样本：基于 benchmark 和人类反馈，使用 SFT model 的回答作为 chosen samples, 然后基于 reward model 或者 rule-based rewards 评估模型输出，选择分数最低的座位 rejected samples&lt;/li>
&lt;li>90,000 纯文本样本： in-house data&lt;/li>
&lt;li>30,000 人类标注样本：使用开源和闭源模型进行回答，然后请人类进行排序&lt;/li>
&lt;/ol>
&lt;h4 id="reasoning-training">&lt;a href="#reasoning-training" class="header-anchor">&lt;/a>Reasoning Training
&lt;/h4>&lt;p>第二个 step 是 reasoning training, 包含了 CoT Cold-Start, Mix-Mode RL 和 Iterative Alignment 三个 stage, 训练 pipeline 如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl/Keye-VL-reasoning-training.png"
width="1358"
height="596"
loading="lazy"
alt="Non reasoning training pipeline"
class="gallery-image"
data-flex-grow="227"
data-flex-basis="546px"
>&lt;/p>
&lt;p>&lt;strong>CoT cold-start&lt;/strong>
作者收集了如下数据：&lt;/p>
&lt;ul>
&lt;li>330, 000 条 non-reasoning 样本：和第一个 step 的数据分布类似，但是不重合&lt;/li>
&lt;li>230,000 reasoning 样本：作者构建了一个 data construction pipeline,用于保证 long-CoT 的正确性&lt;/li>
&lt;li>20,000 automatic reasoning 样本：作者基于 MMPR, MM-Eureka 以及 OpenR1-math 来收集数据，基于这些模型来训练模型自动化决定是否要进行 reasoning&lt;/li>
&lt;li>100,000 agentic reasoning 样本：训练模型的 &amp;ldquo;think with image&amp;rdquo; 能力。基于 3M QA pairs，作者使用 Qwen2.5-72B 来识别需要进行 manipulating 的问题，然后生成对应的代码。接下来作者构建了一部分 OCR 数据，让模型学会 constrast enhancement 或者 rotation 操作。最后对于数学问题，作者让 Gemini-2.5-Pro 生成对应的思考过程，再让 GPT-4o 将对应的计算转换为可执行的代码。&lt;/li>
&lt;li>32, 000 Video data: 包含了 24,000 条 thinking samples 和 80,000 条 non-thinking samples&lt;/li>
&lt;/ul>
&lt;p>训练时，所有样本混在一起进行训练。作者认为，将日常使用的 SFT 数据和 reasoning 数据放在一起训练，可以保持模型在通用场景下的能力。&lt;/p>
&lt;p>&lt;strong>Mix-Mode RL&lt;/strong>
训练数据主要包括 4 个任务：&lt;/p>
&lt;ol>
&lt;li>Multimodal perception: 复杂文本识别和 counting 任务&lt;/li>
&lt;li>Multimodal reasoning: MMPR 和 MM-Eureka&lt;/li>
&lt;li>Text-based mathematical reasoning: 数学推理问题&lt;/li>
&lt;li>Agentic reasoning: 从 DeepEyes 中获取的 47,000 条样本&lt;/li>
&lt;/ol>
&lt;p>作者使用了 GRPO 来训练，reward 基于 MLLM 进行，包括最终结果和思考过程。&lt;/p>
&lt;p>作者还使用 RL 来提高模型的短视频理解能力。作者发现 RL 训练之后，模型的短视频理解能力有了大幅度的提升。&lt;/p>
&lt;p>&lt;strong>Iterative Alignment&lt;/strong>
这一步主要解决模型的重复性输出，或者 reasoning logic 不对的问题。作者使用了 rejection-sampling 数据，包括 instruction following, OCR, mathematics, charts, counting 等。&lt;/p>
&lt;p>作者基于 Rule-based score 和 model-based score 来进行打分，最后使用 MPO 算法进行训练。通过这个过程，模型的输出格式和动态思考能力都有了提升。&lt;/p>
&lt;h2 id="evaluation">&lt;a href="#evaluation" class="header-anchor">&lt;/a>Evaluation
&lt;/h2>&lt;p>作者首先评估了 ViT 的表现，主要有两点：&lt;/p>
&lt;ol>
&lt;li>在 SigLIP 的基础上加入 1D interpolation 之后，模型的表现所有下降，作者认为这是由于 1D 的 position encoding 无法识别 2D 的 patch 排列导致的&lt;/li>
&lt;li>加入 2D RoPE 之后，ViT 与 SigLIP 的表现持平&lt;/li>
&lt;/ol>
&lt;p>接下来是 Keye-VL 在公开 benchmark 上的表现，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl/Keye-VL-performance.png"
width="1068"
height="1161"
loading="lazy"
alt="Performance of Keye-VL"
class="gallery-image"
data-flex-grow="91"
data-flex-basis="220px"
>&lt;/p>
&lt;p>作者还构建了一个内部的 benchmark, 用于进一步评估模型的能力。&lt;/p>
&lt;p>已有 benchmark 的问题：&lt;/p>
&lt;ol>
&lt;li>contamination&lt;/li>
&lt;li>多语种覆盖不足：大部分 benchmark 都是英文的&lt;/li>
&lt;li>任务和 domain 覆盖不足：大部分 benchmark 只考虑基本的 perception 和 reasoning 能力&lt;/li>
&lt;li>任务难度和评估格式单调&lt;/li>
&lt;/ol>
&lt;p>构建 benchmark 的原则：&lt;/p>
&lt;ol>
&lt;li>在中文场景下的真实用户需求，open ended QA, 包括短视频理解能力&lt;/li>
&lt;li>细粒度的评估&lt;/li>
&lt;li>多样性高&lt;/li>
&lt;li>没有 contamination&lt;/li>
&lt;li>多角度评估策略: 正确性，相关性，理解性，流畅性和创造性&lt;/li>
&lt;/ol>
&lt;p>结果如下：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl/Keye-VL-performance-internal.png"
width="1339"
height="559"
loading="lazy"
alt="Performance of Keye-VL on the internal benchmark"
class="gallery-image"
data-flex-grow="239"
data-flex-basis="574px"
>&lt;/p>
&lt;p>分析：&lt;/p>
&lt;ol>
&lt;li>Keye-VL 在 OCR 等任务上的表现有所不足，其细粒度的识别能力也有所不足，会认错人。有时候还会忽略掉一些信息&lt;/li>
&lt;li>描述 temporal action 时会出现不稳定性，模型对镜头的感知能力不足。需要准确定位时间等&lt;/li>
&lt;li>在需要逻辑链条和数学计算时，模型能力不足，对于特定 domain 上的任务会出现事实性错误。写作时，模型倾向于输出通用的回答，而不是定制化的回答。&lt;/li>
&lt;/ol>
&lt;h2 id="discussion">&lt;a href="#discussion" class="header-anchor">&lt;/a>Discussion
&lt;/h2>&lt;p>作者讨论了两点关键发现：&lt;/p>
&lt;ol>
&lt;li>reasoning 和 non-reasoning 的数据可以互相促进彼此的表现，这与 ERNIE 4.5 的发现一致。&lt;/li>
&lt;li>作者认为通过 mix-mode 的训练，模型在简单和复杂任务上的表现都可以提升，因此作者使用了混合数据来进行训练，结果发现效果很好。&lt;/li>
&lt;/ol>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>本文中，作者提出了 Keye-VL 8B，一个短视频理解能力出色的多模态大模型，作者详细介绍了 pre-training 和 post-training. 其中，mix-mode training 可以有效提高模型的表现。&lt;/p>
&lt;p>作者认为 Keye-VL 有如下改进的地方：&lt;/p>
&lt;ol>
&lt;li>并没有优化 video encoder 或者是改进 video encoding 的策略&lt;/li>
&lt;li>Keye-VL 的视觉感知能力有进一步的提升空间，其 &amp;ldquo;reasoning with image&amp;rdquo; 能力依然落后于领先的 reasoning model&lt;/li>
&lt;li>使用一个额外的 MLLM 作为 reward model 会极大消耗算力，如何构建一个更可靠更高效的 reward model 需要进一步探索。&lt;/li>
&lt;/ol>
&lt;h2 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2507.01949" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/Kwai-Keye/Keye/tree/main" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on V-Triune</title><link>https://maosong.website/p/notes-on-v-triune/</link><pubDate>Thu, 17 Jul 2025 09:37:36 +0800</pubDate><guid>https://maosong.website/p/notes-on-v-triune/</guid><description>&lt;p>V-Triune 探究了如何使用一套统一的 RL 训练框架，来同时提高模型的 perception 和 reasoning 能力。&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>已有的 RL 训练方法主要提升模型在特定 domain 上的能力，比如 MM-Eureka, Vision-R1 等专注于提升 MLLM 的 reasoning 能力，而 R1-V, DeepPerception 等专注于提升模型的 perception 能力。&lt;/p>
&lt;p>因此，本文的 motivation 就是，如何用一套统一的 RL 训练框架，来同时提高模型的 perception 和 reasoning 能力。&lt;/p>
&lt;p>V-Triune 包含 3 个关键模块：&lt;/p>
&lt;ul>
&lt;li>Sample-Level data formatting: 即在 sample 的层面定义 reward 等信息&lt;/li>
&lt;li>Verifier-Level Computation: 即分离 verifier 和 generator, 提高整体效率&lt;/li>
&lt;li>Source-Level Metric Monitoring: 基于 data source 来监控模型的表现&lt;/li>
&lt;/ul>
&lt;p>作者还提出了 Dynamic IoU reward, 来提高训练的稳定性。&lt;/p>
&lt;p>基于 V-Triune 和 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5-vl/" target="_blank" rel="noopener"
>Qwen2.5-VL&lt;/a>, 作者提出了 Orsta,一个在 reasoning 和 perception 任务上均有提升的多模态大模型。&lt;/p>
&lt;p>论文的主要贡献为：&lt;/p>
&lt;ol>
&lt;li>提出了 V-Triune, 一个统一的 RL 训练框架，来同时提高模型的 reasoning 和 perception 能力&lt;/li>
&lt;li>在 infra 上进行了改进，提高整体的训练效率和稳定性&lt;/li>
&lt;li>基于 V-Triune, 构建了 Orsta, 相比于 Baseline, 模型的 perception 和 reasoning 能力均有了提升。&lt;/li>
&lt;/ol>
&lt;h2 id="preliminary">&lt;a href="#preliminary" class="header-anchor">&lt;/a>Preliminary
&lt;/h2>&lt;p>作者首先回顾了一下 GRPO 和 &lt;a class="link" href="https://maosong.website/p/notes-on-dapo/" target="_blank" rel="noopener"
>DAPO&lt;/a> , 我们这里就不再赘述。&lt;/p>
&lt;p>然后，作者介绍了一下 reward function, reward function 由两部分组成， 第一部分是 format reward, 第二部分是 accuracy reward,.&lt;/p>
&lt;p>对于 format reward, 作者采取了和 OpenR1 相同的做法，也就是要求输出仅包含 1 个给定的 token, 这里特定的 token 为 $s_1=\texttt{&lt;think>}$, $s_2=\texttt{&lt;/think>}$, $s_3=\texttt{&lt;answer>}$, $s_4=\texttt{&lt;/answer>}$, reward 的定义如下：&lt;/p>
$$
R_{\mathrm{format}}(o_q) = 0.25\sum_{i=1}^4 \mathbb{1}(\mathrm{count}(o_q,s_i)=1)
$$&lt;p>这里 $\mathbb{1}(\cdot)$ 是示性函数。&lt;/p>
&lt;p>对于 accuracy reward, 作者根据不同的任务分别进行处理。&lt;/p>
&lt;p>对于 reasoning 任务，reward function 的定义如下&lt;/p>
$$
R_{\mathrm{acc}}(\hat{a},a) = \begin{cases}
1, &amp;\text{ if }\texttt{verify(parse($\hat{a}$), parse($a$))}\text{ is True}\\
0, &amp;\text{ else}
\end{cases}
$$&lt;p>这里 $\hat{a}$, $a$ 分别是 prediction 和 ground truth.&lt;/p>
&lt;p>对于 perception 任务，reward function 与我们要处理的子任务相关。如果是 OCR 和 counting 任务，则我们采取和 reasoning 一样的 reward function. 如果我们要处理的是 grounding 和 detection 任务，则我们可以使用 IoU 以及 mAP 两种 reward 形式。&lt;/p>
&lt;p>IoU reward 的定义如下：&lt;/p>
$$
R_{\mathrm{acc}}(\hat{a},a) = \begin{cases}
\mathrm{IoU}(\hat{a}, a), &amp;\text{ if }\mathrm{IoU}(\hat{a},a)\geq \epsilon\\
0, &amp;\text{ else}
\end{cases}
$$&lt;p>这里 $\epsilon>0$ 为超参数， IoU 定义如下&lt;/p>
$$
\mathrm{IoU}(\hat{a},a) = \frac{\mathrm{Area}(\hat{a}\cap a)}{\mathrm{Area}(\hat{a}\cup a)}
$$&lt;p>mAP 的定义如下&lt;/p>
$$
\mathrm{AP}_c = \int_0^1 \max_{\tilde{r}\geq r}P_c(\tilde{r}) dr,\quad mAP=\frac1C\sum_{c=1}^C \mathrm{AP}_c
$$&lt;p>最终的 reward 是 format reward 和 accuracy reward 的加权求和：&lt;/p>
$$
R = \alpha_{\mathrm{acc}}R_{\mathrm{acc}} + \alpha_{\mathrm{format}}R_{\mathrm{format}}
$$&lt;h2 id="v-triune">&lt;a href="#v-triune" class="header-anchor">&lt;/a>V-Triune
&lt;/h2>&lt;p>V-Triune 框架如下图所示，其包含三个模块&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-v-triune/V-Triune_framework.png"
width="1351"
height="562"
loading="lazy"
alt="V-Triune System"
class="gallery-image"
data-flex-grow="240"
data-flex-basis="576px"
>&lt;/p>
&lt;h3 id="sample-level-data-formatting">&lt;a href="#sample-level-data-formatting" class="header-anchor">&lt;/a>Sample-Level Data Formatting
&lt;/h3>&lt;p>核心思想就是不同的任务的 reward function 可能是不一样的，如果在 task 层面设计 reward function 的话，RL 训练就没有了 flexibility, 因此，在本文中，作者在 sample 层面设计 reward function. 具体做法就是，每个样本除了 QA 相关信息，还有 reward model 的相关参数，这样就可以更加灵活地调整不同 sample 的损失了。&lt;/p>
&lt;h3 id="verifier-level-reward-computation">&lt;a href="#verifier-level-reward-computation" class="header-anchor">&lt;/a>Verifier-Level Reward Computation
&lt;/h3>&lt;p>核心思想就是构建多个不同的 Verifier, 来分别处理不同的 sample, 因为我们 reward model 也是在 sample 层面构建的，因此我们可以基于 sample 的 metadata 来动态分配对应的 verifier, 这样就提升了 veriier 的可扩展性。本文作者使用了 MathVerifier 和 DetectionVerifier 两种。&lt;/p>
&lt;h3 id="source-level-metric-monitoring">&lt;a href="#source-level-metric-monitoring" class="header-anchor">&lt;/a>Source-Level Metric Monitoring
&lt;/h3>&lt;p>核心思想就是根据 sample 的 metadata 信息来监控不同的 metric, 这样可以防止训练不稳定。&lt;/p>
&lt;h3 id="dynamic-iou-reward">&lt;a href="#dynamic-iou-reward" class="header-anchor">&lt;/a>Dynamic IoU Reward
&lt;/h3>&lt;p>核心思想就是根据训练进程，动态调整 IoU 的阈值。&lt;/p>
&lt;p>作者发现，如果将 IoU 阈值设置的比较低的话，模型很容易拿到比较高的 reward; 如果将 IoU 阈值设置的比较高的话，模型又很难拿到奖励。&lt;/p>
&lt;p>因此，作者的解决方案就是，使用课程学习，随着训练的进行，逐步提高 IoU 的阈值。&lt;/p>
&lt;h2 id="data">&lt;a href="#data" class="header-anchor">&lt;/a>Data
&lt;/h2>&lt;p>数据集列举如下：&lt;/p>
&lt;ul>
&lt;li>Math: mm_math, geometry3K, mmk12&lt;/li>
&lt;li>Puzzle: PuzzleVQA, AlgoPuzzleQA, VisualPuzzles&lt;/li>
&lt;li>Science: ScienceQA, SciVQA, ViRL39K (Broader STEM topics, (GradeSchool) Science)&lt;/li>
&lt;li>Chart: ChartQAPro, ChartX, Table-VQA, ViRL39K (Tables/Diagrams/Charts)&lt;/li>
&lt;li>Detection: V3Det, Object365&lt;/li>
&lt;li>Grounding: D3&lt;/li>
&lt;li>Counting: CLEVR&lt;/li>
&lt;li>OCR: LLaVA-OV (OCR questions), EST-VQA&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Rule-based filtering&lt;/strong>
对于 visual reasoning 数据，作者的过滤如下：&lt;/p>
&lt;ul>
&lt;li>多项选择题以及判断题，防止 reward hacking&lt;/li>
&lt;li>答案包含特殊符号如 &amp;ldquo;=&amp;rdquo;, &amp;ldquo;[&amp;rdquo; 等的数据&lt;/li>
&lt;li>答案字符数超过 20 个，防止答案太复杂&lt;/li>
&lt;/ul>
&lt;p>对于 visual perception 数据，作者过滤流程如下：&lt;/p>
&lt;ul>
&lt;li>Detection: 超过 10 个 bounding box 的，或者 box 占 50% 以上面积的数据，保持 single BB 和 multi BB 的比例为 1:2&lt;/li>
&lt;li>Grounding: box 占 50% 以上面积的数据, label 过于复杂的数据&lt;/li>
&lt;li>Counting: 保持类别平衡，仅使用英文数据&lt;/li>
&lt;li>OCR: 保留英文数据，对 labels 进行 verify&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Difficulty-based filtering&lt;/strong>
主要是移除比较简单的问题。&lt;/p>
&lt;p>经过这两个阶段的过滤之后，得到了 &lt;strong>20.6K&lt;/strong> perception samples 以及 &lt;strong>27.1K&lt;/strong> reasoning samples. 数据保存在 Parquet 格式&lt;/p>
&lt;h2 id="training-recipe">&lt;a href="#training-recipe" class="header-anchor">&lt;/a>Training Recipe
&lt;/h2>&lt;h3 id="disable-vit-training">&lt;a href="#disable-vit-training" class="header-anchor">&lt;/a>Disable ViT Training
&lt;/h3>&lt;p>作者首先发现，全量微调会导致训练崩溃。作者分析原因发现，主要是 ViT 在反向传播过程中，存在 gradient amplify 的问题，第一层与最后一层的梯度的 norm 相差 5-10 倍。作者认为有两点原因：&lt;/p>
&lt;ol>
&lt;li>RL 会强制要求模型的不同模态之间进行对齐&lt;/li>
&lt;li>对比学习训练得到的 VIT 可能使得其不适合 RL 的训练。&lt;/li>
&lt;/ol>
&lt;p>基于这两点考虑，作者再进行训练的时候，冻结了 ViT 模块，结果如下图&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-v-triune/V-Triune-frozen-ViT.png"
width="1383"
height="705"
loading="lazy"
alt="Analysis of ViT training instability"
class="gallery-image"
data-flex-grow="196"
data-flex-basis="470px"
>&lt;/p>
&lt;h3 id="spurious-image-special-tokens">&lt;a href="#spurious-image-special-tokens" class="header-anchor">&lt;/a>Spurious Image Special Tokens
&lt;/h3>&lt;p>作者发现，模型的输出可能会包含一些 &lt;code>&amp;lt;image_pad&amp;gt;&lt;/code> 等 token, 因此，作者对输出进行了过滤，然后再进行计算。&lt;/p>
&lt;h3 id="cot-prompt-pool">&lt;a href="#cot-prompt-pool" class="header-anchor">&lt;/a>CoT Prompt Pool
&lt;/h3>&lt;p>作者还发现，多样化的 CoT prompt 会损害模型的表现。因此，作者构建了 20 个 prompt 模版，每次随机挑选其中一个加入到 instruction 中。&lt;/p>
&lt;h3 id="training-configuration">&lt;a href="#training-configuration" class="header-anchor">&lt;/a>Training Configuration
&lt;/h3>&lt;p>模型基于 Qwen2.5-7B-instruct 和 Qwen2.5VL-32B 来进行开发&lt;/p>
&lt;p>作者探究了 on-policy 和 off-policy 两种配置。roll-out batch size 设置为 1024, 使用 GRPO 进行训练，GRPO 的 group size 设置为 8&lt;/p>
&lt;p>作者还设置 $\epsilon_{high}=0.28$ 以及 $\epsilon_{low}=0.2$ 来提高 token 的采样率。作者去除了 KL divergence loss, 并且在 token 层面上进行平均&lt;/p>
&lt;h2 id="evaluation">&lt;a href="#evaluation" class="header-anchor">&lt;/a>Evaluation
&lt;/h2>&lt;h3 id="performance">&lt;a href="#performance" class="header-anchor">&lt;/a>Performance
&lt;/h3>&lt;p>&lt;img src="https://maosong.website/p/notes-on-v-triune/V-Triune-performance.png"
width="1159"
height="1051"
loading="lazy"
alt="Performance of Orsta on MEGA-Bench core"
class="gallery-image"
data-flex-grow="110"
data-flex-basis="264px"
>&lt;/p>
&lt;h3 id="analysis">&lt;a href="#analysis" class="header-anchor">&lt;/a>Analysis
&lt;/h3>&lt;p>&lt;strong>on-policy v.s. off-policy&lt;/strong>
作者首先对比了以下 on-policy RL 和 off-policy RL 两种训练方式的表现， 结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-v-triune/V-triune-on-policy.png"
width="1137"
height="526"
loading="lazy"
alt="Ablation on on-policy v.s. off-policy"
class="gallery-image"
data-flex-grow="216"
data-flex-basis="518px"
>&lt;/p>
&lt;p>结果有两点发现：&lt;/p>
&lt;ol>
&lt;li>on-policy 的表现基本上都是比 off-policy 要好的。&lt;/li>
&lt;li>小模型通过 RL 带来的表现提升更明显，大模型的表现则更慢&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>generalization&lt;/strong>
作者还评估了以下模型的泛化性，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-v-triune/V-Triune-generalization.png"
width="422"
height="673"
loading="lazy"
alt="Generalization of V-Triune"
class="gallery-image"
data-flex-grow="62"
data-flex-basis="150px"
>&lt;/p>
&lt;p>结果发现，RL 的训练更像是对齐，而不是泛化。也就是说，模型在 in-domain 的任务上表现较好，在 OOD 的任务上表现就一般。这与已有的结论是不一样的&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Recall
&lt;a class="link" href="https://maosong.website/p/notes-on-magistral/" target="_blank" rel="noopener"
>Magistral&lt;/a> 发现模型在 math domain 上进行训练，在 code domain 上的表现也会提升&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Training Dynamics&lt;/strong>
作者还分析了不同任务不同指标随训练进行的变化情况，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-v-triune/V-Triune-training-dynamics.png"
width="1150"
height="900"
loading="lazy"
alt="V-Triune training dynamics"
class="gallery-image"
data-flex-grow="127"
data-flex-basis="306px"
>&lt;/p>
&lt;p>结果显示，reasoning 和 OCR 任务岁训练进行其输出长度和正确率都有所提升。但是 detection 相关任务则变化不是很明显。&lt;/p>
&lt;h3 id="ablation-study">&lt;a href="#ablation-study" class="header-anchor">&lt;/a>Ablation Study
&lt;/h3>&lt;p>作者进行了三个消融实验：&lt;/p>
&lt;ol>
&lt;li>训练策略：冻结 ViT, 冻结 LLM, 两者都不冻结&lt;/li>
&lt;li>任务分解策略：仅使用 reasoning 数据进行训练，仅使用 perception 数据进行训练，同时使用两种数据进行训练。&lt;/li>
&lt;li>learning rate: 不同的学习率&lt;/li>
&lt;/ol>
&lt;p>实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-v-triune/V-Triune-ablation-study.png"
width="1146"
height="585"
loading="lazy"
alt="Ablation study"
class="gallery-image"
data-flex-grow="195"
data-flex-basis="470px"
>&lt;/p>
&lt;p>结果发现，&lt;/p>
&lt;ol>
&lt;li>仅训练 ViT 对表现没有任何帮助，只有训练 LLM 才会带来性能的提升。&lt;/li>
&lt;li>同时使用两种数据进行训练的效果是最好的，其次是仅使用 reasoning 数据进行训练，最后是仅使用 perception 数据进行训练&lt;/li>
&lt;li>较大的学习率会损害模型的表现。&lt;/li>
&lt;/ol>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>本文中，作者提出了 V-Triune, 一个使用 RL 来同时提高 MLLM 的 reasoning 和 perception 能力的 RL 训练框架。&lt;/p>
&lt;p>作者发现 RL 对于 VLM 来说更像是一种 Alignment 策略，而不是一种 Generalization 策略，也就是模型并没有引入新的能力，而是提升模型本身的 utility 以及 robustness.&lt;/p>
&lt;p>作者认为本文有以下 Limitation:&lt;/p>
&lt;ol>
&lt;li>在 perception 任务上没有看到明显的 test-time scaling 现象。作者认为，探究使用 o3 类似的方法或许能提高模型的表现&lt;/li>
&lt;li>探究 RL-zero 在 VLM 中的应用，也就是直接在 base model 上进行 RL 的训练。&lt;/li>
&lt;/ol>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2505.18129" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Magistral</title><link>https://maosong.website/p/notes-on-magistral/</link><pubDate>Wed, 16 Jul 2025 11:04:04 +0800</pubDate><guid>https://maosong.website/p/notes-on-magistral/</guid><description>&lt;p>Magistral 是 Mistral 提出的一个 reasoning model 系列，主要针对 math 和 code 两个 domain&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>Mistral 在 2025 年 6 月 12 日发布了 Magistral ，一个 reasoning model, 包含两个模型，一个是由纯 RL 训练得到的 &lt;em>Magistral Medium&lt;/em>, 另一个是由 SFT 和蒸馏 Magistral Medium 得到的 &lt;strong>Magistral Small&lt;/strong>&lt;/p>
&lt;p>作者首先介绍了一下本文的贡献：&lt;/p>
&lt;ol>
&lt;li>介绍了如何仅使用 RL (而不是用蒸馏) 来训练 Magistral Medium&lt;/li>
&lt;li>infra 上的改进，主要使用最新的权重来更新 generator&lt;/li>
&lt;li>多语种能力，支持多种语言&lt;/li>
&lt;li>系统性探究了 RLVR 的能力边界&lt;/li>
&lt;li>开源了 Magistral small (24B)&lt;/li>
&lt;/ol>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="rl">&lt;a href="#rl" class="header-anchor">&lt;/a>RL
&lt;/h3>&lt;p>RL 算法基于 GRPO 改进，主要有以下几点：&lt;/p>
&lt;ol>
&lt;li>去掉了 KL Divergence loss, 这一点跟 &lt;a class="link" href="https://maosong.website/p/notes-on-dapo/" target="_blank" rel="noopener"
>DAPO&lt;/a> 是一致的，提升模型的探索能力&lt;/li>
&lt;li>Loss Normalization，在 sample 层面做平均，还是跟 DAPO 一致，减少不同长度输出对训练造成的影响&lt;/li>
&lt;li>Advantage Normalization, 作者首先将每个 token 的 Advantage 定义为 $\hat{A}_i=R_i-\mu$, 其中 $\mu$ 是每个 group 的 reward 均值， 然后在每个 mini-batch 里对 advantage 进行 Normalization (这里 $\hat{A}_{mean}$ 和 $\hat{A}_{std}$ 分别为 mini-batch advantage 的均值和方差)：&lt;/li>
&lt;/ol>
$$
\hat{A}_{i,t}^{norm}=\frac{\hat{A}_i-\hat{A}_{mean}}{\hat{A}_{std}}
$$&lt;ol start="4">
&lt;li>CLIP-Higher, 跟 DAPO 一致，提高稀有 token 的被采样概率&lt;/li>
&lt;li>Eliminating non-divsere groups, 跟 DAPO 一致，去掉过于简单和过于难的题目&lt;/li>
&lt;/ol>
&lt;p>最终 RL 阶段训练的损失函数为：&lt;/p>
$$
\mathcal{L}(\theta) = \mathbb{E}_{q\sim P(Q),\{o_i\}_{i=1}^G\sim \pi_{old}(\cdot\mid q)}\frac{1}{\sum_{i=1}^G|o_i|}\sum_{i=1}^G\sum_{t=1}^{|o_i|}\min\left[r_{i,t}(\theta)\hat{A}_{i,t}^{norm}, \mathrm{clip}(r_{i,t}(\theta), 1-\epsilon_{low}, 1+\epsilon_{high})\hat{A}_{i,t}^{norm}\right], \mathrm{s.t.}, \exists 1\leq m &lt; n \leq G, r_m\neq r_n.
$$&lt;p>其中&lt;/p>
$$
r_{i,t}(\theta) = \frac{\pi_{\theta}(o_{i,t}\mid q, o_{i,&lt;t})}{\pi_{old}(o_{i,t}\mid q, o_{i,&lt;t})}
$$&lt;h3 id="reward-shaping">&lt;a href="#reward-shaping" class="header-anchor">&lt;/a>Reward Shaping
&lt;/h3>&lt;p>作者还基于四个维度来构建 reward: formatting, correctness, length, 以及 language consistency&lt;/p>
&lt;p>&lt;strong>Formatting&lt;/strong>
针对数学和代码问题，作者要求模型输出符合特定的格式&lt;/p>
&lt;ul>
&lt;li>Tag requirements: 思考过程用 &lt;code>&amp;lt;think&amp;gt;&amp;lt;/think&amp;gt;&lt;/code> 包含，且只能包含一个 tag&lt;/li>
&lt;li>mathematical responses: 对于数学问题，结果用 &lt;code>\boxed{}&lt;/code> 包含&lt;/li>
&lt;li>code response：包含一个 markdown block&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>[!tip] Future
最新的 &lt;a class="link" href="https://maosong.website/p/notes-on-glm-4.1v-thinking/" target="_blank" rel="noopener"
>GLM-4.1V-Thinking&lt;/a> 认为，不应该在 RL 训练阶段加入 format reward&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>correctness&lt;/strong>
基于答案的正确性分配奖励&lt;/p>
&lt;ul>
&lt;li>math correctness：使用 rule-based verifier 进行打分，使用 parser 和 Sympy 来比较模型输出以及 ground truth&lt;/li>
&lt;li>code correctness: 构建单元测试，评估输出代码是否能通过所有的单元测试&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Length penalty&lt;/strong>
与 DAPO 一致，使用 Length penalty 来惩罚过长的回答。&lt;/p>
&lt;p>&lt;strong>Language consistency&lt;/strong>
减少模型混合语言输出的问题。作者的做法是将 10% 的问题从英语转化为其他语种，然后使用 fastText 进行分类，确保内容都是一个语言。作者发现，通过这个简单的修改，就提高了模型的语言跟随能力。&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Recall
MiMo 中也遇到了混合语言输出的问题，但是其并没有给出解决办法。&lt;/p>
&lt;/blockquote>
&lt;p>作者还在 system prompt 中规定了输出的格式以及语言。&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Observation
作者发现，RL 的训练对于 system prompt 非常的敏感，因为 system prompt 会提高 model 的 entropy, 然后提高模型的探索能力。&lt;/p>
&lt;/blockquote>
&lt;h2 id="data">&lt;a href="#data" class="header-anchor">&lt;/a>Data
&lt;/h2>&lt;p>数据包括 math problems 以及 code problems&lt;/p>
&lt;h3 id="math">&lt;a href="#math" class="header-anchor">&lt;/a>Math
&lt;/h3>&lt;p>作者首先收集了 700K 样本，然后作者通过预处理以及过滤来保证数据的质量：&lt;/p>
&lt;p>&lt;strong>format filtering&lt;/strong>
要求问题完整，答案准确且可验证；去掉证明题和 multi-part 的问题；改写多项选择题为解答题，提高难度防止 reward hacking&lt;/p>
&lt;p>&lt;strong>difficulty filtering&lt;/strong>
使用两阶段的过滤策略。&lt;/p>
&lt;ol>
&lt;li>第一阶段使用 LLM 进行多次采样，然后去除掉比较简单或者比较复杂的&lt;/li>
&lt;li>第二阶段使用 RL checkpoint 进行多次采样，去除掉标准答案可能会存在问题的题目&lt;/li>
&lt;/ol>
&lt;p>最终一共得到 &lt;strong>38K&lt;/strong> 的样本&lt;/p>
&lt;h3 id="code">&lt;a href="#code" class="header-anchor">&lt;/a>Code
&lt;/h3>&lt;p>首先去掉没有 solution 和 test 的问题；然后去除掉标准答案不能通过所有 test 的问题。最终一共得到 &lt;strong>35K&lt;/strong> 的样本，包含 Python 和 C++ 两种语言&lt;/p>
&lt;h2 id="training">&lt;a href="#training" class="header-anchor">&lt;/a>Training
&lt;/h2>&lt;p>训练框架如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-magistral/Magistral_data_training.png"
width="1515"
height="523"
loading="lazy"
alt="Magistral_data_training"
class="gallery-image"
data-flex-grow="289"
data-flex-basis="695px"
>&lt;/p>
&lt;p>对于两个模型，作者采用了不同的方式进行训练&lt;/p>
&lt;ul>
&lt;li>Magistral Medium: 使用 pure RL 进行训练&lt;/li>
&lt;li>Magistral small: 使用 SFT + RL 进行训练&lt;/li>
&lt;/ul>
&lt;p>Magistral Medium 训练时满足的要求：&lt;/p>
&lt;ol>
&lt;li>dataset is not too easy: 太简单的题目对模型提升没有帮助&lt;/li>
&lt;li>Generation length does not stop growing: 逐步提升模型的最大输出长度&lt;/li>
&lt;li>KV-cache memory burden is not too large: 降低 batch size 来减少 KV-cache 的内存占用&lt;/li>
&lt;/ol>
&lt;p>Magistral small 训练&lt;/p>
&lt;p>&lt;strong>SFT&lt;/strong>
数据集包括两部分，一部分是 Magistral Medium 回答正确的这部分数据，第二部分是公开数据集，包括 [[OpenThoughts]] 和 [[OpenR1]] 两个数据集，作者使用 Magistral Medium 来生成回答。作者还加入了 10% 的 instruction tuning 数据来保持模型的通用能力。&lt;/p>
&lt;p>&lt;strong>RL&lt;/strong>
RL 的训练与 Magistral Medium 一致。&lt;/p>
&lt;h2 id="infra">&lt;a href="#infra" class="header-anchor">&lt;/a>Infra
&lt;/h2>&lt;p>作者首先介绍了一下 RL 训练的 infra, infra 主要包括三个模块：&lt;/p>
&lt;ul>
&lt;li>Trainers: 用于更新模型的权重&lt;/li>
&lt;li>Generators: 用于采样，生成 roll-out&lt;/li>
&lt;li>Verifiers: 对模型输出的结果进行打分&lt;/li>
&lt;/ul>
&lt;p>分布式 RL 训练的主要问题在于，不同长度的 roll-out 花费的时间不一致，作者发现，最长的 roll-out 和最短的 roll-out 的时间相差超过 5 倍以上。&lt;/p>
&lt;p>因此，作者就提出了异步生成这个方法。具体的做法就是&lt;/p>
&lt;ol>
&lt;li>首先由 Generator 生成多条 roll-out&lt;/li>
&lt;li>当 roll-out 完成之后，立马用 Verifiers 对轨迹进行打分&lt;/li>
&lt;li>收集 roll-out 以及对应的 reward, 直到达到给定的 batch 大小&lt;/li>
&lt;li>使用 Trainer 更新 Generator 的权重, 将更新后的权重同步给 Generator，这样其他 generator 在生成新的 token 时用的就是新的权重&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-magistral/Magistral_infra.png"
width="1169"
height="968"
loading="lazy"
alt="Magistral_infra"
class="gallery-image"
data-flex-grow="120"
data-flex-basis="289px"
>&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Recall
MiMo 里的做法是，当我们收集到给定数量的 roll-out 之后，我们就基于这些 roll-out 更新权重，然后进行下一次采样&lt;/p>
&lt;/blockquote>
&lt;p>训练时，对于每个 rank, 只要其收集到足够的 roll-out, 就会进行一次梯度更新。&lt;/p>
&lt;h2 id="evaluation">&lt;a href="#evaluation" class="header-anchor">&lt;/a>Evaluation
&lt;/h2>&lt;p>&lt;img src="https://maosong.website/p/notes-on-magistral/Magistral_performance_AIME.png"
width="1810"
height="1002"
loading="lazy"
alt="Performance of Magistral Medium"
class="gallery-image"
data-flex-grow="180"
data-flex-basis="433px"
>&lt;/p>
&lt;h2 id="ablation">&lt;a href="#ablation" class="header-anchor">&lt;/a>Ablation
&lt;/h2>&lt;p>&lt;strong>RL 的泛化性&lt;/strong>
作者探究了 RL 的 cross-domain generalization 能力，实验结果如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>AIME’24&lt;/th>
&lt;th>LiveCodeBench v5&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Starting Checkpoint&lt;/td>
&lt;td>32.2&lt;/td>
&lt;td>22.7&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RL (Math only)&lt;/td>
&lt;td>62.5&lt;/td>
&lt;td>38.3 (+15.6)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RL (Code only)&lt;/td>
&lt;td>49.7 (+17.5)&lt;/td>
&lt;td>42.7&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，不管是使用 math 还是 code 数据单独进行训练，模型在另一个 domain 上的表现都有所提升。&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Future
这个结论与最新的 GLM-4.1V-Thinking 结论一致&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Distillation v.s. RL for small models&lt;/strong>
作者探究了对于小语言模型，使用 RL 进行训练的效果更好，还是使用蒸馏的效果更好。&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-magistral/Magistral_distillation_vs_rl_performance.png"
width="1822"
height="934"
loading="lazy"
alt="Performance of RL v.s. distillation"
class="gallery-image"
data-flex-grow="195"
data-flex-basis="468px"
>&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Observation
实验结果发现，仅使用 RL 的效果与蒸馏差不多，甚至更好&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Batch size&lt;/strong>
作者还探究了 batch size $n_{batch}$ 以及 mini-batch size $n_{mini}$ 的影响。这里 batch size 指的是用于更新梯度的 roll-out 数量，mini-batch size 指的是计算梯度的 roll-out 数量，作者还定义了并行生成 roll-out 的数量 $n_{async}$. 当 $n_{async}>> n_{batch}$ 时，生成的 sequence 就很可能是 off-policy 的。作者固定 $n_{async}=4096$, 然后对比了不同的 $n_{batch}$ 和 $n_{mini}$ 对模型表现的影响，结果如下：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-magistral/Magistral_batch_ablation.png"
width="1602"
height="698"
loading="lazy"
alt="Ablation study on batch size"
class="gallery-image"
data-flex-grow="229"
data-flex-basis="550px"
>&lt;/p>
&lt;blockquote>
&lt;p>![tip] Observation
当 $n_{batch}=n_{mini}$ 时，模型表现差不太多（左图）；当 $n_{batch}$ 为常数，而 $n_{mini}$ 逐渐减小时，模型表现会逐渐变差。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Advantage normalization&lt;/strong>
作者对比了三种针对 advantage 的 normalization 方式：&lt;/p>
&lt;ul>
&lt;li>mini-batch: 在每个 mini-batch 里对模型进行 normalization&lt;/li>
&lt;li>Group normalization: 在每个 group 里进行 normalization&lt;/li>
&lt;li>no normalization: 没有 normalization&lt;/li>
&lt;/ul>
&lt;p>实验结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-magistral/Magistral_normalization.png"
width="1820"
height="500"
loading="lazy"
alt="Ablation study on normalization"
class="gallery-image"
data-flex-grow="364"
data-flex-basis="873px"
>&lt;/p>
&lt;p>作者发现，三种方式的区别并不是很大。&lt;/p>
&lt;h2 id="analysis">&lt;a href="#analysis" class="header-anchor">&lt;/a>Analysis
&lt;/h2>&lt;p>&lt;strong>length dimension&lt;/strong>
作者首先保存模型的 weight, 然后使用 PCA 进行降维，并在 2 维上进行可视化，作者对权重进行扰动，然后记录模型的 reward 以及输出长度。结果发现，模型存在一个 length dimension. 可视化结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-magistral/Magistral_length_dimension.png"
width="1546"
height="602"
loading="lazy"
alt="Magistral length dimension visualization"
class="gallery-image"
data-flex-grow="256"
data-flex-basis="616px"
>&lt;/p>
&lt;p>&lt;strong>multimodal extension&lt;/strong>
由于 Magistral medium 和 Magistral small 都是基于 MLLM 中的 LLM 开发得到的，作者还探究了更换 LLM 的 checkpoint 之后，原始 MLLM 的表现，结果发现，模型在多模态 reasoning 相关任务上的表现也得到了提升，结果如下图所示：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-magistral/Magistral_multimodal_extention.png"
width="1782"
height="822"
loading="lazy"
alt="Magistral multimodal performance"
class="gallery-image"
data-flex-grow="216"
data-flex-basis="520px"
>&lt;/p>
&lt;p>&lt;strong>Impact on other capabilities&lt;/strong>
作者还探究了 RL 训练对模型其他表现的影响，结果发现，RL 训练可以提高模型的 tool calling 和指令跟随能力，实验结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Category&lt;/th>
&lt;th>Benchmark&lt;/th>
&lt;th>Mistral Medium 3&lt;/th>
&lt;th>Magistral Medium&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Function calling&lt;/td>
&lt;td>Internal bench&lt;/td>
&lt;td>87.2&lt;/td>
&lt;td>87.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Instruction following&lt;/td>
&lt;td>IFEval&lt;/td>
&lt;td>86.8&lt;/td>
&lt;td>87.4&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>failed approaches&lt;/strong>
作者还介绍了一些尝试失败的做法：&lt;/p>
&lt;ol>
&lt;li>partial Reward: 对于 coding 任务，作者使用 test 的通过率作为奖励，结果发现效果并不好，这是因为一些错误的解法的 test 通过率也很高&lt;/li>
&lt;li>entropy bonus loss: 作者发现在损失函数中加入 entropy bonus loss 之后，模型的训练变得不稳定，而且效果不如使用更高的 $\epsilon_{high}$&lt;/li>
&lt;li>作者还进一步验证在 PPO loss 中加入 KL divergence loss, 结果发现效果并不好，这与 DAPO 的结论一致&lt;/li>
&lt;li>作者还尝试先 SFT Magistral Medium, 再进行 RL, 结果发现 RL 可以大幅度提高 SFT checkpoint 的表现。&lt;/li>
&lt;/ol>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>本文中，作者提出了 Magistral, 一个针对 math 和 code 的 reasoning model, 作者介绍了训练细节。但是，从方法层面来看，和 DAPO 区别不是很大。关键点应该是作者详细介绍了各种消融实验，为后来相关探索提供了经验。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2506.10910v1" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on SmolLM3</title><link>https://maosong.website/p/notes-on-smollm3/</link><pubDate>Tue, 15 Jul 2025 11:01:13 +0800</pubDate><guid>https://maosong.website/p/notes-on-smollm3/</guid><description>&lt;p>Hugging Face 在 2025 年 7 月 8 号发布了 SmolLM3, 一个 3B 的，128K 上下文，支持 6 种语言，支持 dual mode reasoning 的小语言模型。&lt;/p>
&lt;h2 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h2>&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;p>SmolLM3 是一个基于 transformer 的小语言模型，其模型架构与 SmolLM2 相似，SmolLM3 做了以下改进：&lt;/p>
&lt;ol>
&lt;li>使用 GQA 代替 multi-head attention. 作者通过消融实验发现 &lt;a class="link" href="https://maosong.website/p/notes-on-gqa/" target="_blank" rel="noopener"
>GQA&lt;/a> 和 MHA 的效果差不多，并且还可以减少 KV cache 的 size&lt;/li>
&lt;li>&lt;a class="link" href="https://maosong.website/p/notes-on-nope/" target="_blank" rel="noopener"
>NoPE&lt;/a>: 作者使用了NoPE, 来选择性（每 4 个 layer）移除 position embedding. 这个方法可以在不损害模型短文本能力的同时，提高模型长上下文的表现&lt;/li>
&lt;li>Intra-Document Masking: 不同的文档之间使用 attention masking 隔开&lt;/li>
&lt;li>Training stability: 与 olmo 2 一样，作者移除了 embedding layer 的 weight decay, 来提升训练的稳定性。&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-smollm3/SmolLM3_model_atanomy.png"
width="2554"
height="1470"
loading="lazy"
alt="SmolLM3 model atanomy"
class="gallery-image"
data-flex-grow="173"
data-flex-basis="416px"
>&lt;/p>
&lt;h3 id="data">&lt;a href="#data" class="header-anchor">&lt;/a>Data
&lt;/h3>&lt;p>&lt;img src="https://maosong.website/p/notes-on-smollm3/SmolLM3_pre-training_recipe.png"
width="1932"
height="1196"
loading="lazy"
alt="SmolLM3 pre-training recipe"
class="gallery-image"
data-flex-grow="161"
data-flex-basis="387px"
>&lt;/p>
&lt;p>与 SmolLM2 一样，作者使用了&lt;strong>11.2T&lt;/strong> token 进行训练，训练包括 3 个 stage。作者针对数据混合策略进行了消融实验，实验配置如下：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Stage&lt;/th>
&lt;th>Stable phase&lt;/th>
&lt;th>Stable phase&lt;/th>
&lt;th>Decay phase&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Description&lt;/td>
&lt;td>Base training&lt;/td>
&lt;td>High quality injection&lt;/td>
&lt;td>LR Decay&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Tokens&lt;/td>
&lt;td>8T&lt;/td>
&lt;td>2T&lt;/td>
&lt;td>1.1T&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Web&lt;/td>
&lt;td>85%&lt;/td>
&lt;td>75%&lt;/td>
&lt;td>63%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Code&lt;/td>
&lt;td>12%&lt;/td>
&lt;td>15%&lt;/td>
&lt;td>24%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Math&lt;/td>
&lt;td>3%&lt;/td>
&lt;td>10%&lt;/td>
&lt;td>13%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>datasets&lt;/td>
&lt;td>&lt;strong>web&lt;/strong>: FineWeb-Edu, DCLM, FineWeb2, Fineweb2-HQ&lt;br>&lt;strong>code&lt;/strong>: The Stack v2, StarCoder2 PRS, Jupyter and Kaggle notebooks, Github issues, StackExchange&lt;br>&lt;strong>math&lt;/strong>: FineMath3, InfiWebMath3+&lt;/td>
&lt;td>Adding Stack-Edu, FineMath4+, InfiWebMath4+, MegaMath&lt;/td>
&lt;td>upsampling of high-quality code data&lt;br>upsampling math data and introducing instruction and reasoning datasets such as OpenMathReasoning&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>其中，Web data 包含 12% 的多语种数据。&lt;/p>
&lt;h3 id="training-recipe">&lt;a href="#training-recipe" class="header-anchor">&lt;/a>Training Recipe
&lt;/h3>&lt;p>训练过程中，作者使用了 2.36M tokens 的 batch size, 上下文长度为 4096. 优化器为 AdamW&lt;/p>
&lt;p>作者使用了 nanotron 进行训练， datatrove 来处理数据， lighteval 来评估模型的表现。&lt;/p>
&lt;p>模型在 384 张 H100 上训练了 24 天。分布式训练的配置如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-smollm3/SmolLM3_distributed_training.png"
width="1672"
height="1024"
loading="lazy"
alt="SmolLM3 distributed training"
class="gallery-image"
data-flex-grow="163"
data-flex-basis="391px"
>&lt;/p>
&lt;h2 id="mid-training">&lt;a href="#mid-training" class="header-anchor">&lt;/a>Mid-training
&lt;/h2>&lt;p>Mid-training 的主要目标为扩展模型的上下文以及提升模型的 reasoning 能力。&lt;/p>
&lt;h3 id="long-context-extension">&lt;a href="#long-context-extension" class="header-anchor">&lt;/a>Long Context Extension
&lt;/h3>&lt;p>&lt;img src="https://maosong.website/p/notes-on-smollm3/SmolLM3_long_context_training.png"
width="1942"
height="1128"
loading="lazy"
alt="SmolLM3 long context training"
class="gallery-image"
data-flex-grow="172"
data-flex-basis="413px"
>&lt;/p>
&lt;p>在预训练阶段结束之后，作者使用了额外的 &lt;strong>100B&lt;/strong> tokens 来扩展模型的上下文。作者将扩展过程分为两个 stage:&lt;/p>
&lt;ol>
&lt;li>Stage 1: 将模型的上下文从 4K 提升到 32K. 具体做法是将 RoPE 的 base frequency 提升到 1.5M&lt;/li>
&lt;li>Stage 2: 将，模型的上下文从 32K 提升到 64K. 具体做法是将 RoPE 的 base frequency 提升到 5M&lt;/li>
&lt;/ol>
&lt;p>训练过程中，作者对 math, code 和 reasoning data 进行了上采样。作者发现，对长文本数据进行上采样并不会提高模型在 RULER 和 HELMET 上的表现。&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Recall
&lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5-1m/" target="_blank" rel="noopener"
>Qwen2.5-1M&lt;/a> 里也分析了长文本数据的问题，也就是大部分长文本数据依然是局部相关性强，而全局相关性弱&lt;/p>
&lt;/blockquote>
&lt;p>与 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5/" target="_blank" rel="noopener"
>Qwen2.5&lt;/a> 一样，作者还是用了 &lt;a class="link" href="https://maosong.website/p/notes-on-yarn/" target="_blank" rel="noopener"
>YARN&lt;/a> 来在推理阶段进一步提高模型的上下文长度，作者发现模型可以处理 128K 上下文长度的文本。&lt;/p>
&lt;h3 id="reasoning-mid-training">&lt;a href="#reasoning-mid-training" class="header-anchor">&lt;/a>Reasoning Mid-training
&lt;/h3>&lt;p>扩展模型上下文长度之后，作者还额外增加了一个 mid-training stage 来提高模型的 reasoning 能力。这个阶段的目标在于提升模型的通用能力。作者希望模型不针对特定的 domain, 如 math 或者 code 等。&lt;/p>
&lt;p>训练过程中，作者使用了 &lt;strong>35B&lt;/strong> 的 token. 数据来源包括 Open-Thoughts3-1.2M 以及 NVIDIA 的 Llama-Nemetron-Post-Training-Dataset-v1.1. Reasoning trace 由 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-r1/" target="_blank" rel="noopener"
>DeepSeek-R1&lt;/a> 生成。作者使用了 ChatML 的格式，还使用了 Packing 来提升训练效率。训练持续了 4 个 epoch.&lt;/p>
&lt;h2 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h2>&lt;p>如何构建 dual instruction model 来同时支持 reasoning 和 non-reasoning 任务并没有一个共识，大部分模型的数据都是非公开的。因此，作者就构建了一个 training pipeline, 用于提升模型在两种模式下的能力。&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-smollm3/SmolLM3_post-training_recipe.png"
width="1796"
height="1208"
loading="lazy"
alt="SmolLM3 post-training recipe"
class="gallery-image"
data-flex-grow="148"
data-flex-basis="356px"
>&lt;/p>
&lt;h3 id="chat-template">&lt;a href="#chat-template" class="header-anchor">&lt;/a>Chat Template
&lt;/h3>&lt;p>作者首先构建了一个 chat template, 用于支持 reasoning 和 non-reasoning 两种模式。该 chat template 支持用户使用 &lt;code>/think&lt;/code> 和 &lt;code>/no_think&lt;/code> flag 来控制模型的思考模式。在 non-reasoning 模式下，作者还在模型输出中 prefill 了 &lt;code>&amp;lt;think&amp;gt;\n\n&amp;lt;/think&amp;gt;&lt;/code>, 这一点与 Qwen3 一致。&lt;/p>
&lt;p>SmolLM3 还支持工具调用，其在 chat template 中加入了两种描述方式：XLM tools 和 Python tools.&lt;/p>
&lt;p>SmolLM3 还在 system prompt 中加入了 metadata, 如知识的截止时间，当前的 reasoning mode 等。&lt;/p>
&lt;h3 id="sft">&lt;a href="#sft" class="header-anchor">&lt;/a>SFT
&lt;/h3>&lt;p>&lt;img src="https://maosong.website/p/notes-on-smollm3/SmolLM3_synthetic_SFT_data.png"
width="1562"
height="940"
loading="lazy"
alt="SmolLM3 Synthetic SFT data"
class="gallery-image"
data-flex-grow="166"
data-flex-basis="398px"
>&lt;/p>
&lt;p>作者针对 math, code, general reasoning, instruction following, 以及 multilinguality 这几个领域来提升模型的表现。&lt;/p>
&lt;p>作者首先使用 reasoning mode 的 Qwen3-32B 来合成数据，合成数据的过程如上图所示。&lt;/p>
&lt;p>最终，SFT 阶段的数据包括 &lt;strong>1.8B&lt;/strong> token, 其中 &lt;strong>1B&lt;/strong> 为 non-reasoning mode 的 token, 覆盖 12 个数据集， &lt;strong>0.8B&lt;/strong> 为 reasoning token, 覆盖 10 个数据集。作者训练了 4 个 epoch, 使用了 Packing 的技巧。&lt;/p>
&lt;h3 id="apo">&lt;a href="#apo" class="header-anchor">&lt;/a>APO
&lt;/h3>&lt;p>&lt;img src="https://maosong.website/p/notes-on-smollm3/SmolLM3_synthetic_preference_data.png"
width="1542"
height="760"
loading="lazy"
alt="SmolLM3 synthetic preference data"
class="gallery-image"
data-flex-grow="202"
data-flex-basis="486px"
>&lt;/p>
&lt;p>SFT 阶段之后，作者使用了 Tulu3 的 preference dataset 用于 non-reasoning mode 的训练，然后合成了一批数据用于 reasoning mode 的训练，这批合成数据使用 Qwen3 32B 和 Qwen3 0.6B 生成得到，具体做法就是 Qwen3 32B 输出的结果定义为正样本，Qwen3 0.6B 输出的结果定义为负样本。&lt;/p>
&lt;p>作者使用了 APO 算法来进行训练，APO 是 &lt;a class="link" href="https://maosong.website/p/notes-on-dpo/" target="_blank" rel="noopener"
>DPO&lt;/a> 的一个变体， DPO 的目标函数为&lt;/p>
$$
\mathcal{L}_{DPO}(x,y_w,y_{l}; \theta) = -\log \sigma(r_\theta(x,y_w) - r_\theta(x, y_l))
$$&lt;p>其中&lt;/p>
$$
r_\theta(x,y) = \beta\log \frac{\pi_{\theta}(y\mid x)}{\pi_{ref}(y\mid x)}
$$&lt;p>$\beta>0$ 是一个超参数。APO 的目标函数如下&lt;/p>
$$
\mathcal{L}_{APO}(x,y_w,y_l;\theta) = -\sigma(r_\theta(x,y_w)) + \sigma(r_\theta(x,y_l))
$$&lt;p>作者发现，模型的 reasoning 能力提升之后，其长上下文能力反而下降了。作者认为这是因为在 reasoning mid-training stage 的训练中，提升 reasoning 能力损害了模型的 long context 能力。并且，APO 的训练数据上下文长度大多都在 24K 左右。为了解决这个问题，作者提出了 Model merging 的方法&lt;/p>
&lt;h3 id="model-merging">&lt;a href="#model-merging" class="header-anchor">&lt;/a>Model Merging
&lt;/h3>&lt;p>作者使用 MergeKit 来完成 model merging 的任务。merge 的过程包括两步：&lt;/p>
&lt;ol>
&lt;li>构造一个 model soup, 包括 APO 的每个 checkpoint&lt;/li>
&lt;li>将 model soup 与 mid-training 的一个拥有强上下文能力的 checkpoint 结合起来，作者使用了 linear merge, APO model soup 和 mid-training checkpoint 的权重分别为 0.9 和 0.1.&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>[!tip] Recall
&lt;a class="link" href="https://maosong.website/p/notes-on-kimi-k1.5/" target="_blank" rel="noopener"
>Kimi-k1.5&lt;/a> 也使用了 model merging 的方法，来将 long CoT 模型的能力迁移到 short-CoT 模型上去&lt;/p>
&lt;/blockquote>
&lt;h2 id="evaluation">&lt;a href="#evaluation" class="header-anchor">&lt;/a>Evaluation
&lt;/h2>&lt;p>作者首先评估了一下 base model 的表现，评估使用了 12 个 benchmark, 对比的模型包括 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a>, Gemma3, LLaMA 3.2. 结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-smollm3/SmolLM3_base_model_performance.png"
width="2992"
height="2059"
loading="lazy"
alt="SmolLM3 base model performance"
class="gallery-image"
data-flex-grow="145"
data-flex-basis="348px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-smollm3/SmolLM3_base_model_extact_performance.png"
width="2194"
height="954"
loading="lazy"
alt="SmolLM3 base model performance"
class="gallery-image"
data-flex-grow="229"
data-flex-basis="551px"
>&lt;/p>
&lt;p>模型的多语种表现如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-smollm3/SmolLM3_multilingual_performance.png"
width="1600"
height="1062"
loading="lazy"
alt="SmolLM3 multilingual performance"
class="gallery-image"
data-flex-grow="150"
data-flex-basis="361px"
>&lt;/p>
&lt;p>Instruction (non-reasoning) 版本的评估结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-smollm3/SmolLM3_instruction_performance.png"
width="2992"
height="2064"
loading="lazy"
alt="SmolLM3 Instruct models performance (w/o reasoning)"
class="gallery-image"
data-flex-grow="144"
data-flex-basis="347px"
>&lt;/p>
&lt;p>Reasoning 版本的评估结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-smollm3/SmolLM3_reasoning_performance.png"
width="2158"
height="1208"
loading="lazy"
alt="SmolLM3 reasoning performance"
class="gallery-image"
data-flex-grow="178"
data-flex-basis="428px"
>&lt;/p>
&lt;p>可以看到，Qwen3-4B 的表现是最好的。而 SmolLM3 的表现在 3B 左右也是非常强劲的&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者提出了 SmolLM3, 一个拥有长上下，文多语种以及 dual mode reasoning 能力的大语言模型，作者详细介绍了数据，训练以及 model merging 的技巧，来提高模型的表现。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://huggingface.co/blog/smollm3" target="_blank" rel="noopener"
>blog&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on GLM-4.1V-Thinking</title><link>https://maosong.website/p/notes-on-glm-4.1v-thinking/</link><pubDate>Mon, 14 Jul 2025 10:32:04 +0800</pubDate><guid>https://maosong.website/p/notes-on-glm-4.1v-thinking/</guid><description>&lt;p>智谱 AI 在 25 年 7 月份发布了 GLM-4.1V-Thinking, 一个 9B 的多模态大语言模型，其在多个 benchmark 上达到了相同大小 MLLM 的 SOTA&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>已有工作如 &lt;a class="link" href="https://maosong.website/p/notes-on-mimo-vl/" target="_blank" rel="noopener"
>MiMo-VL&lt;/a> 和 V-Triune 主要集中在特定的领域，目前还没有一个在所有领域都能超过 non-reasoning model 表现的多模态 reasoning model.&lt;/p>
&lt;p>基于这个目标，作者提出了 GLM-4.1V-Thinking, 一个 9B 的，用于通用领域多模态 reasoning 的 MLLM. 在预训练阶段，作者通过构建多样化的数据来提升模型的基础能力。在 post-training 阶段，作者构建了 domain-specific 的数据来让模型学会 reasoning. 最后，作者提出了 Reinforcement Learning with Curriculum Sampling (RLCS) 来扩展模型的 reasoning 能力。&lt;/p>
&lt;p>作者主要发现如下：&lt;/p>
&lt;ol>
&lt;li>multi-domain RL 的泛化性非常强，在某一个 domain 上进行 RL 训练也可以提高模型在其他 domain 上的表现&lt;/li>
&lt;li>动态选择 RL 训练的数据可以有效提高模型的表现和训练效率&lt;/li>
&lt;li>一个 robust 以及 precise 的 reward model 对于 multi-domain RL 是至关重要的&lt;/li>
&lt;/ol>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;p>GLM-4.1V-Thinking 是一个标准的 ViT-MLP-LLM 架构，其中：&lt;/p>
&lt;ul>
&lt;li>ViT: ViT 使用的是 AIMv2-Huge&lt;/li>
&lt;li>MLP: 是一个 3 层的 MLP，架构为 &lt;code>linear-LayerNorm-GELU-SwiGLU&lt;/code>, 并且在进入 MLP 之前，GLM-4.1V-Thinking 还会在 spatial 上进行降采样，降采样率为 2&lt;/li>
&lt;li>LLM: LLM 使用的是 GLM&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_Thinking_architecture.png"
width="1804"
height="988"
loading="lazy"
alt="Architecture of GLM4.1V-Thinking"
class="gallery-image"
data-flex-grow="182"
data-flex-basis="438px"
>&lt;/p>
&lt;p>与 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5-vl/" target="_blank" rel="noopener"
>Qwen2.5-VL&lt;/a> 一样，作者将 encoder 的 2D convolution 替换为了 3D convolution，用于处理视频输入，然后对于 single-image inputs，GLM-4.1V-Thinking 会将输入复制，变成一个含有两帧相同图片的 video 输入&lt;/p>
&lt;p>为了支持不同分辨率图片输入，作者进行了两点改进：&lt;/p>
&lt;p>第一是使用了 2D RoPE 来处理不同分辨率的图片输入&lt;/p>
&lt;p>第二是使用 bicubic interpolation 来将不同分辨率的图片适应到 encoder 的 position embedding 上。具体来讲，对一个拥有 $H_p\times W_p$ patches 的图片，每个 patch 的坐标 $(w,h)$ 首先会被 normalized 到 $[-1,1]$ 中：&lt;/p>
$$
g_{norm} = (w_{norm}, h_{norm}) = 2* \left(\frac{w+0.5}{W_p}, \frac{h+0.5}{H_p}\right) - 1
$$&lt;p>然后，我们再使用 bicubic interpolation $\mathcal{I}_{bicubic}$ 将坐标映射到原始的 position embedding 上：&lt;/p>
$$
P_{adapted} = \mathcal{I}_{bicubic}(P_{orig}, g_{norm})
$$&lt;p>对于视频输入，作者在每一帧之后插入了一个 time index token, 用每一帧的 timestamp string 来表示。这个做法与 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5-vl/" target="_blank" rel="noopener"
>Qwen2.5-VL&lt;/a> 一样。&lt;/p>
&lt;h3 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h3>&lt;h4 id="pre-training-data">&lt;a href="#pre-training-data" class="header-anchor">&lt;/a>Pre-training Data
&lt;/h4>&lt;p>Pre-training 数据包括 image-caption data, interleaved image-text data, OCR data, grounding data, video data 和 instruction tuning data&lt;/p>
&lt;p>&lt;strong>Image caption data&lt;/strong>
Image caption data 用于提升模型的世界知识。作者从 LAION, DataComp, DNF 以及 Wukong 等数据集收集了 10B 的 image-text pairs, 然后进行数据清洗：&lt;/p>
&lt;ol>
&lt;li>Heuristic-based filtering: 基于规则，如 resolution, caption length 等过滤掉低质量的图片&lt;/li>
&lt;li>Relevance filtering: 计算 CLIP score, 过滤掉低质量的数据&lt;/li>
&lt;li>Concept-balanced resampling: 基于 MetaCLIP , 来提升数据的覆盖程度，解决长尾分布的问题。这里 &lt;a class="link" href="https://maosong.website/p/notes-on-seed1.5-vl/" target="_blank" rel="noopener"
>Seed1.5-VL&lt;/a> 有一个类似的消融实验。&lt;/li>
&lt;li>Factual-centered re-captioning: 通过 rewrite caption 来提升 caption 的质量和信息密度。Idefics2 通过实验发现，rewriting caption 可以提高模型的表现。作者在最终的数据集里，混合了一部分 re-caption 的数据。&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Interleaved image-text data&lt;/strong>
相比于 image-caption data, interleaved image-text data 体量更大，且逻辑关系更强。但是噪声也更多，因此作者针对不同来源的数据构建了不同的数据清洗策略：&lt;/p>
&lt;ol>
&lt;li>Web data processing pipeline: 数据来源为 MINT, MMC4, OmniCorpus 等。作者首先基于 CLIP-score，去掉与上下文不相关的图片；然后，作者去除掉广告，二维码等低质量的图片；最后，作者将图多文少的样本也给去除掉，比如相册图片。作者还训练了一个 high-knowledge-density image classifier, 来识别信息密度高的样本。&lt;/li>
&lt;li>Academic book processing pipeline: 作者从电子书中收集了 100M 的样本，这些电子书主要是 STEM domain，然后作者构建了一个 PDF parsing tool 来提取文档内容。&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>OCR data&lt;/strong>
OCR 数据包含&lt;strong>220M images&lt;/strong>, 数据集由三部分组成：&lt;/p>
&lt;ol>
&lt;li>Synthetic document images: 基于预训练语料，使用不同的格式来渲染文字生成不同的图片，然后基于 LAION 的图片作为背景&lt;/li>
&lt;li>Natural scene text images: 使用 Paddle-OCR 来从图片中提取文字以及对应的 bounding box&lt;/li>
&lt;li>Academic documents: 使用类似 Nougat 的方法来构建 PDF page-Markdown 的数据。&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Grounding data&lt;/strong>
主要包含自然场景和 GUI 两个 domain 的 grounding 数据：&lt;/p>
&lt;ol>
&lt;li>Natural image grounding: 基于 LAION-115M 得到。作者使用 GLIPv2, 来自动化解析 caption 中的实体，然后预测对应的 bounding box, 作者将 bounding box 较少的样本去除掉。最终得到&lt;strong>40M&lt;/strong>高质量的自然场景数据&lt;/li>
&lt;li>GUI grounding: 基于 CC 提取对应的 url, 然后使用 Playwright 来与网页进行交互，进而获取到所有的 DOM elements 与其对应的 bounding box. 最终收集到 &lt;strong>140M&lt;/strong>高质量的 QA 样本&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Video data&lt;/strong>
Video data 从多个来源得到，作者使用了 fine-grained human annotation 来保证数据质量。作者还标注了 camera motion 等信息&lt;/p>
&lt;p>为了保证数据的质量，作者进行了去重，来减少数据语义的相似性。&lt;/p>
&lt;p>&lt;strong>Instruction tuning data&lt;/strong>
instruction tuning data 的策略如下：&lt;/p>
&lt;ol>
&lt;li>Task coverage and taxonomy: 优化数据分布&lt;/li>
&lt;li>Complex scenario augmentation: 构建复杂的指令跟随数据&lt;/li>
&lt;li>Data contamination check: 数据污染检查&lt;/li>
&lt;/ol>
&lt;p>最终一共收集到 &lt;strong>50M&lt;/strong>样本，覆盖 visual perception, multimodal reasoning, GUI agent 等任务。&lt;/p>
&lt;h4 id="pre-training-training">&lt;a href="#pre-training-training" class="header-anchor">&lt;/a>Pre-training Training
&lt;/h4>&lt;p>Pre-training 由两个 stage 组成：&lt;/p>
&lt;ol>
&lt;li>Multimodal pre-training: 提高模型的通用多模态能力，上下文长度为 8192, global batch size 为 1536. 使用了 data packing 策略，使用了 2-way tensor parallelism&lt;/li>
&lt;li>Long-context continue training: 提升模型在高精度图片，长视频，长上下文场景下的表现，上下文长度为 32768, 使用了 2-way tensor parallelism 和 4-way context parallelism.&lt;/li>
&lt;/ol>
&lt;h3 id="sft">&lt;a href="#sft" class="header-anchor">&lt;/a>SFT
&lt;/h3>&lt;h4 id="sft-data">&lt;a href="#sft-data" class="header-anchor">&lt;/a>SFT Data
&lt;/h4>&lt;p>数据包括 long CoT reasoning 数据，来让模型以标准格式输出 multi-step solution.&lt;/p>
&lt;p>数据包括 verifiable tasks 以及 non-verifiable tasks, 覆盖中英两种语言。作者过滤了非常简单和非常困难的样本。&lt;/p>
&lt;p>数据格式如下&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;lt;think&amp;gt; {think_content} &amp;lt;/think&amp;gt; &amp;lt;answer&amp;gt; {answer_content} &amp;lt;/answer&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>对于 verifiable tasks, 输出格式为&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;lt;answer&amp;gt; &amp;lt;|begin_of_box|&amp;gt; {answer_content} &amp;lt;|end_of_box|&amp;gt; &amp;lt;/answer&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>数据清洗策略包括：&lt;/p>
&lt;ol>
&lt;li>严格遵守格式要求&lt;/li>
&lt;li>reasoning style 不一致或者有噪声&lt;/li>
&lt;li>混合语言输出。&lt;/li>
&lt;/ol>
&lt;p>作者还使用 RL checkpoints 来生成一部分高质量数据，加入到 SFT 阶段的数据集里面。&lt;/p>
&lt;h4 id="sft-training">&lt;a href="#sft-training" class="header-anchor">&lt;/a>SFT Training
&lt;/h4>&lt;p>全参数微调。上下文长度为 32768， batch size 为 32.数据包括 long-form reasoning data 以及纯文本数据，包括 math, multi-turn conversation, agent planning 以及 instruction following 等.&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Observation
作者发现，在 SFT 阶段即使是使用带噪声的 reasoning data，模型在 RL 阶段也能继续训练。也就是说，不完美的 reasoning trace 也能提供 guidance. 但是，使用更高质量的数据可以让模型 RL 阶段的训练更稳定且表现更好。&lt;/p>
&lt;/blockquote>
&lt;h3 id="rl">&lt;a href="#rl" class="header-anchor">&lt;/a>RL
&lt;/h3>&lt;p>与 &lt;a class="link" href="https://maosong.website/p/notes-on-seed1.5-vl/" target="_blank" rel="noopener"
>Seed1.5-VL&lt;/a> 一样，作者在 RL 阶段结合了 RLVR 和 RLHF 两个训练目标。任务包括 STEM problem solving (such as mathematics, physics, chemistry), grounding, optical character recognition (OCR), video understanding, GUI agents, chart and document understanding, logical reasoning, and instruction following.&lt;/p>
&lt;p>&lt;strong>Data&lt;/strong>
作者首先构建了一个任务集合，然后基于这些任务，来过滤或者生成对应的 QA pair, 接下来作者基于难度和质量来过滤，最后作者在每个 domain 上进行 RL 的训练来保证数据的有效性。&lt;/p>
&lt;p>&lt;strong>Reward modelling&lt;/strong>
作者构建了一个 reward system, 用于给不同 domain 的任务进行奖励。为了探究不同 domain 的 reward 对模型整体表现的影响，作者设计了一个 low-quality reward system，实验结果如下，可以看到模型在训练后期出现了 collapse 或者 reward hacking 的情况。&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_reward_ablation.png"
width="1778"
height="876"
loading="lazy"
alt="GLM-4.1V-Thinking reward ablation"
class="gallery-image"
data-flex-grow="202"
data-flex-basis="487px"
>&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Observation
作者发现，不同任务的不同信号奖励会对模型最终表现产生巨大影响。&lt;/p>
&lt;/blockquote>
&lt;p>在使用 LLM 提取模型回答的 answer 的时候，作者要求模型的最终答案为 &lt;code>&amp;lt;|begin_of_box|&amp;gt; {answer_content} &amp;lt;|end_of_box|&amp;gt;&lt;/code> 这种形式，避免大语言模型提取出错。&lt;/p>
&lt;p>作者构建的 reward system 包含如下模块：&lt;/p>
&lt;ol>
&lt;li>shared verification functions: 包括格式检查等&lt;/li>
&lt;li>domain specific modules: 与 domain 相关的模块&lt;/li>
&lt;li>unit testing: 观测 domain 输出的分布，然后基于输出的分布来调整 reward logic&lt;/li>
&lt;/ol>
&lt;p>最终 reward 的 design 如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_reward_design.png"
width="1812"
height="712"
loading="lazy"
alt="GLM-4.1V-Thinking reward design"
class="gallery-image"
data-flex-grow="254"
data-flex-basis="610px"
>&lt;/p>
&lt;p>&lt;strong>RLCS&lt;/strong>
作者使用了 GRPO 来进行优化训练。作者发现，仅需 200 training steps 模型在一半以上问题的准确率就达到了 $90\%$.&lt;/p>
&lt;p>为了提升模型的训练效率，作者提出了 Reinforcement Learning with Curriculum Sampling (RLCS), 也就是将课程学习与 RL 结合起来。作者基于模型训练情况动态调整训练样本的难度，来保证每个样本对模型训练的提升都是最大的。&lt;/p>
&lt;p>在 RLCS 训练之前，作者先使用模型评测得到每个样本的 pass@k, 然后根据结果将样本进行分类。在训练过程中，作者记录每个样本的 pass@k， 然后动态更新其分类结果。在采样的时候，作者基于难度来对样本进行加权，来降低太简单或者太难样本的采样概率。&lt;/p>
&lt;p>作者还提出了一些提高 RL 表现的方法：&lt;/p>
&lt;ol>
&lt;li>Larger batch size: 使用更大的 batch size 效果更好&lt;/li>
&lt;li>Dynamic sampling expansion via ratio EMA: 作者定义了 naive 样本和高质量样本的比例，然后根据这个比例进行采样，来提升整体的采样效率。&lt;/li>
&lt;li>Force answering: 与 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a> 一样，对于比较难的问题，当输出的 token 数过长时，作者对模型输出进行截断，然后让模型基于已有思考过程直接输出结果。&lt;/li>
&lt;li>Discard KL loss: 与 DAPO 一样，作者也移除了 KL divergence loss&lt;/li>
&lt;li>Clip-higher: 与 DAPO 一样，作者通过修改超参数来提高模型的探索能力&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>[!tip] Observation&lt;/p>
&lt;ol>
&lt;li>RL 阶段的表现与 cold-start SFT 的表现并不完全相关。作者发现，cold-start SFT 取得更好的表现并不一定保证 RL 的表现就更好。&lt;/li>
&lt;li>RL 阶段各个 domain 数据的影响是正交的，这与 cold-start SFT 阶段的结果不同。&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>作者还提出了一些提高训练稳定性的方法：&lt;/p>
&lt;ol>
&lt;li>提高 cold-start SFT 阶段数据的质量&lt;/li>
&lt;li>移除 KL divergence loss&lt;/li>
&lt;li>使用 top-p 为 1，而不是 0.9 (如 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen-llm/" target="_blank" rel="noopener"
>Qwen-LLM&lt;/a> 中的设置)，top-p 为 1 可以 cover 整个 vocabulary，避免某些 token 没有参与训练&lt;/li>
&lt;li>per-sample loss 和 per-token loss 没有显著区别，但是 per-sample loss 稳定性更高&lt;/li>
&lt;li>在 cold-start SFT 阶段让模型学会格式要求，而不是在 RL 阶段加入 format reward&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Infra&lt;/strong>
在 infra 方面，作者做了如下改进：&lt;/p>
&lt;ol>
&lt;li>Load balancing of sequence lengths across DP ranks. 平衡每个 rank 的 sequensequence length 以及 compute load&lt;/li>
&lt;li>Intra-rank training with sequence packing and gradient accumulation. 将 sequence packing 和 gradient accumulation 结合起来&lt;/li>
&lt;li>Sample packing and reorganization within DP ranks. data packing&lt;/li>
&lt;li>Dynamic sampling expansion via ratio EMA. 平衡 naive 样本和高质量样本之间的比例&lt;/li>
&lt;/ol>
&lt;h2 id="evaluation">&lt;a href="#evaluation" class="header-anchor">&lt;/a>Evaluation
&lt;/h2>&lt;h3 id="performance">&lt;a href="#performance" class="header-anchor">&lt;/a>Performance
&lt;/h3>&lt;p>&lt;img src="https://maosong.website/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_performance.png"
width="1066"
height="1446"
loading="lazy"
alt="Performance of GLM4.1V Thinking"
class="gallery-image"
data-flex-grow="73"
data-flex-basis="176px"
>&lt;/p>
&lt;p>可以看到，GLM-4.1V-Thinking 在多个 benchmark 上都达到了 SOTA.&lt;/p>
&lt;h3 id="ablation-study">&lt;a href="#ablation-study" class="header-anchor">&lt;/a>Ablation Study
&lt;/h3>&lt;p>作者评估了一下不同 domain 的 RL 训练对模型整体表现的影响。作者使用不同的数据来进行训练，然后分析结果的相关性，结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_cross_domain_generalization.png"
width="1226"
height="962"
loading="lazy"
alt="Cross-domain generalization in reinforcement learning"
class="gallery-image"
data-flex-grow="127"
data-flex-basis="305px"
>&lt;/p>
&lt;p>实验结果发现：&lt;/p>
&lt;ol>
&lt;li>在某个 domain 上的训练会提高模型在其他 domain 上的表现&lt;/li>
&lt;li>联合多个 domain 进行训练最终的表现会更好&lt;/li>
&lt;/ol>
&lt;p>作者还发现，不同的任务之间存在关联，如 GUI-agent 和 grounding 这两个任务是高度相关的。OCR &amp;amp; Chart 以及 GUI-agent 也是高度相关的。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>在本文中，作者提出了 GLM-4.1V-Thinking, 一个 9B 的通用多模态 reasoning model, 作者通过构建高质量的数据，多阶段的训练，让模型在 reasoning 和 non-reasoning 任务上均超过了已有的 SOTA 模型的表现&lt;/p>
&lt;p>作者认为，模型有以下局限：&lt;/p>
&lt;ol>
&lt;li>模型并没有提升 reasoning 的质量，在某些情况下，模型结果正确，但是中间过程错误。作者分析这是因为目前只有针对结果的奖励机制。因此，未来需要能够评估中间过程的 reward model&lt;/li>
&lt;li>RL 的训练具有不稳定性，作者发现模型对设置比较敏感，这对 RL 训练的 scaling 提出了挑战。&lt;/li>
&lt;li>模型在复杂场景下表现依旧不太好，比如图片中有物体遮挡的情况，这些感知误差也会影响最终的 reasoning 表现。作者认为如何同时提高 perception 和 reasoning 的表现是一个需要研究的课题。&lt;/li>
&lt;/ol>
&lt;p>未来的工作有：&lt;/p>
&lt;ol>
&lt;li>如何构建更好的 reward 机制，来同时评估结果和中间过程。从而防止模型 reward hacking&lt;/li>
&lt;li>如何基于 multimodal training 来提升模型在纯文本任务上的表现。探究 visual reasoning 和 text reasoning 如何互相促进也是一个课题&lt;/li>
&lt;li>如何更好评估模型的表现，即如何更好评估模型的 failure modes, 比如幻觉，short reasoning 等&lt;/li>
&lt;/ol>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2507.01006" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://huggingface.co/THUDM/GLM-4.1V-9B-Thinking" target="_blank" rel="noopener"
>Huggingface&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="appendix">&lt;a href="#appendix" class="header-anchor">&lt;/a>Appendix
&lt;/h2>&lt;p>GLM-4.1V-Thinking 的 patch merger 代码&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Glm4vVisionPatchMerger&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">context_dim&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_act&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">bool&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="fm">__init__&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">bias&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">post_projection_norm&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">LayerNorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">gate_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">context_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">bias&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">up_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">context_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">bias&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">down_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">context_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">bias&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">act1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">GELU&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">act_fn&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ACT2FN&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">hidden_act&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_state&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_state&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_state&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_state&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">act1&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">post_projection_norm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_state&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">down_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">act_fn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">gate_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_state&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">up_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_state&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Glm4vVisionModel&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">downsample&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Conv2d&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">in_channels&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">out_channels&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">out_hidden_size&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">kernel_size&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">spatial_merge_size&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">stride&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">spatial_merge_size&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">merger&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Glm4vVisionPatchMerger&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">out_hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">context_dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">intermediate_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_act&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_act&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">...&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">spatial_merge_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">spatial_merge_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">permute&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">downsample&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">out_hidden_size&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">merger&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>Notes on QwQ-32B</title><link>https://maosong.website/p/notes-on-qwq-32b/</link><pubDate>Sat, 08 Mar 2025 09:46:16 +0800</pubDate><guid>https://maosong.website/p/notes-on-qwq-32b/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>通义团队在3月6号发布了QwQ-32B，一个基于RL的，32B参数的reasoning model，QwQ-32B的表现可以与DeepSeek-R1相比&lt;/p>
&lt;h2 id="模型架构">&lt;a href="#%e6%a8%a1%e5%9e%8b%e6%9e%b6%e6%9e%84" class="header-anchor">&lt;/a>模型架构
&lt;/h2>&lt;p>QwQ-32B基于Qwen2.5-32B开发。主要通过基于outcome-based rewards的RL进行训练。训练过程包括两个stage&lt;/p>
&lt;ol>
&lt;li>Stage 1：本阶段在math和coding domain上进行RL的训练，作者使用了一个verifier来保证最终数学问题结果的正确性，使用了一个code execution server来保证最终生成代码的正确性。&lt;/li>
&lt;li>Stage 2：本阶段在通用领域上进行RL的训练。模型基于general reward model和一些rule-based verifier进行训练。应该是类似 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-r1/" target="_blank" rel="noopener"
>DeepSeek-R1&lt;/a> 的规则。&lt;/li>
&lt;/ol>
&lt;h2 id="实验结果">&lt;a href="#%e5%ae%9e%e9%aa%8c%e7%bb%93%e6%9e%9c" class="header-anchor">&lt;/a>实验结果
&lt;/h2>&lt;p>实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwq-32b/qwq-32b-final.jpg"
width="3035"
height="1713"
loading="lazy"
alt="QwQ_evaluation"
class="gallery-image"
data-flex-grow="177"
data-flex-basis="425px"
>&lt;/p>
&lt;h2 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://qwenlm.github.io/blog/qwq-32b/" target="_blank" rel="noopener"
>blog&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://chat.qwen.ai/?models=Qwen2.5-Plus" target="_blank" rel="noopener"
>demo&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Kimi k1.5</title><link>https://maosong.website/p/notes-on-kimi-k1.5/</link><pubDate>Sat, 08 Feb 2025 10:09:52 +0800</pubDate><guid>https://maosong.website/p/notes-on-kimi-k1.5/</guid><description>&lt;p>论文提出了Kimi k1.5， 一个基于强化学习训练的多模态推理模型。作者介绍了Kimi k1.5的训练方法，以及infra上的优化。作者的主要贡献如下：&lt;/p>
&lt;ol>
&lt;li>作者发现，模型的上下文长度可以有效提升LLM的推理能力。&lt;/li>
&lt;li>作者提出了基于online mirror descent的强化学习训练方法。&lt;/li>
&lt;li>作者发现可以通过long context scaling和policy optimization来提升模型的推理能力。&lt;/li>
&lt;li>作者提出了long2short的推理方法，可以有效提升short CoT模型的推理能力。&lt;/li>
&lt;/ol>
&lt;h2 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h2>&lt;p>论文中包括两个模型，一个是k1.5 base model， 其是一个多模态大模型。另一个是Kimi-1.5 reasoning model（简称为k1.5），k1.5是基于kimi-1.5 base model， 通过强化学习训练得到的推理模型。&lt;/p>
&lt;h3 id="kimi-15-base-model">&lt;a href="#kimi-15-base-model" class="header-anchor">&lt;/a>Kimi-1.5 base model
&lt;/h3>&lt;p>k1.5 base model是一个基于transformer的多模态大模型，论文没有给出详细的模型结构，只有如下的示意图&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k1.5/k1-5-model-arch.png"
width="1210"
height="287"
loading="lazy"
alt="Architecture of k1.5 base model"
class="gallery-image"
data-flex-grow="421"
data-flex-basis="1011px"
>&lt;/p>
&lt;p>k1.5 base model的训练包括三个阶段：&lt;/p>
&lt;ol>
&lt;li>Vision-language pretraining stage: 这一阶段的目的是让模型拥有语言和视觉的表征能力。训练时，模型先在纯文本的数据上进行训练，然后提高加入图文交错的数据进行训练，直到训练数据中图文交错的数据占比达到30%。模型的更新方式为先冻结LLM更新visual tower, 然后解冻LLM更新所有参数。&lt;/li>
&lt;li>Vision-language cooldown stage: 这一阶段的目的是让模型保持多模态理解能力。作者发现，使用合成数据可以提高模型在这一阶段的表现。因此，作者使用闭源大语言模型，基于math, knowledge和code domain的pretraining data来合成了一些QA pair数据对，然后讲这些QA pair数据对加入到训练数据中。&lt;/li>
&lt;li>Long-context activation stage: 这一阶段的目的是让模型在长上下文的数据上进行训练，以提升模型在长上下文上的理解能力。这一阶段包含了40%的full attention data以及60%的partial attention data。其中，full attention data是基于真实数据和合成的QA以及summary数据得到的，partial attention data是在cooldown data找那个通过均匀采样得到的。这一阶段，模型的上下文长度从4096逐步增加到131072。&lt;/li>
&lt;/ol>
&lt;h3 id="kimi-k15">&lt;a href="#kimi-k15" class="header-anchor">&lt;/a>Kimi k1.5
&lt;/h3>&lt;p>Kimi k1.5是基于k1.5 base model， 通过进一步训练得到的推理模型。k1.5的训练包括四个阶段：&lt;/p>
&lt;ol>
&lt;li>pretraining：这一步就是k1.5 base model的训练， 包括三个小阶段。前面已经介绍过了。&lt;/li>
&lt;li>vanilla SFT：这一步的目的是使得k1.5 base model具有指令跟随能力。对于non-reasoning任务，作者先使用人类标注产生一个seed dataset，然后基于seed dataset训练一个seed model，最后根据收集的prompt来使用seed model来生成回答。对于reasoning任务，作者使用了rejection sampling来扩展SFT数据集&lt;/li>
&lt;li>long-CoT SFT： 这一步的目的是让模型能够像人类一样进行推理，能够掌握最基本的推理策略，即：planning，evaluation，reflection和exploration。从而为RL训练提供一个良好的初始化。&lt;/li>
&lt;li>RL：这一步就是通过RL来提高模型的推理能力。我们在下一节中进行详细的介绍。&lt;/li>
&lt;/ol>
&lt;h2 id="rl">&lt;a href="#rl" class="header-anchor">&lt;/a>RL
&lt;/h2>&lt;h3 id="problem-definition">&lt;a href="#problem-definition" class="header-anchor">&lt;/a>Problem Definition
&lt;/h3>&lt;p>给定一个训练数据集 $\mathcal{D} = \{(x_i, y_i^\star)\}_ {i=1}^n$， 其中 $x_ i$ 是问题， $y_{i}^\star$ 是ground truth。我们希望找到一个模型 $\pi_{\theta}$ ，来解决这个问题。通常问题比较难，因此我们使用chain of thought（CoT）的方法来解决这个问题。具体做法就是让模型输出中间步骤 $z=(z_1, z_2, ..., z_m)$， 来连接问题 $x_i$ 和答案 $y$。其中， $z_j$ 是模型在第 $j$ 步的推理结果，即 $z_t\sim\pi_{\theta}(x_i, z_{&lt;t})$, $y\sim \pi_{\theta}(x_i, z)$。&lt;/p>
&lt;p>通常，我们还会使用一个reward model $r_{\phi}$，来评估模型输出的答案的质量。一个常用的reward model是基于答案的正确性来评估的，即 $r_{\phi}(x_i, y, y^\star) = \mathbb{I}[y=y_i^\star]$ 。这样，我们要求解的问题就变成了最大化中间步骤和最终答案的reward，即&lt;/p>
$$
\max_{\theta} \mathbb{E}_ {(x, y^\star)\sim\mathcal{D},(y,z,)\sim\pi_{\theta}} r_{\phi}(x, y, y^\star)
$$&lt;h3 id="policy-optimization">&lt;a href="#policy-optimization" class="header-anchor">&lt;/a>Policy Optimization
&lt;/h3>&lt;p>作者使用了online policy mirror descent来解决上面提到的优化问题。在每个iteration中，存在一个reference model $\pi_{\theta_r}$ ， 以及一个当前要更新的模型 $\pi_{\theta}$ 。online policy mirror descent要解决的优化问题为：&lt;/p>
$$
\min_{\theta} \mathbb{E}_ {(x, y^\star)\sim\mathcal{D}} \left[\mathbb{E}_ {(y,z,)\sim\pi_{\theta}}\left[r_{\phi}(x, y, y^\star) - \beta \mathcal{D}_ {KL}(\pi_{\theta}(y,z\mid x)\Vert \pi_{\theta_r}(y,z\mid x))\right]\right]
$$&lt;p>其中， $\beta$ 是超参数，用于平衡reward和KL散度 $\mathcal{D}_{KL}(\cdot)$.&lt;/p>
&lt;p>上述问题有一个闭式解，即：&lt;/p>
$$
\pi^\star(y,z\mid x)=\pi_{\theta_r}(y,z\mid x)\exp\left(\frac{r_{\phi}(x, y, y^\star)}{\beta}\right)/Z(x;\beta)
$$&lt;p>其中，&lt;/p>
$$
Z(x;\beta):=\sum_{y,z}\pi_{\theta_r}(y,z\mid x)\exp\left(\frac{r_{\phi}(x, y, y^\star)}{\beta}\right)
$$&lt;p>是归一化因子。在闭式解的表达式中，我们对两边取对数，得到：&lt;/p>
$$
r_{\phi}(x, y, y^\star) - \beta \log Z - \beta \log\frac{\pi^\star(y,z\mid x)}{\pi_{\theta_r}(y,z\mid x)}=0
$$&lt;p>这是最优策略满足的等式。作者这里使用了L2 loss来优化当前策略 $\pi_{\theta}$ ，即：&lt;/p>
$$
\min_{\theta} \mathbb{E}_ {(x, y^\star)\sim\mathcal{D}} \left[\mathbb{E}_ {(y,z,)\sim\pi_ {\theta_r}}\left[r_{\phi}(x, y, y^\star) - \beta \log Z - \beta \log\frac{\pi_{\theta}(y,z\mid x)}{\pi_{\theta_r}(y,z\mid x)}\right]^2\right]
$$&lt;p>这里需要注意的一点是response $(y,z)$ 是基于reference model $\pi_{\theta_r}$ 生成的，而不再是基于当前策略 $\pi_{\theta}$ 生成的。这是Kimi-1.5的RL训练和传统RL训练的一个不同点。这也是为什么k1.5说他是off-policy的原因。&lt;/p>
&lt;p>接下来，我们就可以对上面的目标函数求梯度，我们将里层的函数展开为 $(a-b-c)^2=(a-b)^2-2(a-b)c+c^2$ 的形式，然后对 $\theta$ 求梯度，得到：&lt;/p>
$$
\mathbb{E}_ {(x, y^\star)\sim\mathcal{D}} \left[\mathbb{E}_ {(y,z,)\sim\pi_{\theta_r}}\left[\nabla_{\theta}\log \pi_{\theta}(y,z\mid x)(r_{\phi}(x, y, y^\star) - \beta \log Z) - \frac{\beta}{2} \nabla_{\theta}\left(\log\frac{\pi_{\theta}(y,z\mid x)}{\pi_{\theta_r}(y,z\mid x)}\right)^2\right]\right]
$$&lt;p>如果我们对每个问题$x$,采样 $k$ 个response $(y,z)$ ，那么我们可以近似上式中内部的期望，得到：&lt;/p>
$$
\frac{1}{k}\sum_{i=1}^k\left[\nabla_{\theta}\log \pi_{\theta}(y_i,z_i\mid x)(r_{\phi}(x, y_i, y^\star) - \bar{r}) - \frac{\beta}{2} \nabla_{\theta}\left(\log\frac{\pi_{\theta}(y_i,z_i\mid x)}{\pi_{\theta_r}(y_i,z_i\mid x)}\right)^2\right]
$$&lt;p>这里， $\bar{r}$ 是 $\beta \log Z$ 的估计值。作者提供了两种估计方法：&lt;/p>
&lt;ol>
&lt;li>基于采样的 $(y_i,z_i)\sim \pi_{\theta_r}$ 对 $\beta \log Z$ 进行估计：&lt;/li>
&lt;/ol>
$$
\bar{r}=\beta\log\frac{1}{k}\sum_{j=1}^k\exp\left(\frac{r_{\phi}(x, y_j, y^\star)}{\beta}\right)
$$&lt;ol start="2">
&lt;li>直接使用samples rewards的平均值来近似：&lt;/li>
&lt;/ol>
$$
\bar{r}=\frac{1}{k}\sum_{i=1}^k r_{\phi}(x, y_i, y^\star)
$$&lt;p>作者提到，第二种方法效果很好并且计算效率更高，因此作者在实验中使用了第二种方法。&lt;/p>
&lt;p>训练时，作者每次从数据集中采样一个batch的样本，然后使用上述的梯度更新模型。模型参数更新完之后，作者将新的模型作为reference model，然后重复上述过程。&lt;/p>
&lt;blockquote>
&lt;p>Remark
作者还探究了为什么不使用value network。作者认为，在强化学习中，value network通常用于评估当前状态的价值，但是在reasoning model中，训练的目的是让模型能够充分探索，以提高模型在不同任务中的泛化性。因此，作者认为使用value network可能会限制模型的探索能力。&lt;/p>
&lt;/blockquote>
&lt;h3 id="training-strategy">&lt;a href="#training-strategy" class="header-anchor">&lt;/a>Training Strategy
&lt;/h3>&lt;p>作者提出了两个提高采样效率的方法：&lt;/p>
&lt;ol>
&lt;li>Curriculum Sampling. 这种采样方式会将问题按照难度进行排序，然后按照难度从易到难进行采样。以逐步提高模型的推理能力。&lt;/li>
&lt;li>Prioritized Sampling.作者还采用了prioritized sampling来提高采样效率。也就是说，对于回答错误的问题，模型会进行更多的采样。&lt;/li>
&lt;/ol>
&lt;h2 id="infra">&lt;a href="#infra" class="header-anchor">&lt;/a>Infra
&lt;/h2>&lt;h3 id="training-system">&lt;a href="#training-system" class="header-anchor">&lt;/a>Training System
&lt;/h3>&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k1.5/k1-5-training-system.png"
width="1112"
height="530"
loading="lazy"
alt="Large Scale Reinforcement Learning Training System for LLM"
class="gallery-image"
data-flex-grow="209"
data-flex-basis="503px"
>&lt;/p>
&lt;p>为了高效地进行RL训练，作者构建了一个iterative synchronous RL训练框架。框架如上图所示。框架的创新之处在于&lt;strong>Partial Rollout&lt;/strong>，该方法可以更高效的处理复杂的推理轨迹。框架包含了以下几部分：&lt;/p>
&lt;ol>
&lt;li>rollout worker: 该部分负责生成推理轨迹。然后将生成的推理轨迹加入到replay buffer中。&lt;/li>
&lt;li>master: 管理data flow和replay buffer和train worker之间的通信。&lt;/li>
&lt;li>train worker: 根据获取的rollout轨迹更新模型。&lt;/li>
&lt;li>code execution service: 在代码相关任务中，该部分负责执行代码，并返回执行结果。&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Partial Rollout&lt;/strong>：Partial Rollout的目的是为了整体的训练效率。其做法就是给定一个固定的outpu token budget，然后限制推理轨迹的长度。当长度超过token限制时，没有完成的部分就会被存入到replay buffer中，以供后续的训练使用。&lt;/p>
&lt;h3 id="deployment-framework">&lt;a href="#deployment-framework" class="header-anchor">&lt;/a>Deployment Framework
&lt;/h3>&lt;p>根据前面的介绍，我们知道k1.5每个iteration的训练包括两步：&lt;/p>
&lt;ol>
&lt;li>inference phase: 基于Reference model进行采样，生成推理轨迹。&lt;/li>
&lt;li>training phase: 基于采样得到的推理轨迹，更新模型。&lt;/li>
&lt;/ol>
&lt;p>如果我们直接使用Megatron和vLLM来实现上述的训练和推理，那么会存在以下问题：&lt;/p>
&lt;ol>
&lt;li>Megatron和vLLM的并行策略可能并不一致，因此很难进行参数共享&lt;/li>
&lt;li>在训练过程中，vLLM可能会保留一些GPU，这就导致了训练和推理的资源分配不均。&lt;/li>
&lt;li>推理和训练的资源需求可能并不一致，无法动态调控显卡资源&lt;/li>
&lt;/ol>
&lt;p>因此，作者构建了一个hybrid deployment framework来解决上述问题，其框架如下图所示。&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k1.5/k1-5-hybrid-deployment-framework.png"
width="867"
height="560"
loading="lazy"
alt="Hybrid Deployment Framework"
class="gallery-image"
data-flex-grow="154"
data-flex-basis="371px"
>&lt;/p>
&lt;p>该框架的主要优势在于使用Kubernetes Sidecar containers来在一个pod中共享所有的GPU资源。这样就避免了资源分配不均的问题。&lt;/p>
&lt;h2 id="数据">&lt;a href="#%e6%95%b0%e6%8d%ae" class="header-anchor">&lt;/a>数据
&lt;/h2>&lt;h3 id="数据处理">&lt;a href="#%e6%95%b0%e6%8d%ae%e5%a4%84%e7%90%86" class="header-anchor">&lt;/a>数据处理
&lt;/h3>&lt;ol>
&lt;li>RL prompt set. 作者认为prompt的quality和diversity对RL训练至关重要。因此作者基于diverse coverage, balanced difficuty以及accurate evaluation来构建了一个RL prompt set。对于多样性，作者确保数据集覆盖多个domain。对于平衡难度，作者通过多次采样，根据通过率来给每个样本的难易程度打分。对于准确性，作者筛掉了多选题，正确题和证明题，确保模型能够生成正确的推理步骤。最后，作者还让模型进行猜测，如果模型猜测的正确率比较高，那么就认为该样本是easy-to-hack的，就会筛掉。&lt;/li>
&lt;li>Test Case Generation for Coding. 作者使用&lt;a class="link" href="https://github.com/luogu-dev/cyaron" target="_blank" rel="noopener"
>CYaRon&lt;/a>来生成coding任务的测试用例。通过与Kimi k1.5结合，作者构建了一个包含323个测试用例的训练集。&lt;/li>
&lt;li>Reward modeling for Math. 作者使用了两种方法来提高reward model的准确性。第一个是classic RM，作者基于InstructGPT，构造了大约800K的数据训练了一个value-head based RM。模型会基于问题，参考答案和模型生成的答案来判断模型生成的答案是否正确。第二个是Chain of Thought RM，作者基于收集的800k CoT数据对kimi进行了finetune，然后对模型生成的推理步骤进行打分。最终发现，Chain of Thought RM的效果更好。因此，作者在RL训练中使用了Chain of Thought RM。&lt;/li>
&lt;li>Vision RL Data。 作者从三个方面构建了vision RL数据集：real-world data, synthetic visual reasoning data以及text-rendered data.&lt;/li>
&lt;/ol>
&lt;h3 id="数据集">&lt;a href="#%e6%95%b0%e6%8d%ae%e9%9b%86" class="header-anchor">&lt;/a>数据集
&lt;/h3>&lt;ol>
&lt;li>pretraining：其中，纯文本数据包括Engligh, Chinese, Code, math reasoning以及Knowledge这5个领域。多模态数据包括captioning, image-text interleaving, OCR, Knowledge以及QA数据集等。附录B介绍了数据集的来源和处理过程。&lt;/li>
&lt;li>SFT： vanilla SFT数据集包含1M的纯文本数据，其中包含500K的general QA数据，200K的coding数据，200K的数学和科学数据，5K的创作写作数据以及20K的长上下文任务（总结，QA，翻译和写作等）。作者还构建了1M的图文数据，包括chart理解，OCR，grounding，visual coding, visual reasoning以及visual aided math/science problems等&lt;/li>
&lt;li>long-CoT SFT：该阶段的数据基于refined RL prompt set，使用了prompt engineering来构建了一个少量但高质量的long-CoT数据集。用于将reasoning能力内化到模型中。&lt;/li>
&lt;/ol>
&lt;h2 id="long2short">&lt;a href="#long2short" class="header-anchor">&lt;/a>Long2short
&lt;/h2>&lt;p>为了降低推理成本，作者提出了long2short的推理方法。该方法通过将长上下文推理转化为短上下文推理，让模型能够更加高效的完成推理。作者尝试了几种方法来实现long2short：&lt;/p>
&lt;ol>
&lt;li>Model merging:将long-CoT模型和short-CoT模型进行合并，然后进行推理。这里采用的办法是对两个模型的权重取平均。&lt;/li>
&lt;li>shortest rejection sampling：通过对同一个问题进行多次采样（论文中每个问题采样8次），然后选择最短的，正确的推理步骤作为最终答案。&lt;/li>
&lt;li>&lt;a class="link" href="https://maosong.website/p/notes-on-dpo/" target="_blank" rel="noopener"
>DPO&lt;/a>：通过对同一个问题进行多次采样，选择最短的正确的回答作为正样本，最长的回答作为负样本，然后使用DPO进行训练。&lt;/li>
&lt;li>Long2short RL：作者在RL阶段，选择一个在performance和token efficiency之间达到平衡的模型作为 base model，然后加入了一个long2short RL训练阶段。该阶段，作者加入了length penalty以及降低了maximum rollout length来鼓励模型生成更短的推理步骤。&lt;/li>
&lt;/ol>
&lt;h3 id="length-penalty">&lt;a href="#length-penalty" class="header-anchor">&lt;/a>Length penalty
&lt;/h3>&lt;p>Length penalty的目的是降低模型的overthinking和训练成本。作者加入了一个length reward，对于问题 $x$ 和采样的回答 $(y_i,z_i)$ ，回答 $(y_i,z_i)$ 的length定义为 $len(x_i)=\text{length}([z_i, y_i])$ length reward定义为：&lt;/p>
$$
r_{\text{length}}(x, (y_i,z_i))=\begin{cases}
\lambda, &amp; \text{if } r(x, y_i, y^\star) = 1 \\
\min(0,\lambda), &amp; \text{if } r(x, y_i, y^\star) = 0 \\
\end{cases}
$$&lt;p>其中&lt;/p>
$$
\lambda = 0.5 - \frac{len(x_i)-\min_{j}len(x_j)}{\max_{j}len(x_j)-\min_{j}len(x_j)}
$$&lt;p>也就是说，当回答正确时，我们鼓励模型生成更长的推理步骤。反之，我们鼓励模型生成更短的推理步骤。&lt;/p>
&lt;h2 id="实验">&lt;a href="#%e5%ae%9e%e9%aa%8c" class="header-anchor">&lt;/a>实验
&lt;/h2>&lt;h3 id="实验结果">&lt;a href="#%e5%ae%9e%e9%aa%8c%e7%bb%93%e6%9e%9c" class="header-anchor">&lt;/a>实验结果
&lt;/h3>&lt;ol>
&lt;li>K1.5在long-CoT任务上的表现&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k1.5/k1-5-long-cot-benchmark.png"
width="1118"
height="425"
loading="lazy"
alt="Long-CoT-benchmark"
class="gallery-image"
data-flex-grow="263"
data-flex-basis="631px"
>&lt;/p>
&lt;ol start="2">
&lt;li>K1.5在Short-CoT任务上的表现&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k1.5/k1-5-short-cot-benchmark.png"
width="1357"
height="566"
loading="lazy"
alt="Short-CoT-benchmark"
class="gallery-image"
data-flex-grow="239"
data-flex-basis="575px"
>&lt;/p>
&lt;ol start="3">
&lt;li>
&lt;p>Long Context Scaling
作者在实验中还探究了上下文长度对模型推理能力的影响。作者针对几个模型进行了实验，结果在图5和图6里面。结果发现，随着上下文长度的增加，模型的推理能力会逐渐提升。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Long2short&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k1.5/k1-5-long2short.png"
width="1354"
height="590"
loading="lazy"
alt="Long2short"
class="gallery-image"
data-flex-grow="229"
data-flex-basis="550px"
>&lt;/p>
&lt;h3 id="消融实验">&lt;a href="#%e6%b6%88%e8%9e%8d%e5%ae%9e%e9%aa%8c" class="header-anchor">&lt;/a>消融实验
&lt;/h3>&lt;ol>
&lt;li>Scaling of model size and context length. 作者探究了模型大小和上下文长度对模型推理能力的影响，结果在图8里面。结果发现
&lt;ol>
&lt;li>小模型可以通过Long CoT来达到大模型的效果&lt;/li>
&lt;li>大模型具有更高的token效率&lt;/li>
&lt;li>理想情况下，使用具有更长上下文的大模型，可以同时达到更高的推理能力和更高的token效率，否则，使用上下文长度更长的小模型&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Effects of using negative gradients. 作者探究了使用负梯度对模型推理能力的影响。作者与ReST方法进行了对比，结果在图10里面。结果发现，ReST回到只模型产生更长的推理步骤。作者认为，选取合适的优化方法，可以有效提升模型的推理能力。&lt;/li>
&lt;li>Sampling strategies. 作者还探究了采样策略对模型推理能力的影响。作者与均匀采样进行了对比，结果在图9里面。结果发现，论文提出的采样策略可以有效提升模型的推理能力。&lt;/li>
&lt;/ol>
&lt;h2 id="结论">&lt;a href="#%e7%bb%93%e8%ae%ba" class="header-anchor">&lt;/a>结论
&lt;/h2>&lt;p>作者提出了Kimi k1.5，一个基于强化学习训练的多模态推理模型。作者介绍了Kimi k1.5的训练方法，数据集，infra上的优化以及long2short的推理方法。作者的主要贡献在于：&lt;/p>
&lt;ol>
&lt;li>作者发现，模型的上下文长度可以有效提升LLM的推理能力。&lt;/li>
&lt;li>作者提出了基于online mirror descent的强化学习训练方法。&lt;/li>
&lt;li>作者提出了long2short的推理方法，可以有效提升short CoT模型的推理能力。&lt;/li>
&lt;/ol>
&lt;p>论文的问题在于对于多模态的内容介绍较少，感觉还是更倾向于文本推理任务。&lt;/p>
&lt;h2 id="参考文献">&lt;a href="#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae" class="header-anchor">&lt;/a>参考文献
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2501.12599" target="_blank" rel="noopener"
>Kimi k1.5: Scaling Reinforcement Learning with LLMs&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>