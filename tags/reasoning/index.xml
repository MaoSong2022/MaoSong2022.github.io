<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Reasoning on Mao Song(毛松)'s Homepage</title><link>https://maosong2022.github.io/tags/reasoning/</link><description>Recent content in Reasoning on Mao Song(毛松)'s Homepage</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 11 Sep 2025 11:33:31 +0800</lastBuildDate><atom:link href="https://maosong2022.github.io/tags/reasoning/index.xml" rel="self" type="application/rss+xml"/><item><title>Notes on Keye-VL 1.5</title><link>https://maosong2022.github.io/p/notes-on-keye-vl-1.5/</link><pubDate>Thu, 11 Sep 2025 11:33:31 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-keye-vl-1.5/</guid><description>&lt;p>快手提出了 Keye-VL 1.5, 一个强调 reasoning, video understanding 的 8B 多模态大模型。作者提出了 slow-fast video encoding strategy 来提高模型的视频理解能力，作者通过在预训练和后训练提高了模型的长上下文能力和 reasoning 能力&lt;/p>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>作者回顾了多模态大模型的进展，然后提到视频理解能力仍然是一个挑战。为了解决这个问题，作者提出了 Keye-VL-1.5, 一个 8B 的视频理解多模态大模型。&lt;/p>
&lt;p>Keye-VL-1.5 主要做了三点改进：&lt;/p>
&lt;ol>
&lt;li>在架构上，使用了 Slow-Fast Video Encoding&lt;/li>
&lt;li>在预训练阶段，使用多个 stage 来提升模型的长上下文能力&lt;/li>
&lt;li>在 post-training 阶段，提高模型的 reasoning 能力和 alignment 表现&lt;/li>
&lt;/ol>
&lt;h2 id="method">Method
&lt;/h2>&lt;h3 id="architecture">Architecture
&lt;/h3>&lt;p>Keye-VL 1.5 的架构与 &lt;a class="link" href="https://maosong.website/p/notes-on-keye-vl/" target="_blank" rel="noopener"
>Keye-VL&lt;/a> 一致。&lt;/p>
&lt;p>Keye-VL 1.5 主要做出的改进点为针对视频的 encoding 方式&lt;/p>
&lt;p>作者回顾了已有 MLLM 处理视频的方式，比如 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5-vl/" target="_blank" rel="noopener"
>Qwen2.5-VL&lt;/a> 使用 3D convolution 来 merge 相邻的两帧，&lt;a class="link" href="https://maosong.website/p/notes-on-seed1.5-vl/" target="_blank" rel="noopener"
>Seed1.5-VL&lt;/a> 采用了 Dynamic Frame-Resolution Sampling 技巧，来根据 budget 和处理的任务来动态调整采样率 (frame) 和每一帧的图片精度 (resolution)。&lt;/p>
&lt;p>但是这些方法很难进行泛化。因此，作者在本文中就提出了 &lt;strong>SlowFast video encoding stratrgy&lt;/strong>:&lt;/p>
&lt;ol>
&lt;li>Slow Pathway: 空间信息丰富（high resolution），时间信息简略 (low number of frames)&lt;/li>
&lt;li>Fast Pathway: 时间信息丰富（high number of frames），空间信息简略 (low resolution)&lt;/li>
&lt;/ol>
&lt;p>为了区分 slow/fast frames, 作者提出了一个基于 patch similarity 的 metric:&lt;/p>
&lt;ol>
&lt;li>第一帧始终定义为 slow frame&lt;/li>
&lt;li>接下来的每一帧，如果其和上一帧的相似度超过 $95%$, 则定义为 fast frame; 反之则定义为 slow frame.&lt;/li>
&lt;/ol>
&lt;p>得到 slow/fast frames 之后，作者将 fast-frame 的 token budget 限制为 slow frame token budget 的 $30%$ 来平衡时间信息以及空间信息。接下来，作者使用二分搜索来决定 slow frame 的 token budget. 为了区分 slow frame 和 fast frame 的 token, 作者使用了特殊的 token 来进行分离。&lt;/p>
&lt;p>最终的处理结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-video-encoding.png"
width="1364"
height="335"
srcset="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-video-encoding_hu3751451477354070023.png 480w, https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-video-encoding_hu15836414842667305534.png 1024w"
loading="lazy"
alt="SlowFast Video Encoding"
class="gallery-image"
data-flex-grow="407"
data-flex-basis="977px"
>&lt;/p>
&lt;h3 id="pre-training">Pre-training
&lt;/h3>&lt;p>预训练的数据和 Keye-VL 基本一致，我们主要介绍改进的点&lt;/p>
&lt;p>对于 Image caption 数据，作者认为这批数据可能会损害模型的指令跟随和 reasoning 能力，因此作者对数据进行了增广，主要是调整了数据的格式：&lt;/p>
&lt;ol>
&lt;li>QA, 数据格式为 &lt;code>&amp;lt;image, caption, [eos], question, answer&amp;gt;&lt;/code>&lt;/li>
&lt;li>reverse QA, 数据格式为 &lt;code>&amp;lt;image, question, answer, [eos], caption&amp;gt;&lt;/code>&lt;/li>
&lt;li>instruction following: 随机给一批数据作为输入，然后让模型基于特定 image 输出 caption&lt;/li>
&lt;/ol>
&lt;p>作者还构建了一个 trap question 来提高模型的 robustness 以及 faithfulness.&lt;/p>
&lt;p>OCR 数据在 Keye-VL 的基础上加入了两点：&lt;/p>
&lt;ol>
&lt;li>Structured Document and Code Understanding: 基于 markdown 和 HTML 等数据来获取 code OCR 数据&lt;/li>
&lt;li>Instruction Following OCR: 基于特定指令进行 OCR&lt;/li>
&lt;/ol>
&lt;p>对于 grounding 数据，作者进一步加入了 temporal grounding 数据，作者首先使用 TEMPURA&lt;/p>
&lt;p>来将短视频分割成若干个 video clips. 然后作者使用 SOTA MLLM 来过滤数据，最后作者基于 Gemini2.5 来生成对应的 QA.&lt;/p>
&lt;p>预训练和 Keye-VL 一样，包含 3 个 stage&lt;/p>
&lt;p>前两个 stage，作者将模型的上下文限制为 8K, 使用了 DP 和 Zero-2 来减少内存开销。在 stage 3, 作者将模型的上下文从 8K 扩展到 128K, 对应的 base frequency 从 1M 提升到 8M. 训练数据包括长视频，长文本和大规模图片。作者将优化策略调整为 Zero-1, CP 和 PP 来支持 long-context 的训练。训练时数据分布为 video:images:text=24:50:26.&lt;/p>
&lt;h3 id="post-training">Post-training
&lt;/h3>&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-post-training-pipeline.png"
width="1360"
height="449"
srcset="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-post-training-pipeline_hu4088885002935100923.png 480w, https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-post-training-pipeline_hu14194521321835783259.png 1024w"
loading="lazy"
alt="Post-Training Pipeline"
class="gallery-image"
data-flex-grow="302"
data-flex-basis="726px"
>&lt;/p>
&lt;p>SFT 阶段使用了 &lt;strong>7.5M&lt;/strong> 多模态 QA 样本进行训练。&lt;/p>
&lt;p>MPO 阶段的数据相比 Keye-VL 有所减少，包含：&lt;/p>
&lt;ol>
&lt;li>250K 开源样本&lt;/li>
&lt;li>150K 纯文本数据&lt;/li>
&lt;li>26K 人类标注数据&lt;/li>
&lt;/ol>
&lt;p>对于 reward model 的训练，作者使用了 SFT 和 RL 两个阶段。SFT 阶段的数据包括 R1-Reward 和 MMPR, 训练之后作者还是用比较短的 good response 来避免产生较长的回答&lt;/p>
&lt;p>在 SFT 和 MPO 阶段之后，作者使用 LongCoT code-start 初步激活模型的 reasoning 能力。&lt;/p>
&lt;p>作者构建了一个 5 部的自动化数据生成 pipeline, 如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-LongCoT-data-generation-pipeline.png"
width="1362"
height="389"
srcset="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-LongCoT-data-generation-pipeline_hu10802193350758216375.png 480w, https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-LongCoT-data-generation-pipeline_hu15923562071597966044.png 1024w"
loading="lazy"
alt="LongCoT data generation pipeline"
class="gallery-image"
data-flex-grow="350"
data-flex-basis="840px"
>&lt;/p>
&lt;p>步骤如下：&lt;/p>
&lt;ol>
&lt;li>Multi-Source Data Collection and Enhancement：收集数据&lt;/li>
&lt;li>Multi-Path Reasoning Generation with Confidence Quantification: 基于 confidence 来挑选数据&lt;/li>
&lt;li>Comprehensive Two-Level Quality Assessment: 基于答案和过程的正确性来提高数据质量&lt;/li>
&lt;li>Human-in-the-Loop Quality Enhancement: 对于中等质量的数据请人类进一步进行标注&lt;/li>
&lt;li>Dynamic Quality Scoring and Data Utilization Strategy: 对数据进行打分，高质量数据进行上采样&lt;/li>
&lt;/ol>
&lt;p>接下来就是 General RL 过程。&lt;/p>
&lt;p>对于通用的 RLVR 训练，作者使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-gspo/" target="_blank" rel="noopener"
>GSPO&lt;/a> 算法来进行训练。&lt;/p>
&lt;p>在训练过程中，作者采取了 progressive hint sampling 方式，也就是提供不同程度的 hint 来提高模型的训练效率。作者将 hint 分为五个等级：&lt;/p>
&lt;ol>
&lt;li>Level 1 (Concept / Observation)&lt;/li>
&lt;li>Level 2 (Strategy / Method)&lt;/li>
&lt;li>Level 3 (Tools / Formula)&lt;/li>
&lt;li>Level 4 (Steps / Calculation)&lt;/li>
&lt;li>Level 5 (Complete Solution)&lt;/li>
&lt;/ol>
&lt;p>来提供不同程度的辅助，作者使用 Keye-VL 1.5 来确定最小的 hint level, 然后基于这个 level 提供信息进行 RL 的训练&lt;/p>
&lt;p>为了进一步提高模型的表现，作者采用了一个和 &lt;a class="link" href="https://maosong.website/p/notes-on-seed1.5-vl/" target="_blank" rel="noopener"
>Seed1.5-VL&lt;/a> 一样的迭代式训练策略，即反复进行 SFT 和 RL 来降低训练成本，提高训练效率。&lt;/p>
&lt;p>最后，再通用 RL 阶段之后，作者加入了 alignment RL 的训练来进行对齐，reward 包括三个方面：&lt;/p>
&lt;ol>
&lt;li>rule-based reward&lt;/li>
&lt;li>generative reward&lt;/li>
&lt;li>model-based reward&lt;/li>
&lt;/ol>
&lt;p>任务主要包括三个方面：&lt;/p>
&lt;ol>
&lt;li>instruction following&lt;/li>
&lt;li>format adherence&lt;/li>
&lt;li>preference alignment&lt;/li>
&lt;/ol>
&lt;p>数据介绍如下：&lt;/p>
&lt;ol>
&lt;li>instruction following:25 类硬约束，20 类软约束，数据包括 17K 多模态数据和 23K 纯文本数据，奖励包括 rule-based reward 和 generative reward&lt;/li>
&lt;li>reasoning: 12K 数学和逻辑推理数据&lt;/li>
&lt;li>RAG: 提高模型的搜索能力，作者使用 GSPO 算法进行训练&lt;/li>
&lt;/ol>
&lt;h2 id="experiments">Experiments
&lt;/h2>&lt;p>模型表现如下&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-Performance.png"
width="1366"
height="988"
srcset="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-Performance_hu12685109521298202886.png 480w, https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-Performance_hu1121153072311157701.png 1024w"
loading="lazy"
alt="Performance of Keye-VL 1.5"
class="gallery-image"
data-flex-grow="138"
data-flex-basis="331px"
>&lt;/p>
&lt;p>接下来作者进行了消融实验。&lt;/p>
&lt;p>首先是不同训练阶段对模型表现的影响，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-training-stage.png"
width="1369"
height="458"
srcset="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-training-stage_hu14045805214300333895.png 480w, https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-training-stage_hu10663261281529444295.png 1024w"
loading="lazy"
alt="Ablation study on training strategy"
class="gallery-image"
data-flex-grow="298"
data-flex-basis="717px"
>&lt;/p>
&lt;p>实验结果显示，提高 SFT 训练数据可以有效提高模型在数学推理，逻辑推理和 OCR 任务上的表现。MPO 可以进一步提高模型的表现，Long CoT cold start 可以有效提高模型的 reasoning 表现。&lt;/p>
&lt;p>作者还探究了 model merging 对模型表现的影响，作者首先基于 base model 和 OCR 数据训练得到 OCR expert, 然后进行 merge, 实验结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-model-merging.png"
width="1244"
height="340"
srcset="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-model-merging_hu5485713103224105634.png 480w, https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-model-merging_hu4736239008410953171.png 1024w"
loading="lazy"
alt="Ablation study on model merging"
class="gallery-image"
data-flex-grow="365"
data-flex-basis="878px"
>&lt;/p>
&lt;p>实验结果显示，model merging 可以有效提高模型在 special domain 上的表现，并且还可以维持模型的通用能力&lt;/p>
&lt;p>作者还发现：&lt;/p>
&lt;ol>
&lt;li>expert model 训练时间过长会影响最终 merge model 的表现&lt;/li>
&lt;li>expert mode 训练的学习率应该要设置比较小&lt;/li>
&lt;/ol>
&lt;p>接下来，作者探究了 alignment RL 对模型表现的影响，&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-alignment-RL.png"
width="1385"
height="275"
srcset="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-alignment-RL_hu16960946704897994390.png 480w, https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-alignment-RL_hu16484448956614414637.png 1024w"
loading="lazy"
alt="Ablation on alignment RL"
class="gallery-image"
data-flex-grow="503"
data-flex-basis="1208px"
>&lt;/p>
&lt;p>实验结果说明，alignment RL 可以在保持模型 reasoning 能力的同时提高模型的指令跟随能力&lt;/p>
&lt;p>作者还探究了 hint 对模型表现的影响，使用不同 level hint 进行训练对模型表现影响的结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-hint.png"
width="1209"
height="310"
srcset="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-hint_hu5531435168214557685.png 480w, https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-hint_hu1703494621381694086.png 1024w"
loading="lazy"
alt="Ablation on hint level"
class="gallery-image"
data-flex-grow="390"
data-flex-basis="936px"
>&lt;/p>
&lt;p>实验结果显示，使用 high level 的 hint 可以有效提高模型输出的正确率。&lt;/p>
&lt;p>最后作者探究了 rejection sampling 对模型表现的影响，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-rejection-sampling.png"
width="1384"
height="340"
srcset="https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-rejection-sampling_hu12066184384347437079.png 480w, https://maosong2022.github.io/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-rejection-sampling_hu1521405363756042548.png 1024w"
loading="lazy"
alt="Ablation on rejection sampling"
class="gallery-image"
data-flex-grow="407"
data-flex-basis="976px"
>&lt;/p>
&lt;p>结果发现，通过 rejection sampling，模型的表现有了进一步的提升。因此作者采用了 ST-RL-(RFT-SFT)-(RFT-RL) 的训练方式来进行训练&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 Keye-VL1.5, 一个强调 video understanding 和 reasoning 的 MLLM. Keye-VL 1.5 使用了 SlowFast video encoding strategy 来提高模型的效率和视频理解能力。作者详细介绍了模型的数据，训练和评估。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2509.01563" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on InternVL3.5</title><link>https://maosong2022.github.io/p/notes-on-internvl3.5/</link><pubDate>Mon, 01 Sep 2025 11:30:50 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-internvl3.5/</guid><description>&lt;p>上海 AI LAB 提出了 InternVL 3.5 系列多模态大模型，InternVL 3.5 主要强调了模型的 reasoning 能力以及 inference 效率。&lt;/p>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>作者首先介绍了已有多模态大模型的进展，然后提到有两个未解决的问题：&lt;/p>
&lt;ol>
&lt;li>如何构建一个 stable, effective, scalable 的 RL 框架来训练 MLLM&lt;/li>
&lt;li>如何降低 MLLM 在长上下文场景下的计算开销过高的问题&lt;/li>
&lt;/ol>
&lt;p>为了解决这两个问题，作者提出了 InternVL3.5 多模态大模型系列，相比于 InternVL3, 模型在 reasoning 能力和 efficiency 上均进行了提升。作者主要通过 Cascade RL 框架来提高模型的 reasoning 表现，以及使用 Visual Resolution Router (ViR) 和 Decoupled Vision Language Deployment (DvD) 来提高模型的推理效率。&lt;/p>
&lt;p>总结起来，InternVL3.5 模型的贡献如下：&lt;/p>
&lt;ol>
&lt;li>开源了 InternVL3.5 系列多模态大模型，模型拥有先进的 reasoning 能力以及推理效率&lt;/li>
&lt;li>提出了 Cascade RL, ViR 以及 DvD 等模块来提高模型的表现/推理效率&lt;/li>
&lt;li>系统性评估了模型的表现&lt;/li>
&lt;/ol>
&lt;h2 id="method">Method
&lt;/h2>&lt;h3 id="architecture">Architecture
&lt;/h3>&lt;p>InternVL3.5 的架构与 InternVL3 的架构基本一致，包括一个 ViT, 一个 MLP 和一个 LLM, 模型的架构如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-architecture.png"
width="1295"
height="419"
srcset="https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-architecture_hu13393200266970951905.png 480w, https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-architecture_hu14009408115549874616.png 1024w"
loading="lazy"
alt="Architecture of InternVl3.5"
class="gallery-image"
data-flex-grow="309"
data-flex-basis="741px"
>&lt;/p>
&lt;p>模型配置如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-model-configuration.png"
width="1227"
height="484"
srcset="https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-model-configuration_hu18167488677887436257.png 480w, https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-model-configuration_hu9269566815942944406.png 1024w"
loading="lazy"
alt="Configuration of InternVL3.5"
class="gallery-image"
data-flex-grow="253"
data-flex-basis="608px"
>&lt;/p>
&lt;p>InternVl3.5 延续了 InternVL 系列的做法，即 20B 以下的模型使用 InternViT-300M, 20B 以上的模型使用 InternViT-6B 作为 Visual encoder. 大语言模型基于 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/notes-on-gpt-oss/" target="_blank" rel="noopener"
>gpt-oss&lt;/a>.&lt;/p>
&lt;p>在 InternVL3.5 的基础上，作者还构建了 InternVL3.5-Flash 模型，InternVL3.5-Flash 不同的地方在于，其在 MLP 之前加入了一个 Visual Resolution Router (ViR) 模块，来处理不同分辨率的图片。在 InternVL3.5 中，每个 image patch 由 1024 个视觉 token 来表示，然后通过 pixel shuffle 压缩至 256 个。 在 InternVL3.5-Flash 中，ViR 首先会判断每个 patch 的 semantic richness, 然后基于 semantic richness 来将对应的 token 路由到不同的 pixel shuffle 模块中，最高可以将视觉 token 压缩到 64 个，这样就可以大幅度提高模型的推理效率。&lt;/p>
&lt;h3 id="pre-training">Pre-training
&lt;/h3>&lt;p>Pre-training 使用 next-token-prediction loss 来更新模型权重，给定多模态 sequence $x=(x_1,\dots,x_L)$, 损失函数定义为&lt;/p>
$$
\mathcal{L}(\theta) = -\sum_{x_i\text{ is text token}} \log p_{\theta}(x_i
\mid x_1,\dots,x_{i-1})
$$&lt;p>与 InternVL2.5 一样，作者还对不同长度的 sequence 进行了加权，避免模型产生 bias, 加权后的 loss 为&lt;/p>
$$
\mathcal{L}(\theta) = -\sum_{x_i\text{ is text token}}\frac{w_i}{\sum_j w_j} \log p_{\theta}(x_i
\mid x_1,\dots,x_{i-1}),\quad w_i = \frac{1}{N^{0.5}}
$$&lt;p>其中 $N$ 是训练样本中需要计算损失的 token 个数。&lt;/p>
&lt;p>训练数据蛀牙包含两部分：&lt;/p>
&lt;ol>
&lt;li>多模态数据，这部分数据基于 InternVL3&lt;/li>
&lt;li>纯文本数据，基于 InternLM 系列和开源的数据集&lt;/li>
&lt;/ol>
&lt;p>最终，预训练数据一共包含&lt;strong>116M&lt;/strong> 样本，&lt;strong>250B&lt;/strong> token. 纯文本数据和多模态数据的比例为 $1:2.5$. 模型上下文长度为 $32K$.&lt;/p>
&lt;h3 id="post-training">Post-training
&lt;/h3>&lt;p>Post-training 包含三个阶段：&lt;/p>
&lt;ol>
&lt;li>SFT： 使用高质量对话数据提高模型表现&lt;/li>
&lt;li>Cascade RL: 提高模型的 reasoning 能力&lt;/li>
&lt;li>Visual Consistency Learning: 训练 ViR 模块，提高模型处理动态分辨率图片的能力&lt;/li>
&lt;/ol>
&lt;p>训练 pipeline 如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-post-training-pipeline.png"
width="1290"
height="335"
srcset="https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-post-training-pipeline_hu6577490402609280168.png 480w, https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-post-training-pipeline_hu10277305099796222533.png 1024w"
loading="lazy"
alt="Post-training pipeline of InternVL3.5"
class="gallery-image"
data-flex-grow="385"
data-flex-basis="924px"
>&lt;/p>
&lt;p>SFT 阶段的训练数据包括三个方面：&lt;/p>
&lt;ol>
&lt;li>InternVL3 的指令跟随数据&lt;/li>
&lt;li>多模态 reasoning 数据&lt;/li>
&lt;li>能力扩展数据，包括 GUI 交互，具身交互以及 SVG 理解与生成的数据&lt;/li>
&lt;/ol>
&lt;p>Cascade RL 阶段结合了 offline RL 和 online RL 的优点。基本思想是先使用 offline RL 来提高模型的能力，降低训练成本。然后再使用 online RL 进一步提高模型的 reasoning 表现。&lt;/p>
&lt;p>offline RL 阶段使用的是 InternVL-MPO 算法，其损失函数为&lt;/p>
$$
\mathcal{L}_{MPO} = w_p\mathcal{L}_p+w_q\mathcal{L}_q+w_g\mathcal{L}_g
$$&lt;p>其中，$\mathcal{L}_p$ 为 DPO 的损失函数，$\mathcal{L}_q$ 为 Quality loss, $\mathcal{L}_g$ 为 SFT 使用的 next-token-prediction loss.&lt;/p>
&lt;p>online RL 阶段使用的是 &lt;a class="link" href="https://maosong.website/p/notes-on-gspo/" target="_blank" rel="noopener"
>GSPO&lt;/a> 算法，核心思想是在 sample 层面做归一化。这个阶段的目的是进一步提高模型的表现。&lt;/p>
&lt;p>Cascade RL 的优势在于：&lt;/p>
&lt;ol>
&lt;li>训练更稳定：offline RL 可以避免模型产生 reward hacking 现象，online RL 阶段可以进一步提高模型的表现&lt;/li>
&lt;li>训练效率更高：offline RL 可以有效提高采样效率&lt;/li>
&lt;li>表现更好：先 MPO 再 RL 可以达到更好的表现&lt;/li>
&lt;/ol>
&lt;p>Visual Consistency Learning (ViCO) 的目的是降低 InternVL3.5 的推理开销。这个阶段主要训练 ViR 模块&lt;/p>
&lt;p>ViCO 包括两个 stage:&lt;/p>
&lt;p>Stage 1: Consistency training&lt;/p>
&lt;p>这一阶段的目的是使得不同压缩率处理的视觉 token 对应的输出与 ground truth 尽可能一致，作者使用另外一个 InternVL3.5 模型作为 reference model, 然后损失函数为&lt;/p>
$$
\mathcal{L}_{ViCO} = \mathbb{E}_{\xi\sim\mathcal{R}}\left[\frac{1}{N}\sum_{i=1}^N\mathrm{KL}\left(\pi_{\mathrm{ref}}(y_i\mid y_{&lt;i}, I)\ \Vert\ \pi_{\theta}(y_i\mid y_{&lt;i}, I_{\xi})\right)\right]
$$&lt;p>$\xi\in{1/4,1/16}$ 为 compression rate, 训练是随机采样。$\xi=1/4$ 代表最终会有 256 个视觉 token, $\xi=1/16$ 代表最终会有 64 个 token.&lt;/p>
&lt;p>Stage 2: Router training&lt;/p>
&lt;p>这个阶段的目的是训练 ViR 来选取合适的精度/压缩率。ViR 此时作为一个 binary classifier 来进行训练，训练的损失函数为 cross-entropy loss. 模型其他部分参数冻结，仅训练 ViR 模块，首先，我们将计算压缩 16 倍后的损失与压缩 4 倍的损失的比例&lt;/p>
$$
r_i = \frac{\mathcal{L}_{ViCO}(y_i\mid I_{1/16})}{\mathcal{L}_{ViCO}(y_i\mid I_{1/4})}
$$&lt;p>该比例衡量了压缩 token 之后带来的性能下降程度，接下来，作者设定了一个阈值 $\tau$, 当性能下降超过阈值 $\tau$, 则认为应该使用高精度，也就是 $\xi=1/4$, 反之则说明不需要过度视觉 token, 可以使用低精度，也就是 $\xi=1/16$. 总结得到&lt;/p>
$$
y_i^{router} = \begin{cases}
0, &amp;r_i&lt;\tau\\
1, &amp;r_i>\tau
\end{cases}
$$&lt;p>训练时，作者基于 sliding window 中的 $r_i$ 来动态调整 $\tau$ 的值。&lt;/p>
&lt;p>post-training 的训练数据如下：&lt;/p>
&lt;ol>
&lt;li>SFT 训练数据包括&lt;strong>56M&lt;/strong> 样本，&lt;strong>130B&lt;/strong> token, 纯文本数据与多模态数据的比例为 $1:3.5$.&lt;/li>
&lt;li>Cascade RL 的训练数据主要基于 MMPR, 包含 200K 左右的样本，通过过滤最终得到&lt;strong>70K&lt;/strong>左右的样本。online RL 同样使用这批数据进行训练&lt;/li>
&lt;li>ViCO 的训练数据与 SFT 阶段的训练数据一致。主要是 OCR 以及 VQA 数据&lt;/li>
&lt;/ol>
&lt;h3 id="test-time-scaling">Test-time Scaling
&lt;/h3>&lt;p>test time scaling 主要基于两个方面：&lt;/p>
&lt;ol>
&lt;li>deep thinking: 就是 reasoning mode&lt;/li>
&lt;li>parallel thinking: 基于 VisualPRM 进行多次采样，然后基于 BoN 策略得到最终的输出。&lt;/li>
&lt;/ol>
&lt;h3 id="infra">Infra
&lt;/h3>&lt;p>模型使用 XTuner 框架进行训练，使用了包括 FSDP, data packing, FP8, flashattention3 等策略。&lt;/p>
&lt;p>作者还提出了 DvD 的策略来提高推理效率，核心思想就是将 ViT-MLP 模块放在一个 server 上，然后将 LLM 放在另一个 server 上，这样就也可以提高整体的通信效率。框架示意图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-DvD.png"
width="1297"
height="580"
srcset="https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-DvD_hu5225375899242682523.png 480w, https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-DvD_hu2947329161828488830.png 1024w"
loading="lazy"
alt="DvD of InternVL3.5"
class="gallery-image"
data-flex-grow="223"
data-flex-basis="536px"
>&lt;/p>
&lt;h2 id="experiments">Experiments
&lt;/h2>&lt;p>模型整体表现如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-model-performance.png"
width="964"
height="1113"
srcset="https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-model-performance_hu14525544515045453778.png 480w, https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-model-performance_hu5961582846464991529.png 1024w"
loading="lazy"
alt="Performance of InternVL3.5"
class="gallery-image"
data-flex-grow="86"
data-flex-basis="207px"
>&lt;/p>
&lt;h3 id="ablation-study">Ablation Study
&lt;/h3>&lt;p>作者首先探究了 Cascade RL 对模型表现的影响&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-ablation-cascade-RL.png"
width="1299"
height="331"
srcset="https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-ablation-cascade-RL_hu1333440004729449373.png 480w, https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-ablation-cascade-RL_hu14908798937935143641.png 1024w"
loading="lazy"
alt="Ablation study on Cascade RL"
class="gallery-image"
data-flex-grow="392"
data-flex-basis="941px"
>&lt;/p>
&lt;p>实验结果显示，SFT, MPO 和 offline RL 每个阶段都可以提高模型的多模态 reasoning 表现。&lt;/p>
&lt;p>作者进一步探究了不同阶段的投入产出比，也就是训练时间与最终模型表现的变化情况，如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-training-efficiency-effectiveness.png"
width="1237"
height="213"
srcset="https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-training-efficiency-effectiveness_hu5719553098653551965.png 480w, https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-training-efficiency-effectiveness_hu5489765521449568152.png 1024w"
loading="lazy"
alt="Training efficiency and effectiveness"
class="gallery-image"
data-flex-grow="580"
data-flex-basis="1393px"
>&lt;/p>
&lt;p>实验结果显示，尽管 GSPO 的效果提升比较明显，但是训练所需要的时间比较长。如果先 MPO 再进行 GSPO 的话，我们可以在较短的时间里取得较好的表现。&lt;/p>
&lt;p>接下来，作者探究了 ViR 的有效性和效率。作者首先对比了 InternVL3.5 以及 InternVL3.5-flash 的表现。结果显示，大部分情况下，这两个模型的表现没有较大差距。说明 ViR 不会损害模型的性能。&lt;/p>
&lt;p>作者进一步探究了 ViR 和 DvD 对提升模型效率的影响，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-ablation-DvD.png"
width="877"
height="397"
srcset="https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-ablation-DvD_hu383731560352212400.png 480w, https://maosong2022.github.io/p/notes-on-internvl3.5/InternVL-3-5-ablation-DvD_hu14394963354227445597.png 1024w"
loading="lazy"
alt="Ablation study on DvD and ViR"
class="gallery-image"
data-flex-grow="220"
data-flex-basis="530px"
>&lt;/p>
&lt;p>实验结果说明，对于高精度图片输入，DvD 和 ViR 均可以提高模型的推理效率。&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>在本文中，作者提出了 InternVL3.5 系列多模态大模型，作者提出了 Cascade RL 框架，该框架结合了 offline RL 以及 online RL 来提高模型的 reasoning 表现以及训练效率。作者还提出了 ViR 以及 DvD 模块来提高模型处理高分辨率图片的效率。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2508.18265" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on GLM-4.5</title><link>https://maosong2022.github.io/p/notes-on-glm-4.5/</link><pubDate>Wed, 13 Aug 2025 12:27:48 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-glm-4.5/</guid><description>&lt;p>智谱 AI 提出了 GLM4.5, 包含 GLM4.5 和 GLM-4.5-Air,两个 MoE LLM. 模型大小分别为 355B-A22B 和 106B-A12B, GLM4.5 主要关注 agentic, reasoning 以及 coding 三个领域。&lt;/p>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>作者认为，通用模型有三个关键能力，即 ARC：&lt;/p>
&lt;ol>
&lt;li>Agent: 与外部工具以及真实世界进行交互&lt;/li>
&lt;li>Reasoning: 解决数学和科学领域的复杂问题&lt;/li>
&lt;li>Coding: 解决真实世界软件工程相关问题&lt;/li>
&lt;/ol>
&lt;p>已有的商业模型如 o1/o3, Claude Sonnet 4 已经在 ARC 上达到了非常好的表现，但是开源模型仍然比较稀缺&lt;/p>
&lt;p>基于这个目标，作者就提出了 GLM4.5 和 GLM-4.5-Air, 来统一完成三个不同的目标。&lt;/p>
&lt;h2 id="method">Method
&lt;/h2>&lt;h2 id="pre-training">Pre-training
&lt;/h2>&lt;h3 id="architecture">Architecture
&lt;/h3>&lt;p>GLM-4.5 是一个基于 MoE 架构的 LLM, 架构与 DeepSeek-MoE 相似，作者做了如下几点改变：&lt;/p>
&lt;ol>
&lt;li>在 MoE layer 中，使用了 loss-free balance routing, 然后使用了 sigmoid function 作为 routing score 的 normalization.&lt;/li>
&lt;li>与 &lt;a class="link" href="https://maosong.website/p/notes-on-kimi-k2/" target="_blank" rel="noopener"
>Kimi-k2&lt;/a> 和 DeepSeek-V3 相比，作者降低了 head dimension, 提升了 number of layers. 作者认为更深的模型更有利于提高模型的 Reasoning 表现&lt;/li>
&lt;li>attention 上，作者使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-gqa/" target="_blank" rel="noopener"
>GQA&lt;/a>, 对于 #RoPE, 作者使用了 partial RoPE, 只旋转每个 token 的前半部分， 作者还将 attention heads 的个数增加到了 2.5 倍，作者发现增加 attention heads 可以提高模型的 Reasoning 表现&lt;/li>
&lt;li>作者还使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-qk-norm/" target="_blank" rel="noopener"
>QK-Norm&lt;/a> 来防止 attention logits 爆炸&lt;/li>
&lt;li>作者还使用了一个 MoE layer 作为 MTP layer 来支持 speculative decoding.&lt;/li>
&lt;/ol>
&lt;p>模型与 DeepSeek-V3 和 Kimi-k2 的对比如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>GLM-4.5&lt;/th>
&lt;th>GLM-4.5-Air&lt;/th>
&lt;th>Step 3&lt;/th>
&lt;th>Kimi K2&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Date&lt;/td>
&lt;td>2025/8/8&lt;/td>
&lt;td>2025/8/8&lt;/td>
&lt;td>2025/7/25&lt;/td>
&lt;td>2025/7/28&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Total Parameters&lt;/td>
&lt;td>355B&lt;/td>
&lt;td>106B&lt;/td>
&lt;td>316B&lt;/td>
&lt;td>1043B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Activated Parameters&lt;/td>
&lt;td>32B&lt;/td>
&lt;td>12B&lt;/td>
&lt;td>38B&lt;/td>
&lt;td>32B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Dense Layers&lt;/td>
&lt;td>3&lt;/td>
&lt;td>1&lt;/td>
&lt;td>5&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># MoE Layers&lt;/td>
&lt;td>89&lt;/td>
&lt;td>45&lt;/td>
&lt;td>56&lt;/td>
&lt;td>60&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># MTP Layers&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;td>0&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Hidden Dim&lt;/td>
&lt;td>5120&lt;/td>
&lt;td>4096&lt;/td>
&lt;td>7168&lt;/td>
&lt;td>7168&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Dense Intermediate Dim&lt;/td>
&lt;td>12288&lt;/td>
&lt;td>10944&lt;/td>
&lt;td>18432&lt;/td>
&lt;td>18432&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MoE Intermediate Dim&lt;/td>
&lt;td>1536&lt;/td>
&lt;td>1408&lt;/td>
&lt;td>5120&lt;/td>
&lt;td>2048&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Attention&lt;/td>
&lt;td>GQA&lt;/td>
&lt;td>GQA&lt;/td>
&lt;td>MFA&lt;/td>
&lt;td>MLA&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Attention Head Dim&lt;/td>
&lt;td>128&lt;/td>
&lt;td>128&lt;/td>
&lt;td>256&lt;/td>
&lt;td>192&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Attention Heads&lt;/td>
&lt;td>96&lt;/td>
&lt;td>96&lt;/td>
&lt;td>64&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Key-Value Heads&lt;/td>
&lt;td>8&lt;/td>
&lt;td>8&lt;/td>
&lt;td>1&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>scoring&lt;/td>
&lt;td>sigmoid&lt;/td>
&lt;td>sigmoid&lt;/td>
&lt;td>softmax&lt;/td>
&lt;td>softmax&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Experts (total)&lt;/td>
&lt;td>160&lt;/td>
&lt;td>128&lt;/td>
&lt;td>48&lt;/td>
&lt;td>384&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Experts Active Per Token&lt;/td>
&lt;td>8&lt;/td>
&lt;td>8&lt;/td>
&lt;td>3&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Shared Experts&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>QK-Norm&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>No&lt;/td>
&lt;td>No&lt;/td>
&lt;td>No&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="pre-training-data">Pre-training Data
&lt;/h3>&lt;p>预训练数据包括四个方面&lt;/p>
&lt;ol>
&lt;li>Web: 过滤低质量数据和使用模版产生的数据&lt;/li>
&lt;li>Multilingual: 基于 webpages 和 Fineweb-2&lt;/li>
&lt;li>Code: 基于 GitHub 和其他代码平台，作者使用了 [[Fill in the middle]] 来训练模型。&lt;/li>
&lt;li>Math &amp;amp; Scirence: 训练一个 classifier 来给数据进行打分。&lt;/li>
&lt;/ol>
&lt;p>最终，预训练数据一共包括 &lt;strong>23T token&lt;/strong>.&lt;/p>
&lt;h3 id="pre-training-recipe">Pre-training Recipe
&lt;/h3>&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4_5-pre-training.png"
width="1173"
height="406"
srcset="https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4_5-pre-training_hu3166061419089618432.png 480w, https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4_5-pre-training_hu7846610875110494798.png 1024w"
loading="lazy"
alt="Pre-training recipe of GLM4.5"
class="gallery-image"
data-flex-grow="288"
data-flex-basis="693px"
>&lt;/p>
&lt;p>预训练包括 2 个阶段:&lt;/p>
&lt;ol>
&lt;li>Pre-training: 使用网页数据进行训练&lt;/li>
&lt;li>Mid-training: 加入 code, math, science 数据进行训练，在这个阶段，作者使用了 repo-level 的 code 数据，合成的 reasoning 数据以及长上下文数据。作者将模型上下文从 4K 扩展到 32K，然后在扩展到 128K.&lt;/li>
&lt;/ol>
&lt;p>作者在 pre-training 的时候使用了 random truncation, 在 mid-training 的时候使用了 best-fit packing 技巧&lt;/p>
&lt;p>训练时，与 Kimi-k2 一样，作者使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-moonlight/" target="_blank" rel="noopener"
>Muon&lt;/a> 作为优化器。作者使用了 cosine decay schedule. batch size 从 16M token 到 64M token.&lt;/p>
&lt;h2 id="post-training">Post-training
&lt;/h2>&lt;p>Post-training 分为两个阶段：&lt;/p>
&lt;ul>
&lt;li>Stage 1, Expert Training. 构建 agent, reasoning, General chat 三个 domain 的专家模型&lt;/li>
&lt;li>Stage 2, Unified Training. 使用 self-distillation 来汇总多个模型的能力&lt;/li>
&lt;/ul>
&lt;p>训练框架如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-post-training-framework.png"
width="1337"
height="1001"
srcset="https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-post-training-framework_hu6853955760118926496.png 480w, https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-post-training-framework_hu15055473525730948009.png 1024w"
loading="lazy"
alt="Post-training of GLM4.5"
class="gallery-image"
data-flex-grow="133"
data-flex-basis="320px"
>&lt;/p>
&lt;h3 id="sft">SFT
&lt;/h3>&lt;p>两个 stage 都由 SFT 开始，&lt;/p>
&lt;ul>
&lt;li>在 Stage 1 里，SFT 的目标是让 expert model 掌握初步的 chat, reasoning 以及 tool-use 的能力。作者使用了一小部分包含 CoT 的 SFT 数据进行训练&lt;/li>
&lt;li>在 Stage 2 中，SFT 的目标是将不同的 expert model 蒸馏到一个模型中，作者使用了百万级的数据，包含 reasoning 任务和通用的 chat 数据，来训练模型的 hybrid reasoning 能力&lt;/li>
&lt;/ul>
&lt;p>在训练模型的 tool-use 能力是，作者发现，function call 在 code 场景下会出现混淆，提高了模型的学习成本。因此，作者的解决方法是使用了类似 XML 的 special token tags&lt;/p>
&lt;blockquote>
&lt;p>Recall
与之相反，Kimi-K2 认为模板应该尽可能简洁，因此 Kimi 采取了 TypeScript 作为 function call 的语言&lt;/p>
&lt;/blockquote>
&lt;p>从专家模型进行采样是，作者进行了数据过滤。还对数据进行了分级结果发现，使用难题进行训练可以提升模型 $2%\sim4%$ 的表现，多次采样也可以提高模型的表现&lt;/p>
&lt;p>Agentic SFT 数据的构建包括四个步骤：&lt;/p>
&lt;ol>
&lt;li>Agentic Framework and Tool Collection: 收集 MCP 和 tool API&lt;/li>
&lt;li>Task Synthesis: 合成不同的 agentic 任务&lt;/li>
&lt;li>Trajectory Generation: 采样生成的 rollout&lt;/li>
&lt;li>Quality Filtering: 过滤低质量的数据&lt;/li>
&lt;/ol>
&lt;h3 id="rl">RL
&lt;/h3>&lt;h4 id="reasoning-rl">Reasoning RL
&lt;/h4>&lt;p>这个阶段使用了 GRPO 算法进行训练，与 &lt;a class="link" href="https://maosong.website/p/notes-on-dapo/" target="_blank" rel="noopener"
>DAPO&lt;/a> 一样，作者去除了损失函数中的 KL divergence。&lt;/p>
&lt;p>首先，作者探究了课程学习对模型表现的影响，结果发现，课程学习可以有效提高模型的性能。因此，作者构建了一个 2 阶段的课程学习框架。实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-curriculum-RL.png"
width="995"
height="504"
srcset="https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-curriculum-RL_hu15212439347355957989.png 480w, https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-curriculum-RL_hu10130863368084009164.png 1024w"
loading="lazy"
alt="Performance of curriculum RL"
class="gallery-image"
data-flex-grow="197"
data-flex-basis="473px"
>&lt;/p>
&lt;p>可以看到，在第二个阶段，模型可以进一步通过更难的题目获得提升。&lt;/p>
&lt;p>其次，作者探究了以下渐进式扩展模型上下文对模型表现的影响。DeepScaleR 认为，逐步提高模型的上下文长度，可以有效提高模型的表现。但是，本文确认为这种方法会损害模型的性能，原因在于，模型在 SFT 阶段的上下文长度就是 64K, 如果我们降低模型的上下文长度，这会导致训练数据分布不一致，从而影响模型的长上下文表现。因此作者直接在 64K 的上下文上进行训练。&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-multi-stage-context-extension.png"
width="1159"
height="453"
srcset="https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-multi-stage-context-extension_hu8500596761537484178.png 480w, https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-multi-stage-context-extension_hu4453680687905474818.png 1024w"
loading="lazy"
alt="Ablation on progressive context extension"
class="gallery-image"
data-flex-grow="255"
data-flex-basis="614px"
>&lt;/p>
&lt;p>接下来，作者探究了以下采样温度对模型表现的影响，温度太低会导致模型探索能力下降，太高的话会导致输出质量下降。因此作者动态调整采样温度来平衡模型的性能以及探索能力。&lt;/p>
&lt;blockquote>
&lt;p>[!tip]
Kimi-K2 认为随着 RL 训练的进行，我们应该逐步降低采样温度来稳定模型的表现&lt;/p>
&lt;/blockquote>
&lt;p>最后，作者分析了以下 code 以及 Science RL 中的一些问题。对于 code RL, 作者发现，我们应该在 sequence 层面而不是 token 层面进行平均。对于 Science RL, 作者强调了高质量数据的重要性。实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-code-science-RL.png"
width="1146"
height="450"
srcset="https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-code-science-RL_hu12963305608509671776.png 480w, https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-code-science-RL_hu15562279798747579185.png 1024w"
loading="lazy"
alt="Ablation on Code and Science RL"
class="gallery-image"
data-flex-grow="254"
data-flex-basis="611px"
>&lt;/p>
&lt;h4 id="agent-rl">Agent RL
&lt;/h4>&lt;p>作者主要关注 web-search 以及 code generation 两个任务。对于 web-search, 作者构建了一个数据合成 pipeline, 用于生成 multi-step reasoning 的 QA 数据。构建过程包括基于知识图谱的 multi-hop reasoning 和 human-in-the-loop 的内容提取。对于 code generation, 作者基于 GitHub 的 PR 以及 issues 构建了 benchmark&lt;/p>
&lt;p>RL 的训练目标如下&lt;/p>
$$
\mathcal{L}(\theta) = \mathbb{E}_{x\sim\mathcal{D}}\left[\frac1K\sum_{i=1}^K(r(x,y_i) - \bar{r}(x))\right]
$$&lt;p>其中 $(x,y_i)$ 是基于 $\pi_{\mathrm{old}}$ 采样的 trace, $\bar{r}(x) = 1/k\sum_{i=1}^Kr(x,y_i)$ 是平均的 reward. 计算损失时，只有模型的回答参与计算。&lt;/p>
&lt;p>作者发现，通过训练模型的 web-search 以及 code generation 能分，模型在 tool-use 以及 coding 任务上的表现也有了提升。作者还是用了 format penalty 来保证模型输出格式的正确性。如果格式不对的话，模型获得的奖励是 0&lt;/p>
&lt;blockquote>
&lt;p>Recall
在 &lt;a class="link" href="https://maosong.website/p/notes-on-glm-4.1v-thinking/" target="_blank" rel="noopener"
>GLM-4.V-Thinking&lt;/a> 中，作者认为应该在 cold-start SFT 阶段让模型学会格式要求，而不是在 RL 阶段加入 format reward&lt;/p>
&lt;/blockquote>
&lt;p>由于 agent RL 的训练比较耗时，为了提高训练效率。作者首先基于 SFT 模型进行 agent RL 训练，训练到一定步数之后，作者使用 self-distillation 来将能力蒸馏回 SFT model, 接下来再基于 Self-distillation 后的 SFT 模型来进行 agent RL 训练&lt;/p>
&lt;blockquote>
&lt;p>Recall
&lt;a class="link" href="https://maosong.website/p/notes-on-seed1.5-vl/" target="_blank" rel="noopener"
>Seed1.5-VL&lt;/a> 和 LlaMA3.2 都提到了使用 multi-round SFT-RL 的形式来提高模型的表现&lt;/p>
&lt;/blockquote>
&lt;p>作者还发现，随着交互轮数的提升，模型的表现也有相应提升。实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-interaction-turns.png"
width="696"
height="428"
srcset="https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-interaction-turns_hu12097979109763063333.png 480w, https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-interaction-turns_hu17021798138852578449.png 1024w"
loading="lazy"
alt="Interaction turn scaling"
class="gallery-image"
data-flex-grow="162"
data-flex-basis="390px"
>&lt;/p>
&lt;h4 id="general-rl">General RL
&lt;/h4>&lt;p>General RL 用于提高模型的整体表现，解决潜在的问题以及提升关键能力，作者主要使用了 RLHF 和 RLAIF 两种方法&lt;/p>
&lt;p>对于 Holistic RL, 作者收集了 5000 条 prompt, reward 基于人类反馈和 AI 反馈。人类反馈用于训练一个 reward model, 对于 AI 反馈，作者构建了 scoring rubrics. 然后作者将两种反馈结合在一起&lt;/p>
&lt;p>对于 Instruction following RL, 作者构建了基于规则的奖励，reward model 的奖励以及 critical model 的奖励。实验结果显示，这种奖励方式可以有效降低模型的 reward hacking&lt;/p>
&lt;p>对于 function calling RL, 作者使用了 step-wise rule-based RL 来提高模型表现。对于 end-to-end multi-turn RL, 作者训练了一个 expert model 来蒸馏专家到模型。&lt;/p>
&lt;p>最后，对于 Pathology RL, 作者希望通过 RL 来解决潜在的问题，比如语言混合输出，重复输出以及格式错误等。作者构建了一批模型容易出错的数据，然后来训练模型。&lt;/p>
&lt;h4 id="infra">Infra
&lt;/h4>&lt;p>作者针对不同任务分别构建了不同的 scheduling 模式：&lt;/p>
&lt;ul>
&lt;li>对于通用 RL 任务，作者将 training engine 和 inference engine 放在一个 worker 来提高效率&lt;/li>
&lt;li>对于 agentic RL 任务，作者将 training 和 inference engine 分开，来提高 data throughput&lt;/li>
&lt;/ul>
&lt;p>在训练时，作者使用了 BF16 精度，在推理时，作者使用了 FP8 精度来提高推理效率。&lt;/p>
&lt;p>针对 agentic RL 任务，作者还进行了优化。与 Kimi-k2 类似，作者让 inference engine 持续产出 rollout, 然后让 training engine 来更新模型权重，最后同步到 inference engine 上&lt;/p>
&lt;h2 id="experiments">Experiments
&lt;/h2>&lt;p>整体表现如下图所示，GLm4.5 在 ARC benchmark 上的平均表现达到了第三名。&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-average_performance.png"
width="1071"
height="776"
srcset="https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-average_performance_hu7451238329626406108.png 480w, https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-average_performance_hu1953441247019446580.png 1024w"
loading="lazy"
alt="Average performance on ARC benchmarks"
class="gallery-image"
data-flex-grow="138"
data-flex-basis="331px"
>&lt;/p>
&lt;p>具体来看，&lt;/p>
&lt;ol>
&lt;li>在 agentic benchmark 上, GLM4.5 仅次于 o3 的表现&lt;/li>
&lt;li>在 coding benchmark 上，GLM4.5 次于 Claude Opus 4 和 Claude Sonnet 4, 排第三名&lt;/li>
&lt;li>在通用能力上，GLM&lt;/li>
&lt;/ol>
&lt;p>人工对比 coding agent 能力的结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-agent-coding-performance.png"
width="1143"
height="514"
srcset="https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-agent-coding-performance_hu12062937037509217689.png 480w, https://maosong2022.github.io/p/notes-on-glm-4.5/GLM4.5-agent-coding-performance_hu10453388556220715451.png 1024w"
loading="lazy"
alt="Comparison of GLM4.5 aginst other models"
class="gallery-image"
data-flex-grow="222"
data-flex-basis="533px"
>&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者提出了 GLM4.5， 一个基于 MoE 架构的大语言模型系列，包含 GLM4.5(355B-A22B) 和 GLM4.5-Air(106B-A12B) 两个模型，作者详细介绍了模型的架构，训练，数据和评估。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2508.06471" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Kimi-k2</title><link>https://maosong2022.github.io/p/notes-on-kimi-k2/</link><pubDate>Thu, 24 Jul 2025 10:56:50 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-kimi-k2/</guid><description>&lt;p>Kimi-k2 是一个总参数为 1T, 激活参数为 32B 的 MoE 大语言模型，模型使用 15.5T token 进行训练，optimizer 使用了 MuonClip. 作者主要关注模型的 agent 能力&lt;/p>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>作者首先强调现在 LLM 发展主要是 Agentic Intelligence, 也就是让模型自主感知，规划，思考和与环境交互。&lt;/p>
&lt;p>基于这个目标，作者就提出了 Kimi K2, 一个 1.04T 总参数，32B 激活参数的 MoE 模型，用于解决实现 agent Intelligence 中遇到的问题。作者主要进行了三点改进：&lt;/p>
&lt;ol>
&lt;li>MuonClip, 一个基于 &lt;a class="link" href="https://maosong.website/p/notes-on-moonlight/" target="_blank" rel="noopener"
>Muon&lt;/a> 的优化算法，来提高 Kimi K2 对 token 的利用效率以及提高训练的稳定性&lt;/li>
&lt;li>大规模的 agentic 数据合成 pipeline: 作者构建了一个用于合成工具调用，agent 数据的 pipeline&lt;/li>
&lt;li>通用的 RL 框架，作者将 RLVR 和 self-critic rubric reward mechanism 结合起来，用于提升模型的表现&lt;/li>
&lt;/ol>
&lt;h2 id="pre-training">Pre-training
&lt;/h2>&lt;h3 id="architecture">Architecture
&lt;/h3>&lt;p>Kimi-K2 的架构与 DeepSeek-V3 相似，配置如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>指标&lt;/th>
&lt;th>DeepSeek-V3&lt;/th>
&lt;th>Kimi K2&lt;/th>
&lt;th>$\Delta$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td># Layers&lt;/td>
&lt;td>61&lt;/td>
&lt;td>61&lt;/td>
&lt;td>=&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Total Parameters&lt;/td>
&lt;td>$671\text{B}$&lt;/td>
&lt;td>$1.04\text{T}$&lt;/td>
&lt;td>$\uparrow 54%$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Activated Parameters&lt;/td>
&lt;td>$37\text{B}$&lt;/td>
&lt;td>$32.6\text{B}$&lt;/td>
&lt;td>$\downarrow 13%$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Experts (total)&lt;/td>
&lt;td>256&lt;/td>
&lt;td>384&lt;/td>
&lt;td>$\uparrow 50%$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Experts Active per Token&lt;/td>
&lt;td>8&lt;/td>
&lt;td>8&lt;/td>
&lt;td>=&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Shared Experts&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;td>=&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Attention Heads&lt;/td>
&lt;td>128&lt;/td>
&lt;td>64&lt;/td>
&lt;td>$\downarrow 50%$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Number of Dense Layers&lt;/td>
&lt;td>3&lt;/td>
&lt;td>1&lt;/td>
&lt;td>$\downarrow 67%$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Expert Grouping&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>No&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>与 DeepSeek-V3 相比，模型主要进行了以下改动：&lt;/p>
&lt;ol>
&lt;li>作者认为提高专家的稀疏性，可以有效提高模型表现。因此作者将专家个数提升了 50%.&lt;/li>
&lt;li>为了降低模型在 inference 阶段的算力开销，作者将 attention heads 的个数降低了 50%.&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Sparsity Scaling Law&lt;/strong>
作者首先构建了基于 Muon 的 sparsity law, 这里 sparsity 定义为激活专家个数与总专家个数之比，即：&lt;/p>
$$
\mathrm{sparsity} = \frac{\# \mathrm{activated\ experts}}{\# \mathrm{total\ experts}}
$$&lt;p>作者在小规模上的实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-K2-sparsity-scaling-law.png"
width="673"
height="678"
srcset="https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-K2-sparsity-scaling-law_hu12013290483963777320.png 480w, https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-K2-sparsity-scaling-law_hu15866147703512200734.png 1024w"
loading="lazy"
alt="Sparsity Scaling Law"
class="gallery-image"
data-flex-grow="99"
data-flex-basis="238px"
>&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Observation&lt;/strong>
实验结果表明，在相同算力（激活专家个数）下，模型的表现随 sparsity 提高而增加。&lt;/p>
&lt;/blockquote>
&lt;p>但是，进一步提高 sparsity, 会让 infra 变的难以优化，因此在 Kimi-K2 里，将 sparsity 定义为 $48$.&lt;/p>
&lt;p>&lt;strong>Number of attention heads&lt;/strong>
作者还分析了探究了 attention heads 的最优配置。DeepSeek-V3 将 attention heads 的个数设置为 layers 层数的 2 倍，来提高带宽利用率以及提高计算效率。但是，当上下文长度增加之后，attention head 变成了 computational bound. 作者对比了不同配置模型的表现，实验结果如下：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-K2-attention-heads.png"
width="671"
height="675"
srcset="https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-K2-attention-heads_hu8958375214150279472.png 480w, https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-K2-attention-heads_hu10448179446271388634.png 1024w"
loading="lazy"
alt="Scaling curves for attention heads"
class="gallery-image"
data-flex-grow="99"
data-flex-basis="238px"
>&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Observation&lt;/strong>
实验结果表明，使用双倍 attention head 带来的收益是比较小的，只有 $0.5%$ 到 $1.2%$ 左右&lt;/p>
&lt;/blockquote>
&lt;p>因此，作者在 Kimi-K2 中，奖 attention head 的个数设置为 $64$.&lt;/p>
&lt;h3 id="data">Data
&lt;/h3>&lt;p>Kimi-K2 主要强调了 token efficiency, 即每个 token 对模型表现提升的贡献。token efficiency 越高，说明每个 token 对最终模型的贡献也就越高&lt;/p>
&lt;p>相比于 &lt;a class="link" href="https://maosong.website/p/notes-on-kimi-k1.5/" target="_blank" rel="noopener"
>Kimi-k1.5&lt;/a>, Kimi-K2 使用了一个 rephrasing pipeline, 来提高高质量 token 的利用率，作者在 knowledge 和 mathematics domain 上进行了实验。&lt;/p>
&lt;p>最终，Kimi-K2 的预训练数据包括了 &lt;strong>15.5T&lt;/strong> token, 主要覆盖 Web text, code, mathmatics 以及 Knowledge 四个 domain. 大部分的数据处理与 Kimi-k1.5 相同。&lt;/p>
&lt;p>&lt;strong>Knowledge Data Rephrasing&lt;/strong>
作者构建了一个 synthetic rephrasing 框架来提高 knowledge token 的利用率，框架主要包含以下几个模块：&lt;/p>
&lt;ol>
&lt;li>Style and perspective-diverse prompting: 作者构建了一系列 prompt, 来让 LLM 从多个角度和风格来重新表述原始文本&lt;/li>
&lt;li>Chunk-wise auto-regressive generation: 为了保持模型在长文档中的 coherence 以及避免信息损失，作者使用了一个 chunk-based rewriting 策略，也就是对每个 chunk 分别进行改写，然后将它门汇总在一起&lt;/li>
&lt;li>Fidelity verification: 作者对原始文本和改写文本进行了 fidelity 检查来保证语义的一致性。&lt;/li>
&lt;/ol>
&lt;p>作者对比了以下三个设置对模型在 SimpleQA benchmark 上表现的影响，这三个设置分别是：&lt;/p>
&lt;ol>
&lt;li>原始数据集训练 10 epoch&lt;/li>
&lt;li>改写数据一次，然后训练 10 epoch&lt;/li>
&lt;li>改写数据一次，训练 1 epoch&lt;/li>
&lt;/ol>
&lt;p>实验结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th># Rephrasings&lt;/th>
&lt;th># Epochs SimpleQA&lt;/th>
&lt;th>Accuracy&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>0 (raw wiki-text)&lt;/td>
&lt;td>10&lt;/td>
&lt;td>23.76&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>10&lt;/td>
&lt;td>27.39&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>1&lt;/td>
&lt;td>28.94&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，改写的策略可以有效提高模型在 SimpleQA benchmark 上的表现&lt;/p>
&lt;p>&lt;strong>Math Data Rephrasing&lt;/strong>
对于数学相关数据，作者基于 SwallowMath, 将数据改写成了学习笔记 (learning-note) 的风格。然后，作者还将其他语言的高质量文本翻译为英文。&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Recall&lt;/strong>
个人认为，数据改写在一定程度上也算是合成数据，对于合成数据这一块，微软提出的 phi 系列是一个比较好的参考&lt;/p>
&lt;/blockquote>
&lt;h3 id="muonclip-optimizer">MuonClip Optimizer
&lt;/h3>&lt;p>Kimi-K2 使用了 Muon optimizer, 之前的实验结果说明了在相同的 compute budget 下，Muon optimizer 的表现超过了 AdamW. 也就是说，Muon optimizer 更加高效。&lt;/p>
&lt;p>但是，Muon optimizer 的问题是训练不稳定。为了解决这个问题，作者提出了 QK-clip, 一个 weight clipping 机制，来显示约束 attention logits. QK-clip 的机制是&lt;strong>当输出的 logits 超过某一个阈值之后，就对齐进行截断&lt;/strong>。&lt;/p>
&lt;p>每个 head 的 attention 的计算公式如下&lt;/p>
$$
O = \mathrm{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V
$$&lt;p>其中, $d$ 是 hidden size, $Q, K,V$ 分别是 query, key, value, 定义如下：&lt;/p>
$$
Q = XW_Q, K=XW_K, V=XW_V
$$&lt;p>这里 $W_Q,W_K,W_V$ 是模型可学习的参数。&lt;/p>
&lt;p>作者定义每个 head 的 max logit 如下：&lt;/p>
$$
S_{\max}^h = \frac{1}{\sqrt{d}} \max_{X\in\mathcal{B}}\max_{i,j} [QK^T]_{ij}
$$&lt;p>最简单的做法就是直接进行截断，也就是&lt;/p>
$$
W_Q\gets \gamma^\alpha W_q, W_K\gets \gamma^{1-\alpha}W_K
$$&lt;p>其中 $\gamma=\min(1, \tau S_{\max})$, 这里 $S_{\max}=\max_h S_{\max}^h$ 是所有 head 对应 $S_{\max}^h$ 的最大值。&lt;/p>
&lt;p>但是，实际中，作者发现只有少部分 head 会出现 logits 爆炸现象。为了提高计算效率，作者针对每个 head 单独进行 scaling, 也就是 $\gamma_{h}=\min(1, \tau S_{\max}^h)$. 对于 MLA 架构，作者仅在 unshared 模块使用 clipping:&lt;/p>
&lt;ul>
&lt;li>$q^c$ 以及 $k^c$, scaling factor 为 $\sqrt{\gamma_h}$&lt;/li>
&lt;li>$q^R$, scaling factor 为 $\gamma_h$&lt;/li>
&lt;/ul>
&lt;p>最后，作者将 Muon, weight decay, RMS matching 和 QK-clip 汇总在一起，得到 Kimi-k2 使用的 MuonClip optimizer, 算法如下所示：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-muonclip.png"
width="1385"
height="668"
srcset="https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-muonclip_hu15126210376126559992.png 480w, https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-muonclip_hu12041395682263694561.png 1024w"
loading="lazy"
alt="MuonClip Optimizer"
class="gallery-image"
data-flex-grow="207"
data-flex-basis="497px"
>&lt;/p>
&lt;p>接下来，作者对比了以下 Muon 和 MuonClip 两个优化器的表现，作者分别使用这两个优化器训练 53B-A9B 的 MoE 模型，实验结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-muonclip-performance.png"
width="1365"
height="551"
srcset="https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-muonclip-performance_hu6393944337521741241.png 480w, https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-muonclip-performance_hu16210125810578172496.png 1024w"
loading="lazy"
alt="Comparison between Muon and MuonClip"
class="gallery-image"
data-flex-grow="247"
data-flex-basis="594px"
>&lt;/p>
&lt;p>实验结果表明，Muon 优化器在 1000 步左右就出现了 logits 爆炸的现象，但是 MuonClip 通过优化可以避免这个问题。作者还发现，MuonClip 可以减少 loss spike 的产生，整体的 loss 变化情况如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-model-training-dynamics.png"
width="977"
height="561"
srcset="https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-model-training-dynamics_hu4565157971618681387.png 480w, https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-model-training-dynamics_hu4579571635122099452.png 1024w"
loading="lazy"
alt="Training loss curve"
class="gallery-image"
data-flex-grow="174"
data-flex-basis="417px"
>&lt;/p>
&lt;p>作者还在附录中说明 QK-clip 并不影响最终的收敛性，并且，为了尽可能减少对模型训练的干扰，作者发现，仅在训练初期 QK-clip 被激活:&lt;/p>
&lt;ol>
&lt;li>在初始的 70, 000 步里，有 12.7% 的 attention heads 至少触发了一次 QK-clip, 并且将它们的 $S_\max$ 降到了 100 以下&lt;/li>
&lt;li>接下来的 70, 000 步里，QK-clip 就不再被激活&lt;/li>
&lt;/ol>
&lt;h3 id="infra">Infra
&lt;/h3>&lt;p>Kimi-k2 使用了 16-way 的 PP, 16-way 的 EP 以及 ZeRO-1 Data Parallelism. 具体流程如下&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-communication-computation.png"
width="1362"
height="318"
srcset="https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-communication-computation_hu9768363041919262943.png 480w, https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-communication-computation_hu17009654877161129053.png 1024w"
loading="lazy"
alt="Kimi-K2 parallelism"
class="gallery-image"
data-flex-grow="428"
data-flex-basis="1027px"
>&lt;/p>
&lt;p>作者发现通过增加 warm-up mircro-batches, 我们可以有效重叠 EP 的 all-to-all communication 以及 computation. 但是，对于 [[DualPipe]], 其要求双倍的存储来保存参数和梯度，并且提高 PP 粒度会引入更多的 bubble, 因此 Kimi-K2 没有使用 DualPipe.&lt;/p>
&lt;p>为了减少 [[1F1B]] 的 PP 通信开销，作者在反向传播的过程中同时进行 PP 通信，来进一步重合通信与计算。&lt;/p>
&lt;p>作者还发现，使用更小的 EP group 可以提高整体的训练速度，因此作者将 EP 设置为 16.&lt;/p>
&lt;p>作者发现，剩余的 GPU 内存不足以存放 MoE 的 activation, 因此作者采取了三个办法解决这个问题：&lt;/p>
&lt;ol>
&lt;li>Selective recomputation: 作者对 LayerNorm, SwiGLU, MLA 的 up-projection 和 MoE 的 down-projections 等进行重新计算。&lt;/li>
&lt;li>FP8 storage for insentive activations: 作者使用了 FP8 精度来存储 MoE up-projections 以及 SwiGLU. 作者发现使用 FP8 进行计算可能会导致性能下降，因此作者并没有使用 FP8 进行计算。&lt;/li>
&lt;li>Activation CPU offload: 作者将其余的 activation 放在 CPU RAM 上，在 1F1B 的过程中，作者再进行加载&lt;/li>
&lt;/ol>
&lt;h3 id="training-recipe">Training Recipe
&lt;/h3>&lt;p>模型上下文长度为 4096, 使用 MuonClip 进行优化，使用 WSD lr Scheduler.weight decay 为 0.1, global batch size 为 67M tokens.&lt;/p>
&lt;p>预训练结束之后，作者加入了一个 long-context activation stage. 这个阶段，作者使用了 400B 的 4K 上下文的 token 和 60B 的 32K 上下文的 60B token. 最后作者使用 &lt;a class="link" href="https://maosong.website/p/notes-on-yarn/" target="_blank" rel="noopener"
>YARN&lt;/a> 将模型上下文长度扩展到 128K.&lt;/p>
&lt;h2 id="post-training">Post-training
&lt;/h2>&lt;h3 id="sft">SFT
&lt;/h3>&lt;p>数据的构建主要是基于：&lt;/p>
&lt;ol>
&lt;li>prompt 的多样性&lt;/li>
&lt;li>Response 的质量&lt;/li>
&lt;/ol>
&lt;p>作者构建了一系列的数据生成 pipeline, 然后使用 Kimi-k1.5 来生成多样化的回答，最后再进行质量评估和过滤。这里，作者主要介绍了一下 tool-use 数据的构建&lt;/p>
&lt;p>受 ACEBench 启发，作者构建了一个模仿真实世界 tool-use 的数据合成 pipeline. pipeline 如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-tool-use-synthetic-pipeline.png"
width="1366"
height="439"
srcset="https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-tool-use-synthetic-pipeline_hu8910712181245528206.png 480w, https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-tool-use-synthetic-pipeline_hu2723101591411375395.png 1024w"
loading="lazy"
alt="Data synthesis pipeline for tool use"
class="gallery-image"
data-flex-grow="311"
data-flex-basis="746px"
>&lt;/p>
&lt;p>pipeline 主要包含三个阶段：&lt;/p>
&lt;ul>
&lt;li>tool spec generation: 作者基于 MCP tools 和 self-evolved 的数据合成策略来构建 tool repository. MCP tools 包括 3000+ 的 tools,合成了 20，000 个 tools&lt;/li>
&lt;li>Agent and task generation: 作者还用不同的 system prompt 以及不同的 tools combination 来构建对应的 agent. 然后对不同的 agent, 作者构建了对应的成功标准均，工具调用模式以及评估方式&lt;/li>
&lt;li>Trajectory generation: 作者首先构建不同风格的 LLM, 然后作者使用一个 simulator 来执行 tool call 并给予反馈。&lt;/li>
&lt;/ul>
&lt;p>最后，作者对数据进行了过滤。&lt;/p>
&lt;p>作者还加入了 coding 以及软件工程等任务相关数据来进一步提高模型的 agent 能力&lt;/p>
&lt;h3 id="rl">RL
&lt;/h3>&lt;p>RL 与 Kimi-K1.5 差不多。作者进一步提高了 RL 阶段的算力并做出了亮点改进：&lt;/p>
&lt;ol>
&lt;li>作者构建了类似 Gym 的框架，用于扩展 RL 的能力&lt;/li>
&lt;li>作者加入了更多 RLVR 的任务&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Data&lt;/strong>
数据主要包括以下几类：&lt;/p>
&lt;ol>
&lt;li>Math, STEM and logical tasks: 数据构建的原则为多样化和中等难度&lt;/li>
&lt;li>Instruction following: 作者基于 hybrid rule verification 和 multi-source instruction generation 来合成复杂的指令跟随数据&lt;/li>
&lt;li>Faithfulness: 作者训练了一个 judge model 来提供 reward&lt;/li>
&lt;li>Coding &amp;amp; Software Engineering: 作者从开源数据收集并合成了代码相关数据&lt;/li>
&lt;li>Safety. 提高模型的安全性，防止 jailbreak&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Reward&lt;/strong>
作者使用了 self-critique rubric reward 的奖励机制。&lt;/p>
&lt;p>首先，对于 Kimi-k2 的回答，作者会使用另一个 kimi-k2 作为 critic，基于三个方面进行排序：&lt;/p>
&lt;ul>
&lt;li>core rubric: AI 的核心价值观&lt;/li>
&lt;li>prescriptive rubric: 避免 reward hacking&lt;/li>
&lt;li>human-annotated rubric: 特定的上下文&lt;/li>
&lt;/ul>
&lt;p>在训练的过程中，critic 也会基于 verifiable signals 进行 refine&lt;/p>
&lt;p>&lt;strong>RL training&lt;/strong>
RL 的训练目标与 Kimi-k1.5 相同&lt;/p>
$$
\mathcal{L}(\pi_\theta) = \mathbb{E}_{x\sim \mathcal{D}}\left[\frac1K\sum_{i=1}^K \left(r(x,y_i)-\bar{r}(x) -\tau\log\frac{\pi_{\mathrm{\theta}}(y_i\mid x)}{\pi_{\mathrm{old}}(y_i\mid x)}\right)^2\right]
$$&lt;p>其中 $\bar{r}(x)=1/K\sum_{i=1}^Kr(x,y_i)$ 是 sample response 的平均奖励。&lt;/p>
&lt;p>作者做了以下几点改进来提高模型在不同 domain 上的表现：&lt;/p>
&lt;ol>
&lt;li>Budget control: 作者针对不同任务分别设置了最大 token 限制，模型超过这个限制会受到惩罚。猜测应该是 length penalty 或者类似 Qwen3 一样，直接中断思考过程输出最终答案&lt;/li>
&lt;li>PTX loss: 作者使用了 PTX loss 来提高模型对于高质量数据的利用率以及降低模型的过拟合&lt;/li>
&lt;li>Temperature Decay: 作者发现，训练后期保持较高的采样温度会影响模型的表现，因此作者设置了一个 schedule, 来逐步降低采样温度。&lt;/li>
&lt;/ol>
&lt;h3 id="rl-infra">RL Infra
&lt;/h3>&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-RL-infra.png"
width="734"
height="477"
srcset="https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-RL-infra_hu7836952951472259143.png 480w, https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-RL-infra_hu929938601854115055.png 1024w"
loading="lazy"
alt="Kimi-K2 RL infra"
class="gallery-image"
data-flex-grow="153"
data-flex-basis="369px"
>&lt;/p>
&lt;p>RL infra 与 Kimi-k1.5 类似。主要包含三个模块。其中 train engine 和 inference engine 两者互相切换，一个 engine 进行 training 的时候，另一个 engine 就进行 inference, 为了提高 engine 切换的效率，作者使用了 checkpoint engine 来传输更新后的参数。&lt;/p>
&lt;h2 id="evaluation">Evaluation
&lt;/h2>&lt;p>模型评估结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-performance.png"
width="696"
height="979"
srcset="https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-performance_hu17285728062120188885.png 480w, https://maosong2022.github.io/p/notes-on-kimi-k2/Kimi-k2-performance_hu8344921060842393810.png 1024w"
loading="lazy"
alt="Performance of Kimi-k2"
class="gallery-image"
data-flex-grow="71"
data-flex-basis="170px"
>&lt;/p>
&lt;p>评估结果显示，模型在 coding 和通用任务上的表现仅次于 Claude 4, 大部分 benchmark 上都是第二名甚至是 sota.&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 Kimi-K2, 一个总参数为 1B 的 MoE 大语言模型。作者在架构，数据和优化器上进行了创新。并且通过 post-training 显著提高了模型的 agent 以及 tool-use 能力&lt;/p>
&lt;p>作者发现模型主要存在的问题有：&lt;/p>
&lt;ol>
&lt;li>reasoning 任务过难或者 tool 的定义不清晰的时候，模型会使用很多 token&lt;/li>
&lt;li>有时候工具调用可能会降低模型的表现&lt;/li>
&lt;li>模型在 agentic coding 任务上的能力需要进一步提升&lt;/li>
&lt;/ol>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://github.com/MoonshotAI/Kimi-K2" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Keye-VL</title><link>https://maosong2022.github.io/p/notes-on-keye-vl/</link><pubDate>Wed, 23 Jul 2025 11:11:43 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-keye-vl/</guid><description>&lt;p>Keye-VL 是快手在 25 年 7 月份提出的一个 8B 的多模态大模型，其亮点为短视频理解能力。预训练包括 4 个 stage，使用了 600B token，后训练包括 2 个 stage，用于提升模型的 reasoning 和 non-reasoning 能力。&lt;/p>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>作者首先回顾了已有的 MLLM 工作，然后强调理解短视频仍然是一个很难的任务，特别是要求模型基于 video 和 audio 来理解视频。因此，在本文中，作者提出了 Kwai Keye-VL，一个 8B 的多模态大模型，主要用于短视频理解任务。&lt;/p>
&lt;h2 id="method">Method
&lt;/h2>&lt;h3 id="architecture">Architecture
&lt;/h3>&lt;p>Keye-VL 是一个标准的 ViT-MLP-LLM 的架构，其中，ViT 是 SigLIP-400M-384-14, MLP 是一个基于 SwiGLU 的 2 层 MLP，使用了和 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5-vl/" target="_blank" rel="noopener"
>Qwen2.5-VL&lt;/a> 一样的 patch merge 方法，LLM 使用的是 Qwen3-8B，模型架构示意图如下&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-keye-vl/Keye-VL-architecture.png"
width="1371"
height="1000"
srcset="https://maosong2022.github.io/p/notes-on-keye-vl/Keye-VL-architecture_hu8715583027172451106.png 480w, https://maosong2022.github.io/p/notes-on-keye-vl/Keye-VL-architecture_hu3357172034037663162.png 1024w"
loading="lazy"
alt="Keye-VL model architecture"
class="gallery-image"
data-flex-grow="137"
data-flex-basis="329px"
>&lt;/p>
&lt;p>作者针对 ViT 和 visual encoding 分别做了如下改进&lt;/p>
&lt;h4 id="navit">NaViT
&lt;/h4>&lt;p>作者实现了 native resolution ViT，来处理不同分辨率的图片。&lt;/p>
&lt;p>具体做法为，作者基于 SigLIP-400M-384-14 来初始化 ViT。&lt;/p>
&lt;p>然后，作者首先采用了 interpolation 来将 ViT 的 position encoding 扩展到不同的图片精度下面去。&lt;/p>
&lt;p>接下来，作者提出了 2D RoPE 来进一步提升 position encoding 的表现。&lt;/p>
&lt;p>最后，作者加入了 NaViT 的 packing 技巧来继续预训练 ViT.&lt;/p>
&lt;p>在 ViT 预训练的过程中，作者使用了 &lt;strong>500B&lt;/strong> 的 token&lt;/p>
&lt;h4 id="visual-encoding">Visual Encoding
&lt;/h4>&lt;p>为了提升模型理解图片和视频的能力，作者针对图片和视频进行了进一步的处理。&lt;/p>
&lt;p>对于不同精度的图片，作者将最大 token 个数设置为 16384。&lt;/p>
&lt;p>对于视频，作者将每帧的 token 数限制在 $[128,768]$, 每个视频的最大 token 个数设置为 24576&lt;/p>
&lt;p>对于提取的 frames，作者重新计算了 FPS, 然后在 3D RoPE 中让时间维度与真实时间严格对齐。&lt;/p>
&lt;h3 id="pre-training">Pre-training
&lt;/h3>&lt;h4 id="data">Data
&lt;/h4>&lt;p>预训练数据一共包括 600B token，覆盖了 6 个类别：&lt;/p>
&lt;ul>
&lt;li>Image caption: 包括中英文数据，来源为 LAION, DataComp 以及 Coyo. 作者基于 CLIP 来计算相速度，然后过滤掉相似度比较低的数据，作者还对数据进行了 re-caption，实验发现 re-caption 可以提高模型的细粒度图片理解能力&lt;/li>
&lt;li>OCR &amp;amp; VQA: 数据包括开源数据和合成数据。合成数据包括 Synthesis 和 Rendering 两个方法，第一是基于 text-dense image 构建 OCR 数据以及基于 image-caption pair 数据使用 MLLM 构建 VQA 数据。第二是使用字体渲染工具合成高质量的 OCR 数据&lt;/li>
&lt;li>Grounding &amp;amp; Counting: Grounding 数据主要包括 RefCoCo, VisualGenome, TolokaVQA, Counting 数据包括 PixMo. 作者仍然使用 CLIP 对数据进行过滤&lt;/li>
&lt;li>Interleaved text-image data: 作者发现图文交错数据可以提供通用知识，并且还可以提高模型的视觉语言对齐能力，第三就是提升模型的泛化能力。作者主要从 academic PDF 以及结构化知识中提取对应的数据。作者基于 Garbled character recognition, low-resolution/broken image filtering 以及 text-image similarity validation 来保证数据的质量&lt;/li>
&lt;li>Video understanding: 作者使用 Qwen2.5-omni 从 interleaved video-asr 来将视频数据转化图文交错数据， 然后基于 ASR 的结果进行 recaption，最后对每一帧进行 OCR. 作者还构建了 Frame-level re-ordering 以及 multiple video matching 两个任务来提高模型的上下文理解能力&lt;/li>
&lt;li>Pure Text: 未提及&lt;/li>
&lt;/ul>
&lt;p>对于源数据，作者进行了数据清洗：&lt;/p>
&lt;ol>
&lt;li>使用 CLIP 对数据进行打分，然后过滤掉低质量的数据&lt;/li>
&lt;li>使用开源的 MLLM 作为 discriminator 来选择高质量的数据&lt;/li>
&lt;li>去重&lt;/li>
&lt;/ol>
&lt;h4 id="training-recipe">Training Recipe
&lt;/h4>&lt;p>预训练包括 4 个 stage：&lt;/p>
&lt;ul>
&lt;li>Stage 0: 使用 SigLIP 损失函数来继续训练 ViT&lt;/li>
&lt;li>Stage 1: cross-modal Alignment，仅训练 MLP&lt;/li>
&lt;li>Stage 2: multi-task pre-training, 解冻所有参数，使用 Multi-task 数据来训练模型&lt;/li>
&lt;li>Stage 3: annealing, 在高质量数据集上进行 fine-tune，进一步提升模型的能力&lt;/li>
&lt;/ul>
&lt;p>作者发现，预训练后的模型在下游任务上的表现对训练数据配比非常敏感。为了解决这个问题，在最后一个训练阶段，作者使用了一个 merging 的技巧，来保持模型的能力。&lt;/p>
&lt;h3 id="post-training">Post-training
&lt;/h3>&lt;p>post-training 阶段一共包含了 2 个 step, 5 个 stage, 第一个 step 包含 2 个 stage，用于提升模型的 non-reasoning 能力。第二个 step 包含 3 个 stage, 用于提升模型的 reasoning 能力&lt;/p>
&lt;h4 id="no-reasoning-training">No-reasoning Training
&lt;/h4>&lt;p>第一个 step 是 non-reasoning training, 包含了 SFT 和 &lt;a class="link" href="MPO.md" >MPO&lt;/a> 两个 stage, 训练 pipeline 如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-keye-vl/Keye-VL-non-reasoning-training.png"
width="1340"
height="472"
srcset="https://maosong2022.github.io/p/notes-on-keye-vl/Keye-VL-non-reasoning-training_hu4483349681294792353.png 480w, https://maosong2022.github.io/p/notes-on-keye-vl/Keye-VL-non-reasoning-training_hu13267003824404863093.png 1024w"
loading="lazy"
alt="Non reasoning training pipeline"
class="gallery-image"
data-flex-grow="283"
data-flex-basis="681px"
>&lt;/p>
&lt;p>&lt;strong>SFT&lt;/strong>
SFT 阶段一共使用了 &lt;strong>5M&lt;/strong> 的多模态 QA 样本，为了提升数据的多样性，作者使用 TaskGalaxy 来将数据分类为 70,000 种任务类型，然后作者对每条数据，使用 MLLM 评估问题的难度，来过滤掉过于简单的问题。最后，作者请人类来进行标注，保证数据的可靠性。&lt;/p>
&lt;p>&lt;strong>MPO&lt;/strong>
训练方面，作者使用了 MPO 进行训练。
数据方面，作者使用了：&lt;/p>
&lt;ol>
&lt;li>400,000 开源的数据，作者主要进行了去重以及过滤低质量的数据&lt;/li>
&lt;li>50,000 偏好数据，基于 MM-RLHF 和 MMPR 等数据构建，然后构建高质量的 negative examples&lt;/li>
&lt;li>10,000 条 self-improvement 样本：基于 benchmark 和人类反馈，使用 SFT model 的回答作为 chosen samples, 然后基于 reward model 或者 rule-based rewards 评估模型输出，选择分数最低的座位 rejected samples&lt;/li>
&lt;li>90,000 纯文本样本： in-house data&lt;/li>
&lt;li>30,000 人类标注样本：使用开源和闭源模型进行回答，然后请人类进行排序&lt;/li>
&lt;/ol>
&lt;h4 id="reasoning-training">Reasoning Training
&lt;/h4>&lt;p>第二个 step 是 reasoning training, 包含了 CoT Cold-Start, Mix-Mode RL 和 Iterative Alignment 三个 stage, 训练 pipeline 如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-keye-vl/Keye-VL-reasoning-training.png"
width="1358"
height="596"
srcset="https://maosong2022.github.io/p/notes-on-keye-vl/Keye-VL-reasoning-training_hu3508552858551465121.png 480w, https://maosong2022.github.io/p/notes-on-keye-vl/Keye-VL-reasoning-training_hu15300135858013059770.png 1024w"
loading="lazy"
alt="Non reasoning training pipeline"
class="gallery-image"
data-flex-grow="227"
data-flex-basis="546px"
>&lt;/p>
&lt;p>&lt;strong>CoT cold-start&lt;/strong>
作者收集了如下数据：&lt;/p>
&lt;ul>
&lt;li>330, 000 条 non-reasoning 样本：和第一个 step 的数据分布类似，但是不重合&lt;/li>
&lt;li>230,000 reasoning 样本：作者构建了一个 data construction pipeline,用于保证 long-CoT 的正确性&lt;/li>
&lt;li>20,000 automatic reasoning 样本：作者基于 MMPR, MM-Eureka 以及 OpenR1-math 来收集数据，基于这些模型来训练模型自动化决定是否要进行 reasoning&lt;/li>
&lt;li>100,000 agentic reasoning 样本：训练模型的 &amp;ldquo;think with image&amp;rdquo; 能力。基于 3M QA pairs，作者使用 Qwen2.5-72B 来识别需要进行 manipulating 的问题，然后生成对应的代码。接下来作者构建了一部分 OCR 数据，让模型学会 constrast enhancement 或者 rotation 操作。最后对于数学问题，作者让 Gemini-2.5-Pro 生成对应的思考过程，再让 GPT-4o 将对应的计算转换为可执行的代码。&lt;/li>
&lt;li>32, 000 Video data: 包含了 24,000 条 thinking samples 和 80,000 条 non-thinking samples&lt;/li>
&lt;/ul>
&lt;p>训练时，所有样本混在一起进行训练。作者认为，将日常使用的 SFT 数据和 reasoning 数据放在一起训练，可以保持模型在通用场景下的能力。&lt;/p>
&lt;p>&lt;strong>Mix-Mode RL&lt;/strong>
训练数据主要包括 4 个任务：&lt;/p>
&lt;ol>
&lt;li>Multimodal perception: 复杂文本识别和 counting 任务&lt;/li>
&lt;li>Multimodal reasoning: MMPR 和 MM-Eureka&lt;/li>
&lt;li>Text-based mathematical reasoning: 数学推理问题&lt;/li>
&lt;li>Agentic reasoning: 从 DeepEyes 中获取的 47,000 条样本&lt;/li>
&lt;/ol>
&lt;p>作者使用了 GRPO 来训练，reward 基于 MLLM 进行，包括最终结果和思考过程。&lt;/p>
&lt;p>作者还使用 RL 来提高模型的短视频理解能力。作者发现 RL 训练之后，模型的短视频理解能力有了大幅度的提升。&lt;/p>
&lt;p>&lt;strong>Iterative Alignment&lt;/strong>
这一步主要解决模型的重复性输出，或者 reasoning logic 不对的问题。作者使用了 rejection-sampling 数据，包括 instruction following, OCR, mathematics, charts, counting 等。&lt;/p>
&lt;p>作者基于 Rule-based score 和 model-based score 来进行打分，最后使用 MPO 算法进行训练。通过这个过程，模型的输出格式和动态思考能力都有了提升。&lt;/p>
&lt;h2 id="evaluation">Evaluation
&lt;/h2>&lt;p>作者首先评估了 ViT 的表现，主要有两点：&lt;/p>
&lt;ol>
&lt;li>在 SigLIP 的基础上加入 1D interpolation 之后，模型的表现所有下降，作者认为这是由于 1D 的 position encoding 无法识别 2D 的 patch 排列导致的&lt;/li>
&lt;li>加入 2D RoPE 之后，ViT 与 SigLIP 的表现持平&lt;/li>
&lt;/ol>
&lt;p>接下来是 Keye-VL 在公开 benchmark 上的表现，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-keye-vl/Keye-VL-performance.png"
width="1068"
height="1161"
srcset="https://maosong2022.github.io/p/notes-on-keye-vl/Keye-VL-performance_hu2244647248615998183.png 480w, https://maosong2022.github.io/p/notes-on-keye-vl/Keye-VL-performance_hu12384064493261148122.png 1024w"
loading="lazy"
alt="Performance of Keye-VL"
class="gallery-image"
data-flex-grow="91"
data-flex-basis="220px"
>&lt;/p>
&lt;p>作者还构建了一个内部的 benchmark, 用于进一步评估模型的能力。&lt;/p>
&lt;p>已有 benchmark 的问题：&lt;/p>
&lt;ol>
&lt;li>contamination&lt;/li>
&lt;li>多语种覆盖不足：大部分 benchmark 都是英文的&lt;/li>
&lt;li>任务和 domain 覆盖不足：大部分 benchmark 只考虑基本的 perception 和 reasoning 能力&lt;/li>
&lt;li>任务难度和评估格式单调&lt;/li>
&lt;/ol>
&lt;p>构建 benchmark 的原则：&lt;/p>
&lt;ol>
&lt;li>在中文场景下的真实用户需求，open ended QA, 包括短视频理解能力&lt;/li>
&lt;li>细粒度的评估&lt;/li>
&lt;li>多样性高&lt;/li>
&lt;li>没有 contamination&lt;/li>
&lt;li>多角度评估策略: 正确性，相关性，理解性，流畅性和创造性&lt;/li>
&lt;/ol>
&lt;p>结果如下：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-keye-vl/Keye-VL-performance-internal.png"
width="1339"
height="559"
srcset="https://maosong2022.github.io/p/notes-on-keye-vl/Keye-VL-performance-internal_hu13798451161994777213.png 480w, https://maosong2022.github.io/p/notes-on-keye-vl/Keye-VL-performance-internal_hu3362434457835284134.png 1024w"
loading="lazy"
alt="Performance of Keye-VL on the internal benchmark"
class="gallery-image"
data-flex-grow="239"
data-flex-basis="574px"
>&lt;/p>
&lt;p>分析：&lt;/p>
&lt;ol>
&lt;li>Keye-VL 在 OCR 等任务上的表现有所不足，其细粒度的识别能力也有所不足，会认错人。有时候还会忽略掉一些信息&lt;/li>
&lt;li>描述 temporal action 时会出现不稳定性，模型对镜头的感知能力不足。需要准确定位时间等&lt;/li>
&lt;li>在需要逻辑链条和数学计算时，模型能力不足，对于特定 domain 上的任务会出现事实性错误。写作时，模型倾向于输出通用的回答，而不是定制化的回答。&lt;/li>
&lt;/ol>
&lt;h2 id="discussion">Discussion
&lt;/h2>&lt;p>作者讨论了两点关键发现：&lt;/p>
&lt;ol>
&lt;li>reasoning 和 non-reasoning 的数据可以互相促进彼此的表现，这与 ERNIE 4.5 的发现一致。&lt;/li>
&lt;li>作者认为通过 mix-mode 的训练，模型在简单和复杂任务上的表现都可以提升，因此作者使用了混合数据来进行训练，结果发现效果很好。&lt;/li>
&lt;/ol>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>本文中，作者提出了 Keye-VL 8B，一个短视频理解能力出色的多模态大模型，作者详细介绍了 pre-training 和 post-training. 其中，mix-mode training 可以有效提高模型的表现。&lt;/p>
&lt;p>作者认为 Keye-VL 有如下改进的地方：&lt;/p>
&lt;ol>
&lt;li>并没有优化 video encoder 或者是改进 video encoding 的策略&lt;/li>
&lt;li>Keye-VL 的视觉感知能力有进一步的提升空间，其 &amp;ldquo;reasoning with image&amp;rdquo; 能力依然落后于领先的 reasoning model&lt;/li>
&lt;li>使用一个额外的 MLLM 作为 reward model 会极大消耗算力，如何构建一个更可靠更高效的 reward model 需要进一步探索。&lt;/li>
&lt;/ol>
&lt;h2 id="reference">Reference
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2507.01949" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/Kwai-Keye/Keye/tree/main" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on V-Triune</title><link>https://maosong2022.github.io/p/notes-on-v-triune/</link><pubDate>Thu, 17 Jul 2025 09:37:36 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-v-triune/</guid><description>&lt;p>V-Triune 探究了如何使用一套统一的 RL 训练框架，来同时提高模型的 perception 和 reasoning 能力。&lt;/p>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>已有的 RL 训练方法主要提升模型在特定 domain 上的能力，比如 MM-Eureka, Vision-R1 等专注于提升 MLLM 的 reasoning 能力，而 R1-V, DeepPerception 等专注于提升模型的 perception 能力。&lt;/p>
&lt;p>因此，本文的 motivation 就是，如何用一套统一的 RL 训练框架，来同时提高模型的 perception 和 reasoning 能力。&lt;/p>
&lt;p>V-Triune 包含 3 个关键模块：&lt;/p>
&lt;ul>
&lt;li>Sample-Level data formatting: 即在 sample 的层面定义 reward 等信息&lt;/li>
&lt;li>Verifier-Level Computation: 即分离 verifier 和 generator, 提高整体效率&lt;/li>
&lt;li>Source-Level Metric Monitoring: 基于 data source 来监控模型的表现&lt;/li>
&lt;/ul>
&lt;p>作者还提出了 Dynamic IoU reward, 来提高训练的稳定性。&lt;/p>
&lt;p>基于 V-Triune 和 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5-vl/" target="_blank" rel="noopener"
>Qwen2.5-VL&lt;/a>, 作者提出了 Orsta,一个在 reasoning 和 perception 任务上均有提升的多模态大模型。&lt;/p>
&lt;p>论文的主要贡献为：&lt;/p>
&lt;ol>
&lt;li>提出了 V-Triune, 一个统一的 RL 训练框架，来同时提高模型的 reasoning 和 perception 能力&lt;/li>
&lt;li>在 infra 上进行了改进，提高整体的训练效率和稳定性&lt;/li>
&lt;li>基于 V-Triune, 构建了 Orsta, 相比于 Baseline, 模型的 perception 和 reasoning 能力均有了提升。&lt;/li>
&lt;/ol>
&lt;h2 id="preliminary">Preliminary
&lt;/h2>&lt;p>作者首先回顾了一下 GRPO 和 &lt;a class="link" href="https://maosong.website/p/notes-on-dapo/" target="_blank" rel="noopener"
>DAPO&lt;/a> , 我们这里就不再赘述。&lt;/p>
&lt;p>然后，作者介绍了一下 reward function, reward function 由两部分组成， 第一部分是 format reward, 第二部分是 accuracy reward,.&lt;/p>
&lt;p>对于 format reward, 作者采取了和 OpenR1 相同的做法，也就是要求输出仅包含 1 个给定的 token, 这里特定的 token 为 $s_1=\texttt{&lt;think>}$, $s_2=\texttt{&lt;/think>}$, $s_3=\texttt{&lt;answer>}$, $s_4=\texttt{&lt;/answer>}$, reward 的定义如下：&lt;/p>
$$
R_{\mathrm{format}}(o_q) = 0.25\sum_{i=1}^4 \mathbb{1}(\mathrm{count}(o_q,s_i)=1)
$$&lt;p>这里 $\mathbb{1}(\cdot)$ 是示性函数。&lt;/p>
&lt;p>对于 accuracy reward, 作者根据不同的任务分别进行处理。&lt;/p>
&lt;p>对于 reasoning 任务，reward function 的定义如下&lt;/p>
$$
R_{\mathrm{acc}}(\hat{a},a) = \begin{cases}
1, &amp;\text{ if }\texttt{verify(parse($\hat{a}$), parse($a$))}\text{ is True}\\
0, &amp;\text{ else}
\end{cases}
$$&lt;p>这里 $\hat{a}$, $a$ 分别是 prediction 和 ground truth.&lt;/p>
&lt;p>对于 perception 任务，reward function 与我们要处理的子任务相关。如果是 OCR 和 counting 任务，则我们采取和 reasoning 一样的 reward function. 如果我们要处理的是 grounding 和 detection 任务，则我们可以使用 IoU 以及 mAP 两种 reward 形式。&lt;/p>
&lt;p>IoU reward 的定义如下：&lt;/p>
$$
R_{\mathrm{acc}}(\hat{a},a) = \begin{cases}
\mathrm{IoU}(\hat{a}, a), &amp;\text{ if }\mathrm{IoU}(\hat{a},a)\geq \epsilon\\
0, &amp;\text{ else}
\end{cases}
$$&lt;p>这里 $\epsilon&amp;gt;0$ 为超参数， IoU 定义如下&lt;/p>
$$
\mathrm{IoU}(\hat{a},a) = \frac{\mathrm{Area}(\hat{a}\cap a)}{\mathrm{Area}(\hat{a}\cup a)}
$$&lt;p>mAP 的定义如下&lt;/p>
$$
\mathrm{AP}_c = \int_0^1 \max_{\tilde{r}\geq r}P_c(\tilde{r}) dr,\quad mAP=\frac1C\sum_{c=1}^C \mathrm{AP}_c
$$&lt;p>最终的 reward 是 format reward 和 accuracy reward 的加权求和：&lt;/p>
$$
R = \alpha_{\mathrm{acc}}R_{\mathrm{acc}} + \alpha_{\mathrm{format}}R_{\mathrm{format}}
$$&lt;h2 id="v-triune">V-Triune
&lt;/h2>&lt;p>V-Triune 框架如下图所示，其包含三个模块&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-v-triune/V-Triune_framework.png"
width="1351"
height="562"
srcset="https://maosong2022.github.io/p/notes-on-v-triune/V-Triune_framework_hu10953460521504204366.png 480w, https://maosong2022.github.io/p/notes-on-v-triune/V-Triune_framework_hu16889825761822693623.png 1024w"
loading="lazy"
alt="V-Triune System"
class="gallery-image"
data-flex-grow="240"
data-flex-basis="576px"
>&lt;/p>
&lt;h3 id="sample-level-data-formatting">Sample-Level Data Formatting
&lt;/h3>&lt;p>核心思想就是不同的任务的 reward function 可能是不一样的，如果在 task 层面设计 reward function 的话，RL 训练就没有了 flexibility, 因此，在本文中，作者在 sample 层面设计 reward function. 具体做法就是，每个样本除了 QA 相关信息，还有 reward model 的相关参数，这样就可以更加灵活地调整不同 sample 的损失了。&lt;/p>
&lt;h3 id="verifier-level-reward-computation">Verifier-Level Reward Computation
&lt;/h3>&lt;p>核心思想就是构建多个不同的 Verifier, 来分别处理不同的 sample, 因为我们 reward model 也是在 sample 层面构建的，因此我们可以基于 sample 的 metadata 来动态分配对应的 verifier, 这样就提升了 veriier 的可扩展性。本文作者使用了 MathVerifier 和 DetectionVerifier 两种。&lt;/p>
&lt;h3 id="source-level-metric-monitoring">Source-Level Metric Monitoring
&lt;/h3>&lt;p>核心思想就是根据 sample 的 metadata 信息来监控不同的 metric, 这样可以防止训练不稳定。&lt;/p>
&lt;h3 id="dynamic-iou-reward">Dynamic IoU Reward
&lt;/h3>&lt;p>核心思想就是根据训练进程，动态调整 IoU 的阈值。&lt;/p>
&lt;p>作者发现，如果将 IoU 阈值设置的比较低的话，模型很容易拿到比较高的 reward; 如果将 IoU 阈值设置的比较高的话，模型又很难拿到奖励。&lt;/p>
&lt;p>因此，作者的解决方案就是，使用课程学习，随着训练的进行，逐步提高 IoU 的阈值。&lt;/p>
&lt;h2 id="data">Data
&lt;/h2>&lt;p>数据集列举如下：&lt;/p>
&lt;ul>
&lt;li>Math: mm_math, geometry3K, mmk12&lt;/li>
&lt;li>Puzzle: PuzzleVQA, AlgoPuzzleQA, VisualPuzzles&lt;/li>
&lt;li>Science: ScienceQA, SciVQA, ViRL39K (Broader STEM topics, (GradeSchool) Science)&lt;/li>
&lt;li>Chart: ChartQAPro, ChartX, Table-VQA, ViRL39K (Tables/Diagrams/Charts)&lt;/li>
&lt;li>Detection: V3Det, Object365&lt;/li>
&lt;li>Grounding: D3&lt;/li>
&lt;li>Counting: CLEVR&lt;/li>
&lt;li>OCR: LLaVA-OV (OCR questions), EST-VQA&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Rule-based filtering&lt;/strong>
对于 visual reasoning 数据，作者的过滤如下：&lt;/p>
&lt;ul>
&lt;li>多项选择题以及判断题，防止 reward hacking&lt;/li>
&lt;li>答案包含特殊符号如 &amp;ldquo;=&amp;rdquo;, &amp;ldquo;[&amp;rdquo; 等的数据&lt;/li>
&lt;li>答案字符数超过 20 个，防止答案太复杂&lt;/li>
&lt;/ul>
&lt;p>对于 visual perception 数据，作者过滤流程如下：&lt;/p>
&lt;ul>
&lt;li>Detection: 超过 10 个 bounding box 的，或者 box 占 50% 以上面积的数据，保持 single BB 和 multi BB 的比例为 1:2&lt;/li>
&lt;li>Grounding: box 占 50% 以上面积的数据, label 过于复杂的数据&lt;/li>
&lt;li>Counting: 保持类别平衡，仅使用英文数据&lt;/li>
&lt;li>OCR: 保留英文数据，对 labels 进行 verify&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Difficulty-based filtering&lt;/strong>
主要是移除比较简单的问题。&lt;/p>
&lt;p>经过这两个阶段的过滤之后，得到了 &lt;strong>20.6K&lt;/strong> perception samples 以及 &lt;strong>27.1K&lt;/strong> reasoning samples. 数据保存在 Parquet 格式&lt;/p>
&lt;h2 id="training-recipe">Training Recipe
&lt;/h2>&lt;h3 id="disable-vit-training">Disable ViT Training
&lt;/h3>&lt;p>作者首先发现，全量微调会导致训练崩溃。作者分析原因发现，主要是 ViT 在反向传播过程中，存在 gradient amplify 的问题，第一层与最后一层的梯度的 norm 相差 5-10 倍。作者认为有两点原因：&lt;/p>
&lt;ol>
&lt;li>RL 会强制要求模型的不同模态之间进行对齐&lt;/li>
&lt;li>对比学习训练得到的 VIT 可能使得其不适合 RL 的训练。&lt;/li>
&lt;/ol>
&lt;p>基于这两点考虑，作者再进行训练的时候，冻结了 ViT 模块，结果如下图&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-v-triune/V-Triune-frozen-ViT.png"
width="1383"
height="705"
srcset="https://maosong2022.github.io/p/notes-on-v-triune/V-Triune-frozen-ViT_hu7988052896170322526.png 480w, https://maosong2022.github.io/p/notes-on-v-triune/V-Triune-frozen-ViT_hu8856288206821860516.png 1024w"
loading="lazy"
alt="Analysis of ViT training instability"
class="gallery-image"
data-flex-grow="196"
data-flex-basis="470px"
>&lt;/p>
&lt;h3 id="spurious-image-special-tokens">Spurious Image Special Tokens
&lt;/h3>&lt;p>作者发现，模型的输出可能会包含一些 &lt;code>&amp;lt;image_pad&amp;gt;&lt;/code> 等 token, 因此，作者对输出进行了过滤，然后再进行计算。&lt;/p>
&lt;h3 id="cot-prompt-pool">CoT Prompt Pool
&lt;/h3>&lt;p>作者还发现，多样化的 CoT prompt 会损害模型的表现。因此，作者构建了 20 个 prompt 模版，每次随机挑选其中一个加入到 instruction 中。&lt;/p>
&lt;h3 id="training-configuration">Training Configuration
&lt;/h3>&lt;p>模型基于 Qwen2.5-7B-instruct 和 Qwen2.5VL-32B 来进行开发&lt;/p>
&lt;p>作者探究了 on-policy 和 off-policy 两种配置。roll-out batch size 设置为 1024, 使用 GRPO 进行训练，GRPO 的 group size 设置为 8&lt;/p>
&lt;p>作者还设置 $\epsilon_{high}=0.28$ 以及 $\epsilon_{low}=0.2$ 来提高 token 的采样率。作者去除了 KL divergence loss, 并且在 token 层面上进行平均&lt;/p>
&lt;h2 id="evaluation">Evaluation
&lt;/h2>&lt;h3 id="performance">Performance
&lt;/h3>&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-v-triune/V-Triune-performance.png"
width="1159"
height="1051"
srcset="https://maosong2022.github.io/p/notes-on-v-triune/V-Triune-performance_hu6593213700954064222.png 480w, https://maosong2022.github.io/p/notes-on-v-triune/V-Triune-performance_hu15819943646912053752.png 1024w"
loading="lazy"
alt="Performance of Orsta on MEGA-Bench core"
class="gallery-image"
data-flex-grow="110"
data-flex-basis="264px"
>&lt;/p>
&lt;h3 id="analysis">Analysis
&lt;/h3>&lt;p>&lt;strong>on-policy v.s. off-policy&lt;/strong>
作者首先对比了以下 on-policy RL 和 off-policy RL 两种训练方式的表现， 结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-v-triune/V-triune-on-policy.png"
width="1137"
height="526"
srcset="https://maosong2022.github.io/p/notes-on-v-triune/V-triune-on-policy_hu17883304061716859651.png 480w, https://maosong2022.github.io/p/notes-on-v-triune/V-triune-on-policy_hu14295860107519637628.png 1024w"
loading="lazy"
alt="Ablation on on-policy v.s. off-policy"
class="gallery-image"
data-flex-grow="216"
data-flex-basis="518px"
>&lt;/p>
&lt;p>结果有两点发现：&lt;/p>
&lt;ol>
&lt;li>on-policy 的表现基本上都是比 off-policy 要好的。&lt;/li>
&lt;li>小模型通过 RL 带来的表现提升更明显，大模型的表现则更慢&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>generalization&lt;/strong>
作者还评估了以下模型的泛化性，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-v-triune/V-Triune-generalization.png"
width="422"
height="673"
srcset="https://maosong2022.github.io/p/notes-on-v-triune/V-Triune-generalization_hu7830263504092978837.png 480w, https://maosong2022.github.io/p/notes-on-v-triune/V-Triune-generalization_hu5722992839965848528.png 1024w"
loading="lazy"
alt="Generalization of V-Triune"
class="gallery-image"
data-flex-grow="62"
data-flex-basis="150px"
>&lt;/p>
&lt;p>结果发现，RL 的训练更像是对齐，而不是泛化。也就是说，模型在 in-domain 的任务上表现较好，在 OOD 的任务上表现就一般。这与已有的结论是不一样的&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Recall
&lt;a class="link" href="https://maosong.website/p/notes-on-magistral/" target="_blank" rel="noopener"
>Magistral&lt;/a> 发现模型在 math domain 上进行训练，在 code domain 上的表现也会提升&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Training Dynamics&lt;/strong>
作者还分析了不同任务不同指标随训练进行的变化情况，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-v-triune/V-Triune-training-dynamics.png"
width="1150"
height="900"
srcset="https://maosong2022.github.io/p/notes-on-v-triune/V-Triune-training-dynamics_hu15276947352141942304.png 480w, https://maosong2022.github.io/p/notes-on-v-triune/V-Triune-training-dynamics_hu12555432383874143707.png 1024w"
loading="lazy"
alt="V-Triune training dynamics"
class="gallery-image"
data-flex-grow="127"
data-flex-basis="306px"
>&lt;/p>
&lt;p>结果显示，reasoning 和 OCR 任务岁训练进行其输出长度和正确率都有所提升。但是 detection 相关任务则变化不是很明显。&lt;/p>
&lt;h3 id="ablation-study">Ablation Study
&lt;/h3>&lt;p>作者进行了三个消融实验：&lt;/p>
&lt;ol>
&lt;li>训练策略：冻结 ViT, 冻结 LLM, 两者都不冻结&lt;/li>
&lt;li>任务分解策略：仅使用 reasoning 数据进行训练，仅使用 perception 数据进行训练，同时使用两种数据进行训练。&lt;/li>
&lt;li>learning rate: 不同的学习率&lt;/li>
&lt;/ol>
&lt;p>实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-v-triune/V-Triune-ablation-study.png"
width="1146"
height="585"
srcset="https://maosong2022.github.io/p/notes-on-v-triune/V-Triune-ablation-study_hu17402617248809186274.png 480w, https://maosong2022.github.io/p/notes-on-v-triune/V-Triune-ablation-study_hu10212683979436721237.png 1024w"
loading="lazy"
alt="Ablation study"
class="gallery-image"
data-flex-grow="195"
data-flex-basis="470px"
>&lt;/p>
&lt;p>结果发现，&lt;/p>
&lt;ol>
&lt;li>仅训练 ViT 对表现没有任何帮助，只有训练 LLM 才会带来性能的提升。&lt;/li>
&lt;li>同时使用两种数据进行训练的效果是最好的，其次是仅使用 reasoning 数据进行训练，最后是仅使用 perception 数据进行训练&lt;/li>
&lt;li>较大的学习率会损害模型的表现。&lt;/li>
&lt;/ol>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>本文中，作者提出了 V-Triune, 一个使用 RL 来同时提高 MLLM 的 reasoning 和 perception 能力的 RL 训练框架。&lt;/p>
&lt;p>作者发现 RL 对于 VLM 来说更像是一种 Alignment 策略，而不是一种 Generalization 策略，也就是模型并没有引入新的能力，而是提升模型本身的 utility 以及 robustness.&lt;/p>
&lt;p>作者认为本文有以下 Limitation:&lt;/p>
&lt;ol>
&lt;li>在 perception 任务上没有看到明显的 test-time scaling 现象。作者认为，探究使用 o3 类似的方法或许能提高模型的表现&lt;/li>
&lt;li>探究 RL-zero 在 VLM 中的应用，也就是直接在 base model 上进行 RL 的训练。&lt;/li>
&lt;/ol>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2505.18129" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Magistral</title><link>https://maosong2022.github.io/p/notes-on-magistral/</link><pubDate>Wed, 16 Jul 2025 11:04:04 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-magistral/</guid><description>&lt;p>Magistral 是 Mistral 提出的一个 reasoning model 系列，主要针对 math 和 code 两个 domain&lt;/p>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>Mistral 在 2025 年 6 月 12 日发布了 Magistral ，一个 reasoning model, 包含两个模型，一个是由纯 RL 训练得到的 &lt;em>Magistral Medium&lt;/em>, 另一个是由 SFT 和蒸馏 Magistral Medium 得到的 &lt;strong>Magistral Small&lt;/strong>&lt;/p>
&lt;p>作者首先介绍了一下本文的贡献：&lt;/p>
&lt;ol>
&lt;li>介绍了如何仅使用 RL (而不是用蒸馏) 来训练 Magistral Medium&lt;/li>
&lt;li>infra 上的改进，主要使用最新的权重来更新 generator&lt;/li>
&lt;li>多语种能力，支持多种语言&lt;/li>
&lt;li>系统性探究了 RLVR 的能力边界&lt;/li>
&lt;li>开源了 Magistral small (24B)&lt;/li>
&lt;/ol>
&lt;h2 id="method">Method
&lt;/h2>&lt;h3 id="rl">RL
&lt;/h3>&lt;p>RL 算法基于 GRPO 改进，主要有以下几点：&lt;/p>
&lt;ol>
&lt;li>去掉了 KL Divergence loss, 这一点跟 &lt;a class="link" href="https://maosong.website/p/notes-on-dapo/" target="_blank" rel="noopener"
>DAPO&lt;/a> 是一致的，提升模型的探索能力&lt;/li>
&lt;li>Loss Normalization，在 sample 层面做平均，还是跟 DAPO 一致，减少不同长度输出对训练造成的影响&lt;/li>
&lt;li>Advantage Normalization, 作者首先将每个 token 的 Advantage 定义为 $\hat{A}&lt;em>i=R_i-\mu$, 其中 $\mu$ 是每个 group 的 reward 均值， 然后在每个 mini-batch 里对 advantage 进行 Normalization (这里 $\hat{A}&lt;/em>{mean}$ 和 $\hat{A}_{std}$ 分别为 mini-batch advantage 的均值和方差)：&lt;/li>
&lt;/ol>
$$
\hat{A}_{i,t}^{norm}=\frac{\hat{A}_i-\hat{A}_{mean}}{\hat{A}_{std}}
$$&lt;ol start="4">
&lt;li>CLIP-Higher, 跟 DAPO 一致，提高稀有 token 的被采样概率&lt;/li>
&lt;li>Eliminating non-divsere groups, 跟 DAPO 一致，去掉过于简单和过于难的题目&lt;/li>
&lt;/ol>
&lt;p>最终 RL 阶段训练的损失函数为：&lt;/p>
$$
\mathcal{L}(\theta) = \mathbb{E}_{q\sim P(Q),\{o_i\}_{i=1}^G\sim \pi_{old}(\cdot\mid q)}\frac{1}{\sum_{i=1}^G|o_i|}\sum_{i=1}^G\sum_{t=1}^{|o_i|}\min\left[r_{i,t}(\theta)\hat{A}_{i,t}^{norm}, \mathrm{clip}(r_{i,t}(\theta), 1-\epsilon_{low}, 1+\epsilon_{high})\hat{A}_{i,t}^{norm}\right], \mathrm{s.t.}, \exists 1\leq m &lt; n \leq G, r_m\neq r_n.
$$&lt;p>其中&lt;/p>
$$
r_{i,t}(\theta) = \frac{\pi_{\theta}(o_{i,t}\mid q, o_{i,&lt;t})}{\pi_{old}(o_{i,t}\mid q, o_{i,&lt;t})}
$$&lt;h3 id="reward-shaping">Reward Shaping
&lt;/h3>&lt;p>作者还基于四个维度来构建 reward: formatting, correctness, length, 以及 language consistency&lt;/p>
&lt;p>&lt;strong>Formatting&lt;/strong>
针对数学和代码问题，作者要求模型输出符合特定的格式&lt;/p>
&lt;ul>
&lt;li>Tag requirements: 思考过程用 &lt;code>&amp;lt;think&amp;gt;&amp;lt;/think&amp;gt;&lt;/code> 包含，且只能包含一个 tag&lt;/li>
&lt;li>mathematical responses: 对于数学问题，结果用 &lt;code>\boxed{}&lt;/code> 包含&lt;/li>
&lt;li>code response：包含一个 markdown block&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>[!tip] Future
最新的 &lt;a class="link" href="https://maosong.website/p/notes-on-glm-4.1v-thinking/" target="_blank" rel="noopener"
>GLM-4.1V-Thinking&lt;/a> 认为，不应该在 RL 训练阶段加入 format reward&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>correctness&lt;/strong>
基于答案的正确性分配奖励&lt;/p>
&lt;ul>
&lt;li>math correctness：使用 rule-based verifier 进行打分，使用 parser 和 Sympy 来比较模型输出以及 ground truth&lt;/li>
&lt;li>code correctness: 构建单元测试，评估输出代码是否能通过所有的单元测试&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Length penalty&lt;/strong>
与 DAPO 一致，使用 Length penalty 来惩罚过长的回答。&lt;/p>
&lt;p>&lt;strong>Language consistency&lt;/strong>
减少模型混合语言输出的问题。作者的做法是将 10% 的问题从英语转化为其他语种，然后使用 fastText 进行分类，确保内容都是一个语言。作者发现，通过这个简单的修改，就提高了模型的语言跟随能力。&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Recall
MiMo 中也遇到了混合语言输出的问题，但是其并没有给出解决办法。&lt;/p>
&lt;/blockquote>
&lt;p>作者还在 system prompt 中规定了输出的格式以及语言。&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Observation
作者发现，RL 的训练对于 system prompt 非常的敏感，因为 system prompt 会提高 model 的 entropy, 然后提高模型的探索能力。&lt;/p>
&lt;/blockquote>
&lt;h2 id="data">Data
&lt;/h2>&lt;p>数据包括 math problems 以及 code problems&lt;/p>
&lt;h3 id="math">Math
&lt;/h3>&lt;p>作者首先收集了 700K 样本，然后作者通过预处理以及过滤来保证数据的质量：&lt;/p>
&lt;p>&lt;strong>format filtering&lt;/strong>
要求问题完整，答案准确且可验证；去掉证明题和 multi-part 的问题；改写多项选择题为解答题，提高难度防止 reward hacking&lt;/p>
&lt;p>&lt;strong>difficulty filtering&lt;/strong>
使用两阶段的过滤策略。&lt;/p>
&lt;ol>
&lt;li>第一阶段使用 LLM 进行多次采样，然后去除掉比较简单或者比较复杂的&lt;/li>
&lt;li>第二阶段使用 RL checkpoint 进行多次采样，去除掉标准答案可能会存在问题的题目&lt;/li>
&lt;/ol>
&lt;p>最终一共得到 &lt;strong>38K&lt;/strong> 的样本&lt;/p>
&lt;h3 id="code">Code
&lt;/h3>&lt;p>首先去掉没有 solution 和 test 的问题；然后去除掉标准答案不能通过所有 test 的问题。最终一共得到 &lt;strong>35K&lt;/strong> 的样本，包含 Python 和 C++ 两种语言&lt;/p>
&lt;h2 id="training">Training
&lt;/h2>&lt;p>训练框架如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-magistral/Magistral_data_training.png"
width="1515"
height="523"
srcset="https://maosong2022.github.io/p/notes-on-magistral/Magistral_data_training_hu7985413339294341630.png 480w, https://maosong2022.github.io/p/notes-on-magistral/Magistral_data_training_hu12253124277348638876.png 1024w"
loading="lazy"
alt="Magistral_data_training"
class="gallery-image"
data-flex-grow="289"
data-flex-basis="695px"
>&lt;/p>
&lt;p>对于两个模型，作者采用了不同的方式进行训练&lt;/p>
&lt;ul>
&lt;li>Magistral Medium: 使用 pure RL 进行训练&lt;/li>
&lt;li>Magistral small: 使用 SFT + RL 进行训练&lt;/li>
&lt;/ul>
&lt;p>Magistral Medium 训练时满足的要求：&lt;/p>
&lt;ol>
&lt;li>dataset is not too easy: 太简单的题目对模型提升没有帮助&lt;/li>
&lt;li>Generation length does not stop growing: 逐步提升模型的最大输出长度&lt;/li>
&lt;li>KV-cache memory burden is not too large: 降低 batch size 来减少 KV-cache 的内存占用&lt;/li>
&lt;/ol>
&lt;p>Magistral small 训练&lt;/p>
&lt;p>&lt;strong>SFT&lt;/strong>
数据集包括两部分，一部分是 Magistral Medium 回答正确的这部分数据，第二部分是公开数据集，包括 [[OpenThoughts]] 和 [[OpenR1]] 两个数据集，作者使用 Magistral Medium 来生成回答。作者还加入了 10% 的 instruction tuning 数据来保持模型的通用能力。&lt;/p>
&lt;p>&lt;strong>RL&lt;/strong>
RL 的训练与 Magistral Medium 一致。&lt;/p>
&lt;h2 id="infra">Infra
&lt;/h2>&lt;p>作者首先介绍了一下 RL 训练的 infra, infra 主要包括三个模块：&lt;/p>
&lt;ul>
&lt;li>Trainers: 用于更新模型的权重&lt;/li>
&lt;li>Generators: 用于采样，生成 roll-out&lt;/li>
&lt;li>Verifiers: 对模型输出的结果进行打分&lt;/li>
&lt;/ul>
&lt;p>分布式 RL 训练的主要问题在于，不同长度的 roll-out 花费的时间不一致，作者发现，最长的 roll-out 和最短的 roll-out 的时间相差超过 5 倍以上。&lt;/p>
&lt;p>因此，作者就提出了异步生成这个方法。具体的做法就是&lt;/p>
&lt;ol>
&lt;li>首先由 Generator 生成多条 roll-out&lt;/li>
&lt;li>当 roll-out 完成之后，立马用 Verifiers 对轨迹进行打分&lt;/li>
&lt;li>收集 roll-out 以及对应的 reward, 直到达到给定的 batch 大小&lt;/li>
&lt;li>使用 Trainer 更新 Generator 的权重, 将更新后的权重同步给 Generator，这样其他 generator 在生成新的 token 时用的就是新的权重&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-magistral/Magistral_infra.png"
width="1169"
height="968"
srcset="https://maosong2022.github.io/p/notes-on-magistral/Magistral_infra_hu16434872165372945647.png 480w, https://maosong2022.github.io/p/notes-on-magistral/Magistral_infra_hu16801221237754484869.png 1024w"
loading="lazy"
alt="Magistral_infra"
class="gallery-image"
data-flex-grow="120"
data-flex-basis="289px"
>&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Recall
MiMo 里的做法是，当我们收集到给定数量的 roll-out 之后，我们就基于这些 roll-out 更新权重，然后进行下一次采样&lt;/p>
&lt;/blockquote>
&lt;p>训练时，对于每个 rank, 只要其收集到足够的 roll-out, 就会进行一次梯度更新。&lt;/p>
&lt;h2 id="evaluation">Evaluation
&lt;/h2>&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-magistral/Magistral_performance_AIME.png"
width="1810"
height="1002"
srcset="https://maosong2022.github.io/p/notes-on-magistral/Magistral_performance_AIME_hu6201950020122303348.png 480w, https://maosong2022.github.io/p/notes-on-magistral/Magistral_performance_AIME_hu17050491895656832894.png 1024w"
loading="lazy"
alt="Performance of Magistral Medium"
class="gallery-image"
data-flex-grow="180"
data-flex-basis="433px"
>&lt;/p>
&lt;h2 id="ablation">Ablation
&lt;/h2>&lt;p>&lt;strong>RL 的泛化性&lt;/strong>
作者探究了 RL 的 cross-domain generalization 能力，实验结果如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>AIME’24&lt;/th>
&lt;th>LiveCodeBench v5&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Starting Checkpoint&lt;/td>
&lt;td>32.2&lt;/td>
&lt;td>22.7&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RL (Math only)&lt;/td>
&lt;td>62.5&lt;/td>
&lt;td>38.3 (+15.6)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RL (Code only)&lt;/td>
&lt;td>49.7 (+17.5)&lt;/td>
&lt;td>42.7&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，不管是使用 math 还是 code 数据单独进行训练，模型在另一个 domain 上的表现都有所提升。&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Future
这个结论与最新的 GLM-4.1V-Thinking 结论一致&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Distillation v.s. RL for small models&lt;/strong>
作者探究了对于小语言模型，使用 RL 进行训练的效果更好，还是使用蒸馏的效果更好。&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-magistral/Magistral_distillation_vs_rl_performance.png"
width="1822"
height="934"
srcset="https://maosong2022.github.io/p/notes-on-magistral/Magistral_distillation_vs_rl_performance_hu12804151669788107874.png 480w, https://maosong2022.github.io/p/notes-on-magistral/Magistral_distillation_vs_rl_performance_hu14645921621896337014.png 1024w"
loading="lazy"
alt="Performance of RL v.s. distillation"
class="gallery-image"
data-flex-grow="195"
data-flex-basis="468px"
>&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Observation
实验结果发现，仅使用 RL 的效果与蒸馏差不多，甚至更好&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Batch size&lt;/strong>
作者还探究了 batch size $n_{batch}$ 以及 mini-batch size $n_{mini}$ 的影响。这里 batch size 指的是用于更新梯度的 roll-out 数量，mini-batch size 指的是计算梯度的 roll-out 数量，作者还定义了并行生成 roll-out 的数量 $n_{async}$. 当 $n_{async}&amp;raquo; n_{batch}$ 时，生成的 sequence 就很可能是 off-policy 的。作者固定 $n_{async}=4096$, 然后对比了不同的 $n_{batch}$ 和 $n_{mini}$ 对模型表现的影响，结果如下：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-magistral/Magistral_batch_ablation.png"
width="1602"
height="698"
srcset="https://maosong2022.github.io/p/notes-on-magistral/Magistral_batch_ablation_hu16454842133673153190.png 480w, https://maosong2022.github.io/p/notes-on-magistral/Magistral_batch_ablation_hu3588199010302775526.png 1024w"
loading="lazy"
alt="Ablation study on batch size"
class="gallery-image"
data-flex-grow="229"
data-flex-basis="550px"
>&lt;/p>
&lt;blockquote>
&lt;p>![tip] Observation
当 $n_{batch}=n_{mini}$ 时，模型表现差不太多（左图）；当 $n_{batch}$ 为常数，而 $n_{mini}$ 逐渐减小时，模型表现会逐渐变差。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Advantage normalization&lt;/strong>
作者对比了三种针对 advantage 的 normalization 方式：&lt;/p>
&lt;ul>
&lt;li>mini-batch: 在每个 mini-batch 里对模型进行 normalization&lt;/li>
&lt;li>Group normalization: 在每个 group 里进行 normalization&lt;/li>
&lt;li>no normalization: 没有 normalization&lt;/li>
&lt;/ul>
&lt;p>实验结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-magistral/Magistral_normalization.png"
width="1820"
height="500"
srcset="https://maosong2022.github.io/p/notes-on-magistral/Magistral_normalization_hu658871494775987334.png 480w, https://maosong2022.github.io/p/notes-on-magistral/Magistral_normalization_hu1935902630730719135.png 1024w"
loading="lazy"
alt="Ablation study on normalization"
class="gallery-image"
data-flex-grow="364"
data-flex-basis="873px"
>&lt;/p>
&lt;p>作者发现，三种方式的区别并不是很大。&lt;/p>
&lt;h2 id="analysis">Analysis
&lt;/h2>&lt;p>&lt;strong>length dimension&lt;/strong>
作者首先保存模型的 weight, 然后使用 PCA 进行降维，并在 2 维上进行可视化，作者对权重进行扰动，然后记录模型的 reward 以及输出长度。结果发现，模型存在一个 length dimension. 可视化结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-magistral/Magistral_length_dimension.png"
width="1546"
height="602"
srcset="https://maosong2022.github.io/p/notes-on-magistral/Magistral_length_dimension_hu660748011264665621.png 480w, https://maosong2022.github.io/p/notes-on-magistral/Magistral_length_dimension_hu16323640694693724369.png 1024w"
loading="lazy"
alt="Magistral length dimension visualization"
class="gallery-image"
data-flex-grow="256"
data-flex-basis="616px"
>&lt;/p>
&lt;p>&lt;strong>multimodal extension&lt;/strong>
由于 Magistral medium 和 Magistral small 都是基于 MLLM 中的 LLM 开发得到的，作者还探究了更换 LLM 的 checkpoint 之后，原始 MLLM 的表现，结果发现，模型在多模态 reasoning 相关任务上的表现也得到了提升，结果如下图所示：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-magistral/Magistral_multimodal_extention.png"
width="1782"
height="822"
srcset="https://maosong2022.github.io/p/notes-on-magistral/Magistral_multimodal_extention_hu14193739507602157915.png 480w, https://maosong2022.github.io/p/notes-on-magistral/Magistral_multimodal_extention_hu12191812944879346854.png 1024w"
loading="lazy"
alt="Magistral multimodal performance"
class="gallery-image"
data-flex-grow="216"
data-flex-basis="520px"
>&lt;/p>
&lt;p>&lt;strong>Impact on other capabilities&lt;/strong>
作者还探究了 RL 训练对模型其他表现的影响，结果发现，RL 训练可以提高模型的 tool calling 和指令跟随能力，实验结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Category&lt;/th>
&lt;th>Benchmark&lt;/th>
&lt;th>Mistral Medium 3&lt;/th>
&lt;th>Magistral Medium&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Function calling&lt;/td>
&lt;td>Internal bench&lt;/td>
&lt;td>87.2&lt;/td>
&lt;td>87.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Instruction following&lt;/td>
&lt;td>IFEval&lt;/td>
&lt;td>86.8&lt;/td>
&lt;td>87.4&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>failed approaches&lt;/strong>
作者还介绍了一些尝试失败的做法：&lt;/p>
&lt;ol>
&lt;li>partial Reward: 对于 coding 任务，作者使用 test 的通过率作为奖励，结果发现效果并不好，这是因为一些错误的解法的 test 通过率也很高&lt;/li>
&lt;li>entropy bonus loss: 作者发现在损失函数中加入 entropy bonus loss 之后，模型的训练变得不稳定，而且效果不如使用更高的 $\epsilon_{high}$&lt;/li>
&lt;li>作者还进一步验证在 PPO loss 中加入 KL divergence loss, 结果发现效果并不好，这与 DAPO 的结论一致&lt;/li>
&lt;li>作者还尝试先 SFT Magistral Medium, 再进行 RL, 结果发现 RL 可以大幅度提高 SFT checkpoint 的表现。&lt;/li>
&lt;/ol>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>本文中，作者提出了 Magistral, 一个针对 math 和 code 的 reasoning model, 作者介绍了训练细节。但是，从方法层面来看，和 DAPO 区别不是很大。关键点应该是作者详细介绍了各种消融实验，为后来相关探索提供了经验。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2506.10910v1" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on SmolLM3</title><link>https://maosong2022.github.io/p/notes-on-smollm3/</link><pubDate>Tue, 15 Jul 2025 11:01:13 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-smollm3/</guid><description>&lt;p>Hugging Face 在 2025 年 7 月 8 号发布了 SmolLM3, 一个 3B 的，128K 上下文，支持 6 种语言，支持 dual mode reasoning 的小语言模型。&lt;/p>
&lt;h2 id="pre-training">Pre-training
&lt;/h2>&lt;h3 id="architecture">Architecture
&lt;/h3>&lt;p>SmolLM3 是一个基于 transformer 的小语言模型，其模型架构与 SmolLM2 相似，SmolLM3 做了以下改进：&lt;/p>
&lt;ol>
&lt;li>使用 GQA 代替 multi-head attention. 作者通过消融实验发现 &lt;a class="link" href="https://maosong.website/p/notes-on-gqa/" target="_blank" rel="noopener"
>GQA&lt;/a> 和 MHA 的效果差不多，并且还可以减少 KV cache 的 size&lt;/li>
&lt;li>NoPE: 作者使用了NoPE, 来选择性（每 4 个 layer）移除 position embedding. 这个方法可以在不损害模型短文本能力的同时，提高模型长上下文的表现&lt;/li>
&lt;li>Intra-Document Masking: 不同的文档之间使用 attention masking 隔开&lt;/li>
&lt;li>Training stability: 与 olmo 2 一样，作者移除了 embedding layer 的 weight decay, 来提升训练的稳定性。&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_model_atanomy.png"
width="2554"
height="1470"
srcset="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_model_atanomy_hu13998036533088262709.png 480w, https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_model_atanomy_hu8563067276937063516.png 1024w"
loading="lazy"
alt="SmolLM3 model atanomy"
class="gallery-image"
data-flex-grow="173"
data-flex-basis="416px"
>&lt;/p>
&lt;h3 id="data">Data
&lt;/h3>&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_pre-training_recipe.png"
width="1932"
height="1196"
srcset="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_pre-training_recipe_hu14656950727837741103.png 480w, https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_pre-training_recipe_hu1794311974944927812.png 1024w"
loading="lazy"
alt="SmolLM3 pre-training recipe"
class="gallery-image"
data-flex-grow="161"
data-flex-basis="387px"
>&lt;/p>
&lt;p>与 SmolLM2 一样，作者使用了&lt;strong>11.2T&lt;/strong> token 进行训练，训练包括 3 个 stage。作者针对数据混合策略进行了消融实验，实验配置如下：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Stage&lt;/th>
&lt;th>Stable phase&lt;/th>
&lt;th>Stable phase&lt;/th>
&lt;th>Decay phase&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Description&lt;/td>
&lt;td>Base training&lt;/td>
&lt;td>High quality injection&lt;/td>
&lt;td>LR Decay&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Tokens&lt;/td>
&lt;td>8T&lt;/td>
&lt;td>2T&lt;/td>
&lt;td>1.1T&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Web&lt;/td>
&lt;td>85%&lt;/td>
&lt;td>75%&lt;/td>
&lt;td>63%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Code&lt;/td>
&lt;td>12%&lt;/td>
&lt;td>15%&lt;/td>
&lt;td>24%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Math&lt;/td>
&lt;td>3%&lt;/td>
&lt;td>10%&lt;/td>
&lt;td>13%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>datasets&lt;/td>
&lt;td>&lt;strong>web&lt;/strong>: FineWeb-Edu, DCLM, FineWeb2, Fineweb2-HQ&lt;br>&lt;strong>code&lt;/strong>: The Stack v2, StarCoder2 PRS, Jupyter and Kaggle notebooks, Github issues, StackExchange&lt;br>&lt;strong>math&lt;/strong>: FineMath3, InfiWebMath3+&lt;/td>
&lt;td>Adding Stack-Edu, FineMath4+, InfiWebMath4+, MegaMath&lt;/td>
&lt;td>upsampling of high-quality code data&lt;br>upsampling math data and introducing instruction and reasoning datasets such as OpenMathReasoning&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>其中，Web data 包含 12% 的多语种数据。&lt;/p>
&lt;h3 id="training-recipe">Training Recipe
&lt;/h3>&lt;p>训练过程中，作者使用了 2.36M tokens 的 batch size, 上下文长度为 4096. 优化器为 AdamW&lt;/p>
&lt;p>作者使用了 nanotron 进行训练， datatrove 来处理数据， lighteval 来评估模型的表现。&lt;/p>
&lt;p>模型在 384 张 H100 上训练了 24 天。分布式训练的配置如下&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_distributed_training.png"
width="1672"
height="1024"
srcset="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_distributed_training_hu7285902030689819858.png 480w, https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_distributed_training_hu8235147299335409429.png 1024w"
loading="lazy"
alt="SmolLM3 distributed training"
class="gallery-image"
data-flex-grow="163"
data-flex-basis="391px"
>&lt;/p>
&lt;h2 id="mid-training">Mid-training
&lt;/h2>&lt;p>Mid-training 的主要目标为扩展模型的上下文以及提升模型的 reasoning 能力。&lt;/p>
&lt;h3 id="long-context-extension">Long Context Extension
&lt;/h3>&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_long_context_training.png"
width="1942"
height="1128"
srcset="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_long_context_training_hu16383025728698209251.png 480w, https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_long_context_training_hu13112308143836602144.png 1024w"
loading="lazy"
alt="SmolLM3 long context training"
class="gallery-image"
data-flex-grow="172"
data-flex-basis="413px"
>&lt;/p>
&lt;p>在预训练阶段结束之后，作者使用了额外的 &lt;strong>100B&lt;/strong> tokens 来扩展模型的上下文。作者将扩展过程分为两个 stage:&lt;/p>
&lt;ol>
&lt;li>Stage 1: 将模型的上下文从 4K 提升到 32K. 具体做法是将 RoPE 的 base frequency 提升到 1.5M&lt;/li>
&lt;li>Stage 2: 将，模型的上下文从 32K 提升到 64K. 具体做法是将 RoPE 的 base frequency 提升到 5M&lt;/li>
&lt;/ol>
&lt;p>训练过程中，作者对 math, code 和 reasoning data 进行了上采样。作者发现，对长文本数据进行上采样并不会提高模型在 RULER 和 HELMET 上的表现。&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Recall
&lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5-1m/" target="_blank" rel="noopener"
>Qwen2.5-1M&lt;/a> 里也分析了长文本数据的问题，也就是大部分长文本数据依然是局部相关性强，而全局相关性弱&lt;/p>
&lt;/blockquote>
&lt;p>与 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5/" target="_blank" rel="noopener"
>Qwen2.5&lt;/a> 一样，作者还是用了 &lt;a class="link" href="https://maosong.website/p/notes-on-yarn/" target="_blank" rel="noopener"
>YARN&lt;/a> 来在推理阶段进一步提高模型的上下文长度，作者发现模型可以处理 128K 上下文长度的文本。&lt;/p>
&lt;h3 id="reasoning-mid-training">Reasoning Mid-training
&lt;/h3>&lt;p>扩展模型上下文长度之后，作者还额外增加了一个 mid-training stage 来提高模型的 reasoning 能力。这个阶段的目标在于提升模型的通用能力。作者希望模型不针对特定的 domain, 如 math 或者 code 等。&lt;/p>
&lt;p>训练过程中，作者使用了 &lt;strong>35B&lt;/strong> 的 token. 数据来源包括 Open-Thoughts3-1.2M 以及 NVIDIA 的 Llama-Nemetron-Post-Training-Dataset-v1.1. Reasoning trace 由 DeepSeek-R1 生成。作者使用了 ChatML 的格式，还使用了 Packing 来提升训练效率。训练持续了 4 个 epoch.&lt;/p>
&lt;h2 id="post-training">Post-training
&lt;/h2>&lt;p>如何构建 dual instruction model 来同时支持 reasoning 和 non-reasoning 任务并没有一个共识，大部分模型的数据都是非公开的。因此，作者就构建了一个 training pipeline, 用于提升模型在两种模式下的能力。&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_post-training_recipe.png"
width="1796"
height="1208"
srcset="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_post-training_recipe_hu5481082171162702903.png 480w, https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_post-training_recipe_hu243045709497717267.png 1024w"
loading="lazy"
alt="SmolLM3 post-training recipe"
class="gallery-image"
data-flex-grow="148"
data-flex-basis="356px"
>&lt;/p>
&lt;h3 id="chat-template">Chat Template
&lt;/h3>&lt;p>作者首先构建了一个 chat template, 用于支持 reasoning 和 non-reasoning 两种模式。该 chat template 支持用户使用 &lt;code>/think&lt;/code> 和 &lt;code>/no_think&lt;/code> flag 来控制模型的思考模式。在 non-reasoning 模式下，作者还在模型输出中 prefill 了 &lt;code>&amp;lt;think&amp;gt;\n\n&amp;lt;/think&amp;gt;&lt;/code>, 这一点与 Qwen3 一致。&lt;/p>
&lt;p>SmolLM3 还支持工具调用，其在 chat template 中加入了两种描述方式：XLM tools 和 Python tools.&lt;/p>
&lt;p>SmolLM3 还在 system prompt 中加入了 metadata, 如知识的截止时间，当前的 reasoning mode 等。&lt;/p>
&lt;h3 id="sft">SFT
&lt;/h3>&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_synthetic_SFT_data.png"
width="1562"
height="940"
srcset="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_synthetic_SFT_data_hu11130185069272692077.png 480w, https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_synthetic_SFT_data_hu3647788789608535616.png 1024w"
loading="lazy"
alt="SmolLM3 Synthetic SFT data"
class="gallery-image"
data-flex-grow="166"
data-flex-basis="398px"
>&lt;/p>
&lt;p>作者针对 math, code, general reasoning, instruction following, 以及 multilinguality 这几个领域来提升模型的表现。&lt;/p>
&lt;p>作者首先使用 reasoning mode 的 Qwen3-32B 来合成数据，合成数据的过程如上图所示。&lt;/p>
&lt;p>最终，SFT 阶段的数据包括 &lt;strong>1.8B&lt;/strong> token, 其中 &lt;strong>1B&lt;/strong> 为 non-reasoning mode 的 token, 覆盖 12 个数据集， &lt;strong>0.8B&lt;/strong> 为 reasoning token, 覆盖 10 个数据集。作者训练了 4 个 epoch, 使用了 Packing 的技巧。&lt;/p>
&lt;h3 id="apo">APO
&lt;/h3>&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_synthetic_preference_data.png"
width="1542"
height="760"
srcset="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_synthetic_preference_data_hu9363143912475736506.png 480w, https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_synthetic_preference_data_hu12538931937351185121.png 1024w"
loading="lazy"
alt="SmolLM3 synthetic preference data"
class="gallery-image"
data-flex-grow="202"
data-flex-basis="486px"
>&lt;/p>
&lt;p>SFT 阶段之后，作者使用了 Tulu3 的 preference dataset 用于 non-reasoning mode 的训练，然后合成了一批数据用于 reasoning mode 的训练，这批合成数据使用 Qwen3 32B 和 Qwen3 0.6B 生成得到，具体做法就是 Qwen3 32B 输出的结果定义为正样本，Qwen3 0.6B 输出的结果定义为负样本。&lt;/p>
&lt;p>作者使用了 APO 算法来进行训练，APO 是 DPO 的一个变体， DPO 的目标函数为&lt;/p>
$$
\mathcal{L}_{DPO}(x,y_w,y_{l}; \theta) = -\log \sigma(r_\theta(x,y_w) - r_\theta(x, y_l))
$$&lt;p>其中&lt;/p>
$$
r_\theta(x,y) = \beta\log \frac{\pi_{\theta}(y\mid x)}{\pi_{ref}(y\mid x)}
$$&lt;p>$\beta&amp;gt;0$ 是一个超参数。APO 的目标函数如下&lt;/p>
$$
\mathcal{L}_{APO}(x,y_w,y_l;\theta) = -\sigma(r_\theta(x,y_w)) + \sigma(r_\theta(x,y_l))
$$&lt;p>作者发现，模型的 reasoning 能力提升之后，其长上下文能力反而下降了。作者认为这是因为在 reasoning mid-training stage 的训练中，提升 reasoning 能力损害了模型的 long context 能力。并且，APO 的训练数据上下文长度大多都在 24K 左右。为了解决这个问题，作者提出了 Model merging 的方法&lt;/p>
&lt;h3 id="model-merging">Model Merging
&lt;/h3>&lt;p>作者使用 MergeKit 来完成 model merging 的任务。merge 的过程包括两步：&lt;/p>
&lt;ol>
&lt;li>构造一个 model soup, 包括 APO 的每个 checkpoint&lt;/li>
&lt;li>将 model soup 与 mid-training 的一个拥有强上下文能力的 checkpoint 结合起来，作者使用了 linear merge, APO model soup 和 mid-training checkpoint 的权重分别为 0.9 和 0.1.&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>[!tip] Recall
&lt;a class="link" href="https://maosong.website/p/notes-on-kimi-k1.5/" target="_blank" rel="noopener"
>Kimi-k1.5&lt;/a> 也使用了 model merging 的方法，来将 long CoT 模型的能力迁移到 short-CoT 模型上去&lt;/p>
&lt;/blockquote>
&lt;h2 id="evaluation">Evaluation
&lt;/h2>&lt;p>作者首先评估了一下 base model 的表现，评估使用了 12 个 benchmark, 对比的模型包括 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a>, Gemma3, LLaMA 3.2. 结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_base_model_performance.png"
width="2992"
height="2059"
srcset="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_base_model_performance_hu6476501870165868626.png 480w, https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_base_model_performance_hu9200080401812923224.png 1024w"
loading="lazy"
alt="SmolLM3 base model performance"
class="gallery-image"
data-flex-grow="145"
data-flex-basis="348px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_base_model_extact_performance.png"
width="2194"
height="954"
srcset="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_base_model_extact_performance_hu7414033305597534731.png 480w, https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_base_model_extact_performance_hu18133849881670103547.png 1024w"
loading="lazy"
alt="SmolLM3 base model performance"
class="gallery-image"
data-flex-grow="229"
data-flex-basis="551px"
>&lt;/p>
&lt;p>模型的多语种表现如下&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_multilingual_performance.png"
width="1600"
height="1062"
srcset="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_multilingual_performance_hu8100262855846916136.png 480w, https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_multilingual_performance_hu256869903175579211.png 1024w"
loading="lazy"
alt="SmolLM3 multilingual performance"
class="gallery-image"
data-flex-grow="150"
data-flex-basis="361px"
>&lt;/p>
&lt;p>Instruction (non-reasoning) 版本的评估结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_instruction_performance.png"
width="2992"
height="2064"
srcset="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_instruction_performance_hu6052638395361747764.png 480w, https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_instruction_performance_hu11311995837787702253.png 1024w"
loading="lazy"
alt="SmolLM3 Instruct models performance (w/o reasoning)"
class="gallery-image"
data-flex-grow="144"
data-flex-basis="347px"
>&lt;/p>
&lt;p>Reasoning 版本的评估结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_reasoning_performance.png"
width="2158"
height="1208"
srcset="https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_reasoning_performance_hu1378326931387372470.png 480w, https://maosong2022.github.io/p/notes-on-smollm3/SmolLM3_reasoning_performance_hu8158417547839893481.png 1024w"
loading="lazy"
alt="SmolLM3 reasoning performance"
class="gallery-image"
data-flex-grow="178"
data-flex-basis="428px"
>&lt;/p>
&lt;p>可以看到，Qwen3-4B 的表现是最好的。而 SmolLM3 的表现在 3B 左右也是非常强劲的&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者提出了 SmolLM3, 一个拥有长上下，文多语种以及 dual mode reasoning 能力的大语言模型，作者详细介绍了数据，训练以及 model merging 的技巧，来提高模型的表现。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://huggingface.co/blog/smollm3" target="_blank" rel="noopener"
>blog&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on GLM-4.1V-Thinking</title><link>https://maosong2022.github.io/p/notes-on-glm-4.1v-thinking/</link><pubDate>Mon, 14 Jul 2025 10:32:04 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-glm-4.1v-thinking/</guid><description>&lt;p>智谱 AI 在 25 年 7 月份发布了 GLM-4.1V-Thinking, 一个 9B 的多模态大语言模型，其在多个 benchmark 上达到了相同大小 MLLM 的 SOTA&lt;/p>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>已有工作如 &lt;a class="link" href="https://maosong.website/p/notes-on-mimo-vl/" target="_blank" rel="noopener"
>MiMo-VL&lt;/a> 和 V-Triune 主要集中在特定的领域，目前还没有一个在所有领域都能超过 non-reasoning model 表现的多模态 reasoning model.&lt;/p>
&lt;p>基于这个目标，作者提出了 GLM-4.1V-Thinking, 一个 9B 的，用于通用领域多模态 reasoning 的 MLLM. 在预训练阶段，作者通过构建多样化的数据来提升模型的基础能力。在 post-training 阶段，作者构建了 domain-specific 的数据来让模型学会 reasoning. 最后，作者提出了 Reinforcement Learning with Curriculum Sampling (RLCS) 来扩展模型的 reasoning 能力。&lt;/p>
&lt;p>作者主要发现如下：&lt;/p>
&lt;ol>
&lt;li>multi-domain RL 的泛化性非常强，在某一个 domain 上进行 RL 训练也可以提高模型在其他 domain 上的表现&lt;/li>
&lt;li>动态选择 RL 训练的数据可以有效提高模型的表现和训练效率&lt;/li>
&lt;li>一个 robust 以及 precise 的 reward model 对于 multi-domain RL 是至关重要的&lt;/li>
&lt;/ol>
&lt;h2 id="method">Method
&lt;/h2>&lt;h3 id="architecture">Architecture
&lt;/h3>&lt;p>GLM-4.1V-Thinking 是一个标准的 ViT-MLP-LLM 架构，其中：&lt;/p>
&lt;ul>
&lt;li>ViT: ViT 使用的是 AIMv2-Huge&lt;/li>
&lt;li>MLP: 是一个 3 层的 MLP，架构为 &lt;code>linear-LayerNorm-GELU-SwiGLU&lt;/code>, 并且在进入 MLP 之前，GLM-4.1V-Thinking 还会在 spatial 上进行降采样，降采样率为 2&lt;/li>
&lt;li>LLM: LLM 使用的是 GLM&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_Thinking_architecture.png"
width="1804"
height="988"
srcset="https://maosong2022.github.io/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_Thinking_architecture_hu14497388027985393186.png 480w, https://maosong2022.github.io/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_Thinking_architecture_hu7694229193987378695.png 1024w"
loading="lazy"
alt="Architecture of GLM4.1V-Thinking"
class="gallery-image"
data-flex-grow="182"
data-flex-basis="438px"
>&lt;/p>
&lt;p>与 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5-vl/" target="_blank" rel="noopener"
>Qwen2.5-VL&lt;/a> 一样，作者将 encoder 的 2D convolution 替换为了 3D convolution，用于处理视频输入，然后对于 single-image inputs，GLM-4.1V-Thinking 会将输入复制，变成一个含有两帧相同图片的 video 输入&lt;/p>
&lt;p>为了支持不同分辨率图片输入，作者进行了两点改进：&lt;/p>
&lt;p>第一是使用了 2D RoPE 来处理不同分辨率的图片输入&lt;/p>
&lt;p>第二是使用 bicubic interpolation 来将不同分辨率的图片适应到 encoder 的 position embedding 上。具体来讲，对一个拥有 $H_p\times W_p$ patches 的图片，每个 patch 的坐标 $(w,h)$ 首先会被 normalized 到 $[-1,1]$ 中：&lt;/p>
$$
g_{norm} = (w_{norm}, h_{norm}) = 2* \left(\frac{w+0.5}{W_p}, \frac{h+0.5}{H_p}\right) - 1
$$&lt;p>然后，我们再使用 bicubic interpolation $\mathcal{I}_{bicubic}$ 将坐标映射到原始的 position embedding 上：&lt;/p>
$$
P_{adapted} = \mathcal{I}_{bicubic}(P_{orig}, g_{norm})
$$&lt;p>对于视频输入，作者在每一帧之后插入了一个 time index token, 用每一帧的 timestamp string 来表示。这个做法与 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5-vl/" target="_blank" rel="noopener"
>Qwen2.5-VL&lt;/a> 一样。&lt;/p>
&lt;h3 id="pre-training">Pre-training
&lt;/h3>&lt;h4 id="pre-training-data">Pre-training Data
&lt;/h4>&lt;p>Pre-training 数据包括 image-caption data, interleaved image-text data, OCR data, grounding data, video data 和 instruction tuning data&lt;/p>
&lt;p>&lt;strong>Image caption data&lt;/strong>
Image caption data 用于提升模型的世界知识。作者从 LAION, DataComp, DNF 以及 Wukong 等数据集收集了 10B 的 image-text pairs, 然后进行数据清洗：&lt;/p>
&lt;ol>
&lt;li>Heuristic-based filtering: 基于规则，如 resolution, caption length 等过滤掉低质量的图片&lt;/li>
&lt;li>Relevance filtering: 计算 CLIP score, 过滤掉低质量的数据&lt;/li>
&lt;li>Concept-balanced resampling: 基于 MetaCLIP , 来提升数据的覆盖程度，解决长尾分布的问题。这里 &lt;a class="link" href="https://maosong.website/p/notes-on-seed1.5-vl/" target="_blank" rel="noopener"
>Seed1.5-VL&lt;/a> 有一个类似的消融实验。&lt;/li>
&lt;li>Factual-centered re-captioning: 通过 rewrite caption 来提升 caption 的质量和信息密度。Idefics2 通过实验发现，rewriting caption 可以提高模型的表现。作者在最终的数据集里，混合了一部分 re-caption 的数据。&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Interleaved image-text data&lt;/strong>
相比于 image-caption data, interleaved image-text data 体量更大，且逻辑关系更强。但是噪声也更多，因此作者针对不同来源的数据构建了不同的数据清洗策略：&lt;/p>
&lt;ol>
&lt;li>Web data processing pipeline: 数据来源为 MINT, MMC4, OmniCorpus 等。作者首先基于 CLIP-score，去掉与上下文不相关的图片；然后，作者去除掉广告，二维码等低质量的图片；最后，作者将图多文少的样本也给去除掉，比如相册图片。作者还训练了一个 high-knowledge-density image classifier, 来识别信息密度高的样本。&lt;/li>
&lt;li>Academic book processing pipeline: 作者从电子书中收集了 100M 的样本，这些电子书主要是 STEM domain，然后作者构建了一个 PDF parsing tool 来提取文档内容。&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>OCR data&lt;/strong>
OCR 数据包含&lt;strong>220M images&lt;/strong>, 数据集由三部分组成：&lt;/p>
&lt;ol>
&lt;li>Synthetic document images: 基于预训练语料，使用不同的格式来渲染文字生成不同的图片，然后基于 LAION 的图片作为背景&lt;/li>
&lt;li>Natural scene text images: 使用 Paddle-OCR 来从图片中提取文字以及对应的 bounding box&lt;/li>
&lt;li>Academic documents: 使用类似 Nougat 的方法来构建 PDF page-Markdown 的数据。&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Grounding data&lt;/strong>
主要包含自然场景和 GUI 两个 domain 的 grounding 数据：&lt;/p>
&lt;ol>
&lt;li>Natural image grounding: 基于 LAION-115M 得到。作者使用 GLIPv2, 来自动化解析 caption 中的实体，然后预测对应的 bounding box, 作者将 bounding box 较少的样本去除掉。最终得到&lt;strong>40M&lt;/strong>高质量的自然场景数据&lt;/li>
&lt;li>GUI grounding: 基于 CC 提取对应的 url, 然后使用 Playwright 来与网页进行交互，进而获取到所有的 DOM elements 与其对应的 bounding box. 最终收集到 &lt;strong>140M&lt;/strong>高质量的 QA 样本&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Video data&lt;/strong>
Video data 从多个来源得到，作者使用了 fine-grained human annotation 来保证数据质量。作者还标注了 camera motion 等信息&lt;/p>
&lt;p>为了保证数据的质量，作者进行了去重，来减少数据语义的相似性。&lt;/p>
&lt;p>&lt;strong>Instruction tuning data&lt;/strong>
instruction tuning data 的策略如下：&lt;/p>
&lt;ol>
&lt;li>Task coverage and taxonomy: 优化数据分布&lt;/li>
&lt;li>Complex scenario augmentation: 构建复杂的指令跟随数据&lt;/li>
&lt;li>Data contamination check: 数据污染检查&lt;/li>
&lt;/ol>
&lt;p>最终一共收集到 &lt;strong>50M&lt;/strong>样本，覆盖 visual perception, multimodal reasoning, GUI agent 等任务。&lt;/p>
&lt;h4 id="pre-training-training">Pre-training Training
&lt;/h4>&lt;p>Pre-training 由两个 stage 组成：&lt;/p>
&lt;ol>
&lt;li>Multimodal pre-training: 提高模型的通用多模态能力，上下文长度为 8192, global batch size 为 1536. 使用了 data packing 策略，使用了 2-way tensor parallelism&lt;/li>
&lt;li>Long-context continue training: 提升模型在高精度图片，长视频，长上下文场景下的表现，上下文长度为 32768, 使用了 2-way tensor parallelism 和 4-way context parallelism.&lt;/li>
&lt;/ol>
&lt;h3 id="sft">SFT
&lt;/h3>&lt;h4 id="sft-data">SFT Data
&lt;/h4>&lt;p>数据包括 long CoT reasoning 数据，来让模型以标准格式输出 multi-step solution.&lt;/p>
&lt;p>数据包括 verifiable tasks 以及 non-verifiable tasks, 覆盖中英两种语言。作者过滤了非常简单和非常困难的样本。&lt;/p>
&lt;p>数据格式如下&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;lt;think&amp;gt; {think_content} &amp;lt;/think&amp;gt; &amp;lt;answer&amp;gt; {answer_content} &amp;lt;/answer&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>对于 verifiable tasks, 输出格式为&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;lt;answer&amp;gt; &amp;lt;|begin_of_box|&amp;gt; {answer_content} &amp;lt;|end_of_box|&amp;gt; &amp;lt;/answer&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>数据清洗策略包括：&lt;/p>
&lt;ol>
&lt;li>严格遵守格式要求&lt;/li>
&lt;li>reasoning style 不一致或者有噪声&lt;/li>
&lt;li>混合语言输出。&lt;/li>
&lt;/ol>
&lt;p>作者还使用 RL checkpoints 来生成一部分高质量数据，加入到 SFT 阶段的数据集里面。&lt;/p>
&lt;h4 id="sft-training">SFT Training
&lt;/h4>&lt;p>全参数微调。上下文长度为 32768， batch size 为 32.数据包括 long-form reasoning data 以及纯文本数据，包括 math, multi-turn conversation, agent planning 以及 instruction following 等.&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Observation
作者发现，在 SFT 阶段即使是使用带噪声的 reasoning data，模型在 RL 阶段也能继续训练。也就是说，不完美的 reasoning trace 也能提供 guidance. 但是，使用更高质量的数据可以让模型 RL 阶段的训练更稳定且表现更好。&lt;/p>
&lt;/blockquote>
&lt;h3 id="rl">RL
&lt;/h3>&lt;p>与 &lt;a class="link" href="https://maosong.website/p/notes-on-seed1.5-vl/" target="_blank" rel="noopener"
>Seed1.5-VL&lt;/a> 一样，作者在 RL 阶段结合了 RLVR 和 RLHF 两个训练目标。任务包括 STEM problem solving (such as mathematics, physics, chemistry), grounding, optical character recognition (OCR), video understanding, GUI agents, chart and document understanding, logical reasoning, and instruction following.&lt;/p>
&lt;p>&lt;strong>Data&lt;/strong>
作者首先构建了一个任务集合，然后基于这些任务，来过滤或者生成对应的 QA pair, 接下来作者基于难度和质量来过滤，最后作者在每个 domain 上进行 RL 的训练来保证数据的有效性。&lt;/p>
&lt;p>&lt;strong>Reward modelling&lt;/strong>
作者构建了一个 reward system, 用于给不同 domain 的任务进行奖励。为了探究不同 domain 的 reward 对模型整体表现的影响，作者设计了一个 low-quality reward system，实验结果如下，可以看到模型在训练后期出现了 collapse 或者 reward hacking 的情况。&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_reward_ablation.png"
width="1778"
height="876"
srcset="https://maosong2022.github.io/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_reward_ablation_hu1940628557481340942.png 480w, https://maosong2022.github.io/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_reward_ablation_hu12923493226588456991.png 1024w"
loading="lazy"
alt="GLM-4.1V-Thinking reward ablation"
class="gallery-image"
data-flex-grow="202"
data-flex-basis="487px"
>&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Observation
作者发现，不同任务的不同信号奖励会对模型最终表现产生巨大影响。&lt;/p>
&lt;/blockquote>
&lt;p>在使用 LLM 提取模型回答的 answer 的时候，作者要求模型的最终答案为 &lt;code>&amp;lt;|begin_of_box|&amp;gt; {answer_content} &amp;lt;|end_of_box|&amp;gt;&lt;/code> 这种形式，避免大语言模型提取出错。&lt;/p>
&lt;p>作者构建的 reward system 包含如下模块：&lt;/p>
&lt;ol>
&lt;li>shared verification functions: 包括格式检查等&lt;/li>
&lt;li>domain specific modules: 与 domain 相关的模块&lt;/li>
&lt;li>unit testing: 观测 domain 输出的分布，然后基于输出的分布来调整 reward logic&lt;/li>
&lt;/ol>
&lt;p>最终 reward 的 design 如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_reward_design.png"
width="1812"
height="712"
srcset="https://maosong2022.github.io/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_reward_design_hu7442997355963364551.png 480w, https://maosong2022.github.io/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_reward_design_hu4476507324978246593.png 1024w"
loading="lazy"
alt="GLM-4.1V-Thinking reward design"
class="gallery-image"
data-flex-grow="254"
data-flex-basis="610px"
>&lt;/p>
&lt;p>&lt;strong>RLCS&lt;/strong>
作者使用了 GRPO 来进行优化训练。作者发现，仅需 200 training steps 模型在一半以上问题的准确率就达到了 $90%$.&lt;/p>
&lt;p>为了提升模型的训练效率，作者提出了 Reinforcement Learning with Curriculum Sampling (RLCS), 也就是将课程学习与 RL 结合起来。作者基于模型训练情况动态调整训练样本的难度，来保证每个样本对模型训练的提升都是最大的。&lt;/p>
&lt;p>在 RLCS 训练之前，作者先使用模型评测得到每个样本的 pass@k, 然后根据结果将样本进行分类。在训练过程中，作者记录每个样本的 pass@k， 然后动态更新其分类结果。在采样的时候，作者基于难度来对样本进行加权，来降低太简单或者太难样本的采样概率。&lt;/p>
&lt;p>作者还提出了一些提高 RL 表现的方法：&lt;/p>
&lt;ol>
&lt;li>Larger batch size: 使用更大的 batch size 效果更好&lt;/li>
&lt;li>Dynamic sampling expansion via ratio EMA: 作者定义了 naive 样本和高质量样本的比例，然后根据这个比例进行采样，来提升整体的采样效率。&lt;/li>
&lt;li>Force answering: 与 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a> 一样，对于比较难的问题，当输出的 token 数过长时，作者对模型输出进行截断，然后让模型基于已有思考过程直接输出结果。&lt;/li>
&lt;li>Discard KL loss: 与 DAPO 一样，作者也移除了 KL divergence loss&lt;/li>
&lt;li>Clip-higher: 与 DAPO 一样，作者通过修改超参数来提高模型的探索能力&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>[!tip] Observation&lt;/p>
&lt;ol>
&lt;li>RL 阶段的表现与 cold-start SFT 的表现并不完全相关。作者发现，cold-start SFT 取得更好的表现并不一定保证 RL 的表现就更好。&lt;/li>
&lt;li>RL 阶段各个 domain 数据的影响是正交的，这与 cold-start SFT 阶段的结果不同。&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>作者还提出了一些提高训练稳定性的方法：&lt;/p>
&lt;ol>
&lt;li>提高 cold-start SFT 阶段数据的质量&lt;/li>
&lt;li>移除 KL divergence loss&lt;/li>
&lt;li>使用 top-p 为 1，而不是 0.9 (如 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen-llm/" target="_blank" rel="noopener"
>Qwen-LLM&lt;/a> 中的设置)，top-p 为 1 可以 cover 整个 vocabulary，避免某些 token 没有参与训练&lt;/li>
&lt;li>per-sample loss 和 per-token loss 没有显著区别，但是 per-sample loss 稳定性更高&lt;/li>
&lt;li>在 cold-start SFT 阶段让模型学会格式要求，而不是在 RL 阶段加入 format reward&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Infra&lt;/strong>
在 infra 方面，作者做了如下改进：&lt;/p>
&lt;ol>
&lt;li>Load balancing of sequence lengths across DP ranks. 平衡每个 rank 的 sequensequence length 以及 compute load&lt;/li>
&lt;li>Intra-rank training with sequence packing and gradient accumulation. 将 sequence packing 和 gradient accumulation 结合起来&lt;/li>
&lt;li>Sample packing and reorganization within DP ranks. data packing&lt;/li>
&lt;li>Dynamic sampling expansion via ratio EMA. 平衡 naive 样本和高质量样本之间的比例&lt;/li>
&lt;/ol>
&lt;h2 id="evaluation">Evaluation
&lt;/h2>&lt;h3 id="performance">Performance
&lt;/h3>&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_performance.png"
width="1066"
height="1446"
srcset="https://maosong2022.github.io/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_performance_hu4663931888132339157.png 480w, https://maosong2022.github.io/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_performance_hu2656097992392796714.png 1024w"
loading="lazy"
alt="Performance of GLM4.1V Thinking"
class="gallery-image"
data-flex-grow="73"
data-flex-basis="176px"
>&lt;/p>
&lt;p>可以看到，GLM-4.1V-Thinking 在多个 benchmark 上都达到了 SOTA.&lt;/p>
&lt;h3 id="ablation-study">Ablation Study
&lt;/h3>&lt;p>作者评估了一下不同 domain 的 RL 训练对模型整体表现的影响。作者使用不同的数据来进行训练，然后分析结果的相关性，结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_cross_domain_generalization.png"
width="1226"
height="962"
srcset="https://maosong2022.github.io/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_cross_domain_generalization_hu16221901218333137952.png 480w, https://maosong2022.github.io/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_cross_domain_generalization_hu16525324023796861196.png 1024w"
loading="lazy"
alt="Cross-domain generalization in reinforcement learning"
class="gallery-image"
data-flex-grow="127"
data-flex-basis="305px"
>&lt;/p>
&lt;p>实验结果发现：&lt;/p>
&lt;ol>
&lt;li>在某个 domain 上的训练会提高模型在其他 domain 上的表现&lt;/li>
&lt;li>联合多个 domain 进行训练最终的表现会更好&lt;/li>
&lt;/ol>
&lt;p>作者还发现，不同的任务之间存在关联，如 GUI-agent 和 grounding 这两个任务是高度相关的。OCR &amp;amp; Chart 以及 GUI-agent 也是高度相关的。&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>在本文中，作者提出了 GLM-4.1V-Thinking, 一个 9B 的通用多模态 reasoning model, 作者通过构建高质量的数据，多阶段的训练，让模型在 reasoning 和 non-reasoning 任务上均超过了已有的 SOTA 模型的表现&lt;/p>
&lt;p>作者认为，模型有以下局限：&lt;/p>
&lt;ol>
&lt;li>模型并没有提升 reasoning 的质量，在某些情况下，模型结果正确，但是中间过程错误。作者分析这是因为目前只有针对结果的奖励机制。因此，未来需要能够评估中间过程的 reward model&lt;/li>
&lt;li>RL 的训练具有不稳定性，作者发现模型对设置比较敏感，这对 RL 训练的 scaling 提出了挑战。&lt;/li>
&lt;li>模型在复杂场景下表现依旧不太好，比如图片中有物体遮挡的情况，这些感知误差也会影响最终的 reasoning 表现。作者认为如何同时提高 perception 和 reasoning 的表现是一个需要研究的课题。&lt;/li>
&lt;/ol>
&lt;p>未来的工作有：&lt;/p>
&lt;ol>
&lt;li>如何构建更好的 reward 机制，来同时评估结果和中间过程。从而防止模型 reward hacking&lt;/li>
&lt;li>如何基于 multimodal training 来提升模型在纯文本任务上的表现。探究 visual reasoning 和 text reasoning 如何互相促进也是一个课题&lt;/li>
&lt;li>如何更好评估模型的表现，即如何更好评估模型的 failure modes, 比如幻觉，short reasoning 等&lt;/li>
&lt;/ol>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2507.01006" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://huggingface.co/THUDM/GLM-4.1V-9B-Thinking" target="_blank" rel="noopener"
>Huggingface&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="appendix">Appendix
&lt;/h2>&lt;p>GLM-4.1V-Thinking 的 patch merger 代码&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Glm4vVisionPatchMerger&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">context_dim&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_act&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">bool&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="fm">__init__&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">bias&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">post_projection_norm&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">LayerNorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">gate_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">context_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">bias&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">up_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">context_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">bias&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">down_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">context_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">bias&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">act1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">GELU&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">act_fn&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ACT2FN&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">hidden_act&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_state&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_state&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_state&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_state&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">act1&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">post_projection_norm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_state&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">down_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">act_fn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">gate_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_state&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">up_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_state&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Glm4vVisionModel&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">downsample&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Conv2d&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">in_channels&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">out_channels&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">out_hidden_size&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">kernel_size&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">spatial_merge_size&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">stride&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">spatial_merge_size&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">merger&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Glm4vVisionPatchMerger&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">out_hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">context_dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">intermediate_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_act&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_act&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">...&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">spatial_merge_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">spatial_merge_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">permute&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">downsample&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">out_hidden_size&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">merger&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>Notes on QwQ-32B</title><link>https://maosong2022.github.io/p/notes-on-qwq-32b/</link><pubDate>Sat, 08 Mar 2025 09:46:16 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-qwq-32b/</guid><description>&lt;h1 id="introduction">Introduction
&lt;/h1>&lt;p>通义团队在3月6号发布了QwQ-32B，一个基于RL的，32B参数的reasoning model，QwQ-32B的表现可以与DeepSeek-R1相比&lt;/p>
&lt;h1 id="模型架构">模型架构
&lt;/h1>&lt;p>QwQ-32B基于Qwen2.5-32B开发。主要通过基于outcome-based rewards的RL进行训练。训练过程包括两个stage&lt;/p>
&lt;ol>
&lt;li>Stage 1：本阶段在math和coding domain上进行RL的训练，作者使用了一个verifier来保证最终数学问题结果的正确性，使用了一个code execution server来保证最终生成代码的正确性。&lt;/li>
&lt;li>Stage 2：本阶段在通用领域上进行RL的训练。模型基于general reward model和一些rule-based verifier进行训练。应该是类似DeepSeek-R1的规则。&lt;/li>
&lt;/ol>
&lt;h1 id="实验结果">实验结果
&lt;/h1>&lt;p>实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-qwq-32b/qwq-32b-final.jpg"
width="3035"
height="1713"
srcset="https://maosong2022.github.io/p/notes-on-qwq-32b/qwq-32b-final_hu8354107095607630009.jpg 480w, https://maosong2022.github.io/p/notes-on-qwq-32b/qwq-32b-final_hu6845858612094320689.jpg 1024w"
loading="lazy"
alt="QwQ_evaluation"
class="gallery-image"
data-flex-grow="177"
data-flex-basis="425px"
>&lt;/p>
&lt;h1 id="reference">Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://qwenlm.github.io/blog/qwq-32b/" target="_blank" rel="noopener"
>blog&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://chat.qwen.ai/?models=Qwen2.5-Plus" target="_blank" rel="noopener"
>demo&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Kimi k1.5</title><link>https://maosong2022.github.io/p/notes-on-kimi-k1.5/</link><pubDate>Sat, 08 Feb 2025 10:09:52 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-kimi-k1.5/</guid><description>&lt;p>论文提出了Kimi k1.5， 一个基于强化学习训练的多模态推理模型。作者介绍了Kimi k1.5的训练方法，以及infra上的优化。作者的主要贡献如下：&lt;/p>
&lt;ol>
&lt;li>作者发现，模型的上下文长度可以有效提升LLM的推理能力。&lt;/li>
&lt;li>作者提出了基于online mirror descent的强化学习训练方法。&lt;/li>
&lt;li>作者发现可以通过long context scaling和policy optimization来提升模型的推理能力。&lt;/li>
&lt;li>作者提出了long2short的推理方法，可以有效提升short CoT模型的推理能力。&lt;/li>
&lt;/ol>
&lt;h2 id="architecture">Architecture
&lt;/h2>&lt;p>论文中包括两个模型，一个是k1.5 base model， 其是一个多模态大模型。另一个是Kimi-1.5 reasoning model（简称为k1.5），k1.5是基于kimi-1.5 base model， 通过强化学习训练得到的推理模型。&lt;/p>
&lt;h3 id="kimi-15-base-model">Kimi-1.5 base model
&lt;/h3>&lt;p>k1.5 base model是一个基于transformer的多模态大模型，论文没有给出详细的模型结构，只有如下的示意图&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-kimi-k1.5/k1-5-model-arch.png"
width="1210"
height="287"
srcset="https://maosong2022.github.io/p/notes-on-kimi-k1.5/k1-5-model-arch_hu1937206545208930311.png 480w, https://maosong2022.github.io/p/notes-on-kimi-k1.5/k1-5-model-arch_hu10349051156390148271.png 1024w"
loading="lazy"
alt="Architecture of k1.5 base model"
class="gallery-image"
data-flex-grow="421"
data-flex-basis="1011px"
>&lt;/p>
&lt;p>k1.5 base model的训练包括三个阶段：&lt;/p>
&lt;ol>
&lt;li>Vision-language pretraining stage: 这一阶段的目的是让模型拥有语言和视觉的表征能力。训练时，模型先在纯文本的数据上进行训练，然后提高加入图文交错的数据进行训练，直到训练数据中图文交错的数据占比达到30%。模型的更新方式为先冻结LLM更新visual tower, 然后解冻LLM更新所有参数。&lt;/li>
&lt;li>Vision-language cooldown stage: 这一阶段的目的是让模型保持多模态理解能力。作者发现，使用合成数据可以提高模型在这一阶段的表现。因此，作者使用闭源大语言模型，基于math, knowledge和code domain的pretraining data来合成了一些QA pair数据对，然后讲这些QA pair数据对加入到训练数据中。&lt;/li>
&lt;li>Long-context activation stage: 这一阶段的目的是让模型在长上下文的数据上进行训练，以提升模型在长上下文上的理解能力。这一阶段包含了40%的full attention data以及60%的partial attention data。其中，full attention data是基于真实数据和合成的QA以及summary数据得到的，partial attention data是在cooldown data找那个通过均匀采样得到的。这一阶段，模型的上下文长度从4096逐步增加到131072。&lt;/li>
&lt;/ol>
&lt;h3 id="kimi-k15">Kimi k1.5
&lt;/h3>&lt;p>Kimi k1.5是基于k1.5 base model， 通过进一步训练得到的推理模型。k1.5的训练包括四个阶段：&lt;/p>
&lt;ol>
&lt;li>pretraining：这一步就是k1.5 base model的训练， 包括三个小阶段。前面已经介绍过了。&lt;/li>
&lt;li>vanilla SFT：这一步的目的是使得k1.5 base model具有指令跟随能力。对于non-reasoning任务，作者先使用人类标注产生一个seed dataset，然后基于seed dataset训练一个seed model，最后根据收集的prompt来使用seed model来生成回答。对于reasoning任务，作者使用了rejection sampling来扩展SFT数据集&lt;/li>
&lt;li>long-CoT SFT： 这一步的目的是让模型能够像人类一样进行推理，能够掌握最基本的推理策略，即：planning，evaluation，reflection和exploration。从而为RL训练提供一个良好的初始化。&lt;/li>
&lt;li>RL：这一步就是通过RL来提高模型的推理能力。我们在下一节中进行详细的介绍。&lt;/li>
&lt;/ol>
&lt;h2 id="rl">RL
&lt;/h2>&lt;h3 id="problem-definition">Problem Definition
&lt;/h3>&lt;p>给定一个训练数据集 $\mathcal{D} = {(x_i, y_i^\star)}_ {i=1}^n$， 其中 $x_ i$ 是问题， $y_{i}^\star$ 是ground truth。我们希望找到一个模型 $\pi_{\theta}$ ，来解决这个问题。通常问题比较难，因此我们使用chain of thought（CoT）的方法来解决这个问题。具体做法就是让模型输出中间步骤 $z=(z_1, z_2, &amp;hellip;, z_m)$， 来连接问题 $x_i$ 和答案 $y$。其中， $z_j$ 是模型在第 $j$ 步的推理结果，即 $z_t\sim\pi_{\theta}(x_i, z_{&amp;lt;t})$, $y\sim \pi_{\theta}(x_i, z)$。&lt;/p>
&lt;p>通常，我们还会使用一个reward model $r_{\phi}$，来评估模型输出的答案的质量。一个常用的reward model是基于答案的正确性来评估的，即 $r_{\phi}(x_i, y, y^\star) = \mathbb{I}[y=y_i^\star]$ 。这样，我们要求解的问题就变成了最大化中间步骤和最终答案的reward，即&lt;/p>
$$
\max_{\theta} \mathbb{E}_ {(x, y^\star)\sim\mathcal{D},(y,z,)\sim\pi_{\theta}} r_{\phi}(x, y, y^\star)
$$&lt;h3 id="policy-optimization">Policy Optimization
&lt;/h3>&lt;p>作者使用了online policy mirror descent来解决上面提到的优化问题。在每个iteration中，存在一个reference model $\pi_{\theta_r}$ ， 以及一个当前要更新的模型 $\pi_{\theta}$ 。online policy mirror descent要解决的优化问题为：&lt;/p>
$$
\min_{\theta} \mathbb{E}_ {(x, y^\star)\sim\mathcal{D}} \left[\mathbb{E}_ {(y,z,)\sim\pi_{\theta}}\left[r_{\phi}(x, y, y^\star) - \beta \mathcal{D}_ {KL}(\pi_{\theta}(y,z\mid x)\Vert \pi_{\theta_r}(y,z\mid x))\right]\right]
$$&lt;p>其中， $\beta$ 是超参数，用于平衡reward和KL散度 $\mathcal{D}_{KL}(\cdot)$.&lt;/p>
&lt;p>上述问题有一个闭式解，即：&lt;/p>
$$
\pi^\star(y,z\mid x)=\pi_{\theta_r}(y,z\mid x)\exp\left(\frac{r_{\phi}(x, y, y^\star)}{\beta}\right)/Z(x;\beta)
$$&lt;p>其中，&lt;/p>
$$
Z(x;\beta):=\sum_{y,z}\pi_{\theta_r}(y,z\mid x)\exp\left(\frac{r_{\phi}(x, y, y^\star)}{\beta}\right)
$$&lt;p>是归一化因子。在闭式解的表达式中，我们对两边取对数，得到：&lt;/p>
$$
r_{\phi}(x, y, y^\star) - \beta \log Z - \beta \log\frac{\pi^\star(y,z\mid x)}{\pi_{\theta_r}(y,z\mid x)}=0
$$&lt;p>这是最优策略满足的等式。作者这里使用了L2 loss来优化当前策略 $\pi_{\theta}$ ，即：&lt;/p>
$$
\min_{\theta} \mathbb{E}_ {(x, y^\star)\sim\mathcal{D}} \left[\mathbb{E}_ {(y,z,)\sim\pi_ {\theta_r}}\left[r_{\phi}(x, y, y^\star) - \beta \log Z - \beta \log\frac{\pi_{\theta}(y,z\mid x)}{\pi_{\theta_r}(y,z\mid x)}\right]^2\right]
$$&lt;p>这里需要注意的一点是response $(y,z)$ 是基于reference model $\pi_{\theta_r}$ 生成的，而不再是基于当前策略 $\pi_{\theta}$ 生成的。这是Kimi-1.5的RL训练和传统RL训练的一个不同点。这也是为什么k1.5说他是off-policy的原因。&lt;/p>
&lt;p>接下来，我们就可以对上面的目标函数求梯度，我们将里层的函数展开为 $(a-b-c)^2=(a-b)^2-2(a-b)c+c^2$ 的形式，然后对 $\theta$ 求梯度，得到：&lt;/p>
$$
\mathbb{E}_ {(x, y^\star)\sim\mathcal{D}} \left[\mathbb{E}_ {(y,z,)\sim\pi_{\theta_r}}\left[\nabla_{\theta}\log \pi_{\theta}(y,z\mid x)(r_{\phi}(x, y, y^\star) - \beta \log Z) - \frac{\beta}{2} \nabla_{\theta}\left(\log\frac{\pi_{\theta}(y,z\mid x)}{\pi_{\theta_r}(y,z\mid x)}\right)^2\right]\right]
$$&lt;p>如果我们对每个问题$x$,采样 $k$ 个response $(y,z)$ ，那么我们可以近似上式中内部的期望，得到：&lt;/p>
$$
\frac{1}{k}\sum_{i=1}^k\left[\nabla_{\theta}\log \pi_{\theta}(y_i,z_i\mid x)(r_{\phi}(x, y_i, y^\star) - \bar{r}) - \frac{\beta}{2} \nabla_{\theta}\left(\log\frac{\pi_{\theta}(y_i,z_i\mid x)}{\pi_{\theta_r}(y_i,z_i\mid x)}\right)^2\right]
$$&lt;p>这里， $\bar{r}$ 是 $\beta \log Z$ 的估计值。作者提供了两种估计方法：&lt;/p>
&lt;ol>
&lt;li>基于采样的 $(y_i,z_i)\sim \pi_{\theta_r}$ 对 $\beta \log Z$ 进行估计：&lt;/li>
&lt;/ol>
$$
\bar{r}=\beta\log\frac{1}{k}\sum_{j=1}^k\exp\left(\frac{r_{\phi}(x, y_j, y^\star)}{\beta}\right)
$$&lt;ol start="2">
&lt;li>直接使用samples rewards的平均值来近似：&lt;/li>
&lt;/ol>
$$
\bar{r}=\frac{1}{k}\sum_{i=1}^k r_{\phi}(x, y_i, y^\star)
$$&lt;p>作者提到，第二种方法效果很好并且计算效率更高，因此作者在实验中使用了第二种方法。&lt;/p>
&lt;p>训练时，作者每次从数据集中采样一个batch的样本，然后使用上述的梯度更新模型。模型参数更新完之后，作者将新的模型作为reference model，然后重复上述过程。&lt;/p>
&lt;blockquote>
&lt;p>Remark
作者还探究了为什么不使用value network。作者认为，在强化学习中，value network通常用于评估当前状态的价值，但是在reasoning model中，训练的目的是让模型能够充分探索，以提高模型在不同任务中的泛化性。因此，作者认为使用value network可能会限制模型的探索能力。&lt;/p>
&lt;/blockquote>
&lt;h3 id="training-strategy">Training Strategy
&lt;/h3>&lt;p>作者提出了两个提高采样效率的方法：&lt;/p>
&lt;ol>
&lt;li>Curriculum Sampling. 这种采样方式会将问题按照难度进行排序，然后按照难度从易到难进行采样。以逐步提高模型的推理能力。&lt;/li>
&lt;li>Prioritized Sampling.作者还采用了prioritized sampling来提高采样效率。也就是说，对于回答错误的问题，模型会进行更多的采样。&lt;/li>
&lt;/ol>
&lt;h2 id="infra">Infra
&lt;/h2>&lt;h3 id="training-system">Training System
&lt;/h3>&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-kimi-k1.5/k1-5-training-system.png"
width="1112"
height="530"
srcset="https://maosong2022.github.io/p/notes-on-kimi-k1.5/k1-5-training-system_hu56126063401246354.png 480w, https://maosong2022.github.io/p/notes-on-kimi-k1.5/k1-5-training-system_hu14529664632197223449.png 1024w"
loading="lazy"
alt="Large Scale Reinforcement Learning Training System for LLM"
class="gallery-image"
data-flex-grow="209"
data-flex-basis="503px"
>&lt;/p>
&lt;p>为了高效地进行RL训练，作者构建了一个iterative synchronous RL训练框架。框架如上图所示。框架的创新之处在于&lt;strong>Partial Rollout&lt;/strong>，该方法可以更高效的处理复杂的推理轨迹。框架包含了以下几部分：&lt;/p>
&lt;ol>
&lt;li>rollout worker: 该部分负责生成推理轨迹。然后将生成的推理轨迹加入到replay buffer中。&lt;/li>
&lt;li>master: 管理data flow和replay buffer和train worker之间的通信。&lt;/li>
&lt;li>train worker: 根据获取的rollout轨迹更新模型。&lt;/li>
&lt;li>code execution service: 在代码相关任务中，该部分负责执行代码，并返回执行结果。&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Partial Rollout&lt;/strong>：Partial Rollout的目的是为了整体的训练效率。其做法就是给定一个固定的outpu token budget，然后限制推理轨迹的长度。当长度超过token限制时，没有完成的部分就会被存入到replay buffer中，以供后续的训练使用。&lt;/p>
&lt;h3 id="deployment-framework">Deployment Framework
&lt;/h3>&lt;p>根据前面的介绍，我们知道k1.5每个iteration的训练包括两步：&lt;/p>
&lt;ol>
&lt;li>inference phase: 基于Reference model进行采样，生成推理轨迹。&lt;/li>
&lt;li>training phase: 基于采样得到的推理轨迹，更新模型。&lt;/li>
&lt;/ol>
&lt;p>如果我们直接使用Megatron和vLLM来实现上述的训练和推理，那么会存在以下问题：&lt;/p>
&lt;ol>
&lt;li>Megatron和vLLM的并行策略可能并不一致，因此很难进行参数共享&lt;/li>
&lt;li>在训练过程中，vLLM可能会保留一些GPU，这就导致了训练和推理的资源分配不均。&lt;/li>
&lt;li>推理和训练的资源需求可能并不一致，无法动态调控显卡资源&lt;/li>
&lt;/ol>
&lt;p>因此，作者构建了一个hybrid deployment framework来解决上述问题，其框架如下图所示。&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-kimi-k1.5/k1-5-hybrid-deployment-framework.png"
width="867"
height="560"
srcset="https://maosong2022.github.io/p/notes-on-kimi-k1.5/k1-5-hybrid-deployment-framework_hu6906289130535590140.png 480w, https://maosong2022.github.io/p/notes-on-kimi-k1.5/k1-5-hybrid-deployment-framework_hu10807671325725203551.png 1024w"
loading="lazy"
alt="Hybrid Deployment Framework"
class="gallery-image"
data-flex-grow="154"
data-flex-basis="371px"
>&lt;/p>
&lt;p>该框架的主要优势在于使用Kubernetes Sidecar containers来在一个pod中共享所有的GPU资源。这样就避免了资源分配不均的问题。&lt;/p>
&lt;h2 id="数据">数据
&lt;/h2>&lt;h3 id="数据处理">数据处理
&lt;/h3>&lt;ol>
&lt;li>RL prompt set. 作者认为prompt的quality和diversity对RL训练至关重要。因此作者基于diverse coverage, balanced difficuty以及accurate evaluation来构建了一个RL prompt set。对于多样性，作者确保数据集覆盖多个domain。对于平衡难度，作者通过多次采样，根据通过率来给每个样本的难易程度打分。对于准确性，作者筛掉了多选题，正确题和证明题，确保模型能够生成正确的推理步骤。最后，作者还让模型进行猜测，如果模型猜测的正确率比较高，那么就认为该样本是easy-to-hack的，就会筛掉。&lt;/li>
&lt;li>Test Case Generation for Coding. 作者使用&lt;a class="link" href="https://github.com/luogu-dev/cyaron" target="_blank" rel="noopener"
>CYaRon&lt;/a>来生成coding任务的测试用例。通过与Kimi k1.5结合，作者构建了一个包含323个测试用例的训练集。&lt;/li>
&lt;li>Reward modeling for Math. 作者使用了两种方法来提高reward model的准确性。第一个是classic RM，作者基于InstructGPT，构造了大约800K的数据训练了一个value-head based RM。模型会基于问题，参考答案和模型生成的答案来判断模型生成的答案是否正确。第二个是Chain of Thought RM，作者基于收集的800k CoT数据对kimi进行了finetune，然后对模型生成的推理步骤进行打分。最终发现，Chain of Thought RM的效果更好。因此，作者在RL训练中使用了Chain of Thought RM。&lt;/li>
&lt;li>Vision RL Data。 作者从三个方面构建了vision RL数据集：real-world data, synthetic visual reasoning data以及text-rendered data.&lt;/li>
&lt;/ol>
&lt;h3 id="数据集">数据集
&lt;/h3>&lt;ol>
&lt;li>pretraining：其中，纯文本数据包括Engligh, Chinese, Code, math reasoning以及Knowledge这5个领域。多模态数据包括captioning, image-text interleaving, OCR, Knowledge以及QA数据集等。附录B介绍了数据集的来源和处理过程。&lt;/li>
&lt;li>SFT： vanilla SFT数据集包含1M的纯文本数据，其中包含500K的general QA数据，200K的coding数据，200K的数学和科学数据，5K的创作写作数据以及20K的长上下文任务（总结，QA，翻译和写作等）。作者还构建了1M的图文数据，包括chart理解，OCR，grounding，visual coding, visual reasoning以及visual aided math/science problems等&lt;/li>
&lt;li>long-CoT SFT：该阶段的数据基于refined RL prompt set，使用了prompt engineering来构建了一个少量但高质量的long-CoT数据集。用于将reasoning能力内化到模型中。&lt;/li>
&lt;/ol>
&lt;h2 id="long2short">Long2short
&lt;/h2>&lt;p>为了降低推理成本，作者提出了long2short的推理方法。该方法通过将长上下文推理转化为短上下文推理，让模型能够更加高效的完成推理。作者尝试了几种方法来实现long2short：&lt;/p>
&lt;ol>
&lt;li>Model merging:将long-CoT模型和short-CoT模型进行合并，然后进行推理。这里采用的办法是对两个模型的权重取平均。&lt;/li>
&lt;li>shortest rejection sampling：通过对同一个问题进行多次采样（论文中每个问题采样8次），然后选择最短的，正确的推理步骤作为最终答案。&lt;/li>
&lt;li>DPO：通过对同一个问题进行多次采样，选择最短的正确的回答作为正样本，最长的回答作为负样本，然后使用DPO进行训练。&lt;/li>
&lt;li>Long2short RL：作者在RL阶段，选择一个在performance和token efficiency之间达到平衡的模型作为 base model，然后加入了一个long2short RL训练阶段。该阶段，作者加入了length penalty以及降低了maximum rollout length来鼓励模型生成更短的推理步骤。&lt;/li>
&lt;/ol>
&lt;h3 id="length-penalty">Length penalty
&lt;/h3>&lt;p>Length penalty的目的是降低模型的overthinking和训练成本。作者加入了一个length reward，对于问题 $x$ 和采样的回答 $(y_i,z_i)$ ，回答 $(y_i,z_i)$ 的length定义为 $len(x_i)=\text{length}([z_i, y_i])$ length reward定义为：&lt;/p>
$$
r_{\text{length}}(x, (y_i,z_i))=\begin{cases}
\lambda, &amp; \text{if } r(x, y_i, y^\star) = 1 \\
\min(0,\lambda), &amp; \text{if } r(x, y_i, y^\star) = 0 \\
\end{cases}
$$&lt;p>其中&lt;/p>
$$
\lambda = 0.5 - \frac{len(x_i)-\min_{j}len(x_j)}{\max_{j}len(x_j)-\min_{j}len(x_j)}
$$&lt;p>也就是说，当回答正确时，我们鼓励模型生成更长的推理步骤。反之，我们鼓励模型生成更短的推理步骤。&lt;/p>
&lt;h2 id="实验">实验
&lt;/h2>&lt;h3 id="实验结果">实验结果
&lt;/h3>&lt;ol>
&lt;li>K1.5在long-CoT任务上的表现&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-kimi-k1.5/k1-5-long-cot-benchmark.png"
width="1118"
height="425"
srcset="https://maosong2022.github.io/p/notes-on-kimi-k1.5/k1-5-long-cot-benchmark_hu6952644691331158148.png 480w, https://maosong2022.github.io/p/notes-on-kimi-k1.5/k1-5-long-cot-benchmark_hu1430225520164276888.png 1024w"
loading="lazy"
alt="Long-CoT-benchmark"
class="gallery-image"
data-flex-grow="263"
data-flex-basis="631px"
>&lt;/p>
&lt;ol start="2">
&lt;li>K1.5在Short-CoT任务上的表现&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-kimi-k1.5/k1-5-short-cot-benchmark.png"
width="1357"
height="566"
srcset="https://maosong2022.github.io/p/notes-on-kimi-k1.5/k1-5-short-cot-benchmark_hu3005992739822340121.png 480w, https://maosong2022.github.io/p/notes-on-kimi-k1.5/k1-5-short-cot-benchmark_hu15186336803595586566.png 1024w"
loading="lazy"
alt="Short-CoT-benchmark"
class="gallery-image"
data-flex-grow="239"
data-flex-basis="575px"
>&lt;/p>
&lt;ol start="3">
&lt;li>
&lt;p>Long Context Scaling
作者在实验中还探究了上下文长度对模型推理能力的影响。作者针对几个模型进行了实验，结果在图5和图6里面。结果发现，随着上下文长度的增加，模型的推理能力会逐渐提升。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Long2short&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-kimi-k1.5/k1-5-long2short.png"
width="1354"
height="590"
srcset="https://maosong2022.github.io/p/notes-on-kimi-k1.5/k1-5-long2short_hu3211336959358196266.png 480w, https://maosong2022.github.io/p/notes-on-kimi-k1.5/k1-5-long2short_hu6402801847399781857.png 1024w"
loading="lazy"
alt="Long2short"
class="gallery-image"
data-flex-grow="229"
data-flex-basis="550px"
>&lt;/p>
&lt;h3 id="消融实验">消融实验
&lt;/h3>&lt;ol>
&lt;li>Scaling of model size and context length. 作者探究了模型大小和上下文长度对模型推理能力的影响，结果在图8里面。结果发现
&lt;ol>
&lt;li>小模型可以通过Long CoT来达到大模型的效果&lt;/li>
&lt;li>大模型具有更高的token效率&lt;/li>
&lt;li>理想情况下，使用具有更长上下文的大模型，可以同时达到更高的推理能力和更高的token效率，否则，使用上下文长度更长的小模型&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Effects of using negative gradients. 作者探究了使用负梯度对模型推理能力的影响。作者与ReST方法进行了对比，结果在图10里面。结果发现，ReST回到只模型产生更长的推理步骤。作者认为，选取合适的优化方法，可以有效提升模型的推理能力。&lt;/li>
&lt;li>Sampling strategies. 作者还探究了采样策略对模型推理能力的影响。作者与均匀采样进行了对比，结果在图9里面。结果发现，论文提出的采样策略可以有效提升模型的推理能力。&lt;/li>
&lt;/ol>
&lt;h2 id="结论">结论
&lt;/h2>&lt;p>作者提出了Kimi k1.5，一个基于强化学习训练的多模态推理模型。作者介绍了Kimi k1.5的训练方法，数据集，infra上的优化以及long2short的推理方法。作者的主要贡献在于：&lt;/p>
&lt;ol>
&lt;li>作者发现，模型的上下文长度可以有效提升LLM的推理能力。&lt;/li>
&lt;li>作者提出了基于online mirror descent的强化学习训练方法。&lt;/li>
&lt;li>作者提出了long2short的推理方法，可以有效提升short CoT模型的推理能力。&lt;/li>
&lt;/ol>
&lt;p>论文的问题在于对于多模态的内容介绍较少，感觉还是更倾向于文本推理任务。&lt;/p>
&lt;h2 id="参考文献">参考文献
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2501.12599" target="_blank" rel="noopener"
>Kimi k1.5: Scaling Reinforcement Learning with LLMs&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>