<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Seed on Mao Song(毛松)'s Homepage</title><link>https://maosong2022.github.io/tags/seed/</link><description>Recent content in Seed on Mao Song(毛松)'s Homepage</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 18 Jul 2025 14:59:35 +0800</lastBuildDate><atom:link href="https://maosong2022.github.io/tags/seed/index.xml" rel="self" type="application/rss+xml"/><item><title>Notes on Seed1.6</title><link>https://maosong2022.github.io/p/notes-on-seed1.6/</link><pubDate>Fri, 18 Jul 2025 14:59:35 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-seed1.6/</guid><description>&lt;p>Seed 在 2025 年 6 月 25 号发布了 Seed 1.6，支持 adaptive deep thinking, multimodal understanding, GUI-based interaction, and deep reasoning with a 256K context window&lt;/p>
&lt;p>Seed 1.6 基于 Seed 1.5 开发，是一个 MoE 模型, 总参数为 230B, 激活参数为 23B.&lt;/p>
&lt;h2 id="pre-training">Pre-training
&lt;/h2>&lt;p>pre-training 分为 3 个 stage:&lt;/p>
&lt;ol>
&lt;li>Text-only pre-training: 使用 web pages, books, papers, code 以及其他数据来进行训练，作者使用了 rule-based 以及 model-based 策略来提高数据的质量。&lt;/li>
&lt;li>Multimodal mixed continue training: 提高学科，代码以及 reasoning 相关的数据，来进一步提高知识密度以及 reasoning 的复杂度。同时，加入高质量的视觉数据。&lt;/li>
&lt;li>Long-context continual training: 将模型的上下文长度从 32K 扩展到 256K&lt;/li>
&lt;/ol>
&lt;p>Seed 1.6 Base 的表现&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Seed1.6 Base&lt;/th>
&lt;th>LLaMA-4 -Maverick Base&lt;/th>
&lt;th>DeepSeek-V3Base&lt;/th>
&lt;th>Qwen3-235B-A22B Base&lt;/th>
&lt;th>Seed1.5 Base&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>MMLU&lt;/td>
&lt;td>&lt;strong>88.83&lt;/strong>&lt;/td>
&lt;td>85.16&lt;/td>
&lt;td>87.19&lt;/td>
&lt;td>87.81&lt;/td>
&lt;td>88.35&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MMLU-Pro&lt;/td>
&lt;td>&lt;strong>69.98&lt;/strong>&lt;/td>
&lt;td>63.91&lt;/td>
&lt;td>59.84&lt;/td>
&lt;td>68.18&lt;/td>
&lt;td>66.47&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SuperGPQA&lt;/td>
&lt;td>&lt;strong>45.08&lt;/strong>&lt;/td>
&lt;td>40.85&lt;/td>
&lt;td>41.53&lt;/td>
&lt;td>44.06&lt;/td>
&lt;td>36.81&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>BBH&lt;/td>
&lt;td>&lt;strong>92.08&lt;/strong>&lt;/td>
&lt;td>83.62&lt;/td>
&lt;td>86.22&lt;/td>
&lt;td>88.87&lt;/td>
&lt;td>88.36&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GPQA-Diamond&lt;/td>
&lt;td>43.43&lt;/td>
&lt;td>43.94&lt;/td>
&lt;td>41.92&lt;/td>
&lt;td>&lt;strong>47.47&lt;/strong>&lt;/td>
&lt;td>45.25&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GSM8k&lt;/td>
&lt;td>93.10&lt;/td>
&lt;td>87.72&lt;/td>
&lt;td>87.57&lt;/td>
&lt;td>&lt;strong>94.39&lt;/strong>&lt;/td>
&lt;td>89.99&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MATH&lt;/td>
&lt;td>&lt;strong>72.86&lt;/strong>&lt;/td>
&lt;td>63.32&lt;/td>
&lt;td>62.62&lt;/td>
&lt;td>71.84&lt;/td>
&lt;td>66.18&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MBPP&lt;/td>
&lt;td>&lt;strong>83.60&lt;/strong>&lt;/td>
&lt;td>75.40&lt;/td>
&lt;td>74.20&lt;/td>
&lt;td>81.40&lt;/td>
&lt;td>81.40&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="post-training">Post-training
&lt;/h2>&lt;p>作者在 post-training 构建了两个变体：&lt;/p>
&lt;ol>
&lt;li>Seed1.6-Thinking: 先进的多模态 reasoning 能力&lt;/li>
&lt;li>Seed1.5(Adaptive CoT): 使用 AdaCoT 来压缩 CoT 的长度，提高效率&lt;/li>
&lt;/ol>
&lt;h3 id="seed16-thinking">Seed1.6-Thinking
&lt;/h3>&lt;p>Seed1.6-Thinking 与 Seed-Thinking-v1.5 的训练方式差不多，作者使用了 multi-stage [[rejection fine-tuning]] (RFT) 和 RL 来提高模型的表现。在每一轮中，先进行 RL 的训练，然后进行 RFT 的训练。相比于 Seed-Thinking-v1.5, Seed1.6-Thinking 使用了更多的算力和更高质量的数据，包括数学问题，代码，谜题以及 non-reasoning 数据。&lt;/p>
&lt;p>表现如下：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-seed1.6/Seed1_6_thinking_LLM_performance.png"
width="5436"
height="2498"
srcset="https://maosong2022.github.io/p/notes-on-seed1.6/Seed1_6_thinking_LLM_performance_hu6736598818851231143.png 480w, https://maosong2022.github.io/p/notes-on-seed1.6/Seed1_6_thinking_LLM_performance_hu16256607489974395991.png 1024w"
loading="lazy"
alt="Seed1.6-Thinking performance (LLM)"
class="gallery-image"
data-flex-grow="217"
data-flex-basis="522px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-seed1.6/Seed1_6_thinking_VLM_performance.png"
width="5255"
height="2514"
srcset="https://maosong2022.github.io/p/notes-on-seed1.6/Seed1_6_thinking_VLM_performance_hu6201545990707352627.png 480w, https://maosong2022.github.io/p/notes-on-seed1.6/Seed1_6_thinking_VLM_performance_hu11298797869590931751.png 1024w"
loading="lazy"
alt="Seed1.6-Thinking performance (VLM)"
class="gallery-image"
data-flex-grow="209"
data-flex-basis="501px"
>&lt;/p>
&lt;p>为了进一步提高模型的表现，作者在推理时使用了 parallel decoding, 来让模型在回答之前使用更多的 token, 作者发现 parallel decoding 可以显著提高模型在困难任务上的表现。&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-seed1.6/Seed1_6_thinking_parallel_decoding_performance.png"
width="4468"
height="1468"
srcset="https://maosong2022.github.io/p/notes-on-seed1.6/Seed1_6_thinking_parallel_decoding_performance_hu14877516256314024226.png 480w, https://maosong2022.github.io/p/notes-on-seed1.6/Seed1_6_thinking_parallel_decoding_performance_hu9088461374692967399.png 1024w"
loading="lazy"
alt="Performance of Seed1.6 Thinking parallel decoding"
class="gallery-image"
data-flex-grow="304"
data-flex-basis="730px"
>&lt;/p>
&lt;h3 id="seed15adaptive-cot">Seed1.5(Adaptive CoT)
&lt;/h3>&lt;p>reasoning model 的问题是会出现 overthinking,导致输出的过程中花费很多不必要的 token.&lt;/p>
&lt;p>为了解决这个问题，Seed1.6 提出了 AdaCoT, 来训练得到三个模型：FullCoT, NoCoT 和 AdaCoT&lt;/p>
&lt;p>通过 AdaCoT, 我们可以显著降低 CoT 的长度。为了完成这个目标，作者在 RL 训练阶段构建了一个 reward, 来惩罚模型的 overthinking. 用户可以基于 prompt 来选择不同的模型：&lt;/p>
&lt;ul>
&lt;li>FullCoT: 对每个 prompt，都进行 reasoning, 模型表现与 Seed1.6-Thinking 差不多，但是 CoT length 更小&lt;/li>
&lt;li>NoCoT: 直接回答，不进行 reasoning&lt;/li>
&lt;li>AdaCoT: 由模型自主决定是否需要进行 reasoning, 模型是 FullCoT 和 NoCoT 的一个 fusion 版本&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-seed1.6/Seed1_6_AdaCoT_performance.png"
width="6798"
height="3416"
srcset="https://maosong2022.github.io/p/notes-on-seed1.6/Seed1_6_AdaCoT_performance_hu4571070263511632519.png 480w, https://maosong2022.github.io/p/notes-on-seed1.6/Seed1_6_AdaCoT_performance_hu17493154294080386988.png 1024w"
loading="lazy"
alt="Seed1.6-AdaCoT performance"
class="gallery-image"
data-flex-grow="199"
data-flex-basis="477px"
>&lt;/p>
&lt;p>结果发现，对于不同难度的任务，AdaCoT 的出发概率是不同的，说明模型会基于任务难度调整思考方式。&lt;/p>
&lt;p>作者还评测了以下模型在高考中的表现，结果如下图：&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-seed1.6/Seed1_6_gaokao_performance.png"
width="5195"
height="4645"
srcset="https://maosong2022.github.io/p/notes-on-seed1.6/Seed1_6_gaokao_performance_hu13567853550537972132.png 480w, https://maosong2022.github.io/p/notes-on-seed1.6/Seed1_6_gaokao_performance_hu5316658797452294396.png 1024w"
loading="lazy"
alt="Seed1.6 performance on gaokao"
class="gallery-image"
data-flex-grow="111"
data-flex-basis="268px"
>&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者提出了 Seed1.6 系列多模态大模型，模型在多个 benchmark 上表现优异。作者还提出了 AdaCoT, 让模型基于问题难度动态选择 CoT length&lt;/p>
&lt;p>作者认为下一步工作是：&lt;/p>
&lt;ol>
&lt;li>构建更加高效的架构，来进一步提高 reasoning 的有效性&lt;/li>
&lt;li>扩展模型的多模态能力&lt;/li>
&lt;li>提升模型的 agent 能力，特鄙视 end-to-end task execution&lt;/li>
&lt;/ol>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://seed.bytedance.com/en/blog/introduction-to-techniques-used-in-seed1-6" target="_blank" rel="noopener"
>blog&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Seed1.5-VL</title><link>https://maosong2022.github.io/p/notes-on-seed1.5-vl/</link><pubDate>Wed, 14 May 2025 09:28:07 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-seed1.5-vl/</guid><description>&lt;p>字节Seed在5月11号发布了Seed1.5-VL技术报告。技术报告详细介绍了Seed1.5-VL的架构，训练和评估细节&lt;/p>
&lt;h1 id="简介">简介
&lt;/h1>&lt;p>Seed1.5-VL是多模态大模型，它是一个标准的 Vision Encoder - MLP - LLM 的架构，Vision Encoder是自研的Seed-ViT，参数量为532M，大语言模型的激活参数为20B（虽然论文未提及总参数量，但是从seed-Thinking v1.5来看，总参数量应该是200B）。Seed1.5-VL强调了agent能力，比如GUI control和gameplay。&lt;/p>
&lt;h1 id="介绍">介绍
&lt;/h1>&lt;p>现有的LVLM还不能与人类的通用能力相比，特别是在3D空间理解，物体技术，交互游戏等方面。并且，LVLM训练预料的多样性也不如LLM。最后，多模态数据的异质性也对模型的训练和推理提出了挑战。&lt;/p>
&lt;p>基于这些问题，作者提出了Seed1.5-VL，一个视觉多模态大模型。通过构建高质量的训练数据，作者大幅度提高了模型的表现。作者还进一步探究了模型的scaling law以及pre-training和post training算法的设计。为了提高训练和推理效率，作者在infra上也进行了改进&lt;/p>
&lt;h1 id="架构">架构
&lt;/h1>&lt;p>Seed1.5-VL的架构如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-seed1.5-vl/architecture.png"
width="1387"
height="1062"
srcset="https://maosong2022.github.io/p/notes-on-seed1.5-vl/architecture_hu6358939600119516298.png 480w, https://maosong2022.github.io/p/notes-on-seed1.5-vl/architecture_hu16024849637560253520.png 1024w"
loading="lazy"
alt="Architecture of See1.5-VL"
class="gallery-image"
data-flex-grow="130"
data-flex-basis="313px"
>&lt;/p>
&lt;p>Seed1.5-VL的架构包含三个部分：&lt;/p>
&lt;ol>
&lt;li>Vision Encoder: vision encoder是seed自研的Seed-ViT，拥有532M参数&lt;/li>
&lt;li>Adapter：一个两层的MLP&lt;/li>
&lt;li>LLM：Seed1.5-LLM，一个MoE架构的LLM&lt;/li>
&lt;/ol>
&lt;p>Seed1.5-VL支持文本，图片和视频输入，输入为文本。其可以进行理解和推理(reasoning)。&lt;/p>
&lt;h2 id="seed-vit">Seed-ViT
&lt;/h2>&lt;p>Seed-ViT主要是在两个方面进行了改进：&lt;/p>
&lt;ol>
&lt;li>在pre-training阶段使用了视频数据，来提高模型的时空间感知能力&lt;/li>
&lt;li>使用了2D-RoPE来处理动态分辨率的图片&lt;/li>
&lt;/ol>
&lt;p>Seed-ViT的超参数如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Patch size&lt;/th>
&lt;th>Pos Embedding&lt;/th>
&lt;th>Head dim&lt;/th>
&lt;th>#heads&lt;/th>
&lt;th>Embedding dim&lt;/th>
&lt;th>MLP ratio&lt;/th>
&lt;th>#layers&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>14&lt;/td>
&lt;td>2D RoPE&lt;/td>
&lt;td>64&lt;/td>
&lt;td>20&lt;/td>
&lt;td>1280&lt;/td>
&lt;td>4.0&lt;/td>
&lt;td>27&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>对于输入的图片，Seed-ViT首先将图片resize到28*28的倍数，接下来图片会被分割为14*14的patch. 与NaViT一样，Seed-ViT也是用了token-packing的操作，来将多个图片packing到一个sequence里进行训练。最后，对于Seed-ViT输出的图片特征，作者还是用了 $2\times 2$的 average pooling来降低token个数。&lt;/p>
&lt;p>Seed-ViT的预训练包括三个阶段，其设置和超参数如下表所示：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Categories&lt;/th>
&lt;th>Unlabeled Image&lt;/th>
&lt;th>Image-text pairs&lt;/th>
&lt;th>Video-audio-text tuples&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Training samples&lt;/td>
&lt;td>2.2B&lt;/td>
&lt;td>4.8B&lt;/td>
&lt;td>65M&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Token percentages&lt;/td>
&lt;td>4.0%&lt;/td>
&lt;td>91.2%&lt;/td>
&lt;td>4.8%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Batch sizes&lt;/td>
&lt;td>55,296&lt;/td>
&lt;td>32,768&lt;/td>
&lt;td>1,024&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LR warm up steps&lt;/td>
&lt;td>1,692&lt;/td>
&lt;td>2,000&lt;/td>
&lt;td>12,800&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Maximum LR&lt;/td>
&lt;td>$7.06\times 10^{-3}$&lt;/td>
&lt;td>$1.0\times 10^{-4}$&lt;/td>
&lt;td>$5.0\times 10^{-5}$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Minimum LR&lt;/td>
&lt;td>$1.05\times 10^{-5}$&lt;/td>
&lt;td>$1.2\times 10^{-6}$&lt;/td>
&lt;td>$2.02\times 10^{-7}$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>在预训练时，作者考虑了三点：&lt;/p>
&lt;ol>
&lt;li>训练效率：尽可能提高模型的训练效率&lt;/li>
&lt;li>原生图片精度输入处理：让模型可以尽早处理不同分辨率图片输入&lt;/li>
&lt;li>数据利用率：可以使用不同类型的数据&lt;/li>
&lt;/ol>
&lt;p>基于这三点考虑，Seed-ViT的训练包括三个阶段：&lt;/p>
&lt;ol>
&lt;li>Masked Image modeling with 2D RoPE：这一阶段的目的是提高模型对与几何和结构的感知能力。作者使用 &lt;code>EVA02-CLIP-E&lt;/code>作为教师模型，然后随机将75%的patches进行mask，最后让Seed-ViT输出的特征尽可能匹配教师模型输出的特征。这里的损失函数是cosine similarity. 作者发现，scale up MIM之后，VLM在OCR和文档理解任务上的表现得到了大幅度的提升。&lt;/li>
&lt;li>Native Resolution Contrastive Learning：这一阶段和CLIP,SigLIP的训练差不多，text encoder与&lt;code>EVA02-CLIP-E&lt;/code>相同。损失函数为SigLIP loss和SuperClass loss&lt;/li>
&lt;li>Omni-modal pret-raining：这一阶段使用了MiCo框架，来将video frames, audio, visual captions以及audio captions进行对齐，text encoder编码caption，ViT编码其他部分。通过这一阶段的训练，模型可以学习omni-model representations,并且还可以提高模型的图片和视频理解人物上的表现。&lt;/li>
&lt;/ol>
&lt;h2 id="视频编码">视频编码
&lt;/h2>&lt;p>对于视频输入，Seed1.5-VL采用了 &lt;strong>Dynamic Frame-Resolution Sampling&lt;/strong>技巧，核心思想在于根据budget和处理的任务来动态调整采样率(frame)和每一帧的图片精度(resolution)。&lt;/p>
&lt;p>具体来讲，在时序上，默认的FPS为1，如果要处理有关时序信息任务的话，FPS调整为2，如果要处理motion tracking等任务的话，FPS提高到5.为了进一步提高模型的时间感知能力，Seed 1.5-VL加入了timestamp tokens (e.g., &lt;code>[1.5 second]&lt;/code>)。&lt;/p>
&lt;p>在空间上，每个frame的可选精度为 ${640, 512, 384, 256, 160, 128}$，选择的精度根据我们的budget进行决定。如果说我们使用最低配置还是不能处理视频输入的话，我们就会先进行均匀采样，然后再进行处理，这样可以让模型处理所有的视频信息。&lt;/p>
&lt;h1 id="pre-training">Pre-training
&lt;/h1>&lt;p>pre-training阶段一共包括了3T token. 作者详细介绍了每一个类别。这里我们就简单总结一下。数据一共包括以下7个类别&lt;/p>
&lt;ol>
&lt;li>Generic Image-text pairs &amp;amp; knowledge data. 这部分数据主要是图文交错以及image text-pairs数据。作者介绍了一下针对长尾数据（稀有物种识别）的数据构建方式&lt;/li>
&lt;li>OCR. 这部分数据有1B左右的sample，包括documents, scene texts, tables, charts以及flowcharts等。其中，作者通过SynthDog和Latex合成了200M左右的文本密集图片。作者还对合成的数据进行了增广;Chart数据包括公开数据集和合成的数据集，最终一共有100M的样本; Table数据集也是通过提取得到的，一共包含50M左右的图片。作者还基于OCR结果构建了VQA数据集，来提高模型理解图片文字的能力&lt;/li>
&lt;li>Visual Grounding &amp;amp; Counting: 提高模型识别和定位物体的能力。数据格式包括bounding box和center points. 其中bounding box主要是公开数据集；作者还是用Grounding DINO来标记了一部分数据，最后一共收集了200M的样本; point数据主要是PixMo, 作者还使用了Molmo以及CountGD标注了一部分数据，最后一共包括170M的指令以及110B的token; Counting数据从bounding box 和point数据集中筛选得到，包括了8M的样本和13B的token。最后在训练时，与InternVL一样，坐标会被normalize到 &lt;code>[1,1000]&lt;/code>&lt;/li>
&lt;li>3D spatial understanding. 主要包括相对深度，绝对深度和3D Grounding数据；对于相对深度，作者使用DepthAnything V2来标注，得到了3.2B的token; 对于绝对深度，作者使用了公开数据集，最终包含18M的指令以及28B的token;对于3D Grounding，作者使用了公开数据集，最终包含770的指令以及1.3B的token&lt;/li>
&lt;li>Video. 主要包含general video understanding数据, video temporal grounding and moment retrieval以及video streaming data.&lt;/li>
&lt;li>STEM. 主要包含image comprehension data和problem-solving data. 前者包括收集的3.2M教育相关的数据，以及10M结构化表格，4.5M化学结构表达式，1.5M坐标系数据。后者包括100M K12的习题&lt;/li>
&lt;li>GUI. 作者使用UI-TARS来合成数据。数据主要包括perception, grounding 以及 reasoning&lt;/li>
&lt;/ol>
&lt;p>预训练包括三个stage，其实验设置如下表所示：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Stages&lt;/th>
&lt;th>Stage 0&lt;/th>
&lt;th>Stage 1&lt;/th>
&lt;th>Stage 2&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Training budget (tokens)&lt;/td>
&lt;td>16B&lt;/td>
&lt;td>3T&lt;/td>
&lt;td>240B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Sequence length&lt;/td>
&lt;td>32,768&lt;/td>
&lt;td>32,768&lt;/td>
&lt;td>131,072&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Trainable components&lt;/td>
&lt;td>MLP adaptor&lt;/td>
&lt;td>all&lt;/td>
&lt;td>all&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Batch sizes (tokens)&lt;/td>
&lt;td>8.4M&lt;/td>
&lt;td>71M&lt;/td>
&lt;td>71M&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LR warmup steps&lt;/td>
&lt;td>100&lt;/td>
&lt;td>500&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Maximum LR&lt;/td>
&lt;td>$2.52 \times 10^{-4}$&lt;/td>
&lt;td>$5.22 \times 10^{-5}$&lt;/td>
&lt;td>$5.22 \times 10^{-6}$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Minimum LR&lt;/td>
&lt;td>$4.50 \times 10^{-5}$&lt;/td>
&lt;td>$5.22 \times 10^{-6}$&lt;/td>
&lt;td>$5.22 \times 10^{-6}$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;ul>
&lt;li>Stage 0: 仅训练MLP，用于对齐vision encoder和LLM,使用了16B token&lt;/li>
&lt;li>Stage 1：训练全部参数，用于获取知识以及掌握grounding和OCR能力，使用了3T token&lt;/li>
&lt;li>Stage 2：训练全部参数，提升模型的长上下文能力以及对下游任务的适应性，包括视频理解，coding和3D空间理解，上下文长度从32,768提升到131,072&lt;/li>
&lt;/ul>
&lt;p>作者还进行了消融实验，采取了和Qwen2-VL一样的训练方式，结果发现，本文的训练方式效果更好。作者认为，解冻vision encoder，可能让vision encoder弥补LLM的不足，进而损害其自身的感知能力。&lt;/p>
&lt;p>作者还探究了模型的scaling law，使用的建模函数为：&lt;/p>
$$
\log(\hat{L}) \sim \log(B) - \beta \log(D) = -a \log(D) + b
$$&lt;p>
这里 $L$ 是NLL loss, D是训练的token数，$a$和$b$是待拟合的参数。实验结果如下图所示。
&lt;img src="https://maosong2022.github.io/p/notes-on-seed1.5-vl/scaling_law.png"
width="1392"
height="1065"
srcset="https://maosong2022.github.io/p/notes-on-seed1.5-vl/scaling_law_hu5357680134013757132.png 480w, https://maosong2022.github.io/p/notes-on-seed1.5-vl/scaling_law_hu12709952869440683564.png 1024w"
loading="lazy"
alt="scaling law"
class="gallery-image"
data-flex-grow="130"
data-flex-basis="313px"
>&lt;/p>
&lt;p>结论是在特定领域数据训练的loss可以作为模型在下游任务上表现的一个估计&lt;/p>
&lt;h1 id="post-training">Post-training
&lt;/h1>&lt;p>post-training分为两个阶段：SFT和RL，其中RL包括RLHF, RLVR. 总流程如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-seed1.5-vl/post_training.png"
width="1367"
height="598"
srcset="https://maosong2022.github.io/p/notes-on-seed1.5-vl/post_training_hu5528687103497423085.png 480w, https://maosong2022.github.io/p/notes-on-seed1.5-vl/post_training_hu7246407921116532780.png 1024w"
loading="lazy"
alt="post training"
class="gallery-image"
data-flex-grow="228"
data-flex-basis="548px"
>&lt;/p>
&lt;h2 id="sft">SFT
&lt;/h2>&lt;p>SFT数据主要包括General Instruction data以及Long Chain-of-Thought (LongCoT) data，劝着途胜模拟徐保国的指令跟随能力，后者提升模型的reasoning能力。&lt;/p>
&lt;p>通用指令微调数据包括13,000条收集的数据和40,0000高质量的公开数据集。&lt;/p>
&lt;h2 id="rlhf">RLHF
&lt;/h2>&lt;p>RLHF的数据包括人类标注数据和合成数据。对于reward model，作者使用了LVLM作为reward model，来给不同的response进行打分。最后，使用变体的PPO算法进行训练。&lt;/p>
&lt;h2 id="rlvr">RLVR
&lt;/h2>&lt;p>RLVR的训练数据主要包含Visual STEM和Visual Perception and Reasoning. 前者包含1M左右的问题，主要是STEM和K12的相关问题。后者包括grounding, visual instruction following以及visual puzzles &amp;amp; games等&lt;/p>
&lt;h2 id="hybrid-rl">Hybrid RL
&lt;/h2>&lt;p>RL的训练过程包含了RLHF和RLVR两个RL算法。作者介绍了以下细节：&lt;/p>
&lt;ol>
&lt;li>format reward. 要求模型输出格式为 &lt;code>&amp;lt;think&amp;gt;{thought}&amp;lt;/think&amp;gt;{solution}&lt;/code>&lt;/li>
&lt;li>hybrid reward. reward包括RM和verifier两部分&lt;/li>
&lt;li>Shared critic. 使用一个critic model来估计value function&lt;/li>
&lt;li>KL coefficients. 对于RLHF和RLVE，使用不同的KL divergence稀疏&lt;/li>
&lt;/ol>
&lt;h2 id="iterative-update-by-rejection-sampling-fine-tuning">Iterative Update by Rejection Sampling Fine-tuning
&lt;/h2>&lt;p>在训练的过程中，作者使用了一个迭代训练策略。一个开始模型在低质量的Long-CoT数据上进行SFT，在RL过后，作者会重新评估问题的难度，然后过程掉一部分数据，再进行SFT，这样反复进行迭代训练以提高模型的表现。&lt;/p>
&lt;blockquote>
&lt;p>Infra部分好像都是一些细节，这里就跳过了。&lt;/p>
&lt;/blockquote>
&lt;h1 id="evaluation">Evaluation
&lt;/h1>&lt;p>作者评估了&lt;/p>
&lt;ol>
&lt;li>Seed-ViT的表现&lt;/li>
&lt;li>Seed1.5-VL的通用视觉理解和推理能力&lt;/li>
&lt;li>Seed1.5-VL的视频相关能力&lt;/li>
&lt;li>Seed1.5-VL的agent能力&lt;/li>
&lt;li>Seed1.5-VL在内部benchmark上的表现&lt;/li>
&lt;/ol>
&lt;p>由于Seed1.5-VL是一个API模型，并且其内部benchmark不公开，因此Seed-ViT以及其在内部benchmark上的表现我们就不列出来了。我们主要看一下其和领先多模态大模型的对比&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-seed1.5-vl/vision_benchmarks.png"
width="1240"
height="1184"
srcset="https://maosong2022.github.io/p/notes-on-seed1.5-vl/vision_benchmarks_hu11436903269627804540.png 480w, https://maosong2022.github.io/p/notes-on-seed1.5-vl/vision_benchmarks_hu11903969968901952136.png 1024w"
loading="lazy"
alt="vision benchmarks, 论文表6"
class="gallery-image"
data-flex-grow="104"
data-flex-basis="251px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-seed1.5-vl/video_benchmarks.png"
width="1254"
height="1025"
srcset="https://maosong2022.github.io/p/notes-on-seed1.5-vl/video_benchmarks_hu1700365900879711657.png 480w, https://maosong2022.github.io/p/notes-on-seed1.5-vl/video_benchmarks_hu5982671056146884601.png 1024w"
loading="lazy"
alt="Video benchmarks, 论文表7"
class="gallery-image"
data-flex-grow="122"
data-flex-basis="293px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-seed1.5-vl/gui_benchmarks.png"
width="1190"
height="393"
srcset="https://maosong2022.github.io/p/notes-on-seed1.5-vl/gui_benchmarks_hu10558902132270732879.png 480w, https://maosong2022.github.io/p/notes-on-seed1.5-vl/gui_benchmarks_hu8916295417278482733.png 1024w"
loading="lazy"
alt="GUI benchmarks"
class="gallery-image"
data-flex-grow="302"
data-flex-basis="726px"
>&lt;/p>
&lt;h1 id="conclusion">Conclusion
&lt;/h1>&lt;p>在本文中，作者介绍了Seed1.5-VL，一个领先的多模态大模型，可以处理图片和视频输入。作者介绍了Seed-ViT以及Seed1.5-VL的数据，训练和评估。&lt;/p>
&lt;p>作者还分析了以下Seed1.5-VL的不足以及未来的改进方向&lt;/p>
&lt;ol>
&lt;li>Seed1.5-VL在复杂视觉感知任务中，其计数能力会下降。&lt;/li>
&lt;li>Seed1.5-VL的高阶推理能力还尚有不足，作者拿Klotski举了一个例子&lt;/li>
&lt;li>Seed1.5-VL的3D spatial reasoning能力不足&lt;/li>
&lt;li>Seed1.5-VL的temporal reasoning能力不足&lt;/li>
&lt;li>Seed1.5-VL与其他模型一样，存在幻觉问题&lt;/li>
&lt;/ol>
&lt;p>未来，有以下值得探索的方向：&lt;/p>
&lt;ol>
&lt;li>涌现现象(emergent abilities)&lt;/li>
&lt;li>scaling law，进一步探索模型参数量，算力以及数据对模型表现的影响&lt;/li>
&lt;li>提高模型的3D spatial reasoning能力，降低模型的幻觉，提高模型分组合搜索能力&lt;/li>
&lt;/ol>
&lt;h1 id="参考文献">参考文献
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2505.07062" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>