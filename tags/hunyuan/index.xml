<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Hunyuan on Mao Song(毛松)'s Homepage</title><link>https://maosong2022.github.io/tags/hunyuan/</link><description>Recent content in Hunyuan on Mao Song(毛松)'s Homepage</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 06 Aug 2025 16:46:32 +0800</lastBuildDate><atom:link href="https://maosong2022.github.io/tags/hunyuan/index.xml" rel="self" type="application/rss+xml"/><item><title>Notes on Hunyuan-Large</title><link>https://maosong2022.github.io/p/notes-on-hunyuan-large/</link><pubDate>Wed, 06 Aug 2025 16:46:32 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-hunyuan-large/</guid><description>&lt;p>腾讯混元提出了 Hunyuan-Large, 一个 389B-A52B 的 MoE LLM, 上下文长度为 256K.&lt;/p>
&lt;h2 id="introduction">Introduction
&lt;/h2>&lt;p>Hunyuan-Large 主要在三个方向进行了改进：&lt;/p>
&lt;ol>
&lt;li>使用了更高质量的合成数据：模型使用了 7T 的预训练数据，其中包含了 1.5T 的合成数据&lt;/li>
&lt;li>优化了模型的架构：作者提出了 KV cache compression, recycle routing, expert-specific learning rate scaling 策略来提高模型的表现&lt;/li>
&lt;li>探究了 MoE 模型的 scaling law: 作者探究了 MoE 模型的 scaling law&lt;/li>
&lt;/ol>
&lt;h2 id="pre-training">Pre-training
&lt;/h2>&lt;h3 id="architecture">Architecture
&lt;/h3>&lt;p>Hunyuan-Large 是一个基于 MoE 的 transformer 架构，attention 部分使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-gqa/" target="_blank" rel="noopener"
>GQA&lt;/a>, position encoding 使用了 RoPE, MLP 的激活函数为 SwiGLU. 在 MoE layer 中，Hunyuan-Large 使用了 shared experts. 最终，模型的配置如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-hunyuan-large/hunyuan-large-architecture-config.png"
width="654"
height="399"
srcset="https://maosong2022.github.io/p/notes-on-hunyuan-large/hunyuan-large-architecture-config_hu7926587958527572246.png 480w, https://maosong2022.github.io/p/notes-on-hunyuan-large/hunyuan-large-architecture-config_hu112456650109780077.png 1024w"
loading="lazy"
alt="Configuration of Hunyuan-Large"
class="gallery-image"
data-flex-grow="163"
data-flex-basis="393px"
>&lt;/p>
&lt;h4 id="kv-cache-compression">KV Cache Compression
&lt;/h4>&lt;p>为了减少 KV cache 的内存开销，作者使用了两个技巧：&lt;/p>
&lt;ol>
&lt;li>GQA: 通过共享 KV projection 的参数，来减少内存访问次数&lt;/li>
&lt;li>[[CLA]]: 在相邻的 layer 中共享 KV cache, 来进一步压缩 KV cache&lt;/li>
&lt;/ol>
&lt;p>在 Hunyuan-Large 中，作者将 GQA 的 group size 设置为 8, 然后相邻的 2 层 layer 共享 KV cache.&lt;/p>
&lt;p>假设输入的 batch size 为 $B$, sequence 长度为 $L$, layers 个数为 $\ell$, attention heads 个数为 $h$, KV heads 个数为 $h_{kv}$, 每个 head 的 hidden size 为 $d_h$, 则每一层的 GQA 需要缓存 $K,V\in\mathbb{R}^{B\times _{kv}\times L\times d_h}$， KV cache 的总占用为&lt;/p>
$$
2\times B\times h_{kv}\times L\times d_h \times \ell \times 2=4BLh_{kv}d_h\ell
$$&lt;p>第一个 $2$ 是因为同时缓存 K 和 V, 第二个 $2$ 是因为一般使用 &lt;code>bfloat16&lt;/code> 数据格式。&lt;/p>
&lt;p>对于 CLA, 因为连续两层共享相同的 KV cache，因此结果除以 2; 对于 MHA, $h_{kv}=h$; 对于 &lt;a class="link" href="https://maosong.website/p/notes-on-mqa/" target="_blank" rel="noopener"
>MQA&lt;/a>, $h_{kv}=1$. 最后，KV cache 的内存占用如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Attention Mechanism&lt;/th>
&lt;th>KV Cache Memory&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>MHA&lt;/td>
&lt;td>$4BLhd_h\ell$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GQA&lt;/td>
&lt;td>$4BLh_{kv}d_h\ell$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MQA&lt;/td>
&lt;td>$4BLd_h\ell$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CLA&lt;/td>
&lt;td>$2BLhd_h\ell$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GQA+CLA&lt;/td>
&lt;td>$2BLh_{kv}d_h\ell$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>可以看到，使用 GQA+CLA 之后，模型的 kv cache 占用相比于 MHA 变成了&lt;/p>
$$
\frac{2BLh_{kv}d_h\ell}{4BLhd_h\ell}=\frac{1}{16}
$$&lt;p>也就是说，Hunyuan-Large 的 KV cache 内存占用下降到了 MHA 的 1/16.&lt;/p>
&lt;h4 id="expert-routing-strategy">Expert Routing Strategy
&lt;/h4>&lt;p>作者采用了 shared expert + activated expert 的形式，其中包含 1 个 shared expert, 然后从 16 个专家里激活 1 个专家。&lt;/p>
&lt;p>为了解决 MoE 中 expert capacity 难以设定的问题，作者提出了一个 recycle routing 的策略，基本思想就是，当 activated expert 的容量超出限制时，会从其他没有超出容量限制的专家里重新进行激活。&lt;/p>
&lt;h4 id="expert-specific-learning-rate-scaling">Expert Specific Learning Rate Scaling
&lt;/h4>&lt;p>作者使用 AdamW 作为优化器，作者探讨了如何设定学习率。基于之前的工作，最优的学习率与 batch size 相关：&lt;/p>
$$
\epsilon_{\mathrm{opt}}(B) = \frac{2\epsilon_{\max}}{\sqrt{\frac{\mathcal{B}_{\mathrm{noise}}}{B}}+\sqrt{\frac{B}{\mathcal{B}_{\mathrm{noise}}}}}
$$&lt;p>这里 $\epsilon_{\max}$ 是 AdamW 的学习率, $\mathcal{B}_{\mathrm{noise}}$ 是训练速度与数据使用效率的一个平衡因子。&lt;/p>
&lt;p>但是，在 MoE 模型中，不同专家处理的 token 是不一样的。基于 load balancing loss, shared expert 和 activated expert 处理的 token 个数比例大概是 $n :1$, 其中 $n=16$ 是总的专家个数。因此，对于 shared expert, 作者使用 $\epsilon_{\mathrm{opt}}(B)$ 作为学习率，然后对于 activated expert, 作者使用 $\epsilon_{\mathrm{opt}}(B/n)$ 作为学习率。&lt;/p>
&lt;h3 id="data">Data
&lt;/h3>&lt;p>预训练数据包括收集和合成。收集的数据主要来自互联网，覆盖中英文两种语言。&lt;/p>
&lt;p>合成数据包括 4 个步骤：&lt;/p>
&lt;ol>
&lt;li>instruction generation: 作者使用高质量的语料作为 seed, 然后生成多样的 instruction 覆盖不同的 domain&lt;/li>
&lt;li>Instruction evolution: refine 上一步生成的 instruction&lt;/li>
&lt;li>Response generation: 使用 specialized model 来生成回答&lt;/li>
&lt;li>response filtering: 对生成的回答进行过滤&lt;/li>
&lt;/ol>
&lt;p>数据合成的流程如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-hunyuan-large/Hunyuan-large-data-synthesis.png"
width="1126"
height="565"
srcset="https://maosong2022.github.io/p/notes-on-hunyuan-large/Hunyuan-large-data-synthesis_hu18416123704675275786.png 480w, https://maosong2022.github.io/p/notes-on-hunyuan-large/Hunyuan-large-data-synthesis_hu18395332435503133072.png 1024w"
loading="lazy"
alt="Data synthesis pipeline"
class="gallery-image"
data-flex-grow="199"
data-flex-basis="478px"
>&lt;/p>
&lt;p>tokenizer 大小为 128K, 由 tittoken tokenizer 和额外的 28K token 组成。&lt;/p>
&lt;h3 id="pre-training-recipe">Pre-training Recipe
&lt;/h3>&lt;p>作者首先探究了一个针对 MoE 模型的 scaling law. 结果发现，最优的激活参数量为 58.1B, training token 个数为 5.6T. 经过平滑之后，作者最终将模型的激活参数两定为 &lt;strong>52B&lt;/strong>, 训练 token 数定为 $7T$.&lt;/p>
&lt;p>在训练时，作者将学习率分为了 3 个 stage:&lt;/p>
&lt;ol>
&lt;li>warmup phase&lt;/li>
&lt;li>gradual decay phase&lt;/li>
&lt;li>concise annealing phase&lt;/li>
&lt;/ol>
&lt;p>上面的三个 stage 结束之后，作者加入了两个 stage 来扩展模型的上下文长度从 32K 扩展到 256K. 训练的数据包括 75% 的短文本和 25% 的长文本。两个 stage 训练的 token 数均为 $10B$ 左右。&lt;/p>
&lt;h2 id="post-training">Post-training
&lt;/h2>&lt;p>post-training 分为 SFT 和 RLHF 两个阶段。&lt;/p>
&lt;h3 id="sft">SFT
&lt;/h3>&lt;p>SFT 数据副高 math, coding, logical reasoning 等 domain, 包含超过 1M 的数据。&lt;/p>
&lt;p>SFT 训练了 3 个 epoch, 学习率从 2e-5 降低到 2e-6, 为了避免 overfitting, 作者使用了 0.1 的 attention dropout 和 0.2 的 hidden dropout.&lt;/p>
&lt;blockquote>
&lt;p>[!tip]
作者发现，MoE 模型可以从 dropout 中学习到更多&lt;/p>
&lt;/blockquote>
&lt;h3 id="rlhf">RLHF
&lt;/h3>&lt;p>作者使用 DPO 来进行 RLHF, 作者同时使用了 offline 和 online 的数据来进行训练，前者是收集的数据，后者是当前 policy 生成的数据。与 LLaMA 3 和 Nemotron-4 一样，为了提高训练稳定性，对于 chosen reponse, 作者使用了 SFT loss.&lt;/p>
&lt;p>作者还是用了 exponential moving average 策略来减少 reward hacking 现象，以及降低 alignment tax.&lt;/p>
&lt;h2 id="experiment">Experiment
&lt;/h2>&lt;p>对于 base 版本，作者对比了 LLaMA 3, Mixtral, DeepSeek-V2, 实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-hunyuan-large/Hunyuan-large-base-performance.png"
width="1135"
height="761"
srcset="https://maosong2022.github.io/p/notes-on-hunyuan-large/Hunyuan-large-base-performance_hu9250287511197580218.png 480w, https://maosong2022.github.io/p/notes-on-hunyuan-large/Hunyuan-large-base-performance_hu13135794748576120680.png 1024w"
loading="lazy"
alt="Performance of Hunyuan-Large-Base"
class="gallery-image"
data-flex-grow="149"
data-flex-basis="357px"
>&lt;/p>
&lt;p>Instruction 版本的表现如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-hunyuan-large/Hunyuan-large-intruction-performance.png"
width="1178"
height="538"
srcset="https://maosong2022.github.io/p/notes-on-hunyuan-large/Hunyuan-large-intruction-performance_hu132719345509594511.png 480w, https://maosong2022.github.io/p/notes-on-hunyuan-large/Hunyuan-large-intruction-performance_hu9830594855642638810.png 1024w"
loading="lazy"
alt="Performance of Hunyuan-Large Instuct"
class="gallery-image"
data-flex-grow="218"
data-flex-basis="525px"
>&lt;/p>
&lt;h2 id="conclusion">Conclusion
&lt;/h2>&lt;p>作者提出了 Hunyuan-Large, 一个 389B-A52B 的 LLM, 上下文长度为 256K. 作者详细介绍了模型的架构，数据和训练方式。&lt;/p>
&lt;h2 id="references">References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2411.02265" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>