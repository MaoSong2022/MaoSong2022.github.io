<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Scaling Law on Mao Song(毛松)'s Homepage</title><link>https://maosong.website/tags/scaling-law/</link><description>Recent content in Scaling Law on Mao Song(毛松)'s Homepage</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Wed, 14 Jan 2026 16:21:00 +0800</lastBuildDate><atom:link href="https://maosong.website/tags/scaling-law/index.xml" rel="self" type="application/rss+xml"/><item><title>Notes on Ling-mini-beta</title><link>https://maosong.website/p/notes-on-ling-mini-beta/</link><pubDate>Sat, 13 Dec 2025 15:58:51 +0800</pubDate><guid>https://maosong.website/p/notes-on-ling-mini-beta/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>目前已经有了针对 dense LLM 的 scaling law, 如 &lt;a class="link" href="https://maosong.website/p/kaplan-scaling-law/" target="_blank" rel="noopener"
>Kaplan scaling law&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/chinchilla-scaling-law/" target="_blank" rel="noopener"
>Chinchilla scaling law&lt;/a>.&lt;/p>
&lt;p>但是，对于 MoE 模型，目前还缺乏一个比较系统的 scaling law.&lt;/p>
&lt;p>为了解决这个问题，作者提出了 efficiency leverage (EL), 用于衡量 MoE 模型的效率，其定义为&lt;/p>
$$
EL(\mathcal{X}_{\mathrm{MoE}}\mid \mathrm{Dense})=\frac{C_{\mathrm{Dense}}}{C_{{\mathrm{MoE}}}}
$$&lt;p>其中 $C_{\mathrm{Dense}}, C_{{\mathrm{MoE}}}$ 分别代表了训练模型所需要的算力。EL 衡量了 moe 模型达到对应 dense 模型表现所需要的算力，EL 值越大，说明 MoE 模型效率越高。EL 的可视化如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-ling-mini-beta/EL-definition.png"
width="555"
height="459"
loading="lazy"
alt="Definition of EL"
class="gallery-image"
data-flex-grow="120"
data-flex-basis="290px"
>&lt;/p>
&lt;p>作者通过训练多个模型，探究了 MoE 架构与 EL 之间的关系。作者发现，MoE 模型的表现主要与专家激活比例以及算力相关。基于 scaling law, 作者训练了 Ling-mini-beta, 一个 17.5B-A0.85B 的 MoE 模型，其表现超过了 6.1B dense 模型的表现。&lt;/p>
&lt;h2 id="preliminary">&lt;a href="#preliminary" class="header-anchor">&lt;/a>Preliminary
&lt;/h2>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Notation&lt;/th>
&lt;th>description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$N$&lt;/td>
&lt;td>total parameters&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$N_a$&lt;/td>
&lt;td>active parameters&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$E$&lt;/td>
&lt;td>routed experts&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$E_a$&lt;/td>
&lt;td>activated experts&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$E_s$&lt;/td>
&lt;td>shared experts&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>作者定义 activation ratio 如下:&lt;/p>
$$
A = \frac{E_a+E_s}{E+E_s}
$$&lt;p>定义 sharing ratio 如下&lt;/p>
$$
S=\frac{E_s}{E_a+E_s}
$$&lt;p>定义 expert granularity 如下&lt;/p>
$$
G = \frac{d_{\mathrm{model}}}{d_{\mathrm{Expert}}}
$$&lt;p>与 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-llm/" target="_blank" rel="noopener"
>DeepSeek-LLM&lt;/a> 一样，作者使用 $C=MD$ 来表示算力，non-embedding FLOPs $M$ 和训练 token 数 $D$ 之间的关系。&lt;/p>
&lt;h3 id="hyper-parameters">&lt;a href="#hyper-parameters" class="header-anchor">&lt;/a>Hyper Parameters
&lt;/h3>&lt;p>作者首先探究了针对 MoE 模型的超参数配置，最终你和出来的结果如下图&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-ling-mini-beta/EL-hyperparameter-scaling-law.png"
width="1377"
height="586"
loading="lazy"
alt="Scaling laws for optimal hyperparameters"
class="gallery-image"
data-flex-grow="234"
data-flex-basis="563px"
>&lt;/p>
&lt;p>实验结果说明，相比于 dense model, MoE model 需要更大的 batch size.&lt;/p>
&lt;p>作者基于这个 scaling law 进行了验证，结果说明这个 scaling law 比较准确。&lt;/p>
&lt;h3 id="parameters-and-dataset-size">&lt;a href="#parameters-and-dataset-size" class="header-anchor">&lt;/a>Parameters and Dataset Size
&lt;/h3>&lt;p>接下来作者探究了对于模型参数量以及训练 token 个数之间的 scaling law, 求解的问题如下&lt;/p>
$$
(M^{opt}, D^{opt}) = \arg\min_{M,D}\mathcal{L}(M,D;C,A,G,S)\quad s.t.\ C=MD
$$&lt;p>实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-ling-mini-beta/EL-model-data-scaling.png"
width="1374"
height="537"
loading="lazy"
alt="Scaling laws for optimal model scale and data size"
class="gallery-image"
data-flex-grow="255"
data-flex-basis="614px"
>&lt;/p>
&lt;p>结果说明，不同架构对应的系数接近 $0.5$, 说明我们应该将算力均衡分配到数据和 model size 上。领一方面，MoE 模型可以通过使用更多的数据来达到更优的表现。&lt;/p>
&lt;h2 id="efficiency-leverage">&lt;a href="#efficiency-leverage" class="header-anchor">&lt;/a>Efficiency Leverage
&lt;/h2>&lt;p>作者将 Efficiency Leverage (EL) 定义为给定算力 $C_{target}$ 和一个 MoE 模型 $\mathcal{X}_{MoE}$, 对应 dense 模型达到 $\mathcal{X}_{MoE}$ 相同的表现所需要的算力 $C_{dense}$, 即&lt;/p>
$$
\begin{aligned}
&amp;EL(\mathcal{X}_{\mathrm{MoE}}\mid \mathrm{Dense};C_{target})=\frac{C_{\mathrm{Dense}}}{C_{{\mathrm{MoE}}}}\\
s.t.\ &amp; |\mathcal{L}(C_{MoE}, \mathcal{X}_{\mathrm{MoE}})-\mathcal{L}(C_{dense}, \mathcal{X}_{\mathrm{dense}})|\leq \epsilon (\epsilon\to 0)
\end{aligned}
$$&lt;p>EL 值越高说明 MoE 模型越有效。为了公平起见，dense 模型和 MoE 模型的架构参数基本相同，作者只改变 $d_{model}, d_{ffn}, d_{expert}$ 以及 $n_{layer}$.&lt;/p>
&lt;p>接下来，作者就探究了给定算力的情况下，最优的 MoE 配置，即&lt;/p>
$$
(A^{opt}, G^{opt}, S^{opt}) = \arg\min_{(A,G,S)\in\mathcal{X}_{\mathrm{MoE}}}EL(\mathcal{X}_{\mathrm{MoE}}\mid \mathrm{Dense};C)
$$&lt;h2 id="scaling-law">&lt;a href="#scaling-law" class="header-anchor">&lt;/a>Scaling Law
&lt;/h2>&lt;p>首先，坐着探究了最优的 activation ratio $A$, 即&lt;/p>
$$
A^{opt} = \arg\min_{A}\mathcal{L}(A;C,M,G,S)
$$&lt;p>拟合的结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-ling-mini-beta/EL-impact-of-A.png"
width="1380"
height="487"
loading="lazy"
alt="Impact of the activation ratio"
class="gallery-image"
data-flex-grow="283"
data-flex-basis="680px"
>&lt;/p>
&lt;p>实验结果表明：&lt;/p>
&lt;ol>
&lt;li>模型表现随激活比例降低 er 提高&lt;/li>
&lt;li>更系数的模型对于算力的提升其效率也提升更快&lt;/li>
&lt;/ol>
&lt;p>然后，作者探究了最优的 granularity ratio, 即&lt;/p>
$$
G^{opt} = \arg\min_{G}\mathcal{L}(G;C,M,A,S)
$$&lt;p>拟合的结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-ling-mini-beta/EL-impact-of-G.png"
width="1376"
height="497"
loading="lazy"
alt="Impact of granularity"
class="gallery-image"
data-flex-grow="276"
data-flex-basis="664px"
>&lt;/p>
&lt;p>可以看到，无限制提升 granularity 并不会提高模型的表现。并且，不同的算力对应的最优 granularity 处于一个固定的范围&lt;/p>
&lt;p>接下来，作者探究了最优的 shared expert ratio, 即&lt;/p>
$$
S^{opt} = \arg\min_{S}\mathcal{L}(S;C,M,A,G)
$$&lt;p>拟合的结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-ling-mini-beta/EL-impact-of-S.png"
width="1376"
height="487"
loading="lazy"
class="gallery-image"
data-flex-grow="282"
data-flex-basis="678px"
>&lt;/p>
&lt;p>结果说明 shared expert 的比例也不是越多越好，其存在最优值。并且给定算力的情况下，非零最小值的 shared expert 表现最好。因此作者认为，一个 shared expert 的效果最好。&lt;/p>
&lt;p>作者还探究了其他可能的因素，结论如下：&lt;/p>
&lt;ol>
&lt;li>与 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v3/" target="_blank" rel="noopener"
>DeepSeek-V3&lt;/a> 一样，将 early layer 替换为 dense layer 可以避免 routing imbalance, 并且不会损害模型的表现&lt;/li>
&lt;li>attention 应该占 $30\%\sim40\%$ 左右的算力才能保证模型的表现和效率。进一步提升 attention 的算力占比虽然会提升表现但是会降低推理效率。&lt;/li>
&lt;/ol>
&lt;p>通过前面的发现，作者将 shared expert 设置为 1 个，然后探究 EL 与 activation ratio $A$, granularity $G$, FLOPs $C$ 之间的关系。&lt;/p>
&lt;p>首先，作者分别假设 $EL$ 与 $A$, $G$, $C$ 之间存在如下关系：&lt;/p>
$$
\begin{aligned}
\log EL_{C,G}(\hat{A}) &amp;= a_A\log\hat{A}, \text{ where }\frac{1}{\hat{A}}=\frac{1}{A+(1/A_{start}-1/A_{\max})^{-1}}+\frac{1}{A_{\max}}\\
\log EL_{C,A}(\hat{G}) &amp;= a_G+b_G(\log G(\log G+c_G))\\
\log EL_{A,G}(C) &amp;= a_C\log C+c_C
\end{aligned}
$$&lt;p>拟合的结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-ling-mini-beta/EL-scaling-A-G-C.png"
width="1380"
height="427"
loading="lazy"
alt="Scaling behavior of EL"
class="gallery-image"
data-flex-grow="323"
data-flex-basis="775px"
>&lt;/p>
&lt;p>结果显示：&lt;/p>
&lt;ol>
&lt;li>提升算力以及降低 activation ratio 都可以提高 EL&lt;/li>
&lt;li>granularity 对 EL 的影响在不同算力的情况下都是一致的&lt;/li>
&lt;li>对于 MoE 模型，提升算力可以提高 EL&lt;/li>
&lt;/ol>
&lt;p>作者的结论为，activation ratio 是影响 MoE EL 的核心因素。并且随着算力的提升，MoE EL 会越来越明显。&lt;/p>
&lt;p>作者因此构建了一个统一的公式来统一三个因素&lt;/p>
$$
EL(A,G,C) = \hat{A}^{\alpha+\gamma(\log G)^2+\beta \log G}
$$&lt;p>其中 $\alpha=a+d\log C$ 代表了 EL 和 activation ratio 之间的关系。拟合出来的参数如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>$\alpha$&lt;/th>
&lt;th>$d$&lt;/th>
&lt;th>$\gamma$&lt;/th>
&lt;th>$\beta$&lt;/th>
&lt;th>$A_{start}$&lt;/th>
&lt;th>$A_{\max}$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>1.23&lt;/td>
&lt;td>-7.61e-2&lt;/td>
&lt;td>1.67e-2&lt;/td>
&lt;td>-1.17e-1&lt;/td>
&lt;td>1.63e-2&lt;/td>
&lt;td>5.28e+16&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>基于这个结果，作者发现在 1e22 FLOPs 的算力下，一个 activation ratio 为 $3.1\%$, granularity 为 $12$ 的 MoE 模型，其 EL 为 $7$.&lt;/p>
&lt;h2 id="ling-mini-beta">&lt;a href="#ling-mini-beta" class="header-anchor">&lt;/a>Ling-mini-beta
&lt;/h2>&lt;p>基于上一节的发现，作者构建了 Ling-mini-beta, 一个 17.5B 总参数，激活参数为 0.85B 的 MoE 模型。训练使用了 1T token, 模型参数如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>$n_{\text{layers}}$&lt;/th>
&lt;th>$d_{\text{model}}$&lt;/th>
&lt;th>$d_{\text{ffn}}$&lt;/th>
&lt;th>$d_{\text{expert}}$&lt;/th>
&lt;th>$n_{\text{heads}}$&lt;/th>
&lt;th>$n_{\text{kv\_head}}$&lt;/th>
&lt;th>$E$&lt;/th>
&lt;th>$E_a$&lt;/th>
&lt;th>$E_s$&lt;/th>
&lt;th>$N$&lt;/th>
&lt;th>$N_a$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Dense 6.1B&lt;/td>
&lt;td>28&lt;/td>
&lt;td>4096&lt;/td>
&lt;td>14336&lt;/td>
&lt;td>-&lt;/td>
&lt;td>32&lt;/td>
&lt;td>8&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>6.11B&lt;/td>
&lt;td>6.11B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ling-mini-beta (A0.8B)&lt;/td>
&lt;td>20&lt;/td>
&lt;td>2048&lt;/td>
&lt;td>5120&lt;/td>
&lt;td>384&lt;/td>
&lt;td>16&lt;/td>
&lt;td>4&lt;/td>
&lt;td>384&lt;/td>
&lt;td>12&lt;/td>
&lt;td>1&lt;/td>
&lt;td>17.5B&lt;/td>
&lt;td>0.85B&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>训练的损失变化情况如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-ling-mini-beta/EL-training-dynamics.png"
width="867"
height="596"
loading="lazy"
alt="Dynamic of training loss"
class="gallery-image"
data-flex-grow="145"
data-flex-basis="349px"
>&lt;/p>
&lt;p>从图中我们可以看出，dense model 一开始的损失下降比较快，但是其最终表现不如 moe 模型。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>在本文中，作者提出了 Efficiency Leverage, 一个衡量 MoE 模型相对于 dense 模型计算效率的 metric, 作者构建了针对 MoE 模型的 scaling law. scaling 揭示了两个主要影响 MoE 模型效率的因素：算力与激活参数比例。基于 scaling law, 作者构建了 Ling-mini-beta, 一个 17B-A0.8B 的 MoE 模型，其效率超过了对应 dense 模型的 7 倍。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2507.17702" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Chinchilla Scaling Law</title><link>https://maosong.website/p/chinchilla-scaling-law/</link><pubDate>Wed, 22 Oct 2025 14:39:23 +0800</pubDate><guid>https://maosong.website/p/chinchilla-scaling-law/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>本文中关注的研究问题为：&lt;/p>
&lt;blockquote>
&lt;p>给定一个 FLOPs budget, 如何平衡 model size 和 dataset size 之间的关系？&lt;/p>
&lt;/blockquote>
&lt;p>即，我们希望求解如下优化问题：&lt;/p>
$$
N_{opt}(C), D_{opt}(C) =\arg\min_{N,D,\ \mathrm{s.t.}\ FLOPs(N,D)=C} L(N,D)
$$&lt;p>作者通过训练 400 多个模型，构建了对应的 scaling law.&lt;/p>
&lt;p>已有工作如 &lt;a class="link" href="https://maosong.website/p/kaplan-scaling-law/" target="_blank" rel="noopener"
>Kaplan scaling law&lt;/a> 已经发现模型参数和大语言模型表现之间的关系，一个结论就是计算最优并不代表达到最优的 loss. 在本文中，作者也有相同结论，但是作者认为大模型应该使用比 &lt;a class="link" href="https://maosong.website/p/kaplan-scaling-law/" target="_blank" rel="noopener"
>Kaplan scaling law&lt;/a> 推荐的更多的 training token. 基于这个发现，作者训练了 Chinchilla, 一个 70B 的 LLM, Chinchilla 相比 Gopher 表现有了大幅度的提升。&lt;/p>
&lt;h2 id="scaling-law">&lt;a href="#scaling-law" class="header-anchor">&lt;/a>Scaling Law
&lt;/h2>&lt;h3 id="fix-model-size-and-very-dataset-size">&lt;a href="#fix-model-size-and-very-dataset-size" class="header-anchor">&lt;/a>Fix Model Size and Very Dataset Size
&lt;/h3>&lt;p>这个方法中，作者通过改变训练步数，来研究 FLOPs 与模型表现之间的关系，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/chinchilla-scaling-law/Chinchilla-Training-curve-envelope.png"
width="3018"
height="1304"
loading="lazy"
alt="Training Curve envelope"
class="gallery-image"
data-flex-grow="231"
data-flex-basis="555px"
>&lt;/p>
&lt;p>通过对实验结果进行拟合，作者发现存在关系 $N_{opt}(C)\propto C^a$ 以及 $D_{opt}(C)\propto C^b$, 拟合的结果为 $a=b=0.5$.&lt;/p>
&lt;h3 id="isoflops-profiles">&lt;a href="#isoflops-profiles" class="header-anchor">&lt;/a>IsoFLOPS Profiles
&lt;/h3>&lt;p>这个方法中，作者使用了不同的模型大小以及算力来构建最优模型参数量与算力之间的关系。作者给定 9 个算力配置，然后选取不同参数量的模型，训练的 token 数由算力和模型参数量决定，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/chinchilla-scaling-law/Chinchilla-IsoFLOP-curves.png"
width="3018"
height="1396"
loading="lazy"
alt="IsoFLOP curves"
class="gallery-image"
data-flex-grow="216"
data-flex-basis="518px"
>&lt;/p>
&lt;p>结果显示，不同大小的模型的表现 (loss) 随算力上升先下降后上升。因此给定算力，存在一个最优的 model size. 作者基于拟合出来的曲线得到了 Gopher 使用的算力配置下的最优 model size 和 training tokens. 同样的，作者得到 $a=0.49,b=0.51$.&lt;/p>
&lt;h3 id="fitting-a-parametric-loss-function">&lt;a href="#fitting-a-parametric-loss-function" class="header-anchor">&lt;/a>Fitting a Parametric Loss Function
&lt;/h3>&lt;p>这个方法中，作者对 $L(N,D)$ 进行建模，作者使用了如下的公式&lt;/p>
$$
L(N,D) = E + \frac{A}{N^{\alpha}} + \frac{B}{D^{\beta}}
$$&lt;p>第一项代表了建模的误差，第二项代表了数据集充分大损失与模型参数之间的关系，第三项代表了当模型充分训练时，损失与数据集大小之间的关系。&lt;/p>
&lt;p>为了求解 $(A,B,E,\alpha,\beta)$, 作者基于训练收集到的数据 $L(N_i,D_i)$, 通过 L-BFGS 算法来最小化 Huber loss 进行求解，结果得到 $(A,B,E,\alpha,\beta)=( 406.4, 410.7, 1.69, 0.34, 0.28)$.&lt;/p>
&lt;p>将结果带入带上面的表达式中，然后求出梯度为 0 的点，就得到&lt;/p>
$$
N_{opt}(C) = G\left(\frac C6\right)^a, D_{opt}(C) = G^{-1}\left(\frac C6\right)^b, \text{ where }G=\left(\frac{\alpha A}{\beta B}\right)^{1/(\alpha+\beta)}, a=\frac{\beta}{\alpha+\beta}, b=\frac{\alpha}{\alpha+\beta}
$$&lt;p>带入数值之后就得到 $a=0.46$, $b=0.54$. 作者对结果可视化如下图所示，左图是拟合曲线的 Contour plot, 右图对左图的一个切片&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/chinchilla-scaling-law/Chinchilla-Parametric-fit.png"
width="2980"
height="1284"
loading="lazy"
alt="Parametric fit"
class="gallery-image"
data-flex-grow="232"
data-flex-basis="557px"
>&lt;/p>
&lt;h3 id="optimal-model-scaling">&lt;a href="#optimal-model-scaling" class="header-anchor">&lt;/a>Optimal Model Scaling
&lt;/h3>&lt;p>作者将三种方法的结果以及 &lt;a class="link" href="https://maosong.website/p/kaplan-scaling-law/" target="_blank" rel="noopener"
>Kaplan scaling law&lt;/a> 的结果总结放在下表中，作者假设 $N_{opt}(C)\propto C^a$ 以及 $D_{opt}(C)\propto C^b$&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Approach&lt;/th>
&lt;th>$a$&lt;/th>
&lt;th>$b$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Kaplan&lt;/td>
&lt;td>0.73&lt;/td>
&lt;td>0.26&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Approach 1&lt;/td>
&lt;td>0.50&lt;/td>
&lt;td>0.50&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Approach 2&lt;/td>
&lt;td>0.49&lt;/td>
&lt;td>0.51&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Approach 3&lt;/td>
&lt;td>0.46&lt;/td>
&lt;td>0.54&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>结果表明，三种方法的结论差不多：model size 和 dataset size 增长 debility 差不多。&lt;/p>
&lt;p>作者因此给出来的不同模型大小所需要的算力以及 token, 结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Parameters&lt;/th>
&lt;th>Approach 1&lt;/th>
&lt;th>&lt;/th>
&lt;th>Approach 2&lt;/th>
&lt;th>&lt;/th>
&lt;th>Approach 3&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>FLOPs&lt;/td>
&lt;td>Tokens&lt;/td>
&lt;td>FLOPs&lt;/td>
&lt;td>Tokens&lt;/td>
&lt;td>FLOPs&lt;/td>
&lt;td>Tokens&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>400 M&lt;/td>
&lt;td>1.92e+19&lt;/td>
&lt;td>8.0 B&lt;/td>
&lt;td>1.84e+19&lt;/td>
&lt;td>7.7 B&lt;/td>
&lt;td>1.84e+19&lt;/td>
&lt;td>7.7 B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1 B&lt;/td>
&lt;td>1.21e+20&lt;/td>
&lt;td>20.2 B&lt;/td>
&lt;td>1.20e+20&lt;/td>
&lt;td>20.0 B&lt;/td>
&lt;td>1.20e+20&lt;/td>
&lt;td>20.0 B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10 B&lt;/td>
&lt;td>1.23e+22&lt;/td>
&lt;td>205.1 B&lt;/td>
&lt;td>1.32e+22&lt;/td>
&lt;td>219.5 B&lt;/td>
&lt;td>1.32e+22&lt;/td>
&lt;td>219.5 B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>67 B&lt;/td>
&lt;td>5.76e+23&lt;/td>
&lt;td>1.5 T&lt;/td>
&lt;td>6.88e+23&lt;/td>
&lt;td>1.7 T&lt;/td>
&lt;td>6.88e+23&lt;/td>
&lt;td>1.7 T&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>175 B&lt;/td>
&lt;td>3.85e+24&lt;/td>
&lt;td>3.7 T&lt;/td>
&lt;td>4.54e+24&lt;/td>
&lt;td>4.3 T&lt;/td>
&lt;td>4.54e+24&lt;/td>
&lt;td>4.3 T&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>280 B&lt;/td>
&lt;td>9.90e+24&lt;/td>
&lt;td>5.9 T&lt;/td>
&lt;td>1.18e+25&lt;/td>
&lt;td>7.1 T&lt;/td>
&lt;td>1.18e+25&lt;/td>
&lt;td>7.1 T&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>520 B&lt;/td>
&lt;td>3.43e+25&lt;/td>
&lt;td>11.0 T&lt;/td>
&lt;td>4.19e+25&lt;/td>
&lt;td>13.4 T&lt;/td>
&lt;td>4.19e+25&lt;/td>
&lt;td>13.4 T&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1 T&lt;/td>
&lt;td>1.27e+26&lt;/td>
&lt;td>21.2 T&lt;/td>
&lt;td>1.59e+26&lt;/td>
&lt;td>26.5 T&lt;/td>
&lt;td>1.59e+26&lt;/td>
&lt;td>26.5 T&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10 T&lt;/td>
&lt;td>1.30e+28&lt;/td>
&lt;td>216.2 T&lt;/td>
&lt;td>1.75e+28&lt;/td>
&lt;td>292.0 T&lt;/td>
&lt;td>1.75e+28&lt;/td>
&lt;td>292.0 T&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>作者基于发现的 scaling law, 对已有模型进行了探究，发现现有的大模型都存在 under-training 的现象，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/chinchilla-scaling-law/Chinchilla-Overlaied-predictions.png"
width="2422"
height="1284"
loading="lazy"
alt="Overlaid predictions"
class="gallery-image"
data-flex-grow="188"
data-flex-basis="452px"
>&lt;/p>
&lt;p>实验结果显示，现有的大模型的 size 应该更小（或者需要更大的算力）。作者最终的结论就是，现有的比较小的模型，需要更多的算力才能达到更好的表现。&lt;/p>
&lt;h2 id="chinchilla">&lt;a href="#chinchilla" class="header-anchor">&lt;/a>Chinchilla
&lt;/h2>&lt;p>基于上一节的发现，作者提出了 Chinchilla, 一个 70B 的模型，训练使用了 1.4T token. 训练的数据集为 MassiveText 的扩展版本，训练使用的优化器为 AdamW, tokenizer 为 SentencePiece.&lt;/p>
&lt;p>模型配置如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Layers&lt;/th>
&lt;th>Number Heads&lt;/th>
&lt;th>Key/Value Size&lt;/th>
&lt;th>dmodel&lt;/th>
&lt;th>Max LR&lt;/th>
&lt;th>Batch Size&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Gopher 280B&lt;/td>
&lt;td>80&lt;/td>
&lt;td>128&lt;/td>
&lt;td>128&lt;/td>
&lt;td>16384&lt;/td>
&lt;td>$4\times 10^{-5}$&lt;/td>
&lt;td>$3M\to6M$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Chinchilla 70B&lt;/td>
&lt;td>80&lt;/td>
&lt;td>64&lt;/td>
&lt;td>128&lt;/td>
&lt;td>8192&lt;/td>
&lt;td>$1\times 10^{-5}$&lt;/td>
&lt;td>$1.5M\to3M$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="ablation-study">&lt;a href="#ablation-study" class="header-anchor">&lt;/a>Ablation Study
&lt;/h3>&lt;p>&lt;strong>learning rate schedule&lt;/strong>
作者还通过 ablation study 发现，cosine learning rate cycle length 应该和训练步数差不多，当 cycle length 太长时，模型表现会下降。&lt;/p>
&lt;p>&lt;strong>Optimizer&lt;/strong>
作者对比了 &lt;a class="link" href="https://maosong.website/p/notes-on-adam/" target="_blank" rel="noopener"
>Adam&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/notes-on-adamw/" target="_blank" rel="noopener"
>AdamW&lt;/a> 的表现，结果发现，AdamW 的表现优于 Adam.&lt;/p>
&lt;p>&lt;strong>High Precision&lt;/strong>
训练时，作者使用了高精度也就是 &lt;code>float32&lt;/code> 来保存梯度的状态，结果显示，不管是 Adam 还是 AdamW, 使用高精度都可以提高模型的表现&lt;/p>
&lt;p>&lt;strong>Comparison with Kaplan&lt;/strong>
作者还对比了 Chinchilla 和 Kaplan 的预测结果，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/chinchilla-scaling-law/Chinchilla-Comparison-with-Kaplan.png"
width="2684"
height="1436"
loading="lazy"
alt="Comparison with Kaplan"
class="gallery-image"
data-flex-grow="186"
data-flex-basis="448px"
>&lt;/p>
&lt;p>结果显示，基于 Chinchilla 预测得到的模型训练效果比 Kaplan 的更好。&lt;/p>
&lt;p>&lt;strong>Curvature of the FLOPs-frontier&lt;/strong>
作者发现，FLOP-minimal loss frontier 存在 curvature, 也就是小模型和大模型预测出来的曲线是不一样的，作者将结果展示在下图中&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/chinchilla-scaling-law/Chinchilla-curvature-of-FLOPs-frontier.png"
width="3012"
height="1570"
loading="lazy"
alt="Curvature of the FLOPs-frontier"
class="gallery-image"
data-flex-grow="191"
data-flex-basis="460px"
>&lt;/p>
&lt;p>结果显示，从小模型拟合出来的结果比大模型拥有更高的算力使用效率，作者认为这是未来的一个研究方向。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>本文中作者重新探究了针对 LLM 的 scaling law, 作者发现已有的大模型都存在 under-training 的现象，也就是说，模型需要更多的训练 token, 具体来讲，model size scaling 和 dataset scaling 应该处于同一水平。作者基于这个结论，提出了 Chinchilla, 一个 70B 的 LLM, 其表现超过了 280B 的 LLM.&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2203.15556" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Kaplan Scaling Law</title><link>https://maosong.website/p/kaplan-scaling-law/</link><pubDate>Wed, 22 Oct 2025 14:10:52 +0800</pubDate><guid>https://maosong.website/p/kaplan-scaling-law/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者首先就总结了本文的发现，主要有以下几点：&lt;/p>
&lt;ol>
&lt;li>损失与模型的 scale , 即除开 embedding 的模型参数，数据集大小以及算力强相关，与 model shape 比如 depth 或者 width 关系不大&lt;/li>
&lt;li>scaling law 是非常光滑的，意味着 scaling law 是一个可预测的模型&lt;/li>
&lt;li>overfitting 普遍存在，当参数和数据集大小同时增加时，模型的表现会增加，但是当其中一个量固定时，提升就比较小。并且当我们将模型参数提升 8 倍时，我们只需要将数据集的大小提升 5 倍就可以避免过拟合&lt;/li>
&lt;li>训练的损失函数曲线与 model size 无关，因此我们可以预测给定大小模型的表现&lt;/li>
&lt;li>模型在测试集和训练集上的表现高度相关，因此我们可以基于训练集的损失来预测模型的表现&lt;/li>
&lt;li>大模型比小模型拥有更高的 sample efficiency, 即更小的训练步数就可以达到相同的表现&lt;/li>
&lt;li>convergence 不能说明一切，我们可以通过 early-stopping 来提高算力使用效率，避免模型花费过多的算力在较小的提升上&lt;/li>
&lt;li>最优的 batch size 与 loss 呈一个 power law 的关系&lt;/li>
&lt;/ol>
&lt;h3 id="notation">&lt;a href="#notation" class="header-anchor">&lt;/a>Notation
&lt;/h3>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Symbol&lt;/th>
&lt;th>Notation&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$L$&lt;/td>
&lt;td>cross entropy loss&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$N$&lt;/td>
&lt;td>non-embedding parametters&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$B$&lt;/td>
&lt;td>batch size&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$S$&lt;/td>
&lt;td>number of training steps&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$C\approx 6NBS$&lt;/td>
&lt;td>estimate of total training compute&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$D$&lt;/td>
&lt;td>dataset size in tokens&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$B_{crit}$&lt;/td>
&lt;td>critical batch size&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$C_{\min}$&lt;/td>
&lt;td>estimate of the minimum compute to reach a given value of loss&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$S_{\min}$&lt;/td>
&lt;td>estimate of the minimum steps to reach a given value of loss&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\alpha_X$&lt;/td>
&lt;td>power-law exponents for the scaling law of loss&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$d$&lt;/td>
&lt;td>hidden size of the model&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>为了简便，后续未经特殊说明，我们说模型参数均指的是 non-embedding 的参数&lt;/p>
&lt;h3 id="scaling-law-overview">&lt;a href="#scaling-law-overview" class="header-anchor">&lt;/a>Scaling Law Overview
&lt;/h3>&lt;ol>
&lt;li>当数据集 $D$ 足够大时，损失与模型参数大小 $N$ 之间的关系为&lt;/li>
&lt;/ol>
$$
L(N) = \left(\frac{N_C}{N}\right)^{\alpha_N}, \alpha_N\sim 0.076, N_C\sim 8.8\times 10^{13}
$$&lt;ol start="2">
&lt;li>给定模型参数大小 $N$ , 损失与数据集大小 $D$ 之间的关系为&lt;/li>
&lt;/ol>
$$
L(D) = \left(\frac{D_C}{D}\right)^{\alpha_D}, \alpha_D\sim 0.095, D_C\sim 5.4\times 10^{13}
$$&lt;ol start="3">
&lt;li>给定足够大的数据集 $D$ 和最优模型大小 $N$ 时，损失与算力 $C$ 之间的关系为&lt;/li>
&lt;/ol>
$$
L(C_{\min}) = \left(\frac{C_C^{\min}}{C_{\min}}\right)^{\alpha_D}, \alpha_C^{\min}\sim 0.050, C_C^{\min}\sim 3.1\times 10^{8}
$$&lt;p>作者在不同大小的数据集，算力，模型大小下进行了测试，结果发现 scaling 与模型的 shape, transformer 的超参数之间的关系比较小。$\alpha_N,\alpha_D,\alpha_C^{\min}$ 等决定了当我们 scale up 数据及大小，模型大小和算力时损失的变化情况。比如当我们将模型参数提升至 2 倍时，模型的损失会降低至原来的 $0.95$.&lt;/p>
&lt;p>基于发现 1 和 2, 作者发现当我们将模型的 size 提升至原来的 2 倍时，模型的数据集大小应该提升至原来的 $1.67$ 倍，具体关系为 $D\sim N^{0.74}$.&lt;/p>
&lt;p>作者使用了一个统一的公式来描述损失与数据及大小和模型参数大小之间的关系&lt;/p>
$$
L(N,D) = \left[\left(\frac{N_c}{N}\right)^{\frac{\alpha_N}{\alpha_D}}+ \frac{D_c}{D}\right]^{\alpha_D}
$$&lt;ol start="4">
&lt;li>当数据集充分大时，损失与模型参数大小以及更新步数 $S$ 的关系如下&lt;/li>
&lt;/ol>
$$
L(N, S) = \left(\frac{N_C}{N}\right)^{\alpha_N} +\left(\frac{S_C}{S_{\min}(S)}\right)^{\alpha_S}
$$&lt;p>这里 $S_C\approx 2.1\times 10^3$, $\alpha_S\approx 0.76$, $S_{\min}(S)$ 是估计出来的最小优化步数&lt;/p>
&lt;ol start="5">
&lt;li>最优的 batch size 与损失函数之间的关系如下&lt;/li>
&lt;/ol>
$$
B_{crit}(L) = \frac{B_*}{L^{1/\alpha_B}}, B_*\sim 2*10^8 \text{ tokens}, \alpha_B\sim 0.21
$$&lt;ol start="6">
&lt;li>给定算力 $C$ 且无其他限制时，模型参数，数据及大小，batch size 和更新参数与算力之间的关系如下&lt;/li>
&lt;/ol>
$$
N\propto C^{\alpha_C^{\min}/\alpha_N}, B\propto C^{\alpha_C^{\min}/\alpha_B}, S\propto C^{\alpha_C^{\min}/\alpha_S}, D=BS
$$&lt;p>其中&lt;/p>
$$
\alpha_C^{\min} = \frac{1}{\frac{1}{\alpha_S}+\frac{1}{\alpha_B}+\frac{1}{\alpha_N}}
$$&lt;p>实验的结果为 $N\propto C_{\min}^{0.73}$, $B\propto C_{\min}^{0.24}$ , $S\propto C_{\min}^{0.03}$. 也就是说，当我们提升算力时，提升模型的参数大小带来的收益是最高的。&lt;/p>
&lt;h2 id="background">&lt;a href="#background" class="header-anchor">&lt;/a>Background
&lt;/h2>&lt;p>首先，transformer 的参数量通过计算可以得到&lt;/p>
$$
N\approx 2dn(2d+d_{ff}) = 12nd^2
$$&lt;p>这里 $d$ 是 hidden size, $n$ 是 layer 个数，$d_{ff}$ 是 MLP 的 hidden size, 这里我们 假设 $d_{ff}=4d$. 计算时我们丢掉了 bias 以及 LayerNorm 的参数量。具体计算过程见 &lt;a class="link" href="https://maosong.website/p/llm-parameter-computation/" target="_blank" rel="noopener"
>LLM parameter analysis&lt;/a>&lt;/p>
&lt;p>transformer 一次前向计算的 operations 数量大概为&lt;/p>
$$
C_{forward}\approx 2N + 2nLd
$$&lt;p>这里 $L$ 是输入的 token 长度。&lt;/p>
&lt;p>由于反向传播所需要的 FLOPs 是前向传播两倍，因此 transformer 的计算量为&lt;/p>
$$
C = C_{backward} + C_{forward} = 3C_{forward}\approx 6N
$$&lt;p>具体计算过程见 &lt;a class="link" href="https://maosong.website/p/llm-flops-computation/" target="_blank" rel="noopener"
>LLM FLOPs analysis&lt;/a>。也就是说，对于参数量为 $N$ 的 transformer model, 每个 token 所需要的 FLOPs 为 $C\approx 6N$&lt;/p>
&lt;h2 id="empirical-results-and-basic-power-laws">&lt;a href="#empirical-results-and-basic-power-laws" class="header-anchor">&lt;/a>Empirical Results and Basic Power Laws
&lt;/h2>&lt;h3 id="transformer-shape-and-hyper-parameter-independence">&lt;a href="#transformer-shape-and-hyper-parameter-independence" class="header-anchor">&lt;/a>Transformer Shape and Hyper-parameter Independence
&lt;/h3>&lt;p>作者基于 $N=12nd^2$, 在保持总参数量 $N$ 不变的情况下，分别调整 $n$, $d_{ff}$ 和 number of attention heads 的个数 （变化 $d$ 用于维持总参数量不变），结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/kaplan-scaling-law/Kaplan-model-shape-ablation.png"
width="1264"
height="501"
loading="lazy"
alt="Ablation study on model shape"
class="gallery-image"
data-flex-grow="252"
data-flex-basis="605px"
>&lt;/p>
&lt;p>实验结果发现，损失对于 $d_{ff}/d$, $d/n$, $d/n_h$ 都比较 robust, 说明&lt;strong>模型的损失对模型的 shape 依赖性比较低。&lt;/strong>&lt;/p>
&lt;h3 id="non-embedding-parameter-count">&lt;a href="#non-embedding-parameter-count" class="header-anchor">&lt;/a>Non-embedding Parameter Count
&lt;/h3>&lt;p>作者探究了以下 model size 对损失的影响，作者使用了不同的 $n$ 和 $d$, 然后训练得到的损失情况如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/kaplan-scaling-law/Kaplan-ablation-model-size.png"
width="1285"
height="547"
loading="lazy"
alt="Ablation study on model size"
class="gallery-image"
data-flex-grow="234"
data-flex-basis="563px"
>&lt;/p>
&lt;p>作者发现，当包含 embedding parameter 时，损失不仅依赖于模型参数量，还依赖于 layer 层数 $n$, 但是&lt;strong>当我们排除 embedding parameter 时，模型的损失便与 layer 层数 $n$ 关系不大&lt;/strong>。这个趋势可以用以下模型来表示&lt;/p>
$$
L(N) \approx \left(\frac{N_c}{N}\right)^{\alpha_N}
$$&lt;p>最终拟合的曲线如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/kaplan-scaling-law/Kaplan-parameter-scaling-law-curve.png"
width="404"
height="383"
loading="lazy"
alt="Scaling law with respect to parameters"
class="gallery-image"
data-flex-grow="105"
data-flex-basis="253px"
>&lt;/p>
&lt;h3 id="comparing-to-lstms-and-universal-transformers">&lt;a href="#comparing-to-lstms-and-universal-transformers" class="header-anchor">&lt;/a>Comparing to LSTMs and Universal Transformers
&lt;/h3>&lt;p>作者比较了 LSTM 和 Transformer 结构的损失，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/kaplan-scaling-law/Kaplan-ablation-LSTM.png"
width="1268"
height="467"
loading="lazy"
alt="Ablation study on LSTM"
class="gallery-image"
data-flex-grow="271"
data-flex-basis="651px"
>&lt;/p>
&lt;p>可以看到，&lt;strong>transformer 比 LSTM 拥有更强的学习能力&lt;/strong>， LSTM 架构对于 early context 表现比较好，但是随着 context 增加，LSTM 的表现逐渐弱于 transformer. &lt;strong>即 transformer 的长上下文能力强于 LSTM 架构&lt;/strong>。&lt;/p>
&lt;h3 id="generalization-among-data-distributions">&lt;a href="#generalization-among-data-distributions" class="header-anchor">&lt;/a>Generalization Among Data Distributions
&lt;/h3>&lt;p>模型是在 WebText2 数据集上训练的，作者进一步在其他数据集上评估了以下模型的泛化性，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/kaplan-scaling-law/Kaplan-parameter-scaling-law-generalization.png"
width="1266"
height="591"
loading="lazy"
alt="Generalization performance"
class="gallery-image"
data-flex-grow="214"
data-flex-basis="514px"
>&lt;/p>
&lt;p>结果发现，模型在其他数据集上的泛化性很好。并且，&lt;strong>模型的泛化性能仅与训练阶段的表现相关（validation loss），而与训练阶段（是否收敛）无关&lt;/strong>。&lt;/p>
&lt;p>作者还评估了 model depth 对模型泛化性的影响，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/kaplan-scaling-law/Kaplan-ablation-depth-to-generalization.png"
width="658"
height="421"
loading="lazy"
alt="Ablation study on depth"
class="gallery-image"
data-flex-grow="156"
data-flex-basis="375px"
>&lt;/p>
&lt;p>实验结果显示，&lt;strong>model depth 对模型泛化性基本没有影响&lt;/strong>。&lt;/p>
&lt;h3 id="performance-with-data-size-and-compute">&lt;a href="#performance-with-data-size-and-compute" class="header-anchor">&lt;/a>Performance with Data Size and Compute
&lt;/h3>&lt;p>作者探究了损失与 dataset size $D$ 之间的关系。作者固定一个模型，然后当 test loss 不再下降时停止训练，结果发现 test loss 与 dataset size $D$ 之间存在如下关系&lt;/p>
$$
L(D) \approx \left(\frac{D_c}{D}\right)^{\alpha_D}
$$&lt;p>拟合结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/kaplan-scaling-law/Kaplan-dataset-size-scaling-law.png"
width="419"
height="384"
loading="lazy"
alt="Scaling law with respect to dataset"
class="gallery-image"
data-flex-grow="109"
data-flex-basis="261px"
>&lt;/p>
&lt;p>接下来，基于前面计算的结果，我们有 $C\approx 6ND=6NBS$, 这里 $B$ 是 batch size, $S$ 是训练步数。给定 $C$, 作者使用不同大小的模型进行训练，batch size $B$ 保持不懂，训练步数设置为 $S=C/6BS$,实验结果显示损失与算力 $C$ 之间满足如下关系&lt;/p>
$$
L(C) \approx \left(\frac{C_c}{C}\right)^{\alpha_C}
$$&lt;p>拟合结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/kaplan-scaling-law/Kaplan-compute-scaling-law.png"
width="451"
height="384"
loading="lazy"
alt="Scaling law with respect to compute"
class="gallery-image"
data-flex-grow="117"
data-flex-basis="281px"
>&lt;/p>
&lt;p>作者进一步探究了 sample efficiency 与 model size 之间的关系，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/kaplan-scaling-law/Kaplan-sample-efficiency-vs-model-size.png"
width="1273"
height="513"
loading="lazy"
alt="Sample efficiency with respect to model size"
class="gallery-image"
data-flex-grow="248"
data-flex-basis="595px"
>&lt;/p>
&lt;p>结果显示，&lt;strong>随着 model size 增加，sample efficiency 也在增加&lt;/strong>&lt;/p>
&lt;h2 id="charting-the-infinite-data-limit-and-overfitting">&lt;a href="#charting-the-infinite-data-limit-and-overfitting" class="header-anchor">&lt;/a>Charting the Infinite Data Limit and Overfitting
&lt;/h2>&lt;p>作者在本节探讨了同时变化 $N$ 和 $D$ 对损失变化的影响。&lt;/p>
&lt;h3 id="proposed-equation">&lt;a href="#proposed-equation" class="header-anchor">&lt;/a>Proposed Equation
&lt;/h3>&lt;p>作者基于三个原则进行建模：&lt;/p>
&lt;ol>
&lt;li>改变 vocabulary size 或者 tokenization 会 rescale loss&lt;/li>
&lt;li>固定 $D$ 并且令 $N\to\infty$, 则最终损失应该接近 $L(D)$. 反之固定 $N$, 令 $D\to\infty$, 最终损失应该接近 $L(N)$&lt;/li>
&lt;li>$L(N,D)$ 在 $D=\infty$ 处应该是可解析的&lt;/li>
&lt;/ol>
&lt;p>基于以上三条原则，将模型选择为如下形式&lt;/p>
$$
L(N,D) = \left[\left(\frac{N_c}{N}\right)^{\frac{\alpha_N}{\alpha_D}}+ \frac{D_c}{D}\right]^{\alpha_D}
$$&lt;p>作者基于不同配置进行训练，基于实验结果你和得到的参数如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Parameter&lt;/th>
&lt;th>$\alpha_N$&lt;/th>
&lt;th>$\alpha_D$&lt;/th>
&lt;th>$N_c$&lt;/th>
&lt;th>$D_c$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Value&lt;/td>
&lt;td>0.076&lt;/td>
&lt;td>0.103&lt;/td>
&lt;td>$6.4\times 10^{13}$&lt;/td>
&lt;td>$1.8\times 10^{13}$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>接下来，作者探究了模型的过拟合程度，作者定义如下 metric&lt;/p>
$$
\delta L(N,D) := \frac{L(N,D)}{L(N,\infty)} - 1
$$&lt;p>带入 $L(N,D)$ 定义就得到&lt;/p>
$$
\delta L(N,D) = \left(1 + \left(\frac{N}{N_c}\right)^{\frac{\alpha_N}{\alpha_D}}\frac{D_c}{D}\right) - 1
$$&lt;p>通过测试不同的模型，作者发现 $\delta L$ 的值在 $0.02$ 左右，将实验结果带入到上面的公式就得到&lt;/p>
$$
D \geq (5\times 10^3)N^{0.7379}
$$&lt;p>也就是说对于参数量为 $N$ 的模型，需要 data size $D \geq (5\times 10^3)N^{0.7379}$ 才能避免过拟合。&lt;/p>
&lt;h2 id="scaling-laws-with-model-size-and-training-time">&lt;a href="#scaling-laws-with-model-size-and-training-time" class="header-anchor">&lt;/a>Scaling Laws with Model Size and Training time
&lt;/h2>&lt;p>作者在本节构建了损失函数与 model size $N$ 以及训练时间的 scaling law&lt;/p>
&lt;h3 id="adjustment-for-training-at-critical-batch-size">&lt;a href="#adjustment-for-training-at-critical-batch-size" class="header-anchor">&lt;/a>Adjustment for Training at Critical Batch Size
&lt;/h3>&lt;p>已有结论说明，存在一个 critical batch size $B_{crit}$, 当 batch size 接近 $B_{crit}$ 时，增加 batch size 对计算效率影响比较小，但是当 batch size 大于 $B_{crit}$ 时，带来的提升比较小。另一方面，batch size 会影响梯度的噪声程度。因此，训练步数 $S$ 和处理的样本数 $E=BS$ 应该满足：&lt;/p>
$$
\left(\frac{S}{S_{\min}}-1\right)\left(\frac{E}{E_{\min}}-1\right) = 1
$$&lt;p>这里 $S_{\min}$ 是达到损失 $L$ 所需要的最小训练步数，而 $E_{\min}$ 是最小的训练样本数量。&lt;/p>
&lt;p>作者的实验结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/kaplan-scaling-law/Kaplan-critical-batch-size-relation.png"
width="1264"
height="499"
loading="lazy"
class="gallery-image"
data-flex-grow="253"
data-flex-basis="607px"
>&lt;/p>
&lt;p>作者将 critical batch size 定义为&lt;/p>
$$
B_{crit}(L) := \frac{E_{\min}}{S_{\min}}
$$&lt;p>使用 critical batch size 进行训练可以在计算效率和算力之间达到一个平衡。&lt;/p>
&lt;p>作者基于上面的实验结果探究了 critical batch size 和 model performance 之间的关系，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/kaplan-scaling-law/Kaplan-critical-batch-size-vs-performance.png"
width="946"
height="620"
loading="lazy"
class="gallery-image"
data-flex-grow="152"
data-flex-basis="366px"
>&lt;/p>
&lt;p>可以看到，critical batch size 与 model size 的关系不大，仅与损失 $L$ 有关。作者通过以下模型拟合 critical batch size:&lt;/p>
$$
B_{crit}(L) \approx \frac{B_*}{L^{1/\alpha_B}}
$$&lt;p>这里 $B_*\approx 2\times 10^8$, $\alpha_B\approx 0.21$.&lt;/p>
&lt;p>给定一个 target loss $L$, 当 batch size $B>> B_{crit}$ 时，作者定义最小训练步数为&lt;/p>
$$
S_{\min}(S) := \frac{S}{1+B_{crit}(L)/B}
$$&lt;p>给定 target loss $L$ 和 model size $N$, 当 batch size $B&lt;&lt; B_{crit}$ 时，作者定义最小算力为&lt;/p>
$$
C_{\min}(C) := \frac{C}{1+B_{crit}(L)/B}
$$&lt;h3 id="performance-with-model-size-and-compute">&lt;a href="#performance-with-model-size-and-compute" class="header-anchor">&lt;/a>Performance with Model Size and Compute
&lt;/h3>&lt;p>作者使用如下公式来探究损失与 model size 和 computer 之间的关系&lt;/p>
$$
L(N, S_{\min}) = \left(\frac{N_C}{N}\right)^{\alpha_N} +\left(\frac{S_C}{S_{\min}(S)}\right)^{\alpha_S}
$$&lt;p>拟合结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Parameter&lt;/th>
&lt;th>$\alpha_N$&lt;/th>
&lt;th>$\alpha_S$&lt;/th>
&lt;th>$N_c$&lt;/th>
&lt;th>$S_c$&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Value&lt;/td>
&lt;td>0.077&lt;/td>
&lt;td>0.76&lt;/td>
&lt;td>$6.5\times 10^{13}$&lt;/td>
&lt;td>$2.1\times 10^{3}$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>基于这个拟合结果，作者得到了下图的结果&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/kaplan-scaling-law/Kaplan-loss-vs-model-size-and-training-steps.png"
width="613"
height="347"
loading="lazy"
class="gallery-image"
data-flex-grow="176"
data-flex-basis="423px"
>&lt;/p>
&lt;p>作者还使用了不同的可视化方式，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/kaplan-scaling-law/Kaplan-performance-vs-compute-budget-and-steps.png"
width="1273"
height="560"
loading="lazy"
class="gallery-image"
data-flex-grow="227"
data-flex-basis="545px"
>&lt;/p>
&lt;p>实验结果显示，上面的公式拟合的很好。&lt;/p>
&lt;h3 id="lower-bound-on-early-stopping-step">&lt;a href="#lower-bound-on-early-stopping-step" class="header-anchor">&lt;/a>Lower Bound on Early Stopping step
&lt;/h3>&lt;p>作者还探究了以下 early step 与模型大小以及数据集之间的关系，作者通过分析得到如下结果&lt;/p>
$$
S_{stop}(N,D) \gtrsim \frac{S_c}{[L(N,D)-L(N,\infty)]^{1/\alpha_S}}
$$&lt;p>其中 $L(N,\infty)$ 是在充分大数据集上的收敛损失。作者对实验结果进行了拟合，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/kaplan-scaling-law/Kaplan-early-stop-resultes.png"
width="1277"
height="618"
loading="lazy"
class="gallery-image"
data-flex-grow="206"
data-flex-basis="495px"
>&lt;/p>
&lt;h2 id="optimal-allocation-of-the-compute-budget">&lt;a href="#optimal-allocation-of-the-compute-budget" class="header-anchor">&lt;/a>Optimal Allocation of the Compute Budget
&lt;/h2>&lt;p>作者在本节探究了最优算力与 model size $N$ 和训练数据 $2B_{crit}S_{\min}$ 之间的关系&lt;/p>
&lt;h3 id="optimal-performance-and-allocations">&lt;a href="#optimal-performance-and-allocations" class="header-anchor">&lt;/a>Optimal Performance and Allocations
&lt;/h3>&lt;p>作者首先基于&lt;/p>
$$
C_{\min}(C) := \frac{C}{1+B_{crit}(L)/B}
$$&lt;p>绘制了如下曲线图&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/kaplan-scaling-law/Kaplan-loss-vs-optimal-compute.png"
width="664"
height="439"
loading="lazy"
class="gallery-image"
data-flex-grow="151"
data-flex-basis="363px"
>&lt;/p>
&lt;p>作者发现，相比于 loss 与算力 $C$ 之间的关系，使用 $C_{\min}$ 进行拟合效果更好。&lt;/p>
&lt;p>接下来，作者基于 $L(C_{\min})$ 进一步探究了给定算力如何决定最优的 model size $N(C_{\min})$. 其实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/kaplan-scaling-law/Kaplan-optimal-model-size-given-compute.png"
width="616"
height="405"
loading="lazy"
class="gallery-image"
data-flex-grow="152"
data-flex-basis="365px"
>&lt;/p>
&lt;p>实验结果显示，model size 和算力之间有如下关系&lt;/p>
$$
N(C_{\min}) \propto (C_{\min})^{0.73}
$$&lt;p>作者进一步探究了对于非最优模型大小与算力之间的关系，作者先构建了如下的关系&lt;/p>
$$
\frac{C(N, N_{\mathrm{eff}})}{C(N_{\mathrm{eff}}, N_{\mathrm{eff}})}
= \frac{N}{N_{\mathrm{eff}}}
\left[
1 + \frac{\alpha_S}{\alpha_N}
\left( 1 - \left( \frac{N_{\mathrm{eff}}}{N} \right)^{\alpha_N} \right)
\right]^{-\!1 / \alpha_S}.
$$&lt;p>对应的示意图为&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/kaplan-scaling-law/Kaplan-suboptimal-model-efficiency.png"
width="1256"
height="600"
loading="lazy"
class="gallery-image"
data-flex-grow="209"
data-flex-basis="502px"
>&lt;/p>
&lt;p>实现结果发现，大小为最优模型的 $0.6\sim 2.2$ 倍只需要额外 $20\%$ 的算力。作者强调，这个实验结果对于超大模型不一定适用。&lt;/p>
&lt;p>作者进一步推导了 $S_{\min}$ 和 $C_{\min}$ 之间的关系，由于 $C_{\min}=6NB_{crit}S$, 且我们前面已经有 $B\propto L^{-4.8}$ 和 $L\propto C_{\min}^{-0.05}$, 因此 我们有&lt;/p>
$$
B_{crit}\propto L^{-4.8} \propto (C_{\min})^{-0.05\times (-4.8)}\propto (C_{\min})^{0.24}
$$&lt;p>以及&lt;/p>
$$
S_{\min} \propto \frac{C_{\min}}{6B_{crit}N(C_{\min})} \propto (C_{\min})^{0.03}
$$&lt;p>拟合的结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/kaplan-scaling-law/Kaplan-optimal-step-size-vs-compute.png"
width="620"
height="403"
loading="lazy"
class="gallery-image"
data-flex-grow="153"
data-flex-basis="369px"
>&lt;/p>
&lt;p>因此，基于上面的结果，当我们增加算力时，我们的主要精力应该放在增加模型大小和提高 batch size 上，而训练步数基本可以保持不变。&lt;/p>
&lt;h3 id="another-way-of-derivation">&lt;a href="#another-way-of-derivation" class="header-anchor">&lt;/a>Another way of Derivation
&lt;/h3>&lt;p>作者还给出了另一种建模 $L(C_{\min})$ 的方式，即从 $L(N,S_{\min})$ 中进行推导，作者将 $B_{crit}$ 和 $S_{\min}$ 的表达式带入到 $L(N,S_{\min})$ 然后求解最小值就得到&lt;/p>
$$
L(C_{\min})= \left(\frac{C_C^{\min}}{C_{\min}}\right)^{\alpha_C^{\min}}
$$&lt;p>其中&lt;/p>
$$
\alpha_C^{\min} = \frac{1}{\frac{1}{\alpha_S}+\frac{1}{\alpha_B}+\frac{1}{\alpha_N}} \approx 0.054
$$&lt;p>这和前面的结果基本是吻合的，进一步进行推导得到&lt;/p>
$$
N(C_{\min})\propto (C_{\min})^{\alpha_C^{\min}/\alpha_N}\approx (C_{\min})^{0.71}
$$&lt;p>这个结果也和上面的差不多。&lt;/p>
&lt;h3 id="contradiction-and-a-conjecture">&lt;a href="#contradiction-and-a-conjecture" class="header-anchor">&lt;/a>Contradiction and a Conjecture
&lt;/h3>&lt;p>作者发现，尽管拟合的 scaling law 曲线非常好，但是由于自然语言不可能达到 zero entropy, 因此该曲线最终一定会失效。作者基于更大的模型进行了实验，结果发现，模型在某一点开始就比预测的损失曲线下降的更慢。作者认为这是因为 transformer 模型已经达到了 maximal performance 导致的。&lt;/p>
&lt;p>通过前面的分析，我们发现 $L(C_{\min})$ 比 $L(D)$ 下降的快，因此两者必然在某一点相交。&lt;/p>
&lt;p>在前面的章节中，我们基于以下关系来决定数据集大小&lt;/p>
$$
D\propto N^{0.74}\propto (C_{\min})^{0.74*0.73}\propto (C_{\min})^{0.54}
$$&lt;p>这里我们利用了 $N(C_{\min})$ 的结果&lt;/p>
&lt;p>另一方面，我们有&lt;/p>
$$
D(C_{\min}) = \frac{2C_{\min}}{6N(C_{\min})}\propto (C_{\min})^{0.26}
$$&lt;p>可以看到，基于训练最优导出的数据集大小相比于拟合出来的数据集大小，实际上存在过拟合。&lt;/p>
&lt;p>作者进一步分析出了 $L(D(C_{\min}))$ 和 $L(C_{\min})$ 这两条曲线的交点，结果得到&lt;/p>
$$
C^*\approx 10^4 \text{ PF-Days}, N^*\approx 10^{12}\text{ parameters}, D^*\approx 10^12\text{ tokens}, L^*\approx 1.7\text{1.7nats/token}
$$&lt;p>作者认为出现这种原因有以下几种情况：&lt;/p>
&lt;ol>
&lt;li>$L^*$ 给出了自然语言的 entropy 的一个估计，因此当模型充分大之后，模型可能已经获取到了数据中的所有知识&lt;/li>
&lt;li>$L(C_{\min})$ 可以作为数据集噪声的一个量化表现，其衡量了数据集的质量&lt;/li>
&lt;/ol>
&lt;h2 id="learning-rate-schedule">&lt;a href="#learning-rate-schedule" class="header-anchor">&lt;/a>Learning Rate Schedule
&lt;/h2>&lt;p>附录中，作者还探究了 learning rate 与损失之间的关系，作者使用了不同 learning rate schedule 对模型损失的影响，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/kaplan-scaling-law/Kaplan-learning-rate-schedule-results.png"
width="1278"
height="530"
loading="lazy"
class="gallery-image"
data-flex-grow="241"
data-flex-basis="578px"
>&lt;/p>
&lt;p>实验结果显示，&lt;strong>只要 learning rate 下降的不会太快，模型的表现基本上差不太多&lt;/strong>。&lt;/p>
&lt;p>作者基于实验结果得到了学习率和模型参数之间的关系如下&lt;/p>
$$
\text{lr}(N)\approx 0.003239 - 0.0001395\log N
$$&lt;p>也就是说，小模型用比较大的学习率，大模型用较小的学习率。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中训练了大量不同配置的大模型，然后构建了损失（损失）与模型参数，数据及大小以及算力之间的关系。实验结果发现，损失与架构和优化参数之间的关系比较小，主要由模型参数量决定，更大的模型拥有更高的采样效率。&lt;/p>
&lt;p>作者认为，本文的局限在于损失函数不一定能够反应模型在其他语言任务上的表现。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2001.08361" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>LLM FLOPs Computation</title><link>https://maosong.website/p/llm-flops-computation/</link><pubDate>Wed, 15 Oct 2025 16:33:39 +0800</pubDate><guid>https://maosong.website/p/llm-flops-computation/</guid><description>&lt;p>本文中，我们介绍如何计算基于 transformer 架构的 LLM 的 FLOPs, 计算完成之后，我们可以推导出算力 $C$ 与模型参数量 $N$，数据集大小 $D$ 之间的关系，即 $C\approx 6ND$.&lt;/p>
&lt;h2 id="background">&lt;a href="#background" class="header-anchor">&lt;/a>Background
&lt;/h2>&lt;h3 id="flops">&lt;a href="#flops" class="header-anchor">&lt;/a>FLOPs
&lt;/h3>&lt;p>FLOPs，floating point operations，表示浮点数运算次数，一般计算 FLOPs 仅考虑加法和乘法。&lt;/p>
&lt;blockquote>
&lt;p>假设 $A\in\mathbb{R}^{m\times p}$, $B\in\mathbb{R}^{p\times n}$, 则 计算 $C=AB$ 的过程中一共需要进行 $mnp$ 次乘法运算和 $mnp$ 次加法运算，因此总的 FLOPs 为 $2mnp$.&lt;/p>
&lt;/blockquote>
&lt;h3 id="assumption">&lt;a href="#assumption" class="header-anchor">&lt;/a>Assumption
&lt;/h3>&lt;blockquote>
&lt;p>假设 $A\in\mathbb{R}^{m\times p}$, $B\in\mathbb{R}^{p\times n}$, $C\in\mathbb{R}^{m\times n}$ 则 计算 $D=AB+C$ 的过程中一共需要进行 $mnp$ 次乘法运算和 $mnp+mn$ 次加法运算，因此总的 FLOPs 为 $2mnp+mn\approx 2mnp$.&lt;/p>
&lt;/blockquote>
&lt;p>基于上述结论，我们计算 FLOPs 时，忽略 element-wise 的操作，即我们做如下假设：&lt;/p>
&lt;ol>
&lt;li>忽略 normalization 中的小常数项运算&lt;/li>
&lt;li>忽略 residual connection 和 bias term 的加法&lt;/li>
&lt;li>忽略注意力的 MASK 和 softmax，因为这两者都是 element-wise operation.&lt;/li>
&lt;li>使用 look-up 计算 embedding layer&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>注，基于以上假设，我们的结果与 Chinchilla Scaling law 的结果稍有不同，但结论不变。&lt;/p>
&lt;/blockquote>
&lt;h3 id="notation">&lt;a href="#notation" class="header-anchor">&lt;/a>Notation
&lt;/h3>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Math Variable&lt;/th>
&lt;th>Code Variable&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$n$&lt;/td>
&lt;td>&lt;code>num_hidden_layers&lt;/code>&lt;/td>
&lt;td>Transformer block 个数&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\vert V\vert$&lt;/td>
&lt;td>&lt;code>vocab_size&lt;/code>&lt;/td>
&lt;td>词表大小&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$d$&lt;/td>
&lt;td>&lt;code>hidden_size&lt;/code>&lt;/td>
&lt;td>token embedding 的维度&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$d_{ff}$&lt;/td>
&lt;td>&lt;code>intermediate_size&lt;/code>&lt;/td>
&lt;td>MLP 的中间层的维度&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$h$&lt;/td>
&lt;td>&lt;code>num_attention_heads&lt;/code>&lt;/td>
&lt;td>query head 的个数&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$s$&lt;/td>
&lt;td>&lt;code>seq_len&lt;/code>&lt;/td>
&lt;td>length of token sequence&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;blockquote>
&lt;p>注：为了避免混淆，我们使用 $n$ 来表示 decode layer 的个数。&lt;/p>
&lt;/blockquote>
&lt;h2 id="computation">&lt;a href="#computation" class="header-anchor">&lt;/a>Computation
&lt;/h2>&lt;p>我们计算训练阶段的总 FLOPs, 记为 $C$, &lt;a class="link" href="https://maosong.website/p/kaplan-scaling-law/" target="_blank" rel="noopener"
>Kaplan scaling law&lt;/a> 用 PF-days 作为单位，$1\text{ PF-Days}=10^{15}\times 24\times 3600\ FLOPs$. 训练阶段包括前向阶段 (forward pass) 和反向传播阶段 (backward pass). 因此&lt;/p>
$$
C = FLOPs(\text{forward}) + FLOPs(\text{backward})
$$&lt;h3 id="forward">&lt;a href="#forward" class="header-anchor">&lt;/a>Forward
&lt;/h3>&lt;p>decoder-only transformer 的模型架构包含三个模块：&lt;/p>
&lt;ol>
&lt;li>1 层 embedding layer&lt;/li>
&lt;li>$n$ 层 decoder layer&lt;/li>
&lt;li>1 层 lm head layer&lt;/li>
&lt;/ol>
&lt;p>因此模型总的 FLOPs 为&lt;/p>
$$
FLOPs(\text{forward}) = FLOPs(\text{embedding}) + n*FLOPs(\mathrm{decode\_layer})+FLOPs(\mathrm{lm\_head})
$$&lt;h4 id="embedding--lm-head">&lt;a href="#embedding--lm-head" class="header-anchor">&lt;/a>Embedding &amp;amp; Lm Head
&lt;/h4>&lt;p>首先，对于 embedding layer, embedding layer 本质上是一个 look up table, 计算过程中不涉及浮点数运算，因此 $\boxed{FLOPs(\text{embedding})=0}$.&lt;/p>
&lt;p>接下来，对于 &lt;code>lm_head&lt;/code>, 这是一个 linear layer, 其权重大小为 $W\in\mathbb{R}^{d\times |V|}$, 输入为 $x\in\mathbb{R}^{s\times d}$, 因此 $\boxed{FLOPs(\mathrm{lm\_head})=2sd|V|}$.&lt;/p>
&lt;p>因此，我们有&lt;/p>
$$
FLOPs(\text{forward}) = n*FLOPs(\mathrm{decode\_layer})+ 2sd|V|
$$&lt;h4 id="decode-layer">&lt;a href="#decode-layer" class="header-anchor">&lt;/a>Decode Layer
&lt;/h4>&lt;p>对于 &lt;code>decode_layer&lt;/code>, 其又包含了四个模块：&lt;/p>
&lt;ol>
&lt;li>pre-normalization&lt;/li>
&lt;li>attention&lt;/li>
&lt;li>post-normalization&lt;/li>
&lt;li>FFN&lt;/li>
&lt;/ol>
&lt;p>pre-normalization 和 post-normalization 一般是一样的，因此&lt;/p>
$$
\begin{aligned}
FLOPs(\mathrm{decode\_layer}) &amp;= FLOPs(\mathrm{pre\_normoalization}) + FLOPs(\mathrm{Attention}) + FLOPs(\mathrm{post\_normoalization}) +FLOPs(\mathrm{FFN})\\
&amp;= 2*FLOPs(\mathrm{normoalization}) + FLOPs(\mathrm{Attention})+FLOPs(\mathrm{FFN})
\end{aligned}
$$&lt;h4 id="normalization">&lt;a href="#normalization" class="header-anchor">&lt;/a>Normalization
&lt;/h4>&lt;p>现在有两种常见的 normalization, 也就是 LayerNorm 和 RMSNorm&lt;/p>
&lt;p>LayerNorm 定义如下&lt;/p>
$$
\mathrm{LayerNorm}(x) = \frac{x-\mathbb{E}[x]}{\sqrt{\mathrm{var}[x]+\epsilon}}\odot \beta + \gamma
$$&lt;p>其中 $\beta,\gamma\in\mathbb{R}^d$ 是可学习的参数。&lt;/p>
&lt;p>对输入 $x\in\mathbb{R}^{s\times d}$, 均值需要约 $sd$ 次 FLOPs，方差 $\mathrm{var}[x]$ 只需要约 $3sd$ 次 FLOPs，这两者的计算都可以忽略。接下来就是 scaling 和 shift, 这两者都是 element-wise 操作，我们这里，因此总的 FLOPs 为&lt;/p>
$$
\boxed{FLOPs(\mathrm{normoalization}) = 4sd}
$$&lt;p>RMSNorm 的作用和 LayerNorm 是一样的，但是实现上更简单&lt;/p>
$$
\mathrm{RMSNorm}(x) = \frac{x}{\sqrt{\|x\|_2^2+\epsilon}}\odot \gamma
$$&lt;p>其中 $\gamma\in\mathbb{R}^d$ 是可学习的参数&lt;/p>
&lt;p>对于 RMSNorm，其分析方式与 LayerNorm 基本一致，因此总的 FLOPs 为&lt;/p>
$$
\boxed{FLOPs(\mathrm{normoalization}) = 4sd}
$$&lt;p>总之，不管使用哪种 normalization，其 FLOPs 都是&lt;/p>
$$
\boxed{FLOPs(\mathrm{normoalization}) = 4sd}
$$&lt;h4 id="attention">&lt;a href="#attention" class="header-anchor">&lt;/a>Attention
&lt;/h4>&lt;p>Attention 定义如下&lt;/p>
$$
\mathrm{Attention}(X) = \mathrm{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V\in\mathbb{R}^{m\times d}
$$&lt;p>其中 $X\in\mathbb{R}^{s\times d}$, $W_Q,W_K,W_V\in\mathbb{R}^{d\times d}$&lt;/p>
$$
Q = W_QX\in\mathbb{R}^{s\times d},\quad
K =W_KX\in\mathbb{R}^{s\times d},\quad
V = W_VX\in\mathbb{R}^{s\times d}
$$&lt;p>$Q,K,V$ 计算的 FLOPs 为 $6*sd^2$. $QK^T$ 的 FlOPs 为 $2s^2d$, $\mathrm{softmax}(\cdot)V$ 的 FLOPs 为 $2s^2d$, 最后对于 multi-head attention 还有一个 output projection layer, 其权重为 $W_O\in\mathbb{R}^{d\times d}$, 因此 FLOPs 为 $2sd^2$. 故 attention 最终的 FLOPs 为&lt;/p>
$$
FLOPs(\mathrm{Attention})=6sd^2+2s^2d+2s^2d+2sd^2=\boxed{8sd^2+4s^2d}
$$&lt;h4 id="ffn">&lt;a href="#ffn" class="header-anchor">&lt;/a>FFN
&lt;/h4>&lt;p>对于 FFN，有两种常见的形式，一种基于 ReLU 激活函数，其定义如下&lt;/p>
$$
y = \max(xW_1+b_1, 0)W_2 + b_2
$$&lt;p>其中 $W_1\in\mathbb{R}^{d_{ff}\times d}$, $W_2\in\mathbb{R}^{d\times d_{ff}}$. $b_1\in\mathbb{R}^{d_{ff}}$, $b_2\in\mathbb{R}^{d}$.&lt;/p>
&lt;p>对输入 $x\in\mathbb{R}^{s\times d}$, 其 FLOPs 为&lt;/p>
$$
FLOPs(\mathrm{FFN_{ReLU}}) = 2sdd_{ff} + 2sd_{ff}d =\boxed{4sdd_{ff}}
$$&lt;p>其中第一项和第二项分别为为 $xW_1$ 与 $\max(xW_1+b_1, 0)W_2$ 的 FLOPs.&lt;/p>
&lt;p>另一种基于 SwiGLU 激活函数，其定义为&lt;/p>
$$
\mathrm{SwiGLU}(x) = x\odot \sigma(x)
$$&lt;p>其中 $\sigma(\cdot)$ 是 sigmoid 函数&lt;/p>
&lt;p>FFN 的定义为&lt;/p>
$$
y = W_2(W_3x\odot \mathrm{SwiGLU}(W_1x))
$$&lt;p>其中 $W_3,W_1\in\mathbb{R}^{d_{ff}\times d}$, $W_2\in\mathbb{R}^{d\times d_{ff}}$.&lt;/p>
&lt;p>对输入 $x\in\mathbb{R}^{s\times d}$, 其 FLOPs 为&lt;/p>
$$
FLOPs(\mathrm{FFN_{SwiGLU}}) = 2sdd_{ff} + 2sdd_{ff} + 2sd_{ff}d = \boxed{6sdd_{ff}}
$$&lt;h4 id="summary">&lt;a href="#summary" class="header-anchor">&lt;/a>Summary
&lt;/h4>&lt;p>最终，decoder-only transformer 的 FLOPs 计算量为 （我们假设使用 multi-head attention, 基于 SwiGLU 的 MLP）&lt;/p>
$$
\begin{aligned}
FLOPs(\text{forward}) &amp;= FLOPs(\text{embedding}) + n*FLOPs(\mathrm{decode\_layer})+FLOPs(\mathrm{lm\_head})\\
&amp;= n*FLOPs(\mathrm{decode\_layer})+2sd|V|\\
&amp;= n*(2*FLOPs(\mathrm{normoalization}) + FLOPs(\mathrm{Attention})+FLOPs(\mathrm{FFN}))+2sd|V|\\
&amp;= n*(8sd+8sd^2+4s^2d+6sdd_{ff})+2sd|V|\\
&amp;= nsd^2\left(\frac8d + 8+\frac{4s}{d}+\frac{6d_{ff}}{d}+\frac{2|V|}{nd}\right)\\
&amp;\approx \boxed{nsd^2\left(8+\frac{4s}{d}+\frac{6d_{ff}}{d}+\frac{2|V|}{nd}\right)}
\end{aligned}
$$&lt;p>这里我们丢弃了 normalization 项，因为 normalization 的 FLOPs 是一个低阶项&lt;/p>
&lt;h3 id="backward">&lt;a href="#backward" class="header-anchor">&lt;/a>Backward
&lt;/h3>&lt;p>首先，我们有如下结论：&lt;/p>
&lt;blockquote>
&lt;p>神经网络 backward 过程的计算量（FLOPs）为 forward 过程的两倍&lt;/p>
&lt;/blockquote>
&lt;p>我们使用一个简单的例子来证明这个结论，考虑线性层 $h=Wx$, 其中 $W\in\mathbb{R}^{m\times d}$, 对于输入 $x\in\mathbb{R}^{d\times 1}$ 其 forward 过程的计算量为 $2md$.&lt;/p>
&lt;p>对于反向过程，我们需要分别计算损失 $L$ 对权重和输入的梯度，即&lt;/p>
$$
\frac{\partial L}{\partial x} = W^T\frac{\partial L}{\partial h}\in\mathbb{R}^{d\times 1}, \quad\frac{\partial L}{\partial W} = \frac{\partial L}{\partial h}\otimes x^T\in{m\times d},
$$&lt;p>这里 $\frac{\partial L}{\partial h}\in\mathbb{R}^{m\times 1}$ 为损失对输出 $h$ 的梯度。因此反向传播的总计算量为&lt;/p>
$$
2dm + 2md = 4md
$$&lt;p>这里的两项分别是对 $x$ 和 $W$ 求梯度的 FLOPs.&lt;/p>
&lt;h3 id="overall">&lt;a href="#overall" class="header-anchor">&lt;/a>Overall
&lt;/h3>&lt;p>将前向传播和反向传播的计算量汇总我们就得到一次前向传播和一次反向传播过程中，对于长度为 $s$ 的 token, 其 FLOPs 为 (multi-head attention, SwiGLU-FFN)&lt;/p>
$$
\begin{aligned}
C &amp;= FLOPs(\text{forward}) + FLOPs(\text{backward})\\
&amp;= 3FLOPs(\mathrm{forward}) \\
&amp;\approx \boxed{3nsd^2\left(8+\frac{4s}{d}+\frac{6d_{ff}}{d}+\frac{2V}{nd}\right)}
\end{aligned}
$$&lt;h3 id="extension">&lt;a href="#extension" class="header-anchor">&lt;/a>Extension
&lt;/h3>&lt;h4 id="gqa">&lt;a href="#gqa" class="header-anchor">&lt;/a>GQA
&lt;/h4>&lt;p>GQA 与 MHA 不同的地方在于通过共享 key 和 value 来降低 KV cache 的占用，我们假设 group number 为 $g$, 则 key 和 value 的 FLOPs 现在变成了&lt;/p>
$$
2sdd_h\frac{h}{g}+2sdd_h\frac{h}{g}=4sdd_h\frac{h}{g}
$$&lt;p>因此 attention 部分总的 FLOPs 变成了&lt;/p>
$$
FLOPs(\mathrm{Attention})=4sd^2+2s^2d+2s^2d+4sdd_h\frac{h}{g}=4sd^2+4s^2d+4sdd_h\frac{h}{g}
$$&lt;p>当 $g=h$ 时，GQA 就变成了 MHA, 此时的 FLOPs 也一致。&lt;/p>
&lt;h4 id="moe">&lt;a href="#moe" class="header-anchor">&lt;/a>MoE
&lt;/h4>&lt;p>MoE 是针对 Dense FFN 的一个改进，介绍见 &lt;a class="link" href="https://maosong.website/p/moe-tutorial/" target="_blank" rel="noopener"
>MoE&lt;/a>, 我们假设一共有 $e$ 个路由专家，其中激活 $k$ 个。&lt;/p>
&lt;p>Gate layer 一般是一个 linear layer, 其权重矩阵大小为 $W_{G}\in\mathbb{R}^{d\times e}$, 因此 $FLOPs(\text{router})= 2sde$.&lt;/p>
&lt;p>Expert layer 和前面提到的 FFN 一致，我们每次挑选出 $k$ 个专家进行计算，因此 expert 部分 $FLOPs(\text{expert})=6ksdd_{ff}$.&lt;/p>
&lt;p>从而对于 MoE 来说，FFN 部分的 FLOPs 为&lt;/p>
$$
FLOPs(\text{MoE}) = FLOPs(\text{router})+FLOPs(\text{expert})= \boxed{2sde+6ksdd_{ff}}
$$&lt;h3 id="simplification">&lt;a href="#simplification" class="header-anchor">&lt;/a>Simplification
&lt;/h3>&lt;p>我们已经得到了 transformer 的 FLOPs 计算表达式，但是其表达式比较繁琐，因此，在研究 scaling law 时，一般会进行简化。&lt;/p>
&lt;p>首先，在 &lt;a class="link" href="https://maosong.website/p/llm-parameter-computation/" target="_blank" rel="noopener"
>LLM parameter analysis&lt;/a> 中，我们已经给出了 LLM 参数量 $N$ （基于 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a>）的计算结果&lt;/p>
$$
N=n*(4d+3dd_{ff}+2hh_{d}d + 2h_{kv}h_dd) + d(2|V|+1)
$$&lt;p>我们这里对其进行简化，一般来说 $d_{ff}=8/3d$, $h_{kv}=h$, $h_d=d/h$, 带入就得到&lt;/p>
$$
N = n(4d+8d^2+2d^2+2d^2) + d(2|V|+1)=n(12d^2+4d)+ d(2|V|+1)
$$&lt;p>我们忽略关于 $d$ 的一阶项，并且我们假设 $|V| &lt;&lt; 12nd$, 则最终模型参数量可以近似为&lt;/p>
$$
\boxed{N \approx 12nd^2}
$$&lt;p>接下来，我们基于上面的配置简化 FLOPs 表达式&lt;/p>
$$
\begin{aligned}
C &amp;=
3nsd^2\left(8+\frac{4s}{d}+\frac{6d_{ff}}{d}+\frac{2|V|}{nd}\right) \\
&amp;= 3nsd^2\left(24+\frac{4s}{d}+\frac{2|V|}{nd}\right)\\
&amp;\approx 72nsd^2 \\
&amp;= 6sN
\end{aligned}
$$&lt;p>这里我们利用了前面的 $|V| &lt;&lt; 12nd$ 假设，为了简便我们还舍弃了 $4s/d$.&lt;/p>
&lt;p>注意到 $s$ 代表 token 序列长度，如果训练集的总 token 个数为 $D$, 则最终对于包含 $D$ tokens 的数据集和包含 $N$ 参数量的 LLM, 其训练总 FLOPs 可以近似估计为&lt;/p>
$$
\boxed{C\approx 6ND}
$$&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;h3 id="setting">&lt;a href="#setting" class="header-anchor">&lt;/a>Setting
&lt;/h3>&lt;p>接下来我们定量分析一些模型的 FLOPs. 我们基于 &lt;a class="link" href="https://maosong.website/p/chinchilla-scaling-law/" target="_blank" rel="noopener"
>Chinchilla scaling law&lt;/a> 给出的实验配置 (Table A9), 我们筛掉 &lt;code>kv_size * n_heads != d_model&lt;/code> 的配置，$|V|=32,000$.&lt;/p>
&lt;p>各部分的 FLOPs 计算代码如下&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">compute_flops&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">V&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d_ff&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">lm_head_flops&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">V&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">s&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">attention_flops&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">8&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">s&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">4&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">s&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">s&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">feed_forward_flops&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">4&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">s&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">d_ff&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">lm_head_flops&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">attention_flops&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">feed_forward_flops&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>首先我们看一下不同大小模型的 FLOPs 分布情况&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/llm-flops-computation/LLM-FLOPs-flops_distribution.png"
width="1200"
height="600"
loading="lazy"
alt="FLOPs distribution against model size"
class="gallery-image"
data-flex-grow="200"
data-flex-basis="480px"
>&lt;/p>
&lt;p>可以看到，当模型越来越大，FFN 层的算力占比越来越高，这也是为什么后来采用 MoE 架构的一个原因。&lt;/p>
&lt;p>接下来，我们看一下模型 FLOPs 分布情况随上下文长度变化的情况&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/llm-flops-computation/LLM-FLOPs-flops_vs_context.png"
width="1200"
height="600"
loading="lazy"
alt="FLOPs distribution aginst context length"
class="gallery-image"
data-flex-grow="200"
data-flex-basis="480px"
>&lt;/p>
&lt;p>可以看到，随 context length 增加，attention 的算力占比逐渐上升，这符合 attention 是一个平方复杂度的算法的结论。并且，当上下文足够长之后，计算还出现了 overflow (图像右端的突然下降)。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>在本文中，我们介绍了如何计算基于 decoder-only transformer LLM 的 FLOPs, 并推导除了 Kaplan scaling law 中使用的公式 $C=6ND$, 这为后面的 infra 和 scaling law 的学习提供了基础。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2203.15556" target="_blank" rel="noopener"
>Chinchilla Scaling law&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Embedding.html" target="_blank" rel="noopener"
>pytorch embedding layer&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.adamcasson.com/posts/transformer-flops" target="_blank" rel="noopener"
>transformer flops&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>