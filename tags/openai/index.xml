<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>OpenAI on Mao Song(毛松)'s Homepage</title><link>https://maosong.website/tags/openai/</link><description>Recent content in OpenAI on Mao Song(毛松)'s Homepage</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sat, 06 Dec 2025 18:21:34 +0800</lastBuildDate><atom:link href="https://maosong.website/tags/openai/index.xml" rel="self" type="application/rss+xml"/><item><title>Notes on gpt-oss</title><link>https://maosong.website/p/notes-on-gpt-oss/</link><pubDate>Tue, 19 Aug 2025 16:14:56 +0800</pubDate><guid>https://maosong.website/p/notes-on-gpt-oss/</guid><description>&lt;p>openAI 发布了 gpt-oss 大语言模型，包含 120B-A5.1B 以及 20.9B-A3.6B 两个 size, 作者强调了模型的 instruction following, tool use, 以及 adaptive thinking 能力&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;p>gpt-oss 系列是一个基于 MoE transformer 架构的 LLM. 架构中交替使用 sliding window attention 和 full attention, sliding window size 为 128 token, 架构图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gpt-oss/gpt-oss-architecture.png"
width="876"
height="789"
loading="lazy"
alt="gpt-oss-architecture"
class="gallery-image"
data-flex-grow="111"
data-flex-basis="266px"
>&lt;/p>
&lt;p>模型的配置如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>120B&lt;/th>
&lt;th>20B&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Date&lt;/td>
&lt;td>2025/8/5&lt;/td>
&lt;td>2025/8/5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Total Parameters&lt;/td>
&lt;td>116B&lt;/td>
&lt;td>20B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Activated Parameters&lt;/td>
&lt;td>5.13B&lt;/td>
&lt;td>3.61B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># MoE Layers&lt;/td>
&lt;td>36&lt;/td>
&lt;td>24&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Hidden Dim&lt;/td>
&lt;td>2880&lt;/td>
&lt;td>2880&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MoE Intermediate Dim&lt;/td>
&lt;td>2880&lt;/td>
&lt;td>2880&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Attention&lt;/td>
&lt;td>GQA+sliding window&lt;/td>
&lt;td>GQA+sliding window&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Attention Head Dim&lt;/td>
&lt;td>64&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Attention bias&lt;/td>
&lt;td>True&lt;/td>
&lt;td>True&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Attention Heads&lt;/td>
&lt;td>64&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Key-Value Heads&lt;/td>
&lt;td>8&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Experts (total)&lt;/td>
&lt;td>128&lt;/td>
&lt;td>32&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Experts Active Per Token&lt;/td>
&lt;td>4&lt;/td>
&lt;td>4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td># Shared Experts&lt;/td>
&lt;td>0&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>QK-Norm&lt;/td>
&lt;td>No&lt;/td>
&lt;td>No&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>在架构上，gpt-oss 做的主要改变有：&lt;/p>
&lt;ol>
&lt;li>Q, K, V projection layer, expert layer, routing layer 都使用了 bias&lt;/li>
&lt;li>修改了 expert layer 中 SwiGLU 的定义&lt;/li>
&lt;li>attention 中额外使用了一个 attention sink&lt;/li>
&lt;/ol>
&lt;h4 id="swiglu">&lt;a href="#swiglu" class="header-anchor">&lt;/a>SwiGLU
&lt;/h4>&lt;p>大多数模型使用的基于 SwiGLU 的 MLP 定义如下&lt;/p>
$$
y = W_2(W_3x \odot \mathrm{SwiGLU}(W_1x))
$$&lt;p>其中 $\mathrm{SwiGLU}(x)=x\odot\mathrm{sigmoid}(x)$, 在 gpt-oss 模型中，作者首先定义了两个常数 $\alpha=1.702$, $\mathrm{limit}=7.0$, 然后 SwiGLU MLP 的定义如下&lt;/p>
$$
\begin{aligned}
o_1&amp;=W_1x+b_1,\\
o_3&amp;=W_3x+b_3\\
o_1&amp;=\mathrm{clamp}(o_1,\max=\mathrm{limit})\\
o_3&amp;=\mathrm{clamp}(o_3,\min=-\mathrm{limit},\max=\mathrm{limit})\\
o_3&amp;= o_3\odot \mathrm{sigmoid}(\alpha\cdot o_3)\\
o_3&amp;= (o_1+1)\odot o_3\\
y &amp;= W_2o_3
\end{aligned}
$$&lt;h4 id="attention-sink">&lt;a href="#attention-sink" class="header-anchor">&lt;/a>Attention Sink
&lt;/h4>&lt;p>作者使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-streamingllm/" target="_blank" rel="noopener"
>attention sink&lt;/a> 来避免 window attention 在超过 kv cache size 之后，表现大幅度下降的问题。&lt;/p>
&lt;h3 id="quantization">&lt;a href="#quantization" class="header-anchor">&lt;/a>Quantization
&lt;/h3>&lt;p>为了降低模型的内存占用量，作者使用了 PTQ 来训练 MoE 的权重，使用的精度为 MXFP4, 这样每个参数由 4.25 bits 来表示。最终，模型的参数存储格式如下：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gpt-oss/gpt-oss-precision-format.png"
width="1152"
height="1235"
loading="lazy"
alt="precision format of gpt-oss"
class="gallery-image"
data-flex-grow="93"
data-flex-basis="223px"
>&lt;/p>
&lt;p>通过这个流程，gpt-oss-120B 可以部署在 80GB 内存的 GPU 上，gpt-oss-20B 可以部署在 16GB 内存的 GPU 上。模型各部分参数量如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Component&lt;/th>
&lt;th>120b&lt;/th>
&lt;th>20b&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>MLP&lt;/td>
&lt;td>114.71B&lt;/td>
&lt;td>19.12B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Attention&lt;/td>
&lt;td>0.96B&lt;/td>
&lt;td>0.64B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Embed + Unembed&lt;/td>
&lt;td>1.16B&lt;/td>
&lt;td>1.16B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Active Parameters&lt;/td>
&lt;td>5.13B&lt;/td>
&lt;td>3.61B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Total Parameters&lt;/td>
&lt;td>116.83B&lt;/td>
&lt;td>20.91B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Checkpoint Size&lt;/td>
&lt;td>60.8GiB&lt;/td>
&lt;td>12.8GiB&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>这里在计算激活参数的时候，没有没有考虑 embedding 的参数量。&lt;/p>
&lt;h3 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h3>&lt;p>预训练细节不多，主要是使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-flashattention/" target="_blank" rel="noopener"
>flash attention&lt;/a> 进行加速计算，使用了 Triton 进行了 kernel 的优化，gpt-oss-120b 训练了 120M H100-hours, gpt-oss-20B 的训练时间是 gpt-oss-120Bd 的十分之一左右&lt;/p>
&lt;h3 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h3>&lt;p>post-training 的数据包括 coding, math 以及 science 等，主要使用 RL 进行训练&lt;/p>
&lt;p>作者介绍了以下 post-training 使用的格式，即 &lt;code>harmony chat format&lt;/code>. 角色的优先级如下所示&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">System &amp;gt; Developer &amp;gt; User &amp;gt; Assistant &amp;gt; Tool
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>作者还加入了 channels 来限制可以使用的信息，比如使用 &lt;code>analysis&lt;/code> 来表示 CoT tokens, 使用 &lt;code>commentary&lt;/code> 来表示 function calling 等，一个具体的例子如下：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gpt-oss/gpt-oss-chat-template.png"
width="1351"
height="821"
loading="lazy"
alt="gpt-oss-chat-template"
class="gallery-image"
data-flex-grow="164"
data-flex-basis="394px"
>&lt;/p>
&lt;h2 id="evaluation">&lt;a href="#evaluation" class="header-anchor">&lt;/a>Evaluation
&lt;/h2>&lt;p>gpt-oss 系列的表现如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gpt-oss/gpt-oss-performance.png"
width="1179"
height="1106"
loading="lazy"
alt="performance of gpt-oss"
class="gallery-image"
data-flex-grow="106"
data-flex-basis="255px"
>&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者提出了 gpt-oss 系列大语言模型，gpt-oss 在架构上与已有的主流模型架构如 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a>, &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v3/" target="_blank" rel="noopener"
>DeepSeek-V3&lt;/a> 等都有一定区别&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://openai.com/index/introducing-gpt-oss/" target="_blank" rel="noopener"
>technical report&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>