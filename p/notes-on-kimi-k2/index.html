<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="Kimi-k2 是一个总参数为 1T, 激活参数为 32B 的 MoE 大语言模型，模型使用 15.5T token 进行训练，optimizer 使用了 MuonClip. 作者主要关注模型的 agent 能力"><title>Notes on Kimi-k2</title>
<link rel=canonical href=https://maosong2022.github.io/p/notes-on-kimi-k2/><link rel=stylesheet href=/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css><meta property='og:title' content="Notes on Kimi-k2"><meta property='og:description' content="Kimi-k2 是一个总参数为 1T, 激活参数为 32B 的 MoE 大语言模型，模型使用 15.5T token 进行训练，optimizer 使用了 MuonClip. 作者主要关注模型的 agent 能力"><meta property='og:url' content='https://maosong2022.github.io/p/notes-on-kimi-k2/'><meta property='og:site_name' content="Mao Song(毛松)'s Homepage"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='kimi'><meta property='article:tag' content='Reasoning'><meta property='article:published_time' content='2025-07-24T10:56:50+08:00'><meta property='article:modified_time' content='2025-07-24T10:56:50+08:00'><meta name=twitter:title content="Notes on Kimi-k2"><meta name=twitter:description content="Kimi-k2 是一个总参数为 1T, 激活参数为 32B 的 MoE 大语言模型，模型使用 15.5T token 进行训练，optimizer 使用了 MuonClip. 作者主要关注模型的 agent 能力"><link rel="shortcut icon" href=/favicon.png></head><script async src=https://scripts.simpleanalyticscdn.com/latest.js></script><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu9788782091159651930.jpg width=300 height=240 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>🍥</span></figure><div class=site-meta><h1 class=site-name><a href=/>Mao Song(毛松)'s Homepage</a></h1><h2 class=site-description>Never stop learning.</h2></div></header><ol class=menu-social><li><a href=https://b23.tv/a0Bb9Z1 target=_blank title=bilibili rel=me><svg role="img" viewBox="0 0 24 24"><title>Bilibili</title><path d="M17.813 4.653h.854c1.51.054 2.769.578 3.773 1.574 1.004.995 1.524 2.249 1.56 3.76v7.36c-.036 1.51-.556 2.769-1.56 3.773s-2.262 1.524-3.773 1.56H5.333c-1.51-.036-2.769-.556-3.773-1.56S.036 18.858.0 17.347v-7.36c.036-1.511.556-2.765 1.56-3.76 1.004-.996 2.262-1.52 3.773-1.574h.774l-1.174-1.12a1.234 1.234.0 01-.373-.906c0-.356.124-.658.373-.907l.027-.027c.267-.249.573-.373.92-.373s.653.124.92.373L9.653 4.44c.071.071.134.142.187.213h4.267a.836.836.0 01.16-.213l2.853-2.747c.267-.249.573-.373.92-.373s.662.151.929.4.391.551.391.907c0 .355-.124.657-.373.906zM5.333 7.24c-.746.018-1.373.276-1.88.773-.506.498-.769 1.13-.786 1.894v7.52c.017.764.28 1.395.786 1.893.507.498 1.134.756 1.88.773h13.334c.746-.017 1.373-.275 1.88-.773.506-.498.769-1.129.786-1.893v-7.52c-.017-.765-.28-1.396-.786-1.894-.507-.497-1.134-.755-1.88-.773zM8 11.107c.373.0.684.124.933.373.25.249.383.569.4.96v1.173c-.017.391-.15.711-.4.96-.249.25-.56.374-.933.374s-.684-.125-.933-.374c-.25-.249-.383-.569-.4-.96V12.44c0-.373.129-.689.386-.947.258-.257.574-.386.947-.386zm8 0c.373.0.684.124.933.373.25.249.383.569.4.96v1.173c-.017.391-.15.711-.4.96-.249.25-.56.374-.933.374s-.684-.125-.933-.374c-.25-.249-.383-.569-.4-.96V12.44c.017-.391.15-.711.4-.96.249-.249.56-.373.933-.373z"/></svg></a></li><li><a href=https://github.com/MaoSong2022 target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href='https://scholar.google.com/citations?user=BaqGkQQAAAAJ' target=_blank title="Google Scholar" rel=me><svg role="img" viewBox="0 0 24 24"><title>Google Scholar</title><path d="M5.242 13.769.0 9.5 12 0l12 9.5-5.242 4.269C17.548 11.249 14.978 9.5 12 9.5c-2.977.0-5.548 1.748-6.758 4.269zM12 10a7 7 0 100 14 7 7 0 000-14z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/about/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>About</span></a></li><li><a href=/tags/><svg class="icon icon-tabler icon-tabler-tag" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11 3l9 9a1.5 1.5.0 010 2l-6 6a1.5 1.5.0 01-2 0L3 11V7a4 4 0 014-4h4"/><circle cx="9" cy="9" r="2"/></svg>
<span>Tags</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#introduction>Introduction</a></li><li><a href=#pre-training>Pre-training</a><ol><li><a href=#architecture>Architecture</a></li><li><a href=#data>Data</a></li><li><a href=#muonclip-optimizer>MuonClip Optimizer</a></li><li><a href=#infra>Infra</a></li><li><a href=#training-recipe>Training Recipe</a></li></ol></li><li><a href=#post-training>Post-training</a><ol><li><a href=#sft>SFT</a></li><li><a href=#rl>RL</a></li><li><a href=#rl-infra>RL Infra</a></li></ol></li><li><a href=#evaluation>Evaluation</a></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#references>References</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/llm/>LLM</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/notes-on-kimi-k2/>Notes on Kimi-k2</a></h2><h3 class=article-subtitle>Kimi-k2 是一个总参数为 1T, 激活参数为 32B 的 MoE 大语言模型，模型使用 15.5T token 进行训练，optimizer 使用了 MuonClip. 作者主要关注模型的 agent 能力</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Jul 24, 2025</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>7 minute read</time></div></footer></div></header><section class=article-content><p>Kimi-k2 是一个总参数为 1T, 激活参数为 32B 的 MoE 大语言模型，模型使用 15.5T token 进行训练，optimizer 使用了 MuonClip. 作者主要关注模型的 agent 能力</p><h2 id=introduction>Introduction</h2><p>作者首先强调现在 LLM 发展主要是 Agentic Intelligence, 也就是让模型自主感知，规划，思考和与环境交互。</p><p>基于这个目标，作者就提出了 Kimi K2, 一个 1.04T 总参数，32B 激活参数的 MoE 模型，用于解决实现 agent Intelligence 中遇到的问题。作者主要进行了三点改进：</p><ol><li>MuonClip, 一个基于 <a class=link href=https://maosong.website/p/notes-on-moonlight/ target=_blank rel=noopener>Muon</a> 的优化算法，来提高 Kimi K2 对 token 的利用效率以及提高训练的稳定性</li><li>大规模的 agentic 数据合成 pipeline: 作者构建了一个用于合成工具调用，agent 数据的 pipeline</li><li>通用的 RL 框架，作者将 RLVR 和 self-critic rubric reward mechanism 结合起来，用于提升模型的表现</li></ol><h2 id=pre-training>Pre-training</h2><h3 id=architecture>Architecture</h3><p>Kimi-K2 的架构与 DeepSeek-V3 相似，配置如下表所示</p><div class=table-wrapper><table><thead><tr><th>指标</th><th>DeepSeek-V3</th><th>Kimi K2</th><th>$\Delta$</th></tr></thead><tbody><tr><td># Layers</td><td>61</td><td>61</td><td>=</td></tr><tr><td>Total Parameters</td><td>$671\text{B}$</td><td>$1.04\text{T}$</td><td>$\uparrow 54%$</td></tr><tr><td>Activated Parameters</td><td>$37\text{B}$</td><td>$32.6\text{B}$</td><td>$\downarrow 13%$</td></tr><tr><td>Experts (total)</td><td>256</td><td>384</td><td>$\uparrow 50%$</td></tr><tr><td>Experts Active per Token</td><td>8</td><td>8</td><td>=</td></tr><tr><td>Shared Experts</td><td>1</td><td>1</td><td>=</td></tr><tr><td>Attention Heads</td><td>128</td><td>64</td><td>$\downarrow 50%$</td></tr><tr><td>Number of Dense Layers</td><td>3</td><td>1</td><td>$\downarrow 67%$</td></tr><tr><td>Expert Grouping</td><td>Yes</td><td>No</td><td>-</td></tr></tbody></table></div><p>与 DeepSeek-V3 相比，模型主要进行了以下改动：</p><ol><li>作者认为提高专家的稀疏性，可以有效提高模型表现。因此作者将专家个数提升了 50%.</li><li>为了降低模型在 inference 阶段的算力开销，作者将 attention heads 的个数降低了 50%.</li></ol><p><strong>Sparsity Scaling Law</strong>
作者首先构建了基于 Muon 的 sparsity law, 这里 sparsity 定义为激活专家个数与总专家个数之比，即：</p>$$
\mathrm{sparsity} = \frac{\# \mathrm{activated\ experts}}{\# \mathrm{total\ experts}}
$$<p>作者在小规模上的实验结果如下图所示</p><p><img src=/p/notes-on-kimi-k2/Kimi-K2-sparsity-scaling-law.png width=673 height=678 srcset="/p/notes-on-kimi-k2/Kimi-K2-sparsity-scaling-law_hu12013290483963777320.png 480w, /p/notes-on-kimi-k2/Kimi-K2-sparsity-scaling-law_hu15866147703512200734.png 1024w" loading=lazy alt="Sparsity Scaling Law" class=gallery-image data-flex-grow=99 data-flex-basis=238px></p><blockquote><p><strong>Observation</strong>
实验结果表明，在相同算力（激活专家个数）下，模型的表现随 sparsity 提高而增加。</p></blockquote><p>但是，进一步提高 sparsity, 会让 infra 变的难以优化，因此在 Kimi-K2 里，将 sparsity 定义为 $48$.</p><p><strong>Number of attention heads</strong>
作者还分析了探究了 attention heads 的最优配置。DeepSeek-V3 将 attention heads 的个数设置为 layers 层数的 2 倍，来提高带宽利用率以及提高计算效率。但是，当上下文长度增加之后，attention head 变成了 computational bound. 作者对比了不同配置模型的表现，实验结果如下：</p><p><img src=/p/notes-on-kimi-k2/Kimi-K2-attention-heads.png width=671 height=675 srcset="/p/notes-on-kimi-k2/Kimi-K2-attention-heads_hu8958375214150279472.png 480w, /p/notes-on-kimi-k2/Kimi-K2-attention-heads_hu10448179446271388634.png 1024w" loading=lazy alt="Scaling curves for attention heads" class=gallery-image data-flex-grow=99 data-flex-basis=238px></p><blockquote><p><strong>Observation</strong>
实验结果表明，使用双倍 attention head 带来的收益是比较小的，只有 $0.5%$ 到 $1.2%$ 左右</p></blockquote><p>因此，作者在 Kimi-K2 中，奖 attention head 的个数设置为 $64$.</p><h3 id=data>Data</h3><p>Kimi-K2 主要强调了 token efficiency, 即每个 token 对模型表现提升的贡献。token efficiency 越高，说明每个 token 对最终模型的贡献也就越高</p><p>相比于 <a class=link href=https://maosong.website/p/notes-on-kimi-k1.5/ target=_blank rel=noopener>Kimi-k1.5</a>, Kimi-K2 使用了一个 rephrasing pipeline, 来提高高质量 token 的利用率，作者在 knowledge 和 mathematics domain 上进行了实验。</p><p>最终，Kimi-K2 的预训练数据包括了 <strong>15.5T</strong> token, 主要覆盖 Web text, code, mathmatics 以及 Knowledge 四个 domain. 大部分的数据处理与 Kimi-k1.5 相同。</p><p><strong>Knowledge Data Rephrasing</strong>
作者构建了一个 synthetic rephrasing 框架来提高 knowledge token 的利用率，框架主要包含以下几个模块：</p><ol><li>Style and perspective-diverse prompting: 作者构建了一系列 prompt, 来让 LLM 从多个角度和风格来重新表述原始文本</li><li>Chunk-wise auto-regressive generation: 为了保持模型在长文档中的 coherence 以及避免信息损失，作者使用了一个 chunk-based rewriting 策略，也就是对每个 chunk 分别进行改写，然后将它门汇总在一起</li><li>Fidelity verification: 作者对原始文本和改写文本进行了 fidelity 检查来保证语义的一致性。</li></ol><p>作者对比了以下三个设置对模型在 SimpleQA benchmark 上表现的影响，这三个设置分别是：</p><ol><li>原始数据集训练 10 epoch</li><li>改写数据一次，然后训练 10 epoch</li><li>改写数据一次，训练 1 epoch</li></ol><p>实验结果如下表所示</p><div class=table-wrapper><table><thead><tr><th># Rephrasings</th><th># Epochs SimpleQA</th><th>Accuracy</th></tr></thead><tbody><tr><td>0 (raw wiki-text)</td><td>10</td><td>23.76</td></tr><tr><td>1</td><td>10</td><td>27.39</td></tr><tr><td>10</td><td>1</td><td>28.94</td></tr></tbody></table></div><p>可以看到，改写的策略可以有效提高模型在 SimpleQA benchmark 上的表现</p><p><strong>Math Data Rephrasing</strong>
对于数学相关数据，作者基于 SwallowMath, 将数据改写成了学习笔记 (learning-note) 的风格。然后，作者还将其他语言的高质量文本翻译为英文。</p><blockquote><p><strong>Recall</strong>
个人认为，数据改写在一定程度上也算是合成数据，对于合成数据这一块，微软提出的 phi 系列是一个比较好的参考</p></blockquote><h3 id=muonclip-optimizer>MuonClip Optimizer</h3><p>Kimi-K2 使用了 Muon optimizer, 之前的实验结果说明了在相同的 compute budget 下，Muon optimizer 的表现超过了 AdamW. 也就是说，Muon optimizer 更加高效。</p><p>但是，Muon optimizer 的问题是训练不稳定。为了解决这个问题，作者提出了 QK-clip, 一个 weight clipping 机制，来显示约束 attention logits. QK-clip 的机制是<strong>当输出的 logits 超过某一个阈值之后，就对齐进行截断</strong>。</p><p>每个 head 的 attention 的计算公式如下</p>$$
O = \mathrm{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V
$$<p>其中, $d$ 是 hidden size, $Q, K,V$ 分别是 query, key, value, 定义如下：</p>$$
Q = XW_Q, K=XW_K, V=XW_V
$$<p>这里 $W_Q,W_K,W_V$ 是模型可学习的参数。</p><p>作者定义每个 head 的 max logit 如下：</p>$$
S_{\max}^h = \frac{1}{\sqrt{d}} \max_{X\in\mathcal{B}}\max_{i,j} [QK^T]_{ij}
$$<p>最简单的做法就是直接进行截断，也就是</p>$$
W_Q\gets \gamma^\alpha W_q, W_K\gets \gamma^{1-\alpha}W_K
$$<p>其中 $\gamma=\min(1, \tau S_{\max})$, 这里 $S_{\max}=\max_h S_{\max}^h$ 是所有 head 对应 $S_{\max}^h$ 的最大值。</p><p>但是，实际中，作者发现只有少部分 head 会出现 logits 爆炸现象。为了提高计算效率，作者针对每个 head 单独进行 scaling, 也就是 $\gamma_{h}=\min(1, \tau S_{\max}^h)$. 对于 MLA 架构，作者仅在 unshared 模块使用 clipping:</p><ul><li>$q^c$ 以及 $k^c$, scaling factor 为 $\sqrt{\gamma_h}$</li><li>$q^R$, scaling factor 为 $\gamma_h$</li></ul><p>最后，作者将 Muon, weight decay, RMS matching 和 QK-clip 汇总在一起，得到 Kimi-k2 使用的 MuonClip optimizer, 算法如下所示：</p><p><img src=/p/notes-on-kimi-k2/Kimi-k2-muonclip.png width=1385 height=668 srcset="/p/notes-on-kimi-k2/Kimi-k2-muonclip_hu15126210376126559992.png 480w, /p/notes-on-kimi-k2/Kimi-k2-muonclip_hu12041395682263694561.png 1024w" loading=lazy alt="MuonClip Optimizer" class=gallery-image data-flex-grow=207 data-flex-basis=497px></p><p>接下来，作者对比了以下 Muon 和 MuonClip 两个优化器的表现，作者分别使用这两个优化器训练 53B-A9B 的 MoE 模型，实验结果如下</p><p><img src=/p/notes-on-kimi-k2/Kimi-k2-muonclip-performance.png width=1365 height=551 srcset="/p/notes-on-kimi-k2/Kimi-k2-muonclip-performance_hu6393944337521741241.png 480w, /p/notes-on-kimi-k2/Kimi-k2-muonclip-performance_hu16210125810578172496.png 1024w" loading=lazy alt="Comparison between Muon and MuonClip" class=gallery-image data-flex-grow=247 data-flex-basis=594px></p><p>实验结果表明，Muon 优化器在 1000 步左右就出现了 logits 爆炸的现象，但是 MuonClip 通过优化可以避免这个问题。作者还发现，MuonClip 可以减少 loss spike 的产生，整体的 loss 变化情况如下图所示</p><p><img src=/p/notes-on-kimi-k2/Kimi-k2-model-training-dynamics.png width=977 height=561 srcset="/p/notes-on-kimi-k2/Kimi-k2-model-training-dynamics_hu4565157971618681387.png 480w, /p/notes-on-kimi-k2/Kimi-k2-model-training-dynamics_hu4579571635122099452.png 1024w" loading=lazy alt="Training loss curve" class=gallery-image data-flex-grow=174 data-flex-basis=417px></p><p>作者还在附录中说明 QK-clip 并不影响最终的收敛性，并且，为了尽可能减少对模型训练的干扰，作者发现，仅在训练初期 QK-clip 被激活:</p><ol><li>在初始的 70, 000 步里，有 12.7% 的 attention heads 至少触发了一次 QK-clip, 并且将它们的 $S_\max$ 降到了 100 以下</li><li>接下来的 70, 000 步里，QK-clip 就不再被激活</li></ol><h3 id=infra>Infra</h3><p>Kimi-k2 使用了 16-way 的 PP, 16-way 的 EP 以及 ZeRO-1 Data Parallelism. 具体流程如下</p><p><img src=/p/notes-on-kimi-k2/Kimi-k2-communication-computation.png width=1362 height=318 srcset="/p/notes-on-kimi-k2/Kimi-k2-communication-computation_hu9768363041919262943.png 480w, /p/notes-on-kimi-k2/Kimi-k2-communication-computation_hu17009654877161129053.png 1024w" loading=lazy alt="Kimi-K2 parallelism" class=gallery-image data-flex-grow=428 data-flex-basis=1027px></p><p>作者发现通过增加 warm-up mircro-batches, 我们可以有效重叠 EP 的 all-to-all communication 以及 computation. 但是，对于 [[DualPipe]], 其要求双倍的存储来保存参数和梯度，并且提高 PP 粒度会引入更多的 bubble, 因此 Kimi-K2 没有使用 DualPipe.</p><p>为了减少 [[1F1B]] 的 PP 通信开销，作者在反向传播的过程中同时进行 PP 通信，来进一步重合通信与计算。</p><p>作者还发现，使用更小的 EP group 可以提高整体的训练速度，因此作者将 EP 设置为 16.</p><p>作者发现，剩余的 GPU 内存不足以存放 MoE 的 activation, 因此作者采取了三个办法解决这个问题：</p><ol><li>Selective recomputation: 作者对 LayerNorm, SwiGLU, MLA 的 up-projection 和 MoE 的 down-projections 等进行重新计算。</li><li>FP8 storage for insentive activations: 作者使用了 FP8 精度来存储 MoE up-projections 以及 SwiGLU. 作者发现使用 FP8 进行计算可能会导致性能下降，因此作者并没有使用 FP8 进行计算。</li><li>Activation CPU offload: 作者将其余的 activation 放在 CPU RAM 上，在 1F1B 的过程中，作者再进行加载</li></ol><h3 id=training-recipe>Training Recipe</h3><p>模型上下文长度为 4096, 使用 MuonClip 进行优化，使用 WSD lr Scheduler.weight decay 为 0.1, global batch size 为 67M tokens.</p><p>预训练结束之后，作者加入了一个 long-context activation stage. 这个阶段，作者使用了 400B 的 4K 上下文的 token 和 60B 的 32K 上下文的 60B token. 最后作者使用 <a class=link href=https://maosong.website/p/notes-on-yarn/ target=_blank rel=noopener>YARN</a> 将模型上下文长度扩展到 128K.</p><h2 id=post-training>Post-training</h2><h3 id=sft>SFT</h3><p>数据的构建主要是基于：</p><ol><li>prompt 的多样性</li><li>Response 的质量</li></ol><p>作者构建了一系列的数据生成 pipeline, 然后使用 Kimi-k1.5 来生成多样化的回答，最后再进行质量评估和过滤。这里，作者主要介绍了一下 tool-use 数据的构建</p><p>受 ACEBench 启发，作者构建了一个模仿真实世界 tool-use 的数据合成 pipeline. pipeline 如下图所示</p><p><img src=/p/notes-on-kimi-k2/Kimi-k2-tool-use-synthetic-pipeline.png width=1366 height=439 srcset="/p/notes-on-kimi-k2/Kimi-k2-tool-use-synthetic-pipeline_hu8910712181245528206.png 480w, /p/notes-on-kimi-k2/Kimi-k2-tool-use-synthetic-pipeline_hu2723101591411375395.png 1024w" loading=lazy alt="Data synthesis pipeline for tool use" class=gallery-image data-flex-grow=311 data-flex-basis=746px></p><p>pipeline 主要包含三个阶段：</p><ul><li>tool spec generation: 作者基于 MCP tools 和 self-evolved 的数据合成策略来构建 tool repository. MCP tools 包括 3000+ 的 tools,合成了 20，000 个 tools</li><li>Agent and task generation: 作者还用不同的 system prompt 以及不同的 tools combination 来构建对应的 agent. 然后对不同的 agent, 作者构建了对应的成功标准均，工具调用模式以及评估方式</li><li>Trajectory generation: 作者首先构建不同风格的 LLM, 然后作者使用一个 simulator 来执行 tool call 并给予反馈。</li></ul><p>最后，作者对数据进行了过滤。</p><p>作者还加入了 coding 以及软件工程等任务相关数据来进一步提高模型的 agent 能力</p><h3 id=rl>RL</h3><p>RL 与 Kimi-K1.5 差不多。作者进一步提高了 RL 阶段的算力并做出了亮点改进：</p><ol><li>作者构建了类似 Gym 的框架，用于扩展 RL 的能力</li><li>作者加入了更多 RLVR 的任务</li></ol><p><strong>Data</strong>
数据主要包括以下几类：</p><ol><li>Math, STEM and logical tasks: 数据构建的原则为多样化和中等难度</li><li>Instruction following: 作者基于 hybrid rule verification 和 multi-source instruction generation 来合成复杂的指令跟随数据</li><li>Faithfulness: 作者训练了一个 judge model 来提供 reward</li><li>Coding & Software Engineering: 作者从开源数据收集并合成了代码相关数据</li><li>Safety. 提高模型的安全性，防止 jailbreak</li></ol><p><strong>Reward</strong>
作者使用了 self-critique rubric reward 的奖励机制。</p><p>首先，对于 Kimi-k2 的回答，作者会使用另一个 kimi-k2 作为 critic，基于三个方面进行排序：</p><ul><li>core rubric: AI 的核心价值观</li><li>prescriptive rubric: 避免 reward hacking</li><li>human-annotated rubric: 特定的上下文</li></ul><p>在训练的过程中，critic 也会基于 verifiable signals 进行 refine</p><p><strong>RL training</strong>
RL 的训练目标与 Kimi-k1.5 相同</p>$$
\mathcal{L}(\pi_\theta) = \mathbb{E}_{x\sim \mathcal{D}}\left[\frac1K\sum_{i=1}^K \left(r(x,y_i)-\bar{r}(x) -\tau\log\frac{\pi_{\mathrm{\theta}}(y_i\mid x)}{\pi_{\mathrm{old}}(y_i\mid x)}\right)^2\right]
$$<p>其中 $\bar{r}(x)=1/K\sum_{i=1}^Kr(x,y_i)$ 是 sample response 的平均奖励。</p><p>作者做了以下几点改进来提高模型在不同 domain 上的表现：</p><ol><li>Budget control: 作者针对不同任务分别设置了最大 token 限制，模型超过这个限制会受到惩罚。猜测应该是 length penalty 或者类似 Qwen3 一样，直接中断思考过程输出最终答案</li><li>PTX loss: 作者使用了 PTX loss 来提高模型对于高质量数据的利用率以及降低模型的过拟合</li><li>Temperature Decay: 作者发现，训练后期保持较高的采样温度会影响模型的表现，因此作者设置了一个 schedule, 来逐步降低采样温度。</li></ol><h3 id=rl-infra>RL Infra</h3><p><img src=/p/notes-on-kimi-k2/Kimi-k2-RL-infra.png width=734 height=477 srcset="/p/notes-on-kimi-k2/Kimi-k2-RL-infra_hu7836952951472259143.png 480w, /p/notes-on-kimi-k2/Kimi-k2-RL-infra_hu929938601854115055.png 1024w" loading=lazy alt="Kimi-K2 RL infra" class=gallery-image data-flex-grow=153 data-flex-basis=369px></p><p>RL infra 与 Kimi-k1.5 类似。主要包含三个模块。其中 train engine 和 inference engine 两者互相切换，一个 engine 进行 training 的时候，另一个 engine 就进行 inference, 为了提高 engine 切换的效率，作者使用了 checkpoint engine 来传输更新后的参数。</p><h2 id=evaluation>Evaluation</h2><p>模型评估结果如下图所示</p><p><img src=/p/notes-on-kimi-k2/Kimi-k2-performance.png width=696 height=979 srcset="/p/notes-on-kimi-k2/Kimi-k2-performance_hu17285728062120188885.png 480w, /p/notes-on-kimi-k2/Kimi-k2-performance_hu8344921060842393810.png 1024w" loading=lazy alt="Performance of Kimi-k2" class=gallery-image data-flex-grow=71 data-flex-basis=170px></p><p>评估结果显示，模型在 coding 和通用任务上的表现仅次于 Claude 4, 大部分 benchmark 上都是第二名甚至是 sota.</p><h2 id=conclusion>Conclusion</h2><p>作者在本文中提出了 Kimi-K2, 一个总参数为 1B 的 MoE 大语言模型。作者在架构，数据和优化器上进行了创新。并且通过 post-training 显著提高了模型的 agent 以及 tool-use 能力</p><p>作者发现模型主要存在的问题有：</p><ol><li>reasoning 任务过难或者 tool 的定义不清晰的时候，模型会使用很多 token</li><li>有时候工具调用可能会降低模型的表现</li><li>模型在 agentic coding 任务上的能力需要进一步提升</li></ol><h2 id=references>References</h2><ul><li><a class=link href=https://github.com/MoonshotAI/Kimi-K2 target=_blank rel=noopener>Github</a></li></ul></section><footer class=article-footer><section class=article-tags><a href=/tags/kimi/>Kimi</a>
<a href=/tags/reasoning/>Reasoning</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".main-article");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/notes-on-glm-4.5/><div class=article-details><h2 class=article-title>Notes on GLM-4.5</h2></div></a></article><article><a href=/p/notes-on-moonlight/><div class=article-details><h2 class=article-title>Notes on Moonlight</h2></div></a></article><article><a href=/p/notes-on-muon-blog/><div class=article-details><h2 class=article-title>Notes on Muon blog</h2></div></a></article><article><a href=/p/notes-on-magistral/><div class=article-details><h2 class=article-title>Notes on Magistral</h2></div></a></article><article><a href=/p/notes-on-smollm3/><div class=article-details><h2 class=article-title>Notes on SmolLM3</h2></div></a></article></div></div></aside><script src=https://utteranc.es/client.js repo=MaoSong2022/MaoSong2022.github.io issue-term=pathname crossorigin=anonymous async></script><style>.utterances{max-width:unset}</style><script>let utterancesLoaded=!1;function setUtterancesTheme(e){let t=document.querySelector(".utterances iframe");t&&t.contentWindow.postMessage({type:"set-theme",theme:`github-${e}`},"https://utteranc.es")}addEventListener("message",e=>{if(e.origin!=="https://utteranc.es")return;utterancesLoaded=!0,setUtterancesTheme(document.documentElement.dataset.scheme)}),window.addEventListener("onColorSchemeChange",e=>{if(!utterancesLoaded)return;setUtterancesTheme(e.detail)})</script><footer class=site-footer><section class=copyright>&copy;
2020 -
2025 Mao Song(毛松)'s Homepage</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.31.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>