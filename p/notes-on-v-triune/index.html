<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="统一的RL训练框架，用于提升VLM的感知和推理能力"><title>Notes on V-Triune</title>
<link rel=canonical href=https://maosong2022.github.io/p/notes-on-v-triune/><link rel=stylesheet href=/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css><meta property='og:title' content="Notes on V-Triune"><meta property='og:description' content="统一的RL训练框架，用于提升VLM的感知和推理能力"><meta property='og:url' content='https://maosong2022.github.io/p/notes-on-v-triune/'><meta property='og:site_name' content="Mao Song(毛松)'s Homepage"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='Reasoning'><meta property='article:tag' content='Perception'><meta property='article:published_time' content='2025-07-17T09:37:36+08:00'><meta property='article:modified_time' content='2025-07-17T09:37:36+08:00'><meta name=twitter:title content="Notes on V-Triune"><meta name=twitter:description content="统一的RL训练框架，用于提升VLM的感知和推理能力"><link rel="shortcut icon" href=/favicon.png></head><script async src=https://scripts.simpleanalyticscdn.com/latest.js></script><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu9788782091159651930.jpg width=300 height=240 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>🍥</span></figure><div class=site-meta><h1 class=site-name><a href=/>Mao Song(毛松)'s Homepage</a></h1><h2 class=site-description>Never stop learning.</h2></div></header><ol class=menu-social><li><a href=https://b23.tv/a0Bb9Z1 target=_blank title=bilibili rel=me><svg role="img" viewBox="0 0 24 24"><title>Bilibili</title><path d="M17.813 4.653h.854c1.51.054 2.769.578 3.773 1.574 1.004.995 1.524 2.249 1.56 3.76v7.36c-.036 1.51-.556 2.769-1.56 3.773s-2.262 1.524-3.773 1.56H5.333c-1.51-.036-2.769-.556-3.773-1.56S.036 18.858.0 17.347v-7.36c.036-1.511.556-2.765 1.56-3.76 1.004-.996 2.262-1.52 3.773-1.574h.774l-1.174-1.12a1.234 1.234.0 01-.373-.906c0-.356.124-.658.373-.907l.027-.027c.267-.249.573-.373.92-.373s.653.124.92.373L9.653 4.44c.071.071.134.142.187.213h4.267a.836.836.0 01.16-.213l2.853-2.747c.267-.249.573-.373.92-.373s.662.151.929.4.391.551.391.907c0 .355-.124.657-.373.906zM5.333 7.24c-.746.018-1.373.276-1.88.773-.506.498-.769 1.13-.786 1.894v7.52c.017.764.28 1.395.786 1.893.507.498 1.134.756 1.88.773h13.334c.746-.017 1.373-.275 1.88-.773.506-.498.769-1.129.786-1.893v-7.52c-.017-.765-.28-1.396-.786-1.894-.507-.497-1.134-.755-1.88-.773zM8 11.107c.373.0.684.124.933.373.25.249.383.569.4.96v1.173c-.017.391-.15.711-.4.96-.249.25-.56.374-.933.374s-.684-.125-.933-.374c-.25-.249-.383-.569-.4-.96V12.44c0-.373.129-.689.386-.947.258-.257.574-.386.947-.386zm8 0c.373.0.684.124.933.373.25.249.383.569.4.96v1.173c-.017.391-.15.711-.4.96-.249.25-.56.374-.933.374s-.684-.125-.933-.374c-.25-.249-.383-.569-.4-.96V12.44c.017-.391.15-.711.4-.96.249-.249.56-.373.933-.373z"/></svg></a></li><li><a href=https://github.com/MaoSong2022 target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href='https://scholar.google.com/citations?user=BaqGkQQAAAAJ' target=_blank title="Google Scholar" rel=me><svg role="img" viewBox="0 0 24 24"><title>Google Scholar</title><path d="M5.242 13.769.0 9.5 12 0l12 9.5-5.242 4.269C17.548 11.249 14.978 9.5 12 9.5c-2.977.0-5.548 1.748-6.758 4.269zM12 10a7 7 0 100 14 7 7 0 000-14z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/about/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>About</span></a></li><li><a href=/tags/><svg class="icon icon-tabler icon-tabler-tag" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11 3l9 9a1.5 1.5.0 010 2l-6 6a1.5 1.5.0 01-2 0L3 11V7a4 4 0 014-4h4"/><circle cx="9" cy="9" r="2"/></svg>
<span>Tags</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#introduction>Introduction</a></li><li><a href=#preliminary>Preliminary</a></li><li><a href=#v-triune>V-Triune</a><ol><li><a href=#sample-level-data-formatting>Sample-Level Data Formatting</a></li><li><a href=#verifier-level-reward-computation>Verifier-Level Reward Computation</a></li><li><a href=#source-level-metric-monitoring>Source-Level Metric Monitoring</a></li><li><a href=#dynamic-iou-reward>Dynamic IoU Reward</a></li></ol></li><li><a href=#data>Data</a></li><li><a href=#training-recipe>Training Recipe</a><ol><li><a href=#disable-vit-training>Disable ViT Training</a></li><li><a href=#spurious-image-special-tokens>Spurious Image Special Tokens</a></li><li><a href=#cot-prompt-pool>CoT Prompt Pool</a></li><li><a href=#training-configuration>Training Configuration</a></li></ol></li><li><a href=#evaluation>Evaluation</a><ol><li><a href=#performance>Performance</a></li><li><a href=#analysis>Analysis</a></li><li><a href=#ablation-study>Ablation Study</a></li></ol></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#references>References</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/mllm/>MLLM</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/notes-on-v-triune/>Notes on V-Triune</a></h2><h3 class=article-subtitle>统一的RL训练框架，用于提升VLM的感知和推理能力</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Jul 17, 2025</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>5 minute read</time></div></footer></div></header><section class=article-content><p>V-Triune 探究了如何使用一套统一的 RL 训练框架，来同时提高模型的 perception 和 reasoning 能力。</p><h2 id=introduction>Introduction</h2><p>已有的 RL 训练方法主要提升模型在特定 domain 上的能力，比如 MM-Eureka, Vision-R1 等专注于提升 MLLM 的 reasoning 能力，而 R1-V, DeepPerception 等专注于提升模型的 perception 能力。</p><p>因此，本文的 motivation 就是，如何用一套统一的 RL 训练框架，来同时提高模型的 perception 和 reasoning 能力。</p><p>V-Triune 包含 3 个关键模块：</p><ul><li>Sample-Level data formatting: 即在 sample 的层面定义 reward 等信息</li><li>Verifier-Level Computation: 即分离 verifier 和 generator, 提高整体效率</li><li>Source-Level Metric Monitoring: 基于 data source 来监控模型的表现</li></ul><p>作者还提出了 Dynamic IoU reward, 来提高训练的稳定性。</p><p>基于 V-Triune 和 <a class=link href=https://maosong.website/p/notes-on-qwen2.5-vl/ target=_blank rel=noopener>Qwen2.5-VL</a>, 作者提出了 Orsta,一个在 reasoning 和 perception 任务上均有提升的多模态大模型。</p><p>论文的主要贡献为：</p><ol><li>提出了 V-Triune, 一个统一的 RL 训练框架，来同时提高模型的 reasoning 和 perception 能力</li><li>在 infra 上进行了改进，提高整体的训练效率和稳定性</li><li>基于 V-Triune, 构建了 Orsta, 相比于 Baseline, 模型的 perception 和 reasoning 能力均有了提升。</li></ol><h2 id=preliminary>Preliminary</h2><p>作者首先回顾了一下 GRPO 和 <a class=link href=https://maosong.website/p/notes-on-dapo/ target=_blank rel=noopener>DAPO</a> , 我们这里就不再赘述。</p><p>然后，作者介绍了一下 reward function, reward function 由两部分组成， 第一部分是 format reward, 第二部分是 accuracy reward,.</p><p>对于 format reward, 作者采取了和 OpenR1 相同的做法，也就是要求输出仅包含 1 个给定的 token, 这里特定的 token 为 $s_1=\texttt{<think>}$, $s_2=\texttt{</think>}$, $s_3=\texttt{<answer>}$, $s_4=\texttt{</answer>}$, reward 的定义如下：</p>$$
R_{\mathrm{format}}(o_q) = 0.25\sum_{i=1}^4 \mathbb{1}(\mathrm{count}(o_q,s_i)=1)
$$<p>这里 $\mathbb{1}(\cdot)$ 是示性函数。</p><p>对于 accuracy reward, 作者根据不同的任务分别进行处理。</p><p>对于 reasoning 任务，reward function 的定义如下</p>$$
R_{\mathrm{acc}}(\hat{a},a) = \begin{cases}
1, &\text{ if }\texttt{verify(parse($\hat{a}$), parse($a$))}\text{ is True}\\
0, &\text{ else}
\end{cases}
$$<p>这里 $\hat{a}$, $a$ 分别是 prediction 和 ground truth.</p><p>对于 perception 任务，reward function 与我们要处理的子任务相关。如果是 OCR 和 counting 任务，则我们采取和 reasoning 一样的 reward function. 如果我们要处理的是 grounding 和 detection 任务，则我们可以使用 IoU 以及 mAP 两种 reward 形式。</p><p>IoU reward 的定义如下：</p>$$
R_{\mathrm{acc}}(\hat{a},a) = \begin{cases}
\mathrm{IoU}(\hat{a}, a), &\text{ if }\mathrm{IoU}(\hat{a},a)\geq \epsilon\\
0, &\text{ else}
\end{cases}
$$<p>这里 $\epsilon>0$ 为超参数， IoU 定义如下</p>$$
\mathrm{IoU}(\hat{a},a) = \frac{\mathrm{Area}(\hat{a}\cap a)}{\mathrm{Area}(\hat{a}\cup a)}
$$<p>mAP 的定义如下</p>$$
\mathrm{AP}_c = \int_0^1 \max_{\tilde{r}\geq r}P_c(\tilde{r}) dr,\quad mAP=\frac1C\sum_{c=1}^C \mathrm{AP}_c
$$<p>最终的 reward 是 format reward 和 accuracy reward 的加权求和：</p>$$
R = \alpha_{\mathrm{acc}}R_{\mathrm{acc}} + \alpha_{\mathrm{format}}R_{\mathrm{format}}
$$<h2 id=v-triune>V-Triune</h2><p>V-Triune 框架如下图所示，其包含三个模块</p><p><img src=/p/notes-on-v-triune/V-Triune_framework.png width=1351 height=562 srcset="/p/notes-on-v-triune/V-Triune_framework_hu10953460521504204366.png 480w, /p/notes-on-v-triune/V-Triune_framework_hu16889825761822693623.png 1024w" loading=lazy alt="V-Triune System" class=gallery-image data-flex-grow=240 data-flex-basis=576px></p><h3 id=sample-level-data-formatting>Sample-Level Data Formatting</h3><p>核心思想就是不同的任务的 reward function 可能是不一样的，如果在 task 层面设计 reward function 的话，RL 训练就没有了 flexibility, 因此，在本文中，作者在 sample 层面设计 reward function. 具体做法就是，每个样本除了 QA 相关信息，还有 reward model 的相关参数，这样就可以更加灵活地调整不同 sample 的损失了。</p><h3 id=verifier-level-reward-computation>Verifier-Level Reward Computation</h3><p>核心思想就是构建多个不同的 Verifier, 来分别处理不同的 sample, 因为我们 reward model 也是在 sample 层面构建的，因此我们可以基于 sample 的 metadata 来动态分配对应的 verifier, 这样就提升了 veriier 的可扩展性。本文作者使用了 MathVerifier 和 DetectionVerifier 两种。</p><h3 id=source-level-metric-monitoring>Source-Level Metric Monitoring</h3><p>核心思想就是根据 sample 的 metadata 信息来监控不同的 metric, 这样可以防止训练不稳定。</p><h3 id=dynamic-iou-reward>Dynamic IoU Reward</h3><p>核心思想就是根据训练进程，动态调整 IoU 的阈值。</p><p>作者发现，如果将 IoU 阈值设置的比较低的话，模型很容易拿到比较高的 reward; 如果将 IoU 阈值设置的比较高的话，模型又很难拿到奖励。</p><p>因此，作者的解决方案就是，使用课程学习，随着训练的进行，逐步提高 IoU 的阈值。</p><h2 id=data>Data</h2><p>数据集列举如下：</p><ul><li>Math: mm_math, geometry3K, mmk12</li><li>Puzzle: PuzzleVQA, AlgoPuzzleQA, VisualPuzzles</li><li>Science: ScienceQA, SciVQA, ViRL39K (Broader STEM topics, (GradeSchool) Science)</li><li>Chart: ChartQAPro, ChartX, Table-VQA, ViRL39K (Tables/Diagrams/Charts)</li><li>Detection: V3Det, Object365</li><li>Grounding: D3</li><li>Counting: CLEVR</li><li>OCR: LLaVA-OV (OCR questions), EST-VQA</li></ul><p><strong>Rule-based filtering</strong>
对于 visual reasoning 数据，作者的过滤如下：</p><ul><li>多项选择题以及判断题，防止 reward hacking</li><li>答案包含特殊符号如 &ldquo;=&rdquo;, &ldquo;[&rdquo; 等的数据</li><li>答案字符数超过 20 个，防止答案太复杂</li></ul><p>对于 visual perception 数据，作者过滤流程如下：</p><ul><li>Detection: 超过 10 个 bounding box 的，或者 box 占 50% 以上面积的数据，保持 single BB 和 multi BB 的比例为 1:2</li><li>Grounding: box 占 50% 以上面积的数据, label 过于复杂的数据</li><li>Counting: 保持类别平衡，仅使用英文数据</li><li>OCR: 保留英文数据，对 labels 进行 verify</li></ul><p><strong>Difficulty-based filtering</strong>
主要是移除比较简单的问题。</p><p>经过这两个阶段的过滤之后，得到了 <strong>20.6K</strong> perception samples 以及 <strong>27.1K</strong> reasoning samples. 数据保存在 Parquet 格式</p><h2 id=training-recipe>Training Recipe</h2><h3 id=disable-vit-training>Disable ViT Training</h3><p>作者首先发现，全量微调会导致训练崩溃。作者分析原因发现，主要是 ViT 在反向传播过程中，存在 gradient amplify 的问题，第一层与最后一层的梯度的 norm 相差 5-10 倍。作者认为有两点原因：</p><ol><li>RL 会强制要求模型的不同模态之间进行对齐</li><li>对比学习训练得到的 VIT 可能使得其不适合 RL 的训练。</li></ol><p>基于这两点考虑，作者再进行训练的时候，冻结了 ViT 模块，结果如下图</p><p><img src=/p/notes-on-v-triune/V-Triune-frozen-ViT.png width=1383 height=705 srcset="/p/notes-on-v-triune/V-Triune-frozen-ViT_hu7988052896170322526.png 480w, /p/notes-on-v-triune/V-Triune-frozen-ViT_hu8856288206821860516.png 1024w" loading=lazy alt="Analysis of ViT training instability" class=gallery-image data-flex-grow=196 data-flex-basis=470px></p><h3 id=spurious-image-special-tokens>Spurious Image Special Tokens</h3><p>作者发现，模型的输出可能会包含一些 <code>&lt;image_pad></code> 等 token, 因此，作者对输出进行了过滤，然后再进行计算。</p><h3 id=cot-prompt-pool>CoT Prompt Pool</h3><p>作者还发现，多样化的 CoT prompt 会损害模型的表现。因此，作者构建了 20 个 prompt 模版，每次随机挑选其中一个加入到 instruction 中。</p><h3 id=training-configuration>Training Configuration</h3><p>模型基于 Qwen2.5-7B-instruct 和 Qwen2.5VL-32B 来进行开发</p><p>作者探究了 on-policy 和 off-policy 两种配置。roll-out batch size 设置为 1024, 使用 GRPO 进行训练，GRPO 的 group size 设置为 8</p><p>作者还设置 $\epsilon_{high}=0.28$ 以及 $\epsilon_{low}=0.2$ 来提高 token 的采样率。作者去除了 KL divergence loss, 并且在 token 层面上进行平均</p><h2 id=evaluation>Evaluation</h2><h3 id=performance>Performance</h3><p><img src=/p/notes-on-v-triune/V-Triune-performance.png width=1159 height=1051 srcset="/p/notes-on-v-triune/V-Triune-performance_hu6593213700954064222.png 480w, /p/notes-on-v-triune/V-Triune-performance_hu15819943646912053752.png 1024w" loading=lazy alt="Performance of Orsta on MEGA-Bench core" class=gallery-image data-flex-grow=110 data-flex-basis=264px></p><h3 id=analysis>Analysis</h3><p><strong>on-policy v.s. off-policy</strong>
作者首先对比了以下 on-policy RL 和 off-policy RL 两种训练方式的表现， 结果如下图所示</p><p><img src=/p/notes-on-v-triune/V-triune-on-policy.png width=1137 height=526 srcset="/p/notes-on-v-triune/V-triune-on-policy_hu17883304061716859651.png 480w, /p/notes-on-v-triune/V-triune-on-policy_hu14295860107519637628.png 1024w" loading=lazy alt="Ablation on on-policy v.s. off-policy" class=gallery-image data-flex-grow=216 data-flex-basis=518px></p><p>结果有两点发现：</p><ol><li>on-policy 的表现基本上都是比 off-policy 要好的。</li><li>小模型通过 RL 带来的表现提升更明显，大模型的表现则更慢</li></ol><p><strong>generalization</strong>
作者还评估了以下模型的泛化性，结果如下图所示</p><p><img src=/p/notes-on-v-triune/V-Triune-generalization.png width=422 height=673 srcset="/p/notes-on-v-triune/V-Triune-generalization_hu7830263504092978837.png 480w, /p/notes-on-v-triune/V-Triune-generalization_hu5722992839965848528.png 1024w" loading=lazy alt="Generalization of V-Triune" class=gallery-image data-flex-grow=62 data-flex-basis=150px></p><p>结果发现，RL 的训练更像是对齐，而不是泛化。也就是说，模型在 in-domain 的任务上表现较好，在 OOD 的任务上表现就一般。这与已有的结论是不一样的</p><blockquote><p>[!tip] Recall
<a class=link href=https://maosong.website/p/notes-on-magistral/ target=_blank rel=noopener>Magistral</a> 发现模型在 math domain 上进行训练，在 code domain 上的表现也会提升</p></blockquote><p><strong>Training Dynamics</strong>
作者还分析了不同任务不同指标随训练进行的变化情况，如下图所示</p><p><img src=/p/notes-on-v-triune/V-Triune-training-dynamics.png width=1150 height=900 srcset="/p/notes-on-v-triune/V-Triune-training-dynamics_hu15276947352141942304.png 480w, /p/notes-on-v-triune/V-Triune-training-dynamics_hu12555432383874143707.png 1024w" loading=lazy alt="V-Triune training dynamics" class=gallery-image data-flex-grow=127 data-flex-basis=306px></p><p>结果显示，reasoning 和 OCR 任务岁训练进行其输出长度和正确率都有所提升。但是 detection 相关任务则变化不是很明显。</p><h3 id=ablation-study>Ablation Study</h3><p>作者进行了三个消融实验：</p><ol><li>训练策略：冻结 ViT, 冻结 LLM, 两者都不冻结</li><li>任务分解策略：仅使用 reasoning 数据进行训练，仅使用 perception 数据进行训练，同时使用两种数据进行训练。</li><li>learning rate: 不同的学习率</li></ol><p>实验结果如下图所示</p><p><img src=/p/notes-on-v-triune/V-Triune-ablation-study.png width=1146 height=585 srcset="/p/notes-on-v-triune/V-Triune-ablation-study_hu17402617248809186274.png 480w, /p/notes-on-v-triune/V-Triune-ablation-study_hu10212683979436721237.png 1024w" loading=lazy alt="Ablation study" class=gallery-image data-flex-grow=195 data-flex-basis=470px></p><p>结果发现，</p><ol><li>仅训练 ViT 对表现没有任何帮助，只有训练 LLM 才会带来性能的提升。</li><li>同时使用两种数据进行训练的效果是最好的，其次是仅使用 reasoning 数据进行训练，最后是仅使用 perception 数据进行训练</li><li>较大的学习率会损害模型的表现。</li></ol><h2 id=conclusion>Conclusion</h2><p>本文中，作者提出了 V-Triune, 一个使用 RL 来同时提高 MLLM 的 reasoning 和 perception 能力的 RL 训练框架。</p><p>作者发现 RL 对于 VLM 来说更像是一种 Alignment 策略，而不是一种 Generalization 策略，也就是模型并没有引入新的能力，而是提升模型本身的 utility 以及 robustness.</p><p>作者认为本文有以下 Limitation:</p><ol><li>在 perception 任务上没有看到明显的 test-time scaling 现象。作者认为，探究使用 o3 类似的方法或许能提高模型的表现</li><li>探究 RL-zero 在 VLM 中的应用，也就是直接在 base model 上进行 RL 的训练。</li></ol><h2 id=references>References</h2><ul><li><a class=link href=http://arxiv.org/abs/2505.18129 target=_blank rel=noopener>arxiv</a></li></ul></section><footer class=article-footer><section class=article-tags><a href=/tags/reasoning/>Reasoning</a>
<a href=/tags/perception/>Perception</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".main-article");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/notes-on-keye-vl-1.5/><div class=article-details><h2 class=article-title>Notes on Keye-VL 1.5</h2></div></a></article><article><a href=/p/notes-on-internvl3.5/><div class=article-details><h2 class=article-title>Notes on InternVL3.5</h2></div></a></article><article><a href=/p/notes-on-keye-vl/><div class=article-details><h2 class=article-title>Notes on Keye-VL</h2></div></a></article><article><a href=/p/notes-on-glm-4.1v-thinking/><div class=article-details><h2 class=article-title>Notes on GLM-4.1V-Thinking</h2></div></a></article><article><a href=/p/notes-on-kimi-k1.5/><div class=article-details><h2 class=article-title>Notes on Kimi k1.5</h2></div></a></article></div></div></aside><script src=https://utteranc.es/client.js repo=MaoSong2022/MaoSong2022.github.io issue-term=pathname crossorigin=anonymous async></script><style>.utterances{max-width:unset}</style><script>let utterancesLoaded=!1;function setUtterancesTheme(e){let t=document.querySelector(".utterances iframe");t&&t.contentWindow.postMessage({type:"set-theme",theme:`github-${e}`},"https://utteranc.es")}addEventListener("message",e=>{if(e.origin!=="https://utteranc.es")return;utterancesLoaded=!0,setUtterancesTheme(document.documentElement.dataset.scheme)}),window.addEventListener("onColorSchemeChange",e=>{if(!utterancesLoaded)return;setUtterancesTheme(e.detail)})</script><footer class=site-footer><section class=copyright>&copy;
2020 -
2025 Mao Song(毛松)'s Homepage</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.31.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>