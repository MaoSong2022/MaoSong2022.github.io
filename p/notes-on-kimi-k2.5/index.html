<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><link rel=dns-prefetch href=https://fonts.googleapis.com><link rel=dns-prefetch href=https://cdn.jsdelivr.net><link rel=dns-prefetch href=https://scripts.simpleanalyticscdn.com><link rel=preconnect href=https://fonts.googleapis.com crossorigin><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><link rel=preconnect href=https://scripts.simpleanalyticscdn.com crossorigin><meta http-equiv=x-dns-prefetch-control content="on"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=5,minimum-scale=1"><meta name=format-detection content="telephone=no"><meta name=theme-color content="#4e54c8" media="(prefers-color-scheme: light)"><meta name=theme-color content="#1a1a2e" media="(prefers-color-scheme: dark)"><link rel=apple-touch-icon href><meta name=description content="Kimi 在 2026 年 2 月发布了 Kimi K2.5, 一个 multimodal agentic model, Kimi K2.5 基于 Kimi K2 开发，在预训练阶段使用了图文联合训练的方式，在 post-training 阶段使用了 zero-vision SFT 和 multimodal RL 来提高模型的 reasoning 能力以及泛化能力，Kimi K2.5 还提出了 Agent Swarm 来提高解决复杂任务的效率。"><meta name=keywords content><meta name=author content="Mao Song"><link rel=canonical href=https://maosong.website/p/notes-on-kimi-k2.5/><meta property="og:title" content="Notes on Kimi-k2.5"><meta property="og:description" content="Kimi 在 2026 年 2 月发布了 Kimi K2.5, 一个 multimodal agentic model, Kimi K2.5 基于 Kimi K2 开发，在预训练阶段使用了图文联合训练的方式，在 post-training 阶段使用了 zero-vision SFT 和 multimodal RL 来提高模型的 reasoning 能力以及泛化能力，Kimi K2.5 还提出了 Agent Swarm 来提高解决复杂任务的效率。"><meta property="og:type" content="article"><meta property="og:url" content="https://maosong.website/p/notes-on-kimi-k2.5/"><meta property="og:site_name" content="Mao Song(毛松)'s Homepage"><meta property="og:locale" content="en"><meta property="og:image" content="https://maosong.website/p/notes-on-kimi-k2.5/Kimi-k2.5-PARL.png"><meta property="og:image:alt" content="Notes on Kimi-k2.5"><meta property="article:published_time" content="2026-02-12T11:13:13+08:00"><meta property="article:modified_time" content="2026-02-14T10:00:05+08:00"><meta property="article:author" content="Mao Song"><meta property="article:tag" content="kimi"><meta property="article:tag" content="Reasoning"><meta name=robots content="index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1"><meta name=googlebot content="index, follow"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"Notes on Kimi-k2.5","description":"Kimi 在 2026 年 2 月发布了 Kimi K2.5, 一个 multimodal agentic model, Kimi K2.5 基于 Kimi K2 开发，在预训练阶段使用了图文联合训练的方式，在 post-training 阶段使用了 zero-vision SFT 和 multimodal RL 来提高模型的 reasoning 能力以及泛化能力，Kimi K2.5 还提出了 Agent Swarm 来提高解决复杂任务的效率。","author":{"@type":"Person","name":"Mao Song"},"datePublished":"2026-02-12T11:13:13\u002b08:00","dateModified":"2026-02-14T10:00:05\u002b08:00","image":"https:\/\/maosong.website\/p\/notes-on-kimi-k2.5\/Kimi-k2.5-PARL.png","publisher":{"@type":"Organization","name":"Mao Song(毛松)\u0027s Homepage","logo":{"@type":"ImageObject","url":"https:\/\/maosong.website\/"}},"mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/maosong.website\/p\/notes-on-kimi-k2.5\/"},"keywords":"kimi, Reasoning","articleSection":"post","inLanguage":"en"}</script><title>Notes on Kimi-k2.5</title>
<link rel=stylesheet href=/scss/style.min.fb2ef3860c8335f835ed6d55e46d9c435d7a37375d695eed32deb59ecfadc82b.css><script async src=https://scripts.simpleanalyticscdn.com/latest.js></script></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><a href=#main-content class=skip-to-main>Skip to main content</a><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu9788782091159651930.jpg width=300 height=240 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>Mao Song(毛松)'s Homepage</a></h1><h2 class=site-description>Delving into the Latent Unknown.</h2></div></header><ol class=menu-social><li><a href=https://b23.tv/a0Bb9Z1 target=_blank title=bilibili rel=me><svg role="img" viewBox="0 0 24 24"><title>Bilibili</title><path d="M17.813 4.653h.854c1.51.054 2.769.578 3.773 1.574 1.004.995 1.524 2.249 1.56 3.76v7.36c-.036 1.51-.556 2.769-1.56 3.773s-2.262 1.524-3.773 1.56H5.333c-1.51-.036-2.769-.556-3.773-1.56S.036 18.858.0 17.347v-7.36c.036-1.511.556-2.765 1.56-3.76 1.004-.996 2.262-1.52 3.773-1.574h.774l-1.174-1.12a1.234 1.234.0 01-.373-.906c0-.356.124-.658.373-.907l.027-.027c.267-.249.573-.373.92-.373s.653.124.92.373L9.653 4.44c.071.071.134.142.187.213h4.267a.836.836.0 01.16-.213l2.853-2.747c.267-.249.573-.373.92-.373s.662.151.929.4.391.551.391.907c0 .355-.124.657-.373.906zM5.333 7.24c-.746.018-1.373.276-1.88.773-.506.498-.769 1.13-.786 1.894v7.52c.017.764.28 1.395.786 1.893.507.498 1.134.756 1.88.773h13.334c.746-.017 1.373-.275 1.88-.773.506-.498.769-1.129.786-1.893v-7.52c-.017-.765-.28-1.396-.786-1.894-.507-.497-1.134-.755-1.88-.773zM8 11.107c.373.0.684.124.933.373.25.249.383.569.4.96v1.173c-.017.391-.15.711-.4.96-.249.25-.56.374-.933.374s-.684-.125-.933-.374c-.25-.249-.383-.569-.4-.96V12.44c0-.373.129-.689.386-.947.258-.257.574-.386.947-.386zm8 0c.373.0.684.124.933.373.25.249.383.569.4.96v1.173c-.017.391-.15.711-.4.96-.249.25-.56.374-.933.374s-.684-.125-.933-.374c-.25-.249-.383-.569-.4-.96V12.44c.017-.391.15-.711.4-.96.249-.249.56-.373.933-.373z"/></svg></a></li><li><a href=https://github.com/MaoSong2022 target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href='https://scholar.google.com/citations?user=BaqGkQQAAAAJ' target=_blank title="Google Scholar" rel=me><svg role="img" viewBox="0 0 24 24"><title>Google Scholar</title><path d="M5.242 13.769.0 9.5 12 0l12 9.5-5.242 4.269C17.548 11.249 14.978 9.5 12 9.5c-2.977.0-5.548 1.748-6.758 4.269zM12 10a7 7 0 100 14 7 7 0 000-14z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/tags/><svg class="icon icon-tabler icon-tabler-tag" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11 3l9 9a1.5 1.5.0 010 2l-6 6a1.5 1.5.0 01-2 0L3 11V7a4 4 0 014-4h4"/><circle cx="9" cy="9" r="2"/></svg>
<span>Tags</span></a></li><li><a href=/about/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>About</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#introduction>Introduction</a></li><li><a href=#method>Method</a><ol><li><a href=#architecture>Architecture</a></li><li><a href=#data>Data</a></li><li><a href=#pre-training>Pre-training</a></li><li><a href=#post-training>Post-training</a></li><li><a href=#agent-swarm>Agent Swarm</a></li><li><a href=#infra>Infra</a></li></ol></li><li><a href=#experiments>Experiments</a></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#references>References</a></li></ol></nav></div></section></aside><main id=main-content class="main full-width" role=main aria-label="Main content"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/llm/>LLM
</a><a href=/categories/agent/>Agent
</a><a href=/categories/mllm/>MLLM</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/notes-on-kimi-k2.5/>Notes on Kimi-k2.5</a></h2><h3 class=article-subtitle>Kimi 在 2026 年 2 月发布了 Kimi K2.5, 一个 multimodal agentic model, Kimi K2.5 基于 Kimi K2 开发，在预训练阶段使用了图文联合训练的方式，在 post-training 阶段使用了 zero-vision SFT 和 multimodal RL 来提高模型的 reasoning 能力以及泛化能力，Kimi K2.5 还提出了 Agent Swarm 来提高解决复杂任务的效率。</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>February 12, 2026</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>7 minute read</time></div></footer></div></header><section class=article-content><h2 id=introduction><a href=#introduction class=header-anchor></a>Introduction</h2><p>Kimi-K2.5 的核心有亮点：</p><ol><li>native multi-modal: 通过在预训练，SFT, RL 阶段使用多模态数据来提高模型的多模态能力</li><li>agent: 通过并行 multi-agent 的方式来提高模型解决复杂问题的效率和能力</li></ol><h2 id=method><a href=#method class=header-anchor></a>Method</h2><h3 id=architecture><a href=#architecture class=header-anchor></a>Architecture</h3><p>Kimi K2.5 是一个标准的 ViT-MLP-LLM 架构，其中</p><ol><li>ViT, 基于 <a class=link href=Kimi-VL.md>Kimi-VL</a> 提出的 MoonViT, 并进行了改进, 参数量为 400M</li><li>MLP, 基于 patch merger,</li><li>LLM, 基于 <a class=link href=https://maosong.website/p/notes-on-kimi-k2/ target=_blank rel=noopener>Kimi-k2</a>, 参数量为 1.02T-A32B</li></ol><p><strong>ViT</strong>
作者使用了 <a class=link href=Kimi-VL.md>Kimi-VL</a> 提出的 MoonViT, MoonViT 基于 <a class=link href=SigLIP.md>SigLIP</a> 提出的 SigLIP-SO-400M 开发得到，MoonViT 使用了 <a class=link href=NaViT.md>NaViT</a> 来避免切分图片和使用不同精度图片进行训练。</p><p>在 MoonViT 的基础上，Kimi-K2.5 还进一步提出了 MoonViT-3D, 将 <a class=link href=NaViT.md>NaViT</a> 的思想扩展到了 3D 用于提高模型的视频理解能力，具体做法为将连续 4 帧的视频展开为 1D sequence, 这样在图像上的注意力机制就可以无缝衔接到视频上了。并且，通过这种方式，我们可以让模型关注跨帧的信息（注意力在 4 帧的 token 之间进行），简化代码如下所示</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># config.json temporal_merge_kernel_size</span>
</span></span><span class=line><span class=cl><span class=c1># kimi_k25_vision_processing.py split_video_chunks</span>
</span></span><span class=line><span class=cl><span class=n>video_chunk</span> <span class=o>=</span> <span class=n>frames</span><span class=p>[</span><span class=mi>0</span><span class=p>:</span><span class=mi>4</span><span class=p>]</span> 
</span></span><span class=line><span class=cl><span class=n>patches</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>frame</span> <span class=ow>in</span> <span class=n>video_chunk</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>patches</span><span class=o>.</span><span class=n>extend</span><span class=p>(</span><span class=n>split_into_patches</span><span class=p>(</span><span class=n>frame</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>tokens</span> <span class=o>=</span> <span class=n>patches</span>
</span></span><span class=line><span class=cl><span class=c1># modeling_kimi_k25.py Learnable2DInterpPosEmbDivided_fixed</span>
</span></span><span class=line><span class=cl><span class=n>positions</span> <span class=o>=</span> <span class=n>spatial_embedding</span> <span class=o>+</span> <span class=n>temporal_embedding</span>
</span></span><span class=line><span class=cl><span class=c1># modeling_kimi_k25.py MoonViT3dEncoder</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>transformer</span><span class=p>(</span><span class=n>tokens</span> <span class=o>+</span> <span class=n>positions</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>最后，在进入 MLP 之前，作者还对每个 temporal chunk 内的特征进行 pooling 操作，将时序长度压缩到了原来的 1/4, 进而提高模型可处理的视频长度。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># modeling_kimi_k25.py tpool_patch_merger</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>tpool_patch_merger</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>grid_thws</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>merge_kernel_size</span><span class=p>:</span> <span class=nb>tuple</span><span class=p>[</span><span class=nb>int</span><span class=p>,</span> <span class=nb>int</span><span class=p>]</span> <span class=o>=</span> <span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>),</span>
</span></span><span class=line><span class=cl><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>list</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>    <span class=n>d_model</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>outputs</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=n>pre_sum</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>t</span><span class=p>,</span> <span class=n>h</span><span class=p>,</span> <span class=n>w</span> <span class=ow>in</span> <span class=n>grid_thws</span><span class=o>.</span><span class=n>tolist</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=c1># Get the current sequence</span>
</span></span><span class=line><span class=cl>        <span class=n>seq</span> <span class=o>=</span> <span class=n>x</span><span class=p>[</span><span class=n>pre_sum</span><span class=p>:</span><span class=n>pre_sum</span> <span class=o>+</span> <span class=n>t</span> <span class=o>*</span> <span class=n>h</span> <span class=o>*</span> <span class=n>w</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=c1># Reshape along self.merge_kernel_size and concat to the last dimension</span>
</span></span><span class=line><span class=cl>        <span class=n>kernel_height</span><span class=p>,</span> <span class=n>kernel_width</span> <span class=o>=</span> <span class=n>merge_kernel_size</span>
</span></span><span class=line><span class=cl>        <span class=n>new_height</span><span class=p>,</span> <span class=n>new_width</span> <span class=o>=</span> <span class=n>h</span> <span class=o>//</span> <span class=n>kernel_height</span><span class=p>,</span> <span class=n>w</span> <span class=o>//</span> <span class=n>kernel_width</span>
</span></span><span class=line><span class=cl>        <span class=n>reshaped_seq</span> <span class=o>=</span> <span class=n>seq</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>t</span><span class=p>,</span> <span class=n>new_height</span><span class=p>,</span> <span class=n>kernel_height</span><span class=p>,</span> <span class=n>new_width</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                <span class=n>kernel_width</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>reshaped_seq</span> <span class=o>=</span> <span class=n>reshaped_seq</span><span class=o>.</span><span class=n>permute</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                            <span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>5</span><span class=p>)</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                                                <span class=n>dim</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>  <span class=c1># temporal pooling</span>
</span></span><span class=line><span class=cl>        <span class=n>padded_seq</span> <span class=o>=</span> <span class=n>reshaped_seq</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>new_height</span> <span class=o>*</span> <span class=n>new_width</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                       <span class=n>kernel_height</span> <span class=o>*</span> <span class=n>kernel_width</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>outputs</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>padded_seq</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>pre_sum</span> <span class=o>+=</span> <span class=n>t</span> <span class=o>*</span> <span class=n>h</span> <span class=o>*</span> <span class=n>w</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>outputs</span>
</span></span></code></pre></td></tr></table></div></div><p><strong>MLP</strong>
MLP 使用了 PatchMerger, 用于减少视觉 token 个数，这个方案在之前的 Qwen-VL 系列里已经得到了应用。</p><p><strong>LLM</strong>
LLM 基于 <a class=link href=https://maosong.website/p/notes-on-kimi-k2/ target=_blank rel=noopener>Kimi-k2</a> 的 MoE 模型，总参数为 1T, 激活参数为 32B</p><h3 id=data><a href=#data class=header-anchor></a>Data</h3><h3 id=pre-training><a href=#pre-training class=header-anchor></a>Pre-training</h3><p>预训练阶段一共使用了 15T token, 分为了三个阶段：</p><ol><li>ViT-training: 单独训练 ViT, 实际用了 image caption, grounding, ocr, video 等数据进行训练，训练方式采用了类似 InternVL 的方式，即通过 cross entropy loss 来与一个清凉话的 LLM 进行对齐，这个阶段训练使用了 1T token, 然后作者使用了一个非常短的 stage 来更新 MLP 用于对齐 ViT 和 Kimi-K2</li><li>Joint pre-training: 训练所有参数，长下文长度为 4K, 使用了 15T token. 这里主要强调了提升代码数据的比例</li><li>Long context mid-training: 使用 <a class=link href=https://maosong.website/p/notes-on-yarn/ target=_blank rel=noopener>YARN</a> 来提高模型的上下文长度</li></ol><p>最终预训练阶段 recipe 如下所示</p><p><img src=/p/notes-on-kimi-k2.5/Kimi-k2.5-pre-training.png width=1430 height=415 loading=lazy alt="Pre-training recipe of Kimi-K2.5" class=gallery-image data-flex-grow=344 data-flex-basis=826px></p><p><strong>Native Multimodal pre-training</strong>
在 Joint pre-training stage, Kimi-K2.5 还采用了一个与 <a class=link href=InternVL3.md>InternVL3</a> 类似的策略，即在预训练一开始直接使用多模态数据进行预训练。</p><p>传统的多模态大模型往往基于一个比较成熟的 LLM backbone 来完成多模态大模型的训练，但是其问题在于成熟的 LLM 其表示空间会收敛到语言模态上，多模态信息的迁移能力比较差。<a class=link href=InternVL3.md>InternVL3</a> 虽然也是 native multimodal pre-training, 但是其仍然依赖于成熟的 LLM. Kimi K2.5 则是使用预训练阶段的 Kimi K2 作为 backbone 来避免表示空间的塌缩，在训练一开始即直接加入少量多模态数据来保持模型的多模态能力。</p><p>作者探究了预训练阶段不同的数据对比，试验结果如下图所示</p><div class=table-wrapper><table><thead><tr><th></th><th>Vision Injection Timing</th><th>Vision-Text Ratio</th><th>Vision Knowledge</th><th>Vision Reasoning</th><th>OCR</th><th>Text Knowledge</th><th>Text Reasoning</th><th>Code</th></tr></thead><tbody><tr><td>Early</td><td>0%</td><td>10%:90%</td><td>25.8</td><td>43.8</td><td>65.7</td><td>45.5</td><td>58.5</td><td>24.8</td></tr><tr><td>Mid</td><td>50%</td><td>20%:80%</td><td>25.0</td><td>40.7</td><td>64.1</td><td>43.9</td><td>58.6</td><td>24.0</td></tr><tr><td>Late</td><td>80%</td><td>50%:50%</td><td>24.2</td><td>39.0</td><td>61.5</td><td>43.1</td><td>57.8</td><td>24.0</td></tr></tbody></table></div><p>结果显示，在训练早期加入少部分的多模态数据可以有效提高模型的表现。</p><h3 id=post-training><a href=#post-training class=header-anchor></a>Post-training</h3><p>Post-training 分为了 SFT 和 RL, SFT 阶段作者使用了合成的高质量数据，主要提升模型的交互式推理能力以及工具调用能力。为了解决传统 VLM 工具调用能力比较差且扩展性差的问题，Kimi-k2.5 提出了 Zero-Vision SFT, 其核心思想模型在预训练阶段已经完成了多模态对齐，因此我们可以仅使用纯文本 SFT 数据来激活 VLM 的视觉 agent 能力，具体做法就是将所有图像操作通过 IPython 的代码进行代理操作，这样视觉工具的调用就编程了程序化的图像处理指令。</p><p><img src=/p/notes-on-kimi-k2.5/Kimi-k2.5-zero-vision-sft.png width=1340 height=796 loading=lazy alt="Performance of Vision RL on zero-vision SFT" class=gallery-image data-flex-grow=168 data-flex-basis=404px></p><p>在 RL 阶段，作者基于 <a class=link href=https://maosong.website/p/notes-on-kimi-k1.5/ target=_blank rel=noopener>Kimi-k1.5</a> 提出的策略优化算法加入了一个 token-level clipping 机制来减少 off-policy divergence, 目标函数如下所示</p>$$
\mathcal{L}(\theta)=\mathbb{E}_{x\sim\mathcal{D}}\left[\frac{1}{N}\sum_{j=1}^k\sum_{i=1}^{|y_i|}\mathrm{clip}\left(\log\frac{\pi_{\theta}(y_j^i\mid x, y_{j}^{0:i})}{\pi_{\mathrm{old}}(y_j^i\mid x, y_{j}^{0:i})},\alpha,\beta\right)(r(x, y_j) - \bar{r}(x)) - \tau\left(\log\frac{\pi_{\theta}(y_j^i\mid x, y_{j}^{0:i})}{\pi_{\mathrm{old}}(y_j^i\mid x, y_{j}^{0:i})}\right)^2\right]
$$<p>其中 $k$ 是针对每个回答 $x$ 的采样次数，$N=\sum_{j=1}^k|y_j|$ 是一个 batch 里总的 token 个数， $\alpha,\beta,\tau$ 为超参数，$\bar{r}(x)$ 是对 normalization 的估计，这里采用了 Kimi-K1.5 的 mean reward, 即 $\bar{r}(x)=1/k\sum_{j=1}^Kr(x,y_j)$. 这里的 clipping 机制与 PPO 不同的地方在于针对 log-ratio 进行 clipping, 而不依赖于 advantage 的计算。最终训练时使用了 <a class=link href=https://maosong.website/p/notes-on-moonlight/ target=_blank rel=noopener>Moonlight</a> 的 MuonClip 算法</p><p>对于 reward 的设计，Kimi-k2.5 也使用了基于规则和基于 reward model 的方式，前者针对答案可验证的任务，后者针对开放式的任务。</p><p>作者还构建了 length penalty 来提高模型的推理效率，作者发现 <a class=link href=https://maosong.website/p/notes-on-kimi-k1.5/ target=_blank rel=noopener>Kimi-k1.5</a> 和 <a class=link href=https://maosong.website/p/notes-on-kimi-k2/ target=_blank rel=noopener>Kimi-k2</a> 中的 length penalty 虽然可以生成更准确的 reasoning chain, 但是其很难泛化到更高的算力. 为了解决这个问题，作者提出了 <strong>Toggle</strong> 策略，即在 inference-time scaling 和 budget-constrained optimization 两种模式之间进行切换优化，对应的 reward 定义为</p>$$
\tilde{r}(x,y) = \begin{cases}
r(x,y)*\mathbb{I}\{1/k\sum_{j=1}^kr(x,y_i<\lambda \text{ or }|y_j|\leq \text{budget}(x)\},&\text{if }\lfloor t/m\rfloor\mod 2 == 0\ (\text{Phase }0)\\
r(x,y),&\text{otherwise } (\text{Phase }1)
\end{cases}
$$<p>其中 $\lambda, m$ 都是超参数。budget 基于正确回答的长度的 p 分位得到：</p>$$
\text{budget}(x) = \text{Percentile}(\{|y_j| \mid r(x,y_j)=1, j\in[k]\},\rho)
$$<p>两种模式每隔 $m$ 个 iteration 切换一次：</p><ul><li>phase 0: budget limited phase, 训练模型在给定 token budget 下解决问题，减少 reasoning chain 长度</li><li>phase 1: scaling phase, 训练模型使用更多的算力来解决更复杂的问题，提高模型的智能程度</li></ul><p>作者评估 Toogle 策略得到的结果如下所示</p><p><img src=/p/notes-on-kimi-k2.5/Kimi-k2.5-toggle.png width=1200 height=689 loading=lazy alt="Performance of toggle strategy" class=gallery-image data-flex-grow=174 data-flex-basis=417px></p><p>结果现实，使用 toggle 策略之后，模型的输出长度减少了 30% 左右，且模型的表现并没有明显下降。作者还发现，一些重复的 pattern 也随之降低，且 toggle 策略的泛化程度更高。</p><p>在 Zero-Vision SFT 的基础上，Kimi-k2.5 使用了 Joint multimodal RL 训练策略。现有的多模态 RL 存在的问题为：模型很容易忽略视觉输入而过度依赖于纯文本进行推理。为了解决这个问题，作者构建了需要视觉理解才能得到答案的任务来提高模型对于视觉信息的利用程度，这些任务覆盖三个 domain:</p><ol><li>visual grounding and counting: 定位和计数</li><li>chart and document understanding: 图表文档理解</li><li>vision-critical STEM problems: 需要图片来完成求解的数学物理问题</li></ol><p>作者在 visual RL 之后评估了模型的表现，发现模型在 MMLU-Pro, GPQA-Diamond 等任务上的表现都有了提升，作者认为 visual RL 可以在不损害模型纯文本能力的情况下提高模型跨模态的泛化性</p><h3 id=agent-swarm><a href=#agent-swarm class=header-anchor></a>Agent Swarm</h3><p>Kimi-k2.5 的另一个重大改进为使用并行机制来提高模型的 agent 能力。传统的 agent 往往序列执行 reasoning, tool-use, 这限制了模型处理复杂任务的能力，Kimi-k2.5 通过 Agent Swarm 和 Parallel Agent Reinforcement Learning (PARL) 来解决这个问题，其核心思想就是并行，框架图如下所示</p><p><img src=/p/notes-on-kimi-k2.5/Kimi-k2.5-agent-swarm.png width=1354 height=729 loading=lazy alt="Framework of Agent Swarm" class=gallery-image data-flex-grow=185 data-flex-basis=445px></p><p>agent swarm 架构包含了一个 orchestrator 和若干个 subagent, 为了解决 agent swarm 的 reward 比较难以设置的问题，PARL 构建了三个不同 level 的 reward</p>$$
r_{PARL}(x,y) = \lambda_1 r_{parallel} + \lambda_2r_{finish} + \lambda_3r_{perf}(x,y)
$$<p>其中 $r_{perf}$ 评估了 solution $y$ 的质量， $r_{parallel}$ 则是避免并行模式崩塌，从 multi-agent 崩塌为 single agent, $r_{finish}$ 则是评估模型的完成性。超参数 $\lambda_1,\lambda_2$ 随训练逐渐降为 0 来提高模型整体的表现。</p><p>作者还提出了使用 critical steps 来评估 parallel agent 的计算时间消耗，其计算公式如下</p>$$
CriticalSteps = \sum_{i=1}^T\left(S_{main}^{(t)}+\max_iS_{sub,i}^{(t)}\right)
$$<p>其中 $T$ 为一个 episode 的时间，$S_{main}^{(t)}$ 为 orchestrator 在第 $t$ 步的运行时间， $S_{sub,i}^{(t)}$ 为第 i 个 subagent 的运行时间。</p><p>为了提高模型的并行能力，作者构建了一批广度优先搜索和深度优先搜索的数据，通过这些数据的构建，我们可以提高 orchestrator 的并行调用能力。</p><p>最终，PARL 的表现如下所示</p><p><img src=/p/notes-on-kimi-k2.5/Kimi-k2.5-PARL.png width=1501 height=441 loading=lazy alt="Performance of PARL" class=gallery-image data-flex-grow=340 data-flex-basis=816px></p><h3 id=infra><a href=#infra class=header-anchor></a>Infra</h3><p>Kimi-k2.5 的 infra 基于 <a class=link href=https://maosong.website/p/notes-on-kimi-k2/ target=_blank rel=noopener>Kimi-k2</a>, 作者主要强调了 decouple encoder process (DEP) 这一改进。之前的工作将 vision encoder 和 text embedding 都做为 PP 的第一个 stage, 但是由于 vision encoder 对不同输入的处理时间不同，这个 stage 的算历和内存分配随输入变化比较大。为了解决这个问题，作者提出了 DEP, 包含三个 stage 来提高训练效率:</p><ol><li>balanced vision forward: 由于 vision encoder 比较小 (400M), 因此作者将 vision encoder 复制到所有 GPU 上，然后根据负载来将 visual data 分配到不同的 GPU 上进行处理，这个阶段不保存中间激活值，处理完毕之后所有的结果作为 PP Stage0 的输入</li><li>backbone training: 正常进行训练，与 LLM 的训练优化一致</li><li>vision recomputation & backward: 这个阶段，我们重新计算 vision encoder 的 forward pass, 然后再对 vision encoder 进行反向传播</li></ol><p>通过 DEP, Kimi-k2.5 的训练效率达到了 Kimi-k2 的 90%.</p><h2 id=experiments><a href=#experiments class=header-anchor></a>Experiments</h2><p>首先是 Kimi-k2.5 在 general & reasoning 类任务上的表现，可以看到，Kimi-k2.5 超过了 DeeoSeek-V3.2 的表现，</p><div class=table-wrapper><table><thead><tr><th>Benchmark</th><th>Kimi K2.5</th><th>Claude Opus 4.5</th><th>GPT-5.2 (xhigh)</th><th>Gemini 3 Pro</th><th>DeepSeek-V3.2</th></tr></thead><tbody><tr><td>HLE-Full</td><td>30.1</td><td>30.8</td><td>34.5</td><td>37.5</td><td>25.1†</td></tr><tr><td>HLE-Full w/ tools</td><td>50.2</td><td>43.2</td><td>45.5</td><td>45.8</td><td>40.8†</td></tr><tr><td>AIME 2025</td><td>96.1</td><td>92.8</td><td>100</td><td>95.0</td><td>93.1</td></tr><tr><td>HMMT 2025 (Feb)</td><td>95.4</td><td>92.9*</td><td>99.4</td><td>97.3*</td><td>92.5</td></tr><tr><td>IMO-AnswerBench</td><td>81.8</td><td>78.5*</td><td>86.3</td><td>83.1*</td><td>78.3</td></tr><tr><td>GPQA-Diamond</td><td>87.6</td><td>87.0</td><td>92.4</td><td>91.9</td><td>82.4</td></tr><tr><td>MMLU-Pro</td><td>87.1</td><td>89.3*</td><td>86.7*</td><td>90.1</td><td>85.0</td></tr><tr><td>SimpleQA Verified</td><td>36.9</td><td>44.1</td><td>38.9</td><td>72.1</td><td>27.5</td></tr><tr><td>AdvancedIF</td><td>75.6</td><td>63.1</td><td>81.1</td><td>74.7</td><td>58.8</td></tr><tr><td>LongBench v2</td><td>61.0</td><td>64.4*</td><td>54.5*</td><td>68.2*</td><td>59.8*</td></tr></tbody></table></div><p>接下来是模型在 coding 任务上的表现</p><div class=table-wrapper><table><thead><tr><th>Benchmark</th><th>Kimi K2.5</th><th>Claude Opus 4.5</th><th>GPT-5.2 (xhigh)</th><th>Gemini 3 Pro</th><th>DeepSeek-V3.2</th></tr></thead><tbody><tr><td>SWE-Bench Verified</td><td>76.8</td><td>80.9</td><td>80.0</td><td>76.2</td><td>73.1</td></tr><tr><td>SWE-Bench Pro (public)</td><td>50.7</td><td>55.4*</td><td>55.6</td><td>-</td><td>-</td></tr><tr><td>SWE-Bench Multilingual</td><td>73.0</td><td>77.5</td><td>72.0</td><td>65.0</td><td>70.2</td></tr><tr><td>Terminal Bench 2.0</td><td>50.8</td><td>59.3</td><td>54.0</td><td>54.2</td><td>46.4</td></tr><tr><td>PaperBench (CodeDev)</td><td>63.5</td><td>72.9*</td><td>63.7*</td><td>-</td><td>47.1</td></tr><tr><td>CyberGym</td><td>41.3</td><td>50.6</td><td>-</td><td>39.9*</td><td>17.3*</td></tr><tr><td>SciCode</td><td>48.7</td><td>49.5</td><td>52.1</td><td>56.1</td><td>38.9</td></tr><tr><td>OIBench (cpp)</td><td>57.4</td><td>54.6*</td><td>-</td><td>68.5*</td><td>54.7*</td></tr><tr><td>LiveCodeBench (v6)</td><td>85.0</td><td>82.2*</td><td>-</td><td>87.4*</td><td>83.3</td></tr></tbody></table></div><p>在 agent 任务上的表现</p><div class=table-wrapper><table><thead><tr><th>Benchmark</th><th>Kimi K2.5</th><th>Claude Opus 4.5</th><th>GPT-5.2 (xhigh)</th><th>Gemini 3 Pro</th><th>DeepSeek-V3.2</th></tr></thead><tbody><tr><td>BrowseComp</td><td>60.6</td><td>37.0</td><td>65.8</td><td>37.8</td><td>51.4</td></tr><tr><td>BrowseComp (w/ ctx manage)</td><td>74.9</td><td>57.8</td><td>-</td><td>59.2</td><td>67.6</td></tr><tr><td>BrowseComp (Agent Swarm)</td><td>78.4</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>WideSearch</td><td>72.7</td><td>76.2*</td><td>-</td><td>57.0</td><td>32.5*</td></tr><tr><td>WideSearch (Agent Swarm)</td><td>79.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>DeepSearchQA</td><td>77.1</td><td>76.1*</td><td>71.3*</td><td>63.2*</td><td>60.9*</td></tr><tr><td>FinSearchCompT2&amp;T3</td><td>67.8</td><td>66.2*</td><td>-</td><td>49.9</td><td>59.1*</td></tr><tr><td>Seal-0</td><td>57.4</td><td>47.7*</td><td>45.0</td><td>45.5*</td><td>49.5*</td></tr><tr><td>GDPVal-AA</td><td>41.0</td><td>45.0</td><td>48.0</td><td>35.0</td><td>34.0</td></tr><tr><td>OSWorld-Verified</td><td>63.3</td><td>66.3</td><td>8.6</td><td>20.7</td><td>-</td></tr><tr><td>WebArena</td><td>58.9</td><td>63.4</td><td>-</td><td>-</td><td>-</td></tr></tbody></table></div><p>多模态表现</p><div class=table-wrapper><table><thead><tr><th>Benchmark</th><th>Kimi K2.5</th><th>Claude Opus 4.5</th><th>GPT-5.2 (xhigh)</th><th>Gemini 3 Pro</th><th>Qwen3-VL-235B-A22B</th></tr></thead><tbody><tr><td><strong>Image</strong></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MMMU-Pro</td><td>78.5</td><td>74.0</td><td>79.5*</td><td>81.0</td><td>69.3</td></tr><tr><td>MMMU (val)</td><td>84.3</td><td>80.7</td><td>86.7*</td><td>87.5*</td><td>80.6</td></tr><tr><td>CharXiv (RQ)</td><td>77.5</td><td>67.2*</td><td>82.1</td><td>81.4</td><td>66.1</td></tr><tr><td>MathVision</td><td>84.2</td><td>77.1*</td><td>83.0</td><td>86.1*</td><td>74.6</td></tr><tr><td>MathVista (mini)</td><td>90.1</td><td>80.2*</td><td>82.8*</td><td>89.8*</td><td>85.8</td></tr><tr><td>SimpleVQA</td><td>71.2</td><td>69.7*</td><td>55.8*</td><td>69.7*</td><td>56.8*</td></tr><tr><td>WorldVQA</td><td>46.3</td><td>36.8</td><td>28.0</td><td>47.4</td><td>23.5</td></tr><tr><td>ZeroBench</td><td>9</td><td>3*</td><td>9*</td><td>8*</td><td>4*</td></tr><tr><td>ZeroBench w/ tools</td><td>11</td><td>9*</td><td>7*</td><td>12*</td><td>3*</td></tr><tr><td>BabyVision</td><td>36.5</td><td>14.2</td><td>34.4</td><td>49.7</td><td>22.2</td></tr><tr><td>BLINK</td><td>78.9</td><td>68.8*</td><td>-</td><td>78.7*</td><td>68.9</td></tr><tr><td>MMVP</td><td>87.0</td><td>80.0*</td><td>83.0*</td><td>90.0*</td><td>84.3</td></tr><tr><td>OmniDocBench 1.5</td><td>88.8</td><td>87.7*</td><td>85.7</td><td>88.5</td><td>82.0*</td></tr><tr><td>OCRBench</td><td>92.3</td><td>86.5*</td><td>80.7*</td><td>90.3*</td><td>87.5</td></tr><tr><td>InfoVQA (test)</td><td>92.6</td><td>76.9*</td><td>84*</td><td>57.2*</td><td>89.5</td></tr><tr><td><strong>Video</strong></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>VideoMMMU</td><td>86.6</td><td>84.4*</td><td>85.9</td><td>87.6</td><td>80.0</td></tr><tr><td>MMVU</td><td>80.4</td><td>77.3*</td><td>80.8*</td><td>77.5*</td><td>71.1</td></tr><tr><td>MotionBench</td><td>70.4</td><td>60.3</td><td>64.8</td><td>70.3</td><td>-</td></tr><tr><td>Video-MME</td><td>87.4</td><td>66.0*</td><td>86.0*</td><td>88.4*</td><td>79.0</td></tr><tr><td>LongVideoBench</td><td>79.8</td><td>67.2*</td><td>76.5*</td><td>77.7*</td><td>65.6*</td></tr><tr><td>LVBench</td><td>75.9</td><td>57.3</td><td>-</td><td>73.5*</td><td>63.6</td></tr></tbody></table></div><p>我们这里基于模型在不同类别任务上的排名来进行可视化，结果如下</p><p><img src=/p/notes-on-kimi-k2.5/Kimi-k2.5-rank-frequency.png width=1590 height=790 loading=lazy alt="Rank performance of difference models" class=gallery-image data-flex-grow=201 data-flex-basis=483px></p><p>从结果可以看出，Kimi-K2.5 的 agent 能力达到了 SOTA 级别，其多模态能力也比较强。</p><p>与 <a class=link href=https://maosong.website/p/notes-on-deepseek-v3.2/ target=_blank rel=noopener>DeepSeek-V3.2</a> 一样，作者也对比了不同模型的推理效率，结果如下图所示</p><p><img src=/p/notes-on-kimi-k2.5/Kimi-k2.5-reasoning-efficiency.png width=783 height=262 loading=lazy alt="Reasoning efficiency of Kimi K2.5" class=gallery-image data-flex-grow=298 data-flex-basis=717px></p><p>可以看到，相比与 Kimi-K2, Kimi-K2.5 通过在 RL 层面进行优化，降低了输出长度，但是相比与 DeepSeel-V3.2 和 Gemini3.0 Pro 之间还存在一定差距。</p><h2 id=conclusion><a href=#conclusion class=header-anchor></a>Conclusion</h2><p>在本文中，作者提出了 Kimi-k2.5， 一个多模态的 agent model, Kimi-k2.5 集成了 Kimi-k2 和 Kimi-VL 的能力，扩展了模型的 agent 能力。</p><h2 id=references><a href=#references class=header-anchor></a>References</h2><ul><li><a class=link href=https://huggingface.co/moonshotai/Kimi-K2.5 target=_blank rel=noopener>huggingface</a></li><li><a class=link href=https://arxiv.org/abs/2602.02276 target=_blank rel=noopener>arxiv</a></li></ul></section><footer class=article-footer><section class=article-tags><a href=/tags/kimi/>Kimi</a>
<a href=/tags/reasoning/>Reasoning</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section><section class=article-lastmod><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>Last updated on February 14, 2026 at 10:00 AM</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".main-article");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/notes-on-minimax-01/><div class=article-details><h2 class=article-title>Notes on MiniMax-01</h2></div></a></article><article><a href=/p/notes-on-kimi-k2/><div class=article-details><h2 class=article-title>Notes on Kimi-k2</h2></div></a></article><article class=has-image><a href=/p/notes-on-gemma3/><div class=article-image><img src=/p/notes-on-gemma3/cover.e45426ad0c10f7e180497d614b280d35.png width=1757 height=762 loading=lazy alt="Featured image of post Notes on Gemma3" data-hash="md5-5FQmrQwQ9+GASX1hSygNNQ=="></div><div class=article-details><h2 class=article-title>Notes on Gemma3</h2></div></a></article><article><a href=/p/notes-on-kimi-k1.5/><div class=article-details><h2 class=article-title>Notes on Kimi k1.5</h2></div></a></article><article><a href=/p/notes-on-deepseek-v3.2/><div class=article-details><h2 class=article-title>Notes on DeepSeek-V3.2</h2></div></a></article></div></div></aside><script src=https://utteranc.es/client.js repo=MaoSong2022/MaoSong2022.github.io issue-term=pathname crossorigin=anonymous async></script><style>.utterances{max-width:unset}</style><script>let utterancesLoaded=!1;function setUtterancesTheme(e){let t=document.querySelector(".utterances iframe");t&&t.contentWindow.postMessage({type:"set-theme",theme:`github-${e}`},"https://utteranc.es")}addEventListener("message",e=>{if(e.origin!=="https://utteranc.es")return;utterancesLoaded=!0,setUtterancesTheme(document.documentElement.dataset.scheme)}),window.addEventListener("onColorSchemeChange",e=>{if(!utterancesLoaded)return;setUtterancesTheme(e.detail)})</script><footer class=site-footer><section class=copyright>&copy;
2020 -
2026 Mao Song(毛松)'s Homepage</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.31.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>