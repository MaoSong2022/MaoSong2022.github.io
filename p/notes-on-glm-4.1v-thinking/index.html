<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><link rel=dns-prefetch href=https://fonts.googleapis.com><link rel=dns-prefetch href=https://cdn.jsdelivr.net><link rel=dns-prefetch href=https://scripts.simpleanalyticscdn.com><link rel=preconnect href=https://fonts.googleapis.com crossorigin><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><link rel=preconnect href=https://scripts.simpleanalyticscdn.com crossorigin><meta http-equiv=x-dns-prefetch-control content="on"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=5,minimum-scale=1"><meta name=format-detection content="telephone=no"><meta name=theme-color content="#4e54c8" media="(prefers-color-scheme: light)"><meta name=theme-color content="#1a1a2e" media="(prefers-color-scheme: dark)"><link rel=apple-touch-icon href><meta name=description content="智谱 AI 在 25 年 7 月份发布了 GLM-4.1V-Thinking, 一个 9B 的多模态大语言模型，其在多个 benchmark 上达到了相同大小 MLLM 的 SOTA"><meta name=keywords content><meta name=author content="Mao Song"><link rel=canonical href=https://maosong.website/p/notes-on-glm-4.1v-thinking/><meta property="og:title" content="Notes on GLM-4.1V-Thinking"><meta property="og:description" content="智谱 AI 在 25 年 7 月份发布了 GLM-4.1V-Thinking, 一个 9B 的多模态大语言模型，其在多个 benchmark 上达到了相同大小 MLLM 的 SOTA"><meta property="og:type" content="article"><meta property="og:url" content="https://maosong.website/p/notes-on-glm-4.1v-thinking/"><meta property="og:site_name" content="Mao Song(毛松)'s Homepage"><meta property="og:locale" content="en"><meta property="og:image" content="https://maosong.website/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_Thinking_architecture.png"><meta property="og:image:alt" content="Notes on GLM-4.1V-Thinking"><meta property="article:published_time" content="2025-07-14T10:32:04+08:00"><meta property="article:modified_time" content="2025-07-15T18:11:06+08:00"><meta property="article:author" content="Mao Song"><meta property="article:tag" content="Zhipu"><meta property="article:tag" content="Reasoning"><meta name=robots content="index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1"><meta name=googlebot content="index, follow"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"Notes on GLM-4.1V-Thinking","description":"智谱 AI 在 25 年 7 月份发布了 GLM-4.1V-Thinking, 一个 9B 的多模态大语言模型，其在多个 benchmark 上达到了相同大小 MLLM 的 SOTA","author":{"@type":"Person","name":"Mao Song"},"datePublished":"2025-07-14T10:32:04\u002b08:00","dateModified":"2025-07-15T18:11:06\u002b08:00","image":"https:\/\/maosong.website\/p\/notes-on-glm-4.1v-thinking\/GLM_4_1_V_Thinking_architecture.png","publisher":{"@type":"Organization","name":"Mao Song(毛松)\u0027s Homepage","logo":{"@type":"ImageObject","url":"https:\/\/maosong.website\/"}},"mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/maosong.website\/p\/notes-on-glm-4.1v-thinking\/"},"keywords":"Zhipu, Reasoning","articleSection":"post","inLanguage":"en"}</script><title>Notes on GLM-4.1V-Thinking</title>
<link rel=stylesheet href=/scss/style.min.fb2ef3860c8335f835ed6d55e46d9c435d7a37375d695eed32deb59ecfadc82b.css><script async src=https://scripts.simpleanalyticscdn.com/latest.js></script></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><a href=#main-content class=skip-to-main>Skip to main content</a><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu9788782091159651930.jpg width=300 height=240 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>Mao Song(毛松)'s Homepage</a></h1><h2 class=site-description>Delving into the Latent Unknown.</h2></div></header><ol class=menu-social><li><a href=https://b23.tv/a0Bb9Z1 target=_blank title=bilibili rel=me><svg role="img" viewBox="0 0 24 24"><title>Bilibili</title><path d="M17.813 4.653h.854c1.51.054 2.769.578 3.773 1.574 1.004.995 1.524 2.249 1.56 3.76v7.36c-.036 1.51-.556 2.769-1.56 3.773s-2.262 1.524-3.773 1.56H5.333c-1.51-.036-2.769-.556-3.773-1.56S.036 18.858.0 17.347v-7.36c.036-1.511.556-2.765 1.56-3.76 1.004-.996 2.262-1.52 3.773-1.574h.774l-1.174-1.12a1.234 1.234.0 01-.373-.906c0-.356.124-.658.373-.907l.027-.027c.267-.249.573-.373.92-.373s.653.124.92.373L9.653 4.44c.071.071.134.142.187.213h4.267a.836.836.0 01.16-.213l2.853-2.747c.267-.249.573-.373.92-.373s.662.151.929.4.391.551.391.907c0 .355-.124.657-.373.906zM5.333 7.24c-.746.018-1.373.276-1.88.773-.506.498-.769 1.13-.786 1.894v7.52c.017.764.28 1.395.786 1.893.507.498 1.134.756 1.88.773h13.334c.746-.017 1.373-.275 1.88-.773.506-.498.769-1.129.786-1.893v-7.52c-.017-.765-.28-1.396-.786-1.894-.507-.497-1.134-.755-1.88-.773zM8 11.107c.373.0.684.124.933.373.25.249.383.569.4.96v1.173c-.017.391-.15.711-.4.96-.249.25-.56.374-.933.374s-.684-.125-.933-.374c-.25-.249-.383-.569-.4-.96V12.44c0-.373.129-.689.386-.947.258-.257.574-.386.947-.386zm8 0c.373.0.684.124.933.373.25.249.383.569.4.96v1.173c-.017.391-.15.711-.4.96-.249.25-.56.374-.933.374s-.684-.125-.933-.374c-.25-.249-.383-.569-.4-.96V12.44c.017-.391.15-.711.4-.96.249-.249.56-.373.933-.373z"/></svg></a></li><li><a href=https://github.com/MaoSong2022 target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href='https://scholar.google.com/citations?user=BaqGkQQAAAAJ' target=_blank title="Google Scholar" rel=me><svg role="img" viewBox="0 0 24 24"><title>Google Scholar</title><path d="M5.242 13.769.0 9.5 12 0l12 9.5-5.242 4.269C17.548 11.249 14.978 9.5 12 9.5c-2.977.0-5.548 1.748-6.758 4.269zM12 10a7 7 0 100 14 7 7 0 000-14z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/tags/><svg class="icon icon-tabler icon-tabler-tag" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11 3l9 9a1.5 1.5.0 010 2l-6 6a1.5 1.5.0 01-2 0L3 11V7a4 4 0 014-4h4"/><circle cx="9" cy="9" r="2"/></svg>
<span>Tags</span></a></li><li><a href=/about/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>About</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#introduction>Introduction</a></li><li><a href=#method>Method</a><ol><li><a href=#architecture>Architecture</a></li><li><a href=#pre-training>Pre-training</a><ol><li><a href=#pre-training-data>Pre-training Data</a></li><li><a href=#pre-training-training>Pre-training Training</a></li></ol></li><li><a href=#sft>SFT</a><ol><li><a href=#sft-data>SFT Data</a></li><li><a href=#sft-training>SFT Training</a></li></ol></li><li><a href=#rl>RL</a></li></ol></li><li><a href=#evaluation>Evaluation</a><ol><li><a href=#performance>Performance</a></li><li><a href=#ablation-study>Ablation Study</a></li></ol></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#references>References</a></li><li><a href=#appendix>Appendix</a></li></ol></nav></div></section></aside><main id=main-content class="main full-width" role=main aria-label="Main content"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/mllm/>MLLM</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/notes-on-glm-4.1v-thinking/>Notes on GLM-4.1V-Thinking</a></h2><h3 class=article-subtitle>智谱 AI 在 25 年 7 月份发布了 GLM-4.1V-Thinking, 一个 9B 的多模态大语言模型，其在多个 benchmark 上达到了相同大小 MLLM 的 SOTA</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>July 14, 2025</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>8 minute read</time></div></footer></div></header><section class=article-content><p>智谱 AI 在 25 年 7 月份发布了 GLM-4.1V-Thinking, 一个 9B 的多模态大语言模型，其在多个 benchmark 上达到了相同大小 MLLM 的 SOTA</p><h2 id=introduction><a href=#introduction class=header-anchor></a>Introduction</h2><p>已有工作如 <a class=link href=https://maosong.website/p/notes-on-mimo-vl/ target=_blank rel=noopener>MiMo-VL</a> 和 V-Triune 主要集中在特定的领域，目前还没有一个在所有领域都能超过 non-reasoning model 表现的多模态 reasoning model.</p><p>基于这个目标，作者提出了 GLM-4.1V-Thinking, 一个 9B 的，用于通用领域多模态 reasoning 的 MLLM. 在预训练阶段，作者通过构建多样化的数据来提升模型的基础能力。在 post-training 阶段，作者构建了 domain-specific 的数据来让模型学会 reasoning. 最后，作者提出了 Reinforcement Learning with Curriculum Sampling (RLCS) 来扩展模型的 reasoning 能力。</p><p>作者主要发现如下：</p><ol><li>multi-domain RL 的泛化性非常强，在某一个 domain 上进行 RL 训练也可以提高模型在其他 domain 上的表现</li><li>动态选择 RL 训练的数据可以有效提高模型的表现和训练效率</li><li>一个 robust 以及 precise 的 reward model 对于 multi-domain RL 是至关重要的</li></ol><h2 id=method><a href=#method class=header-anchor></a>Method</h2><h3 id=architecture><a href=#architecture class=header-anchor></a>Architecture</h3><p>GLM-4.1V-Thinking 是一个标准的 ViT-MLP-LLM 架构，其中：</p><ul><li>ViT: ViT 使用的是 AIMv2-Huge</li><li>MLP: 是一个 3 层的 MLP，架构为 <code>linear-LayerNorm-GELU-SwiGLU</code>, 并且在进入 MLP 之前，GLM-4.1V-Thinking 还会在 spatial 上进行降采样，降采样率为 2</li><li>LLM: LLM 使用的是 GLM</li></ul><p><img src=/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_Thinking_architecture.png width=1804 height=988 loading=lazy alt="Architecture of GLM4.1V-Thinking" class=gallery-image data-flex-grow=182 data-flex-basis=438px></p><p>与 <a class=link href=https://maosong.website/p/notes-on-qwen2.5-vl/ target=_blank rel=noopener>Qwen2.5-VL</a> 一样，作者将 encoder 的 2D convolution 替换为了 3D convolution，用于处理视频输入，然后对于 single-image inputs，GLM-4.1V-Thinking 会将输入复制，变成一个含有两帧相同图片的 video 输入</p><p>为了支持不同分辨率图片输入，作者进行了两点改进：</p><p>第一是使用了 2D RoPE 来处理不同分辨率的图片输入</p><p>第二是使用 bicubic interpolation 来将不同分辨率的图片适应到 encoder 的 position embedding 上。具体来讲，对一个拥有 $H_p\times W_p$ patches 的图片，每个 patch 的坐标 $(w,h)$ 首先会被 normalized 到 $[-1,1]$ 中：</p>$$
g_{norm} = (w_{norm}, h_{norm}) = 2* \left(\frac{w+0.5}{W_p}, \frac{h+0.5}{H_p}\right) - 1
$$<p>然后，我们再使用 bicubic interpolation $\mathcal{I}_{bicubic}$ 将坐标映射到原始的 position embedding 上：</p>$$
P_{adapted} = \mathcal{I}_{bicubic}(P_{orig}, g_{norm})
$$<p>对于视频输入，作者在每一帧之后插入了一个 time index token, 用每一帧的 timestamp string 来表示。这个做法与 <a class=link href=https://maosong.website/p/notes-on-qwen2.5-vl/ target=_blank rel=noopener>Qwen2.5-VL</a> 一样。</p><h3 id=pre-training><a href=#pre-training class=header-anchor></a>Pre-training</h3><h4 id=pre-training-data><a href=#pre-training-data class=header-anchor></a>Pre-training Data</h4><p>Pre-training 数据包括 image-caption data, interleaved image-text data, OCR data, grounding data, video data 和 instruction tuning data</p><p><strong>Image caption data</strong>
Image caption data 用于提升模型的世界知识。作者从 LAION, DataComp, DNF 以及 Wukong 等数据集收集了 10B 的 image-text pairs, 然后进行数据清洗：</p><ol><li>Heuristic-based filtering: 基于规则，如 resolution, caption length 等过滤掉低质量的图片</li><li>Relevance filtering: 计算 CLIP score, 过滤掉低质量的数据</li><li>Concept-balanced resampling: 基于 MetaCLIP , 来提升数据的覆盖程度，解决长尾分布的问题。这里 <a class=link href=https://maosong.website/p/notes-on-seed1.5-vl/ target=_blank rel=noopener>Seed1.5-VL</a> 有一个类似的消融实验。</li><li>Factual-centered re-captioning: 通过 rewrite caption 来提升 caption 的质量和信息密度。Idefics2 通过实验发现，rewriting caption 可以提高模型的表现。作者在最终的数据集里，混合了一部分 re-caption 的数据。</li></ol><p><strong>Interleaved image-text data</strong>
相比于 image-caption data, interleaved image-text data 体量更大，且逻辑关系更强。但是噪声也更多，因此作者针对不同来源的数据构建了不同的数据清洗策略：</p><ol><li>Web data processing pipeline: 数据来源为 MINT, MMC4, OmniCorpus 等。作者首先基于 CLIP-score，去掉与上下文不相关的图片；然后，作者去除掉广告，二维码等低质量的图片；最后，作者将图多文少的样本也给去除掉，比如相册图片。作者还训练了一个 high-knowledge-density image classifier, 来识别信息密度高的样本。</li><li>Academic book processing pipeline: 作者从电子书中收集了 100M 的样本，这些电子书主要是 STEM domain，然后作者构建了一个 PDF parsing tool 来提取文档内容。</li></ol><p><strong>OCR data</strong>
OCR 数据包含<strong>220M images</strong>, 数据集由三部分组成：</p><ol><li>Synthetic document images: 基于预训练语料，使用不同的格式来渲染文字生成不同的图片，然后基于 LAION 的图片作为背景</li><li>Natural scene text images: 使用 Paddle-OCR 来从图片中提取文字以及对应的 bounding box</li><li>Academic documents: 使用类似 Nougat 的方法来构建 PDF page-Markdown 的数据。</li></ol><p><strong>Grounding data</strong>
主要包含自然场景和 GUI 两个 domain 的 grounding 数据：</p><ol><li>Natural image grounding: 基于 LAION-115M 得到。作者使用 GLIPv2, 来自动化解析 caption 中的实体，然后预测对应的 bounding box, 作者将 bounding box 较少的样本去除掉。最终得到<strong>40M</strong>高质量的自然场景数据</li><li>GUI grounding: 基于 CC 提取对应的 url, 然后使用 Playwright 来与网页进行交互，进而获取到所有的 DOM elements 与其对应的 bounding box. 最终收集到 <strong>140M</strong>高质量的 QA 样本</li></ol><p><strong>Video data</strong>
Video data 从多个来源得到，作者使用了 fine-grained human annotation 来保证数据质量。作者还标注了 camera motion 等信息</p><p>为了保证数据的质量，作者进行了去重，来减少数据语义的相似性。</p><p><strong>Instruction tuning data</strong>
instruction tuning data 的策略如下：</p><ol><li>Task coverage and taxonomy: 优化数据分布</li><li>Complex scenario augmentation: 构建复杂的指令跟随数据</li><li>Data contamination check: 数据污染检查</li></ol><p>最终一共收集到 <strong>50M</strong>样本，覆盖 visual perception, multimodal reasoning, GUI agent 等任务。</p><h4 id=pre-training-training><a href=#pre-training-training class=header-anchor></a>Pre-training Training</h4><p>Pre-training 由两个 stage 组成：</p><ol><li>Multimodal pre-training: 提高模型的通用多模态能力，上下文长度为 8192, global batch size 为 1536. 使用了 data packing 策略，使用了 2-way tensor parallelism</li><li>Long-context continue training: 提升模型在高精度图片，长视频，长上下文场景下的表现，上下文长度为 32768, 使用了 2-way tensor parallelism 和 4-way context parallelism.</li></ol><h3 id=sft><a href=#sft class=header-anchor></a>SFT</h3><h4 id=sft-data><a href=#sft-data class=header-anchor></a>SFT Data</h4><p>数据包括 long CoT reasoning 数据，来让模型以标准格式输出 multi-step solution.</p><p>数据包括 verifiable tasks 以及 non-verifiable tasks, 覆盖中英两种语言。作者过滤了非常简单和非常困难的样本。</p><p>数据格式如下</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>&lt;think&gt; {think_content} &lt;/think&gt; &lt;answer&gt; {answer_content} &lt;/answer&gt;
</span></span></code></pre></td></tr></table></div></div><p>对于 verifiable tasks, 输出格式为</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>&lt;answer&gt; &lt;|begin_of_box|&gt; {answer_content} &lt;|end_of_box|&gt; &lt;/answer&gt;
</span></span></code></pre></td></tr></table></div></div><p>数据清洗策略包括：</p><ol><li>严格遵守格式要求</li><li>reasoning style 不一致或者有噪声</li><li>混合语言输出。</li></ol><p>作者还使用 RL checkpoints 来生成一部分高质量数据，加入到 SFT 阶段的数据集里面。</p><h4 id=sft-training><a href=#sft-training class=header-anchor></a>SFT Training</h4><p>全参数微调。上下文长度为 32768， batch size 为 32.数据包括 long-form reasoning data 以及纯文本数据，包括 math, multi-turn conversation, agent planning 以及 instruction following 等.</p><blockquote><p>[!tip] Observation
作者发现，在 SFT 阶段即使是使用带噪声的 reasoning data，模型在 RL 阶段也能继续训练。也就是说，不完美的 reasoning trace 也能提供 guidance. 但是，使用更高质量的数据可以让模型 RL 阶段的训练更稳定且表现更好。</p></blockquote><h3 id=rl><a href=#rl class=header-anchor></a>RL</h3><p>与 <a class=link href=https://maosong.website/p/notes-on-seed1.5-vl/ target=_blank rel=noopener>Seed1.5-VL</a> 一样，作者在 RL 阶段结合了 RLVR 和 RLHF 两个训练目标。任务包括 STEM problem solving (such as mathematics, physics, chemistry), grounding, optical character recognition (OCR), video understanding, GUI agents, chart and document understanding, logical reasoning, and instruction following.</p><p><strong>Data</strong>
作者首先构建了一个任务集合，然后基于这些任务，来过滤或者生成对应的 QA pair, 接下来作者基于难度和质量来过滤，最后作者在每个 domain 上进行 RL 的训练来保证数据的有效性。</p><p><strong>Reward modelling</strong>
作者构建了一个 reward system, 用于给不同 domain 的任务进行奖励。为了探究不同 domain 的 reward 对模型整体表现的影响，作者设计了一个 low-quality reward system，实验结果如下，可以看到模型在训练后期出现了 collapse 或者 reward hacking 的情况。</p><p><img src=/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_reward_ablation.png width=1778 height=876 loading=lazy alt="GLM-4.1V-Thinking reward ablation" class=gallery-image data-flex-grow=202 data-flex-basis=487px></p><blockquote><p>[!tip] Observation
作者发现，不同任务的不同信号奖励会对模型最终表现产生巨大影响。</p></blockquote><p>在使用 LLM 提取模型回答的 answer 的时候，作者要求模型的最终答案为 <code>&lt;|begin_of_box|> {answer_content} &lt;|end_of_box|></code> 这种形式，避免大语言模型提取出错。</p><p>作者构建的 reward system 包含如下模块：</p><ol><li>shared verification functions: 包括格式检查等</li><li>domain specific modules: 与 domain 相关的模块</li><li>unit testing: 观测 domain 输出的分布，然后基于输出的分布来调整 reward logic</li></ol><p>最终 reward 的 design 如下表所示</p><p><img src=/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_reward_design.png width=1812 height=712 loading=lazy alt="GLM-4.1V-Thinking reward design" class=gallery-image data-flex-grow=254 data-flex-basis=610px></p><p><strong>RLCS</strong>
作者使用了 GRPO 来进行优化训练。作者发现，仅需 200 training steps 模型在一半以上问题的准确率就达到了 $90\%$.</p><p>为了提升模型的训练效率，作者提出了 Reinforcement Learning with Curriculum Sampling (RLCS), 也就是将课程学习与 RL 结合起来。作者基于模型训练情况动态调整训练样本的难度，来保证每个样本对模型训练的提升都是最大的。</p><p>在 RLCS 训练之前，作者先使用模型评测得到每个样本的 pass@k, 然后根据结果将样本进行分类。在训练过程中，作者记录每个样本的 pass@k， 然后动态更新其分类结果。在采样的时候，作者基于难度来对样本进行加权，来降低太简单或者太难样本的采样概率。</p><p>作者还提出了一些提高 RL 表现的方法：</p><ol><li>Larger batch size: 使用更大的 batch size 效果更好</li><li>Dynamic sampling expansion via ratio EMA: 作者定义了 naive 样本和高质量样本的比例，然后根据这个比例进行采样，来提升整体的采样效率。</li><li>Force answering: 与 <a class=link href=https://maosong.website/p/notes-on-qwen3/ target=_blank rel=noopener>Qwen3</a> 一样，对于比较难的问题，当输出的 token 数过长时，作者对模型输出进行截断，然后让模型基于已有思考过程直接输出结果。</li><li>Discard KL loss: 与 DAPO 一样，作者也移除了 KL divergence loss</li><li>Clip-higher: 与 DAPO 一样，作者通过修改超参数来提高模型的探索能力</li></ol><blockquote><p>[!tip] Observation</p><ol><li>RL 阶段的表现与 cold-start SFT 的表现并不完全相关。作者发现，cold-start SFT 取得更好的表现并不一定保证 RL 的表现就更好。</li><li>RL 阶段各个 domain 数据的影响是正交的，这与 cold-start SFT 阶段的结果不同。</li></ol></blockquote><p>作者还提出了一些提高训练稳定性的方法：</p><ol><li>提高 cold-start SFT 阶段数据的质量</li><li>移除 KL divergence loss</li><li>使用 top-p 为 1，而不是 0.9 (如 <a class=link href=https://maosong.website/p/notes-on-qwen-llm/ target=_blank rel=noopener>Qwen-LLM</a> 中的设置)，top-p 为 1 可以 cover 整个 vocabulary，避免某些 token 没有参与训练</li><li>per-sample loss 和 per-token loss 没有显著区别，但是 per-sample loss 稳定性更高</li><li>在 cold-start SFT 阶段让模型学会格式要求，而不是在 RL 阶段加入 format reward</li></ol><p><strong>Infra</strong>
在 infra 方面，作者做了如下改进：</p><ol><li>Load balancing of sequence lengths across DP ranks. 平衡每个 rank 的 sequensequence length 以及 compute load</li><li>Intra-rank training with sequence packing and gradient accumulation. 将 sequence packing 和 gradient accumulation 结合起来</li><li>Sample packing and reorganization within DP ranks. data packing</li><li>Dynamic sampling expansion via ratio EMA. 平衡 naive 样本和高质量样本之间的比例</li></ol><h2 id=evaluation><a href=#evaluation class=header-anchor></a>Evaluation</h2><h3 id=performance><a href=#performance class=header-anchor></a>Performance</h3><p><img src=/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_performance.png width=1066 height=1446 loading=lazy alt="Performance of GLM4.1V Thinking" class=gallery-image data-flex-grow=73 data-flex-basis=176px></p><p>可以看到，GLM-4.1V-Thinking 在多个 benchmark 上都达到了 SOTA.</p><h3 id=ablation-study><a href=#ablation-study class=header-anchor></a>Ablation Study</h3><p>作者评估了一下不同 domain 的 RL 训练对模型整体表现的影响。作者使用不同的数据来进行训练，然后分析结果的相关性，结果如下</p><p><img src=/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_cross_domain_generalization.png width=1226 height=962 loading=lazy alt="Cross-domain generalization in reinforcement learning" class=gallery-image data-flex-grow=127 data-flex-basis=305px></p><p>实验结果发现：</p><ol><li>在某个 domain 上的训练会提高模型在其他 domain 上的表现</li><li>联合多个 domain 进行训练最终的表现会更好</li></ol><p>作者还发现，不同的任务之间存在关联，如 GUI-agent 和 grounding 这两个任务是高度相关的。OCR & Chart 以及 GUI-agent 也是高度相关的。</p><h2 id=conclusion><a href=#conclusion class=header-anchor></a>Conclusion</h2><p>在本文中，作者提出了 GLM-4.1V-Thinking, 一个 9B 的通用多模态 reasoning model, 作者通过构建高质量的数据，多阶段的训练，让模型在 reasoning 和 non-reasoning 任务上均超过了已有的 SOTA 模型的表现</p><p>作者认为，模型有以下局限：</p><ol><li>模型并没有提升 reasoning 的质量，在某些情况下，模型结果正确，但是中间过程错误。作者分析这是因为目前只有针对结果的奖励机制。因此，未来需要能够评估中间过程的 reward model</li><li>RL 的训练具有不稳定性，作者发现模型对设置比较敏感，这对 RL 训练的 scaling 提出了挑战。</li><li>模型在复杂场景下表现依旧不太好，比如图片中有物体遮挡的情况，这些感知误差也会影响最终的 reasoning 表现。作者认为如何同时提高 perception 和 reasoning 的表现是一个需要研究的课题。</li></ol><p>未来的工作有：</p><ol><li>如何构建更好的 reward 机制，来同时评估结果和中间过程。从而防止模型 reward hacking</li><li>如何基于 multimodal training 来提升模型在纯文本任务上的表现。探究 visual reasoning 和 text reasoning 如何互相促进也是一个课题</li><li>如何更好评估模型的表现，即如何更好评估模型的 failure modes, 比如幻觉，short reasoning 等</li></ol><h2 id=references><a href=#references class=header-anchor></a>References</h2><ul><li><a class=link href=https://arxiv.org/abs/2507.01006 target=_blank rel=noopener>Arxiv</a></li><li><a class=link href=https://huggingface.co/THUDM/GLM-4.1V-9B-Thinking target=_blank rel=noopener>Huggingface</a></li></ul><h2 id=appendix><a href=#appendix class=header-anchor></a>Appendix</h2><p>GLM-4.1V-Thinking 的 patch merger 代码</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>Glm4vVisionPatchMerger</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>dim</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>context_dim</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>hidden_act</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span> <span class=n>bias</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>dim</span><span class=p>,</span> <span class=n>dim</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>bias</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>post_projection_norm</span> <span class=o>=</span> <span class=n>LayerNorm</span><span class=p>(</span><span class=n>dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>gate_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>dim</span><span class=p>,</span> <span class=n>context_dim</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>bias</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>up_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>dim</span><span class=p>,</span> <span class=n>context_dim</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>bias</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>down_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>context_dim</span><span class=p>,</span> <span class=n>dim</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>bias</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>act1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>GELU</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>act_fn</span> <span class=o>=</span> <span class=n>ACT2FN</span><span class=p>[</span><span class=n>hidden_act</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>hidden_state</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>hidden_state</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>proj</span><span class=p>(</span><span class=n>hidden_state</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>hidden_state</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>act1</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>post_projection_norm</span><span class=p>(</span><span class=n>hidden_state</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>down_proj</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>act_fn</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>gate_proj</span><span class=p>(</span><span class=n>hidden_state</span><span class=p>))</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>up_proj</span><span class=p>(</span><span class=n>hidden_state</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>Glm4vVisionModel</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>downsample</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>in_channels</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>out_channels</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>out_hidden_size</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>kernel_size</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>spatial_merge_size</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>stride</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>spatial_merge_size</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>merger</span> <span class=o>=</span> <span class=n>Glm4vVisionPatchMerger</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>dim</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>out_hidden_size</span><span class=p>,</span> <span class=n>context_dim</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>intermediate_size</span><span class=p>,</span> <span class=n>hidden_act</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>hidden_act</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>hidden_states</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=o>...</span>
</span></span><span class=line><span class=cl>        <span class=n>hidden_states</span> <span class=o>=</span> <span class=n>hidden_states</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>spatial_merge_size</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>spatial_merge_size</span><span class=p>,</span> <span class=n>hidden_states</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=n>hidden_states</span> <span class=o>=</span> <span class=n>hidden_states</span><span class=o>.</span><span class=n>permute</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>downsample</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>out_hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>hidden_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>merger</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div></section><footer class=article-footer><section class=article-tags><a href=/tags/zhipu/>Zhipu</a>
<a href=/tags/reasoning/>Reasoning</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section><section class=article-lastmod><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>Last updated on July 15, 2025 at 6:11 PM</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".main-article");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/notes-on-kimi-k2.5/><div class=article-details><h2 class=article-title>Notes on Kimi-k2.5</h2></div></a></article><article><a href=/p/notes-on-keye-vl-1.5/><div class=article-details><h2 class=article-title>Notes on Keye-VL 1.5</h2></div></a></article><article><a href=/p/notes-on-internvl3.5/><div class=article-details><h2 class=article-title>Notes on InternVL3.5</h2></div></a></article><article><a href=/p/notes-on-keye-vl/><div class=article-details><h2 class=article-title>Notes on Keye-VL</h2></div></a></article><article><a href=/p/notes-on-v-triune/><div class=article-details><h2 class=article-title>Notes on V-Triune</h2></div></a></article></div></div></aside><script src=https://utteranc.es/client.js repo=MaoSong2022/MaoSong2022.github.io issue-term=pathname crossorigin=anonymous async></script><style>.utterances{max-width:unset}</style><script>let utterancesLoaded=!1;function setUtterancesTheme(e){let t=document.querySelector(".utterances iframe");t&&t.contentWindow.postMessage({type:"set-theme",theme:`github-${e}`},"https://utteranc.es")}addEventListener("message",e=>{if(e.origin!=="https://utteranc.es")return;utterancesLoaded=!0,setUtterancesTheme(document.documentElement.dataset.scheme)}),window.addEventListener("onColorSchemeChange",e=>{if(!utterancesLoaded)return;setUtterancesTheme(e.detail)})</script><footer class=site-footer><section class=copyright>&copy;
2020 -
2026 Mao Song(毛松)'s Homepage</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.31.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>