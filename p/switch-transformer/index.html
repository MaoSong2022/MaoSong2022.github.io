<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><link rel=dns-prefetch href=https://fonts.googleapis.com><link rel=dns-prefetch href=https://cdn.jsdelivr.net><link rel=dns-prefetch href=https://scripts.simpleanalyticscdn.com><link rel=preconnect href=https://fonts.googleapis.com crossorigin><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><link rel=preconnect href=https://scripts.simpleanalyticscdn.com crossorigin><meta http-equiv=x-dns-prefetch-control content="on"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=5,minimum-scale=1"><meta name=format-detection content="telephone=no"><meta name=theme-color content="#4e54c8" media="(prefers-color-scheme: light)"><meta name=theme-color content="#1a1a2e" media="(prefers-color-scheme: dark)"><link rel=apple-touch-icon href><meta name=description content="Google 在 2022 年 6 月提出了 Switch Transformer, 一个基于 MoE 架构的 Transformer 模型。作者通过改进 MoE 算法，大幅度提高了计算和通信效率，结果发现模型比 dense model 有更高的训练效率。"><meta name=keywords content><meta name=author content="Mao Song"><link rel=canonical href=https://maosong.website/p/switch-transformer/><meta property="og:title" content="Switch Transformer"><meta property="og:description" content="Google 在 2022 年 6 月提出了 Switch Transformer, 一个基于 MoE 架构的 Transformer 模型。作者通过改进 MoE 算法，大幅度提高了计算和通信效率，结果发现模型比 dense model 有更高的训练效率。"><meta property="og:type" content="article"><meta property="og:url" content="https://maosong.website/p/switch-transformer/"><meta property="og:site_name" content="Mao Song(毛松)'s Homepage"><meta property="og:locale" content="en"><meta property="og:image" content="https://maosong.website/p/switch-transformer/Switch-Transformer-Architecture.png"><meta property="og:image:alt" content="Switch Transformer"><meta property="article:published_time" content="2025-10-28T09:38:12+08:00"><meta property="article:modified_time" content="2025-12-24T18:16:03+08:00"><meta property="article:author" content="Mao Song"><meta property="article:tag" content="MoE"><meta property="article:tag" content="Google"><meta name=robots content="index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1"><meta name=googlebot content="index, follow"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"Switch Transformer","description":"Google 在 2022 年 6 月提出了 Switch Transformer, 一个基于 MoE 架构的 Transformer 模型。作者通过改进 MoE 算法，大幅度提高了计算和通信效率，结果发现模型比 dense model 有更高的训练效率。","author":{"@type":"Person","name":"Mao Song"},"datePublished":"2025-10-28T09:38:12\u002b08:00","dateModified":"2025-12-24T18:16:03\u002b08:00","image":"https:\/\/maosong.website\/p\/switch-transformer\/Switch-Transformer-Architecture.png","publisher":{"@type":"Organization","name":"Mao Song(毛松)\u0027s Homepage","logo":{"@type":"ImageObject","url":"https:\/\/maosong.website\/"}},"mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/maosong.website\/p\/switch-transformer\/"},"keywords":"MoE, Google","articleSection":"post","inLanguage":"en"}</script><title>Switch Transformer</title>
<link rel=stylesheet href=/scss/style.min.fb2ef3860c8335f835ed6d55e46d9c435d7a37375d695eed32deb59ecfadc82b.css><script async src=https://scripts.simpleanalyticscdn.com/latest.js></script></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><a href=#main-content class=skip-to-main>Skip to main content</a><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu9788782091159651930.jpg width=300 height=240 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>Mao Song(毛松)'s Homepage</a></h1><h2 class=site-description>Delving into the Latent Unknown.</h2></div></header><ol class=menu-social><li><a href=https://b23.tv/a0Bb9Z1 target=_blank title=bilibili rel=me><svg role="img" viewBox="0 0 24 24"><title>Bilibili</title><path d="M17.813 4.653h.854c1.51.054 2.769.578 3.773 1.574 1.004.995 1.524 2.249 1.56 3.76v7.36c-.036 1.51-.556 2.769-1.56 3.773s-2.262 1.524-3.773 1.56H5.333c-1.51-.036-2.769-.556-3.773-1.56S.036 18.858.0 17.347v-7.36c.036-1.511.556-2.765 1.56-3.76 1.004-.996 2.262-1.52 3.773-1.574h.774l-1.174-1.12a1.234 1.234.0 01-.373-.906c0-.356.124-.658.373-.907l.027-.027c.267-.249.573-.373.92-.373s.653.124.92.373L9.653 4.44c.071.071.134.142.187.213h4.267a.836.836.0 01.16-.213l2.853-2.747c.267-.249.573-.373.92-.373s.662.151.929.4.391.551.391.907c0 .355-.124.657-.373.906zM5.333 7.24c-.746.018-1.373.276-1.88.773-.506.498-.769 1.13-.786 1.894v7.52c.017.764.28 1.395.786 1.893.507.498 1.134.756 1.88.773h13.334c.746-.017 1.373-.275 1.88-.773.506-.498.769-1.129.786-1.893v-7.52c-.017-.765-.28-1.396-.786-1.894-.507-.497-1.134-.755-1.88-.773zM8 11.107c.373.0.684.124.933.373.25.249.383.569.4.96v1.173c-.017.391-.15.711-.4.96-.249.25-.56.374-.933.374s-.684-.125-.933-.374c-.25-.249-.383-.569-.4-.96V12.44c0-.373.129-.689.386-.947.258-.257.574-.386.947-.386zm8 0c.373.0.684.124.933.373.25.249.383.569.4.96v1.173c-.017.391-.15.711-.4.96-.249.25-.56.374-.933.374s-.684-.125-.933-.374c-.25-.249-.383-.569-.4-.96V12.44c.017-.391.15-.711.4-.96.249-.249.56-.373.933-.373z"/></svg></a></li><li><a href=https://github.com/MaoSong2022 target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href='https://scholar.google.com/citations?user=BaqGkQQAAAAJ' target=_blank title="Google Scholar" rel=me><svg role="img" viewBox="0 0 24 24"><title>Google Scholar</title><path d="M5.242 13.769.0 9.5 12 0l12 9.5-5.242 4.269C17.548 11.249 14.978 9.5 12 9.5c-2.977.0-5.548 1.748-6.758 4.269zM12 10a7 7 0 100 14 7 7 0 000-14z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/tags/><svg class="icon icon-tabler icon-tabler-tag" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11 3l9 9a1.5 1.5.0 010 2l-6 6a1.5 1.5.0 01-2 0L3 11V7a4 4 0 014-4h4"/><circle cx="9" cy="9" r="2"/></svg>
<span>Tags</span></a></li><li><a href=/about/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>About</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#introduction>Introduction</a></li><li><a href=#architecture>Architecture</a><ol><li><a href=#moe>MoE</a></li><li><a href=#efficient-sparse-routing>Efficient Sparse Routing</a></li></ol></li><li><a href=#parallelism>Parallelism</a><ol><li><a href=#data-parallelism>Data Parallelism</a></li><li><a href=#model-parallelism>Model Parallelism</a></li><li><a href=#model-and-data-parallelism>Model and Data Parallelism</a></li><li><a href=#expert-and-data-parallelism>Expert and Data Parallelism</a></li><li><a href=#expert-model-and-data-parallelism>Expert, Model and Data Parallelism</a></li></ol></li><li><a href=#results>Results</a><ol><li><a href=#scaling>Scaling</a></li><li><a href=#switch-for-attention>Switch for Attention</a></li><li><a href=#no-token-left-behind>No Token Left behind</a></li><li><a href=#encouraging-exploration-across-experts>Encouraging Exploration Across Experts</a></li><li><a href=#ablation-on-few-experts>Ablation on Few Experts</a></li><li><a href=#downstream-model-performance>Downstream Model Performance</a></li></ol></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#references>References</a></li></ol></nav></div></section></aside><main id=main-content class="main full-width" role=main aria-label="Main content"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/llm/>LLM</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/switch-transformer/>Switch Transformer</a></h2><h3 class=article-subtitle>Google 在 2022 年 6 月提出了 Switch Transformer, 一个基于 MoE 架构的 Transformer 模型。作者通过改进 MoE 算法，大幅度提高了计算和通信效率，结果发现模型比 dense model 有更高的训练效率。</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>October 28, 2025</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>3 minute read</time></div></footer></div></header><section class=article-content><h2 id=introduction><a href=#introduction class=header-anchor></a>Introduction</h2><p><a class=link href=https://maosong.website/p/kaplan-scaling-law/ target=_blank rel=noopener>Kaplan scaling law</a> 和 <a class=link href=https://maosong.website/p/chinchilla-scaling-law/ target=_blank rel=noopener>Chinchilla scaling law</a> 已经给出了针对 dense transformer model 的 scaling law, 即在给定算力下如何找到最优的 model size 和 dataset size. 已有的工作如 T5 主要是扩展 dense model 来达到更高的表现, 但是他们的问题是对算力要求比较高。</p><p>为了解决这个问题，作者尝试在不增加算力的情况下提升模型的参数量，为了达到这个目的，作者构建了基于 MoE 架构的模型 Switch Transformer.</p><p>作者的主要贡献如下：</p><ol><li>提出了基于 MoE 架构的 Switch Transformer model</li><li>探究了针对 MoE 架构的 scaling law</li><li>将 MoE model 的能力蒸馏到 small dense model 里去</li><li>若干提升训练效率和稳定性的技巧</li></ol><h2 id=architecture><a href=#architecture class=header-anchor></a>Architecture</h2><p>Switch Transformer 的架构如下图所示</p><p><img src=/p/switch-transformer/Switch-Transformer-Architecture.png width=1130 height=571 loading=lazy alt="Architecture of Switch Transformer" class=gallery-image data-flex-grow=197 data-flex-basis=474px></p><h3 id=moe><a href=#moe class=header-anchor></a>MoE</h3><p>MoE 的定义见 <a class=link href=https://maosong.website/p/moe-tutorial/ target=_blank rel=noopener>MoE tutorial</a>, 我们假设有 $N$ 个专家，其中激活 $K$ 个专家。</p><p>之前的工作认为我们只有在激活 $>2$ 个专家时，模型才能够比较好的训练，但是在本文中，作者决定只使用 1 个 expert, 也就是 $K=1$. 作者将激活一个专家的 layer 称为 <strong>Switch layer</strong>.</p><p>作者认为 Switch Layer 有三个优势：</p><ol><li>router computation 现在只需要将每个 token route 到 1 个 expert</li><li>每个专家的 capacity 更小，负载更加均衡</li><li>routing 的实现更简单，且通信开销也降低了</li></ol><h3 id=efficient-sparse-routing><a href=#efficient-sparse-routing class=header-anchor></a>Efficient Sparse Routing</h3><p>作者首先定义了<strong>expert capacity</strong>, 也就是每个 expert 处理的 token 数量，其定义如下</p>$$
\text{expert capacity} = \left(\frac{\text{tokens per batch}}{\text{number of experts}}\right) * \text{capacity factor}
$$<p>其示意图如下所示</p><p><img src=/p/switch-transformer/Switch-Transformer-expert-capacity.png width=1269 height=479 loading=lazy alt="Token routing dynamics" class=gallery-image data-flex-grow=264 data-flex-basis=635px></p><p>提升 capacity factor 可以减少 token overflow 的概率，但同时也会导致计算和内存的浪费。作者通过实验发现应该尽可能降低 dropped token 比例。</p><p>为了平衡每个 expert 处理 token 的个数，作者设计了 load balancing loss, 见 <a class=link href=https://maosong.website/p/load-balancing-tutorial/ target=_blank rel=noopener>Load Balancing loss</a> 来要求每个 expert 处理的 token 数基本一致。</p><h2 id=parallelism><a href=#parallelism class=header-anchor></a>Parallelism</h2><p>作者在本节介绍了针对 MoE 模型的并行策略，如下图所示</p><p><img src=/p/switch-transformer/Switch-Transformer-Parallelism.png width=1268 height=746 loading=lazy alt="Data and weight partitioning strategies" class=gallery-image data-flex-grow=169 data-flex-basis=407px></p><p>这里，我们给定 notation 如下</p><div class=table-wrapper><table><thead><tr><th>Term</th><th>Description</th></tr></thead><tbody><tr><td>$B$</td><td>Number of tokens in the batch.</td></tr><tr><td>$N$</td><td>Number of total cores.</td></tr><tr><td>$n$</td><td>Number of ways for data-parallelism sharding.</td></tr><tr><td>$m$</td><td>Number of ways for model-parallelism sharding.</td></tr><tr><td>$E$</td><td>Number of experts in Switch layers.</td></tr><tr><td>$C$</td><td>Expert capacity, the batch size of each expert.</td></tr></tbody></table></div><p>作者将一个 Logical mesh 分为两个维度，一个是 data-parallel sharding, 用 $n$ 表示，另一个是 model-parallel sharding, 用 $m$ 表示，从而总的 cores 数为 $N=n\times m$.</p><h3 id=data-parallelism><a href=#data-parallelism class=header-anchor></a>Data Parallelism</h3><p>对于 data prallelism 来说，每个 core 上的模型是一样的，而一个 batch 的数据被切分为了 $B/n$ 份，此时 $n=N$, $m=1$. 这种并行策略的好处是仅在 forward 和 backward 完成之后才会进行一次通信。</p><h3 id=model-parallelism><a href=#model-parallelism class=header-anchor></a>Model Parallelism</h3><p>对于 model parallelism, 每个 core 上的数据都是全部数据（通过拷贝得到），模型被 shard 到所有 core 上，此时 $n=1,m=N$, 这种情况下，每次 forward 或者 backward，都需要进行 $N$ 次通信（假设我们使用 pipeline parallelism）， 这种并行策略的好处是每个 core 上的计算压力小了，但是缺点是通信开销多了。</p><h3 id=model-and-data-parallelism><a href=#model-and-data-parallelism class=header-anchor></a>Model and Data Parallelism</h3><p>第三种总策略是同时进行 model parallelism 和 data parallelism, 这种情况下，每个 core 上保存 $B/n$ 的数据以及 shard 为 $m$ 份的 sharding weight.</p><h3 id=expert-and-data-parallelism><a href=#expert-and-data-parallelism class=header-anchor></a>Expert and Data Parallelism</h3><p>第四种策略是同时进行 expert parallelism 和 data parallelism, 作者在这里假设 $n=N$ 且 $E=n=N$, 也就是每个 core 上保留 $B/n$ 的数据, 然后还有一个对应的专家。</p><p>首先，对于输入大小为 $[B,d]$ 的 tensor, 我们会进行拆分得到大小为 $[n, B/n, d]$ 的 tensor, 代表 $n$ 个设备上分别存储 $[B/n, d]$ 的数据，首先我们计算路由权重，得到</p>$$
[n,B/n, d]\times [d, E] \to [n, B/n, E]
$$<p>权重的每个值 $[i,j,k]$ 代表了第 $i$ 个 core 上,第 $j$ 个 token 分配一个第 $k$ 个专家的概率，我们对其进行 softmax 与 top-K 操作之后，就得到了对应的专家（注意 Switch Transformer 中的 $K=1$, 我们通过 one-hot 编码将其输出转化为一个二进制矩阵，大小为 $[n,B/n, E, C]$, 这里的每个值 $[i,j,k,l]$ 代表了第 $i$ 个 core 上,第 $j$ 个 token 分配给第 $k$ 个专家第 $l$ 个 token, 接下来我们再基于输入 <code>[n,B/n,d]</code> 计算 core 到 expert 的数据，其大小为 <code>[n,E,C,d]</code>, 计算方式为</p>$$
\mathrm{einsum}([n,B/n, d], [n,B/n,E,C], \mathrm{dim}=[B/n])
$$<p>里面的元素 <code>[i,j,k,:]</code> 代表了第 $i$ 个设备路由到第 $j$ 个专家的第 $k$ ($k<c$) 个 token.</p><p>然后我们就可以执行 <code>all-to-all</code> 通信来把对应的 token 传输给对应专家所在的设备上了，通信完成之后，每个设备上的专家对收集到的输入 token 进行计算，计算完之后，再通过 <code>all-to-all</code> 通信来把输出传输给输入的设备。这样就完成了 MoE 模块的计算与通信</p><h3 id=expert-model-and-data-parallelism><a href=#expert-model-and-data-parallelism class=header-anchor></a>Expert, Model and Data Parallelism</h3><p>目前我们只是通过增加专家个数来提高模型的表现，但是 FLOPs 并没有增加，如果我们希望通过增加 FLOPs 来提高模型表现的话，我们需要增加 expert layer 的 model size, 这就涉及到了 Model parallelism, 此时的做法和前面提到的 Expert and Data Parallelism 一样，我们在收集到对应的设备的输入之后，再执行 model parallelism 就可以了。</p><h2 id=results><a href=#results class=header-anchor></a>Results</h2><p>基于以上改进，作者构建了 Switch Transformer, 模型训练的数据集为 C4, 训练的目标为 masked language modeling, 训练时，作者将 $15\%$ 的 token 替换为 <code>[mask]</code> token.</p><p>模型的配置如下表所示</p><p><img src=/p/switch-transformer/Switch-Transformer-model-configuration.png width=1256 height=252 loading=lazy alt="Model configuration" class=gallery-image data-flex-grow=498 data-flex-basis=1196px></p><p>实验结果如下图所示</p><p><img src=/p/switch-transformer/Switch-Transformer-performance.png width=1206 height=526 loading=lazy alt="Performance of Switch Transformer" class=gallery-image data-flex-grow=229 data-flex-basis=550px></p><p>实验结果发现</p><ol><li>Switch Transformer 的表现和训练效率都超过了 Dense model</li><li>Switch transformer 的训练效率稍微优于使用 2 个 expert 的 MoE-Base 模型</li><li>Switch transformer 在 low capacity factor 的场景下效果更好</li></ol><h3 id=scaling><a href=#scaling class=header-anchor></a>Scaling</h3><p>作者对比了 MoE 模型的 scaling law, 结果如下图所示</p><p><img src=/p/switch-transformer/Switch-Transformer-scaling-law.png width=1244 height=497 loading=lazy alt="Scaling law of MoE model" class=gallery-image data-flex-grow=250 data-flex-basis=600px></p><p>可以看到，当我们增加专家个数的时候，模型的表现是持续提升的。并且当我们增加专家个数之后，模型的训练效率也有所提升。</p><p>作者接下来在训练时间上进行了对比，结果如下图所示</p><p><img src=/p/switch-transformer/Switch-Transformer-speed-comparison.png width=839 height=672 loading=lazy alt="Speed comparison of MoE model" class=gallery-image data-flex-grow=124 data-flex-basis=299px></p><p>实验结果说明，switch transformer 的训练效率比 dense model 快 7 倍左右</p><p>作者进一步对比 switch transformer 和更大的 dense 模型 (T5 large, $3.5\times$ FLOPs), 实验结果证明 Switch transformer 的训练效率仍然更高。</p><h3 id=switch-for-attention><a href=#switch-for-attention class=header-anchor></a>Switch for Attention</h3><p>作者还探究了在 attention layer 加入 MoE 的表现，结果发现尽管效果有提升，但是训练不稳定。</p><h3 id=no-token-left-behind><a href=#no-token-left-behind class=header-anchor></a>No Token Left behind</h3><p>由于 tensorflow 为一个静态计算框架，tensor 的形状必须预先定义好，因此必须为每个 expert 设定 capacity, 但是这样就会导致有些 token 被 drop 掉。</p><p>因此，作者提出了 &ldquo;No token left behind&rdquo; 这种方法，来避免出现 token overflow 的情况。具体做法就是先按照正常的 router 逻辑进行计算，如果 router 选取出来的 top-K expert （本文中 $K=1$） 都已经满载了，则选取 <code>top-(K+1)</code> expert 进行计算，作者发现这种方式可以保证大部分 token 都不会被 drop. 作者通过尝试之后发现，这种方法并没有带来提升。</p><h3 id=encouraging-exploration-across-experts><a href=#encouraging-exploration-across-experts class=header-anchor></a>Encouraging Exploration Across Experts</h3><p>作者还探究了不同选取 top-Kexpert 的方式，作者对比了以下三种方法：</p><ol><li>argmax</li><li>sampling from the softmax distribution</li><li>input dropout on the incoming representation</li><li>multiplicative jitter noise on the incoming representation</li></ol><p>实验结果如下表所示</p><div class=table-wrapper><table><thead><tr><th>Model Quality</th><th>(Neg. Log Perp.) (↑)</th></tr></thead><tbody><tr><td>Argmax</td><td>-1.471</td></tr><tr><td>Sample softmax</td><td>-1.570</td></tr><tr><td>Input dropout</td><td>-1.480</td></tr><tr><td>Input jitter</td><td>-1.468</td></tr></tbody></table></div><p>作者发现，jitter 的效果最好，因此在本文中作者使用 jitter 来加入 noise.</p><h3 id=ablation-on-few-experts><a href=#ablation-on-few-experts class=header-anchor></a>Ablation on Few Experts</h3><p>作者还使用了更少的专家进行实验，结果如下图所示</p><p><img src=/p/switch-transformer/Switch-Transformer-few-experts.png width=846 height=666 loading=lazy alt="Switch Transformer with few experts" class=gallery-image data-flex-grow=127 data-flex-basis=304px></p><p>实验结果显示，即使我们只使用少数 experts, 其模型表现仍然超过了 dense model 的表现，说明了 MoE 架构的有效性。</p><h3 id=downstream-model-performance><a href=#downstream-model-performance class=header-anchor></a>Downstream Model Performance</h3><p>作者还进一步探究了模型的预训练表现与 downstream task 任务上表现的关系，结果如下图所示</p><p><img src=/p/switch-transformer/Switch-Transformer-downstream-performance-scaling.png width=1257 height=499 loading=lazy alt="Upstream pre-trained quality to downstream model quality." class=gallery-image data-flex-grow=251 data-flex-basis=604px></p><p>实验结果显示，不管是 baseline 还是 Switch Transformer 其预训练的表现与下游任务上的表现都是正相关的。但是，作者也发现，MoE 模型在微调之后，其表现并不总是与预训练的表现正相关。因此，作者认为进一步探究这个机制是有必要的。</p><h2 id=conclusion><a href=#conclusion class=header-anchor></a>Conclusion</h2><p>作者在本文中提出了 Switch Transformer，一个基于 MoE 架构的 Transformer 模型。作者通过改进 MoE 算法，大幅度提高了计算和通信效率，结果发现模型比 dense model 有更高的训练效率。</p><p>作者认为，未来的工作有：</p><ol><li>提升大规模模型的训练稳定性</li><li>解决 MoE 模型微调之后效果不如预期的问题</li><li>探究针对 MoE 模型的 scaling law</li><li>支持异构架构的 MoE 模型</li><li>在 FFN 模块意外应用 MoE 架构</li><li>将 Switch Transformer 扩展到其他的模态</li></ol><h2 id=references><a href=#references class=header-anchor></a>References</h2><ul><li><a class=link href=http://arxiv.org/abs/2101.03961 target=_blank rel=noopener>Arxiv</a></li></ul></section><footer class=article-footer><section class=article-tags><a href=/tags/moe/>MoE</a>
<a href=/tags/google/>Google</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section><section class=article-lastmod><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>Last updated on December 24, 2025 at 6:16 PM</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".main-article");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/notes-on-glam/><div class=article-details><h2 class=article-title>Notes on GLaM</h2></div></a></article><article><a href=/p/gshard/><div class=article-details><h2 class=article-title>GShard</h2></div></a></article><article><a href=/p/st-moe/><div class=article-details><h2 class=article-title>ST-MoE</h2></div></a></article><article><a href=/p/notes-on-qwen3-next/><div class=article-details><h2 class=article-title>Notes on Qwen3-Next</h2></div></a></article><article><a href=/p/notes-on-minimax-01/><div class=article-details><h2 class=article-title>Notes on MiniMax-01</h2></div></a></article></div></div></aside><script src=https://utteranc.es/client.js repo=MaoSong2022/MaoSong2022.github.io issue-term=pathname crossorigin=anonymous async></script><style>.utterances{max-width:unset}</style><script>let utterancesLoaded=!1;function setUtterancesTheme(e){let t=document.querySelector(".utterances iframe");t&&t.contentWindow.postMessage({type:"set-theme",theme:`github-${e}`},"https://utteranc.es")}addEventListener("message",e=>{if(e.origin!=="https://utteranc.es")return;utterancesLoaded=!0,setUtterancesTheme(document.documentElement.dataset.scheme)}),window.addEventListener("onColorSchemeChange",e=>{if(!utterancesLoaded)return;setUtterancesTheme(e.detail)})</script><footer class=site-footer><section class=copyright>&copy;
2020 -
2026 Mao Song(毛松)'s Homepage</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.31.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>