<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><link rel=dns-prefetch href=https://fonts.googleapis.com><link rel=dns-prefetch href=https://cdn.jsdelivr.net><link rel=dns-prefetch href=https://scripts.simpleanalyticscdn.com><link rel=preconnect href=https://fonts.googleapis.com crossorigin><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><link rel=preconnect href=https://scripts.simpleanalyticscdn.com crossorigin><meta http-equiv=x-dns-prefetch-control content="on"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=5,minimum-scale=1"><meta name=format-detection content="telephone=no"><meta name=theme-color content="#4e54c8" media="(prefers-color-scheme: light)"><meta name=theme-color content="#1a1a2e" media="(prefers-color-scheme: dark)"><link rel=apple-touch-icon href><meta name=description content="本文中，我们将介绍如何计算 LLM 在训练和推理过程中的内存需求以及简要介绍对应的优化方法。"><meta name=keywords content><meta name=author content="Mao Song"><link rel=canonical href=https://maosong.website/p/llm-memory-computation/><meta property="og:title" content="LLM Memory Computation"><meta property="og:description" content="本文中，我们将介绍如何计算 LLM 在训练和推理过程中的内存需求以及简要介绍对应的优化方法。"><meta property="og:type" content="article"><meta property="og:url" content="https://maosong.website/p/llm-memory-computation/"><meta property="og:site_name" content="Mao Song(毛松)'s Homepage"><meta property="og:locale" content="en"><meta property="og:image" content="https://maosong.website/p/llm-memory-computation/Attention-computation-graph.png"><meta property="og:image:alt" content="LLM Memory Computation"><meta property="article:published_time" content="2026-01-17T10:04:32+08:00"><meta property="article:modified_time" content="2026-01-24T17:06:04+08:00"><meta property="article:author" content="Mao Song"><meta property="article:tag" content="Transformer"><meta property="article:tag" content="Training"><meta property="article:tag" content="Inference"><meta name=robots content="index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1"><meta name=googlebot content="index, follow"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"LLM Memory Computation","description":"本文中，我们将介绍如何计算 LLM 在训练和推理过程中的内存需求以及简要介绍对应的优化方法。","author":{"@type":"Person","name":"Mao Song"},"datePublished":"2026-01-17T10:04:32\u002b08:00","dateModified":"2026-01-24T17:06:04\u002b08:00","image":"https:\/\/maosong.website\/p\/llm-memory-computation\/Attention-computation-graph.png","publisher":{"@type":"Organization","name":"Mao Song(毛松)\u0027s Homepage","logo":{"@type":"ImageObject","url":"https:\/\/maosong.website\/"}},"mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/maosong.website\/p\/llm-memory-computation\/"},"keywords":"Transformer, Training, Inference","articleSection":"post","inLanguage":"en"}</script><title>LLM Memory Computation</title>
<link rel=stylesheet href=/scss/style.min.fb2ef3860c8335f835ed6d55e46d9c435d7a37375d695eed32deb59ecfadc82b.css><script async src=https://scripts.simpleanalyticscdn.com/latest.js></script></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><a href=#main-content class=skip-to-main>Skip to main content</a><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu9788782091159651930.jpg width=300 height=240 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>Mao Song(毛松)'s Homepage</a></h1><h2 class=site-description>Delving into the Latent Unknown.</h2></div></header><ol class=menu-social><li><a href=https://b23.tv/a0Bb9Z1 target=_blank title=bilibili rel=me><svg role="img" viewBox="0 0 24 24"><title>Bilibili</title><path d="M17.813 4.653h.854c1.51.054 2.769.578 3.773 1.574 1.004.995 1.524 2.249 1.56 3.76v7.36c-.036 1.51-.556 2.769-1.56 3.773s-2.262 1.524-3.773 1.56H5.333c-1.51-.036-2.769-.556-3.773-1.56S.036 18.858.0 17.347v-7.36c.036-1.511.556-2.765 1.56-3.76 1.004-.996 2.262-1.52 3.773-1.574h.774l-1.174-1.12a1.234 1.234.0 01-.373-.906c0-.356.124-.658.373-.907l.027-.027c.267-.249.573-.373.92-.373s.653.124.92.373L9.653 4.44c.071.071.134.142.187.213h4.267a.836.836.0 01.16-.213l2.853-2.747c.267-.249.573-.373.92-.373s.662.151.929.4.391.551.391.907c0 .355-.124.657-.373.906zM5.333 7.24c-.746.018-1.373.276-1.88.773-.506.498-.769 1.13-.786 1.894v7.52c.017.764.28 1.395.786 1.893.507.498 1.134.756 1.88.773h13.334c.746-.017 1.373-.275 1.88-.773.506-.498.769-1.129.786-1.893v-7.52c-.017-.765-.28-1.396-.786-1.894-.507-.497-1.134-.755-1.88-.773zM8 11.107c.373.0.684.124.933.373.25.249.383.569.4.96v1.173c-.017.391-.15.711-.4.96-.249.25-.56.374-.933.374s-.684-.125-.933-.374c-.25-.249-.383-.569-.4-.96V12.44c0-.373.129-.689.386-.947.258-.257.574-.386.947-.386zm8 0c.373.0.684.124.933.373.25.249.383.569.4.96v1.173c-.017.391-.15.711-.4.96-.249.25-.56.374-.933.374s-.684-.125-.933-.374c-.25-.249-.383-.569-.4-.96V12.44c.017-.391.15-.711.4-.96.249-.249.56-.373.933-.373z"/></svg></a></li><li><a href=https://github.com/MaoSong2022 target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href='https://scholar.google.com/citations?user=BaqGkQQAAAAJ' target=_blank title="Google Scholar" rel=me><svg role="img" viewBox="0 0 24 24"><title>Google Scholar</title><path d="M5.242 13.769.0 9.5 12 0l12 9.5-5.242 4.269C17.548 11.249 14.978 9.5 12 9.5c-2.977.0-5.548 1.748-6.758 4.269zM12 10a7 7 0 100 14 7 7 0 000-14z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/tags/><svg class="icon icon-tabler icon-tabler-tag" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11 3l9 9a1.5 1.5.0 010 2l-6 6a1.5 1.5.0 01-2 0L3 11V7a4 4 0 014-4h4"/><circle cx="9" cy="9" r="2"/></svg>
<span>Tags</span></a></li><li><a href=/about/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>About</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#introduction>Introduction</a></li><li><a href=#background>Background</a><ol><li><a href=#assumption>Assumption</a></li></ol></li><li><a href=#computation>Computation</a><ol><li><a href=#overview>Overview</a></li><li><a href=#training>Training</a><ol><li><a href=#weights>Weights</a></li><li><a href=#activation>Activation</a></li><li><a href=#gradients--optimizer-states>Gradients & Optimizer States</a></li></ol></li><li><a href=#inference>Inference</a><ol><li><a href=#key-value-cache>Key Value Cache</a></li></ol></li><li><a href=#summary>Summary</a></li></ol></li><li><a href=#analysis--optimizations>Analysis & Optimizations</a><ol><li><a href=#training-1>Training</a><ol><li><a href=#mixed-precision-training>Mixed Precision Training</a></li><li><a href=#data-parallelism>Data Parallelism</a></li><li><a href=#activation-checkpointing>Activation Checkpointing</a></li><li><a href=#model-parallelism>Model Parallelism</a></li><li><a href=#flash-attention>Flash Attention</a></li></ol></li><li><a href=#inference-1>Inference</a><ol><li><a href=#quantization>Quantization</a></li><li><a href=#kv-cache-optimization>KV Cache Optimization</a></li><li><a href=#attention>Attention</a></li><li><a href=#inference-framework>Inference Framework</a></li></ol></li></ol></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#references>References</a></li><li><a href=#appendix>Appendix</a><ol><li><a href=#activation-visualization>Activation Visualization</a></li></ol></li></ol></nav></div></section></aside><main id=main-content class="main full-width" role=main aria-label="Main content"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/llm/>LLM
</a><a href=/categories/infra/>Infra
</a><a href=/categories/tutorial/>Tutorial</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/llm-memory-computation/>LLM Memory Computation</a></h2><h3 class=article-subtitle>本文中，我们将介绍如何计算 LLM 在训练和推理过程中的内存需求以及简要介绍对应的优化方法。</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>January 17, 2026</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>9 minute read</time></div></footer></div></header><section class=article-content><p>本文中，我们将介绍如何计算 LLM 在训练和推理过程中的内存需求以及简要介绍对应的优化方法。</p><h2 id=introduction><a href=#introduction class=header-anchor></a>Introduction</h2><p>我们在本文中回答的核心问题为：</p><blockquote><p>在训练和推理时 LLM 所需要的内存是多少？如何进行优化内存占用？</p></blockquote><p>为了回答这两个问题，我们需要回答以下问题：</p><ol><li>训练和推理时的内存由哪几部分组成？</li><li>训练和推理过程中哪个阶段是 memory-bound? 哪个阶段是 compute bound?</li><li>训练和推理过程中如何进行优化？</li></ol><p>我们将首先介绍如何计算 LLM 在训练阶段和推理阶段的内存。接下来，我们针对可优化部分进行分析以及介绍相应的优化算法。后续，我们将针对每部分的优化进行详细介绍</p><h2 id=background><a href=#background class=header-anchor></a>Background</h2><p>首先我们介绍一下使用的 notation, 这与之前参数量，FLOPs 计算使用的 notation 基本一致。需要注意的是，我们直接使用参数量 $P$ 这个记号，这部分在 <a class=link href=https://maosong.website/p/llm-parameter-computation/ target=_blank rel=noopener>LLM parameter analysis</a> 中已经进行了详细介绍，因此我们略过这部分。</p><div class=table-wrapper><table><thead><tr><th>variable</th><th>description</th></tr></thead><tbody><tr><td>$P$</td><td>number of parameters</td></tr><tr><td>$L$</td><td>layers</td></tr><tr><td>$V$</td><td>vocabulary size</td></tr><tr><td>$d$</td><td>hidden size</td></tr><tr><td>$d_{ff}$</td><td>FFN hidden size</td></tr><tr><td>$s$</td><td>sequence length</td></tr><tr><td>$b$</td><td>batch size</td></tr><tr><td>$h$</td><td>number of attention heads</td></tr><tr><td>$d_h$</td><td>attention head dimension</td></tr></tbody></table></div><h3 id=assumption><a href=#assumption class=header-anchor></a>Assumption</h3><ol><li>没有特别说明的话，我们使用 BF16/FP16 作为精度，此时每个参数需要 $2$ byte 来表示</li><li>不使用 dropout (现代大模型普遍没有 dropout)</li></ol><h2 id=computation><a href=#computation class=header-anchor></a>Computation</h2><h3 id=overview><a href=#overview class=header-anchor></a>Overview</h3><p>我们首先给出训练和推理阶段各部分的内存需求，然后我们给出详细的计算公式</p><div class=table-wrapper><table><thead><tr><th>component</th><th>训练</th><th>推理</th></tr></thead><tbody><tr><td>weights</td><td>Fixed</td><td>Fixed</td></tr><tr><td>optimizer states</td><td>Fixed and massive</td><td>0</td></tr><tr><td>gradients</td><td>Fixed</td><td>0</td></tr><tr><td>activations</td><td>Large (stored for backprop)</td><td>Tiny (discarded after use)</td></tr><tr><td>KV cache</td><td>0</td><td>Large (grows with sequence)</td></tr></tbody></table></div><h3 id=training><a href=#training class=header-anchor></a>Training</h3><p>LLM 训练阶段对的内存开销包含三部分</p>$$
\text{Memory}_{\text{train}} = \text{Memory}(\text{weight}) + \text{Memory}(\text{activation}) + \text{Memory}(\text{optimizer})+\text{Memory}(\text{gradient})
$$<h4 id=weights><a href=#weights class=header-anchor></a>Weights</h4><p>我们在前面已经介绍了如何计算大语言模型的参数量，这里我们就直接记为 $P$, 由于我们使用单精度，因此所需要的内存为 $2P$.</p><h4 id=activation><a href=#activation class=header-anchor></a>Activation</h4><p>激活值（activation）是前向传播过程中产生的中间张量，反向传播计算梯度时需复用这些张量，因此训练阶段需全程存储。我们用一个简单的例子来进行说明，假设我们有一层神经网络，定义为</p>$$
\begin{aligned}
\mathbf{z}_l &= W_l\mathbf{a}_{l-1}+b_l\\
\mathbf{a}_{l} &= \phi(\mathbf{z}_l)
\end{aligned}
$$<p>那么在反向传播过程中，我们有</p>$$
\frac{\partial \mathcal{L}}{\partial W_l} = \frac{\partial \mathcal{L}}{\partial \mathbf{z}_l}\frac{\partial \mathbf{z}_l}{\partial W_l}=\frac{\partial \mathcal{L}}{\partial \mathbf{z}_l} \mathbf{a}_{l-1}
$$<p>也就是说，在计算第 $l$ 层的参数对应的梯度时，我们需要知道对应的输入 $\mathbf{a}_{l-1}$.</p><p>接下来，我们通过计算图来分析 LLM 所需要的 activation</p><p><strong>Attention</strong>
Attention 的计算图如下所示</p><p><img src=/p/llm-memory-computation/Attention-computation-graph.png width=841 height=776 loading=lazy alt="Computation graph of attention" class=gallery-image data-flex-grow=108 data-flex-basis=260px></p><p>根据计算图，对应的 activation 为（注：这里我们不做任何优化，仅此理论上进行分析）：</p><ol><li>query, key, value projection: 共享输入，对应的 activation 大小为 $2bsd$.</li><li>$Q^TK$ : $Q$, $K$ 都需要保存，大小为 $4bsd$.</li><li>softmax: 需要保存 $2bhs^2$ 大小的输入</li><li>weighted sum of values: 两者都需要保存，前者大小为 $2bhs^2$, 后者大小为 $2bsd$</li><li>output projection layer: 需要保存输入，大小为 $2bsd$.</li></ol><p>因此 attention 部分总共需要 $\boxed{10sbd+4bhs^2}$.</p><p><strong>FFN</strong>
FFN 计算图如下所示</p><p><img src=/p/llm-memory-computation/FFN-computation-graph.png width=559 height=742 loading=lazy alt="FFN computation graph" class=gallery-image data-flex-grow=75 data-flex-basis=180px></p><p>根据计算图，对应的 activation （我们假设 MLP 是一个基于 SwiGLU 的 dense MLP, 其 hidden size $d_{ff}=8/3d$,）：</p><ol><li>MLP 的第一层输入大小为 $2sbd$,</li><li>MLP 的第二层输入大小为 $16/3sbd$,</li><li>SwiGLU 的输入为 $16/3sbd$</li></ol><p>因此总的 activation 大小为 $\boxed{18sbd}$.</p><p><strong>LayerNorm</strong>
LayerNorm 需要保存输入，大小为 $\boxed{2bsd}$.</p><p>以上三部分相加，我们就得到单一 transformer layer 所需要的 activation:</p>$$
\begin{aligned}
\mathrm{activation}(\mathrm{transformer}\_{\mathrm{block}})&=\mathrm{activation}(\mathrm{PerNorm})+\mathrm{activation}(\mathrm{Attention})+\mathrm{activation}(\mathrm{PostNorm})+\mathrm{activation}(\mathrm{FFN})\\
&= 2bsd + (10bsd+4bhs^2) + 2bsd + 18bsd\\
&= \boxed{bs(32d+4hs)}
\end{aligned}
$$<p><strong>output</strong>
output 部分的计算图如下所示</p><p><img src=/p/llm-memory-computation/output-computation-graph.png width=381 height=545 loading=lazy alt="Output computation graph" class=gallery-image data-flex-grow=69 data-flex-basis=167px></p><p>根据计算图，对应的 activation 为：</p><ol><li>normalization 的输入大小为大小为 $2sbd$</li><li><code>lm_head</code> 的输入大小为 $2sbd$</li><li>loss 的输入大小为 $2bsV$</li></ol><p>从而输出部分的 activation 大小为</p>$$
\mathrm{activation}(\mathrm{output}) = \mathrm{activation}(\mathrm{FinalNorm})+\mathrm{activation}(\mathrm{lm\ head})+\mathrm{activation}(\mathrm{Loss}) = \boxed{4bsd+2bsV}
$$<p>因此，总的 activation 为</p>$$
\begin{aligned}
\text{Memory}(\text{activation}) &= L*(\mathrm{transformer}\_{\mathrm{block}}) + \mathrm{activation}(\mathrm{output})\\
&= \boxed{Lsb(32d+4hs) +( 4bsd+2bsV)}
\end{aligned}
$$<h4 id=gradients--optimizer-states><a href=#gradients--optimizer-states class=header-anchor></a>Gradients & Optimizer States</h4><p>现代优化器一般会使用高阶近似以及混合精度训练来提高训练的效率，这部分高阶近似也需要考虑内存占用。</p><p><strong>Gradients</strong>
当 gradient 和 weight 精度一致时，对应的内存消耗一致，为 $\boxed{2P}$.</p><p><strong>Optimizer states</strong>
<a class=link href=https://maosong.website/p/notes-on-adamw/ target=_blank rel=noopener>AdamW</a> 优化器会保存一阶和二阶动量，以及一份 master weights, 精度一般为 FP32:</p><ol><li>FP32 master weights: $4P$</li><li>FP32 first-order momentum: $4P$</li><li>FP32 second-order momentum: $4P$</li></ol><p>因此优化器状态需要 $\boxed{12P}$ 内存。</p><p>对于其他优化器，我们也可以算出对应的内存需求，下表总结了 AdamW, bitsandbytes 和 SGD 三种 optimizer</p><div class=table-wrapper><table><thead><tr><th>optimizer</th><th>master weights (FP32)</th><th>momentum</th><th>variance</th><th>TOTAL</th></tr></thead><tbody><tr><td>AdamW</td><td>$4P$</td><td>$4P$</td><td>$4P$</td><td>$12P$</td></tr><tr><td>bitsandbytes</td><td>$4P$</td><td>$P$</td><td>$P$</td><td>$6P$</td></tr><tr><td>SGD</td><td>$4P$</td><td>$4P$</td><td>0</td><td>$8P$</td></tr></tbody></table></div><p>最终，训练阶段所需要的内存为</p>$$
\text{Memory}_{\text{train}} = 16P+bs(32dL+4hsL+4d+2V)
$$<p>下面我们展示 LLaMA 系列训练时不同部分的内存占比 (batch size=64, AdamW, GB)</p><div class=table-wrapper><table><thead><tr><th>Model</th><th>weights</th><th>gradients</th><th>optimizer_states</th><th>activations</th></tr></thead><tbody><tr><td>LLaMA-7B</td><td>12.55</td><td>12.55</td><td>75.31</td><td>1545.81</td></tr><tr><td>LLaMA-13B</td><td>24.24</td><td>24.24</td><td>145.46</td><td>2410.31</td></tr><tr><td>LLaMA-33B</td><td>60.59</td><td>60.59</td><td>363.54</td><td>4691.06</td></tr><tr><td>LLaMA-65B</td><td>121.60</td><td>121.60</td><td>729.62</td><td>7691.81</td></tr></tbody></table></div><h3 id=inference><a href=#inference class=header-anchor></a>Inference</h3><p>LLM 推理阶段对的开销包含三部分</p>$$
\text{Memory}_{\text{Inference}} = \text{Memory}(\text{weight}) + \text{Memory}(\text{activation}) + \text{Memory}(\text{KV cache})
$$<p>weight memory 的内存占用为 $\boxed{2P}$. activation 内存占用比较小，<a class=link href=https://blog.eleuther.ai/transformer-math/ target=_blank rel=noopener>transformer-math</a> 给出了一个经验值，即</p>$$
\text{Memory}(\text{activation})\approx 0.2*\text{Memory}(\text{weight})=0.4P
$$<p>该经验值适用于 batch size = 1 的自回归推理场景。weight 和 activation 这两部分开销只与模型本身有关，第三部分 KV cache 则与我们的生成内容长度相关，下面我们详细进行介绍</p><h4 id=key-value-cache><a href=#key-value-cache class=header-anchor></a>Key Value Cache</h4><p>Key Value Cache (KV Cache) 是 LLM 在推理过程中为了避免重复计算历史 token 对应的 key 和 value 而使用的一个<strong>空间换时间的缓存机制</strong>。</p><p>在 LLM 推理阶段，我们是 token-by-token 进行生成的，每次 attention 的计算都有如下形式</p>$$
\begin{aligned}
\mathbf{q_t} &= W_Q\mathbf{x_t}\\
\mathbf{k}_{:,t}&=W_K[\mathbf{x_1},\dots,\mathbf{x_t}]\\
\mathbf{v}_{:,t}&=W_V[\mathbf{x_1},\dots,\mathbf{x_t}]\\
\mathbf{o}_t&=\mathrm{Attn}(\mathbf{q_t},\mathbf{k}_{:,t}, \mathbf{v}_{:,t})=\sum_{i=1}^t \frac{\alpha_{t,i}}{\sum_{t,i}\alpha_{t,i}}\mathbf{v_i},\ \alpha_{t,i} = \exp\left(\frac{\mathbf{q_t}^T\mathbf{k}_{i}}{\sqrt{d_k}}\right)
\end{aligned}
$$<p>这里 $\mathbf{q_t}$ 是当前 token $\mathbf{x}_t$ 对应的 query, $\mathbf{k}_{:,t}$ 和 $\mathbf{v}_{:,t}$ 是历史 token $[\mathbf{x_1},\dots,\mathbf{x_t}]$ 对应的 key 和 value. 当我们处理下一个 token $\mathbf{x}_{t+1}$ 时， 对应的计算变成了</p>$$
\begin{aligned}
\mathbf{q_t} &= W_Q\mathbf{x_t}\\
\mathbf{k}_{:,t+1}&=W_K[\mathbf{x_1},\dots,\mathbf{x_t},\mathbf{x}_{t+1}]=[\boxed{\mathbf{k}_{:,t}},W_K\mathbf{x}_{t+1}]\\
\mathbf{v}_{:,t+1}&=W_V[\mathbf{x_1},\dots,\mathbf{x_t},\mathbf{x}_{t+1}]=[\boxed{\mathbf{v}_{:,t}},W_V\mathbf{x}_{t+1}]\\
\end{aligned}
$$<p>也就是说，我们每生成一个 token, 都要重新计算一次历史 token 对应的 key 和 value, 因此生成一个包含 $s$ 个 token 的 sequence 时，每个 token 都需要计算其前序 token 的 key 和 value, 其对应的计算量为</p>$$
\sum_{t=1}^s \mathcal{O}(t) = \mathcal{O}(s^2)
$$<p>因此，一个自然的想法就是缓存历史 token 对应的 key 和 value, 在生成新的 token 时，我们只需从内存中加载计算好的结果，然后计算当前 token 对应的值 $W_K\mathbf{x}_{t+1}$ 和 $W_V\mathbf{x}_{t+1}$ 即可，这就是 KV cache. 使用 KV cache 之后，我们每次生成新的 token 时，仅需要计算当前 token 对应的 key 和 value, 此时总的计算复杂度为 $\mathcal{O}(s)$, 对应的空间复杂度为 $\mathcal{O}(s)$. 也就是以空间换时间。</p><p>容易推导出一个基于 Multi-head attention LLM 的 KV cache 如下</p>$$
\text{Memory}(\text{KV cache}) = s \times 2 \times 2 \times L\times h \times d_h
$$<p>可以看到，KV Cache 占用不仅与模型配置有关，还与生成的 sequence length 有关，生成的 token 越多，KV Cache 这部分占用越高。</p><p>最终，推理阶段模型本身的内存占用为</p>$$
\text{Memory}_{\text{Inference}} = 2.4P+4sLhd_h
$$<p>我们还是以 LLaMA 系列为例，结果如下 (batch size=1, GB, 括号里为 sequence length)</p><div class=table-wrapper><table><thead><tr><th>Model</th><th>Weights</th><th>Activations</th><th>KV Cache (1024)</th><th>KV Cache (4096)</th><th>KV Cache (16384)</th><th>KV Cache (32768)</th><th>KV Cache (131072)</th></tr></thead><tbody><tr><td>LLaMA-7B</td><td>12.55</td><td>2.51</td><td>0.25</td><td>1.00</td><td>4.00</td><td>8.00</td><td>32.00</td></tr><tr><td>LLaMA-13B</td><td>24.24</td><td>4.85</td><td>0.39</td><td>1.56</td><td>6.25</td><td>12.50</td><td>50.00</td></tr><tr><td>LLaMA-33B</td><td>60.59</td><td>12.12</td><td>0.76</td><td>3.05</td><td>12.19</td><td>24.38</td><td>97.50</td></tr><tr><td>LLaMA-65B</td><td>121.60</td><td>24.32</td><td>1.25</td><td>5.00</td><td>20.00</td><td>40.00</td><td>160.00</td></tr></tbody></table></div><p>可以看到，随着输出长度增加，KV cache 的开销占比也逐渐了超过模型权重的内存占用。而实际中 KV cache 往往因 page granularity、padding 和 fragmentation 略高于理论值。</p><h3 id=summary><a href=#summary class=header-anchor></a>Summary</h3><p>我们将上面的结果汇总起来就得到下表的结果。</p><div class=table-wrapper><table><thead><tr><th>component</th><th>训练</th><th>推理</th></tr></thead><tbody><tr><td>weights</td><td>$2P$</td><td>$2P$</td></tr><tr><td>optimizer states</td><td>$12P$</td><td>0</td></tr><tr><td>gradients</td><td>$2P$</td><td>0</td></tr><tr><td>activations</td><td>$Lsb(32d+4hs) +( 4bsd+2bsV)$</td><td>$\sim 0.4P$</td></tr><tr><td>KV cache</td><td>0</td><td>$4sLhd_h$</td></tr><tr><td>TOTAL</td><td>$16P+bs(32dL+4hsL+4d+2V)$</td><td>$2.4P+4sLhd_h$</td></tr></tbody></table></div><h2 id=analysis--optimizations><a href=#analysis--optimizations class=header-anchor></a>Analysis & Optimizations</h2><p>接下来，我们将简单介绍一下如何优化训练和推理过程中的内存占用，我们将优化方法总结如下表所示。后面我们将一一进行详细介绍</p><div class=table-wrapper><table><thead><tr><th>Stage</th><th>methods</th></tr></thead><tbody><tr><td>training</td><td>- activation checkpointing<br>- flash attention<br>- Parallelism</td></tr><tr><td>inference</td><td>- KV Cache Optimization<br>- PagedAttention<br>- RadixAttention<br>- Attention mechanism</td></tr></tbody></table></div><h3 id=training-1><a href=#training-1 class=header-anchor></a>Training</h3><h4 id=mixed-precision-training><a href=#mixed-precision-training class=header-anchor></a>Mixed Precision Training</h4><p>混合精度训练的核心思想是计算量大的模块使用低精度，计算量小的模块使用高精度。细节见 Mixed precision training, 最近的 <a class=link href=https://maosong.website/p/notes-on-deepseek-v3/ target=_blank rel=noopener>DeepSeek-V3</a> 还进一步使用了 FP8 精度进行训练，大幅度提高了训练效率。</p><h4 id=data-parallelism><a href=#data-parallelism class=header-anchor></a>Data Parallelism</h4><p>第一个并行策略是数据并行 (data parallelism), 其基本思想是把模型复制到多个 GPU 上，并行处理数据，然后对 loss 进行求和再进行反向传播。现在最常使用的是微软提出的 ZeRO, 其核心思想为把 optimizer states, gradients, weights 分布到不同的 GPU 上，然后需要的时候再汇总到一起。ZeRO 根据切分的部分不同可以分为三种策略，如下图所示</p><p><img src=/p/llm-memory-computation/ZeRO-architecture.png width=1057 height=528 loading=lazy alt="Architecture of ZeRO" class=gallery-image data-flex-grow=200 data-flex-basis=480px></p><p>如上图所示，在 baseline 场景下，我们每个 GPU 上都保存有一份模型的 optimizer states, gradients, weights, 这就限制了 batch size, 进而降低了整体的计算效率。</p><p>ZeRO 的关键改进在于利用 GPU 可以互相通信的性质来将 tensor 存储在不同的 GPU 上，这时<strong>每个 GPU 上不再保存完整的复制，而是独特的一部分数据</strong>，在参与计算时，GPU 通过 all gather 来把数据汇总在一起，如下图所示</p><p><img src=/p/llm-memory-computation/GPU-all-gather.gif width=850 height=383 loading=lazy alt="All-gather of GPU (sourced from How to scale your model)" class=gallery-image data-flex-grow=221 data-flex-basis=532px></p><p>ZeRO1 只对 optimizer states 进行 shard, 因此其内存占用为</p>$$
\text{Memory}_{\text{train}} = \text{Memory}(\text{weight}) + \text{Memory}(\text{activation}) + \frac{\text{Memory}(\text{optimizer})}{\text{\# GPUs}}+\text{Memory}(\text{gradient})
$$<p>ZeRO2 在 ZeRO1 的基础上进一步对 gradient 也进行 shard, 其内存占用为</p>$$
\text{Memory}_{\text{train}} = \text{Memory}(\text{weight}) + \text{Memory}(\text{activation}) + \frac{\text{Memory}(\text{optimizer})+\text{Memory}(\text{gradient})}{\text{\# GPUs}}
$$<p>ZeRO3 在 ZeRO2 的基础上对 weight 也进行 shard, 其内存占用为</p>$$
\text{Memory}_{\text{train}} = \text{Memory}(\text{activation}) + \frac{\text{Memory}(\text{weight}) + \text{Memory}(\text{optimizer})+\text{Memory}(\text{gradient})}{\text{\# GPUs}}
$$<p>一般来说，我们比较少使用 ZeRO3, 因为其通信开销变为了原来的 1.5 倍。</p><h4 id=activation-checkpointing><a href=#activation-checkpointing class=header-anchor></a>Activation Checkpointing</h4><p>上一节我们介绍了使用 DP 来减少固定部分 (weight, optimizer states, gradients) 部分的占用，但实际上训练时占用部分更多的是 activation, 这部分内存占用会严重影响 batch size 的设置进而影响整体计算效率。我们对固定部分（与模型参数量相关）和非固定部分（与 batch size 相关）进行一个对比，结果如下所示</p><div class=table-wrapper><table><thead><tr><th>Metric</th><th>$d$</th><th>$b, s$</th></tr></thead><tbody><tr><td>weight</td><td>quadratic ($d^2$)</td><td>independent</td></tr><tr><td>activation</td><td>linear ($d$)</td><td>linear ($bs$)</td></tr></tbody></table></div><p>我们可以看到，虽然训练时 batch size 越大越好，但是由于 activation 也会随之增大，batch size 可能只能使用一个非常小的值。下图是 LLaMA 系列在 $b=64$ 时不同部分的内存占用：</p><p><img src=/p/llm-memory-computation/memory_usage_bs-64.png width=1200 height=600 loading=lazy alt="memory usage of different components (bs=64)" class=gallery-image data-flex-grow=200 data-flex-basis=480px></p><p>从图表可看出，LLaMA-65B 在 batch size=64 时，激活值占用内存超 80%，远高于权重 / 梯度 / 优化器状态，而且随着 batch size 增加，这个比例会进一步上升。</p><p>为了解决这个问题，我们一般会使用 <strong>activation checkpointing</strong> 方法，这个方法是一个通过重新计算中间激活值，来减少内存占用的方法。其核心思想在于用计算复杂度换空间复杂度。<a class=link href=https://arxiv.org/pdf/2205.05198 target=_blank rel=noopener>Reducing Activation Recomputation in Large Transformer Models</a> 给出了不同的 checkpointing 策略，需要的算力也不同相同，我们下表进行总结</p><div class=table-wrapper><table><thead><tr><th></th><th>No checkpointing</th><th>Selective checkpointing</th><th>full checkpointing</th></tr></thead><tbody><tr><td>description</td><td>stores everything needed</td><td>store states stagely (e.g., the input to each layer)</td><td>only store the input to the model</td></tr><tr><td>memory</td><td>very high ($\text{Memory}(\text{activation})$)</td><td>medium</td><td>very low $2bsd$</td></tr><tr><td>extra compute</td><td>None</td><td>medium</td><td>very high $2Pbs$</td></tr></tbody></table></div><p>一般来说我们会结合 model parallelism 和 selective checkpointing 来实现一个均衡</p><h4 id=model-parallelism><a href=#model-parallelism class=header-anchor></a>Model Parallelism</h4><p>与 DP 在数据维度上进行切分不同，model parallelism 通过对模型进行切分来提高内存使用效率。Model Parallelism 又可以分为 Pipeline Parallelism (PP) 和 Tensor Parallelisim (TP)</p><p>通过 PP 和 TP 我们可以将模型切分部署在多个 GPU 上进而减少内存占用，对应的计算方式为</p>$$
\text{Memory}(\text{weight};\text{parallelism}) = \frac{\text{Memory}(\text{weight})}{\text{PP degree}\times\text{TP degree}}
$$<p>实际情况中，我们还可以结合 ZeRO 以及 Model Paralelism, 我们根据 PP degree 和 TP degree 来决定 DP degree</p>$$
\text{DP degree} = \frac{\text{\# GPUs}}{\text{PP degree}\times\text{TP degree}}
$$<p>最终，我们把以上优化技巧汇总起来就得到 (假设我们采用 ZeRO1 和 Model Parallelism)</p>$$
\text{Memory}_{\text{train}} \approx \frac{\text{Memory}(\text{weight})}{\text{PP degree}\times\text{TP degree}} + \frac{\text{Memory}(\text{activation})}{\text{TP degree}} + \frac{\text{Memory}(\text{optimizer})}{\text{\# GPUs}}+\frac{\text{Memory}(\text{gradient})}{\text{PP degree}}
$$<p>这里> activation 中 <strong>被 tensor-parallel 的部分</strong> 按 TP degree 缩减。</p><p>关于 Parallelism 的具体细节见 Parallelism tutorial</p><h4 id=flash-attention><a href=#flash-attention class=header-anchor></a>Flash Attention</h4><p>在前面的分析中，我们给出了 attention softmax 这一部分的 activation 为 $2bhs^2$ 而 flashattention 通过 tiling 和 online-softmax 降低了这一部分的内存占用，进而提高整体的效率。</p><p>具体细节见 <a class=link href=https://maosong.website/p/notes-on-flashattention/ target=_blank rel=noopener>flash attention</a></p><h3 id=inference-1><a href=#inference-1 class=header-anchor></a>Inference</h3><h4 id=quantization><a href=#quantization class=header-anchor></a>Quantization</h4><p>quantization 是用低精度加载模型权重从而降低推理阶段模型参数内存占用的一个方法。比如说原始模型使用了 BF16 精度，那么我们可以通过使用 int8 量化来将模型权重对应的内存从 $2P$ 降低到 $P$. 现在一些模型还会在训练阶段就加入 quantization, 比如 quantization aware training 以及 post-training quantization 等。这部分细节可以参考 <a class=link href=https://arxiv.org/pdf/2312.03863 target=_blank rel=noopener>Efficient Large Language Models: A Survey</a></p><h4 id=kv-cache-optimization><a href=#kv-cache-optimization class=header-anchor></a>KV Cache Optimization</h4><p>我们在前面已经介绍了 KV cache 可以通过以空间换时间来提高计算效率，但是随着输出长度增加，对应的 KV cache 也会越来越大，因此目前有相当一部分工作旨在降低 KV cache 占用，比如 KV Cache compression, quantization 等。这部分细节可以参考 <a class=link href=https://arxiv.org/pdf/2412.19442 target=_blank rel=noopener>A Survey on Large Language Model Acceleration based on KV Cache Management</a></p><h4 id=attention><a href=#attention class=header-anchor></a>Attention</h4><p>实际上，相当一部分工作都是通过优化 attention 来降低</p><h4 id=inference-framework><a href=#inference-framework class=header-anchor></a>Inference Framework</h4><p>现在也有一些推理框架专注于提高 LLM 的推理效率，下面是两个比较流行的推理框架</p><ul><li>SGLang: 定制化强，适用于复杂任务如 RL 推理等</li><li>vLLM: 简单高效</li></ul><p>对应的轻量化推理框架为</p><ul><li>nano-vLLM</li><li>mini-SGLang</li></ul><p>这部分</p><h2 id=conclusion><a href=#conclusion class=header-anchor></a>Conclusion</h2><p>在本文中，我们详细介绍了 LLM 在训练和推理阶段的内存占用开销以及简要介绍了对应的优化方法。关键结论为：</p><ul><li>训练阶段内存核心瓶颈是激活值（随 batch size / 序列长度线性增长），推理阶段核心瓶颈是 KV Cache（随序列长度增长）；</li><li>训练优化优先通过 ZeRO（多卡）+ activation checkpointing（单卡）降低内存，推理优化优先通过 KV Cache 优化 + 量化降低内存；</li><li>所有内存计算均为理论值，实际落地需考虑显存碎片、硬件特性、通信开销等工程因素。</li></ul><p>需要注意的是，所有内存计算均为理论值，实际落地需考虑显存碎片、硬件特性、通信开销等工程因素。下一步，我们将分别针对不同的优化方法来进行展开并详细介绍。</p><h2 id=references><a href=#references class=header-anchor></a>References</h2><ul><li><a class=link href=https://blog.eleuther.ai/transformer-math/ target=_blank rel=noopener>transformer-math</a></li><li><a class=link href=https://kipp.ly/transformer-inference-arithmetic/ target=_blank rel=noopener>transformer inference arithmetic</a></li><li><a class=link href=https://zhuanlan.zhihu.com/p/687226668 target=_blank rel=noopener>https://zhuanlan.zhihu.com/p/687226668</a></li><li><a class=link href=https://arxiv.org/pdf/2205.05198 target=_blank rel=noopener>Reducing Activation Recomputation in Large Transformer Models</a></li><li><a class=link href=https://blog.eleuther.ai/transformer-math/ target=_blank rel=noopener>https://blog.eleuther.ai/transformer-math/</a></li><li><a class=link href=https://arxiv.org/pdf/2412.19442 target=_blank rel=noopener>A Survey on Large Language Model Acceleration based on KV Cache Management</a></li><li><a class=link href=https://arxiv.org/pdf/2312.03863 target=_blank rel=noopener>Efficient Large Language Models: A Survey</a></li></ul><h2 id=appendix><a href=#appendix class=header-anchor></a>Appendix</h2><h3 id=activation-visualization><a href=#activation-visualization class=header-anchor></a>Activation Visualization</h3><p>LLaMA 系列的配置如下表所示</p><div class=table-wrapper><table><thead><tr><th>Model</th><th>s</th><th>V</th><th>L</th><th>d</th><th>d_ff</th><th>h</th><th>h_d</th><th>P</th></tr></thead><tbody><tr><td>LLaMA-7B</td><td>2048</td><td>32000</td><td>32</td><td>4096</td><td>11008</td><td>32</td><td>128</td><td>6738411520</td></tr><tr><td>LLaMA-13B</td><td>2048</td><td>32000</td><td>40</td><td>5120</td><td>13824</td><td>40</td><td>128</td><td>13015859200</td></tr><tr><td>LLaMA-33B</td><td>2048</td><td>32000</td><td>60</td><td>6656</td><td>17920</td><td>52</td><td>128</td><td>32528936960</td></tr><tr><td>LLaMA-65B</td><td>2048</td><td>32000</td><td>80</td><td>8192</td><td>22016</td><td>64</td><td>128</td><td>65285652480</td></tr></tbody></table></div><p>对应的可视化代码如下</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>compute_memory</span><span class=p>(</span><span class=n>L</span><span class=p>,</span> <span class=n>d</span><span class=p>,</span> <span class=n>h</span><span class=p>,</span> <span class=n>h_d</span><span class=p>,</span> <span class=n>V</span><span class=p>,</span> <span class=n>s</span><span class=p>,</span> <span class=n>P</span><span class=p>,</span> <span class=n>b</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>weights</span> <span class=o>=</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>P</span>
</span></span><span class=line><span class=cl>    <span class=n>gradients</span> <span class=o>=</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>P</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer_states</span> <span class=o>=</span> <span class=mi>12</span> <span class=o>*</span> <span class=n>P</span>
</span></span><span class=line><span class=cl>    <span class=n>activations</span> <span class=o>=</span> <span class=n>L</span><span class=o>*</span><span class=n>s</span><span class=o>*</span><span class=n>b</span><span class=o>*</span><span class=p>(</span><span class=mi>32</span> <span class=o>*</span> <span class=n>d</span> <span class=o>+</span> <span class=mi>4</span> <span class=o>*</span> <span class=n>h</span> <span class=o>*</span> <span class=n>s</span><span class=p>)</span> <span class=o>+</span> <span class=p>(</span><span class=mi>4</span> <span class=o>*</span> <span class=n>b</span> <span class=o>*</span> <span class=n>s</span> <span class=o>*</span> <span class=n>d</span> <span class=o>+</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>b</span> <span class=o>*</span> <span class=n>s</span> <span class=o>*</span> <span class=n>V</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;weights&#34;</span><span class=p>:</span> <span class=n>weights</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;gradients&#34;</span><span class=p>:</span> <span class=n>gradients</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;optimizer_states&#34;</span><span class=p>:</span> <span class=n>optimizer_states</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;activations&#34;</span><span class=p>:</span> <span class=n>activations</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>b</span> <span class=o>=</span> <span class=mi>64</span>  <span class=c1># batch size for memory calculation</span>
</span></span><span class=line><span class=cl><span class=n>memory_data</span> <span class=o>=</span> <span class=p>{}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>model</span><span class=p>,</span> <span class=n>params</span> <span class=ow>in</span> <span class=n>models</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>memory</span> <span class=o>=</span> <span class=n>compute_memory</span><span class=p>(</span><span class=n>params</span><span class=p>[</span><span class=s2>&#34;L&#34;</span><span class=p>],</span> <span class=n>params</span><span class=p>[</span><span class=s2>&#34;d&#34;</span><span class=p>],</span> <span class=n>params</span><span class=p>[</span><span class=s2>&#34;h&#34;</span><span class=p>],</span> <span class=n>params</span><span class=p>[</span><span class=s2>&#34;h_d&#34;</span><span class=p>],</span> <span class=n>params</span><span class=p>[</span><span class=s2>&#34;V&#34;</span><span class=p>],</span> <span class=n>params</span><span class=p>[</span><span class=s2>&#34;s&#34;</span><span class=p>],</span> <span class=n>params</span><span class=p>[</span><span class=s2>&#34;P&#34;</span><span class=p>],</span> <span class=n>b</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>memory_data</span><span class=p>[</span><span class=n>model</span><span class=p>]</span> <span class=o>=</span> <span class=n>memory</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>fig</span><span class=p>,</span> <span class=n>ax</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>12</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model_names</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>memory_data</span><span class=o>.</span><span class=n>keys</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=n>GB</span> <span class=o>=</span> <span class=mi>1024</span> <span class=o>**</span> <span class=mi>3</span>  <span class=c1># 1 GB in bytes</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>weights</span> <span class=o>=</span> <span class=p>[</span><span class=n>memory_data</span><span class=p>[</span><span class=n>m</span><span class=p>][</span><span class=s2>&#34;weights&#34;</span><span class=p>]</span> <span class=o>/</span> <span class=n>GB</span> <span class=k>for</span> <span class=n>m</span> <span class=ow>in</span> <span class=n>model_names</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>gradients</span> <span class=o>=</span> <span class=p>[</span><span class=n>memory_data</span><span class=p>[</span><span class=n>m</span><span class=p>][</span><span class=s2>&#34;gradients&#34;</span><span class=p>]</span> <span class=o>/</span> <span class=n>GB</span> <span class=k>for</span> <span class=n>m</span> <span class=ow>in</span> <span class=n>model_names</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>optimizer_states</span> <span class=o>=</span> <span class=p>[</span><span class=n>memory_data</span><span class=p>[</span><span class=n>m</span><span class=p>][</span><span class=s2>&#34;optimizer_states&#34;</span><span class=p>]</span> <span class=o>/</span> <span class=n>GB</span> <span class=k>for</span> <span class=n>m</span> <span class=ow>in</span> <span class=n>model_names</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>activations</span> <span class=o>=</span> <span class=p>[</span><span class=n>memory_data</span><span class=p>[</span><span class=n>m</span><span class=p>][</span><span class=s2>&#34;activations&#34;</span><span class=p>]</span> <span class=o>/</span> <span class=n>GB</span> <span class=k>for</span> <span class=n>m</span> <span class=ow>in</span> <span class=n>model_names</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>model_names</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>width</span> <span class=o>=</span> <span class=mf>0.6</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Stacked bar chart</span>
</span></span><span class=line><span class=cl><span class=n>p1</span> <span class=o>=</span> <span class=n>ax</span><span class=o>.</span><span class=n>bar</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>weights</span><span class=p>,</span> <span class=n>width</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Weights&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>p2</span> <span class=o>=</span> <span class=n>ax</span><span class=o>.</span><span class=n>bar</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>gradients</span><span class=p>,</span> <span class=n>width</span><span class=p>,</span> <span class=n>bottom</span><span class=o>=</span><span class=n>weights</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Gradients&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>p3</span> <span class=o>=</span> <span class=n>ax</span><span class=o>.</span><span class=n>bar</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>optimizer_states</span><span class=p>,</span> <span class=n>width</span><span class=p>,</span> <span class=n>bottom</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>weights</span><span class=p>)</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>gradients</span><span class=p>),</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Optimizer States&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>p4</span> <span class=o>=</span> <span class=n>ax</span><span class=o>.</span><span class=n>bar</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>activations</span><span class=p>,</span> <span class=n>width</span><span class=p>,</span> <span class=n>bottom</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>weights</span><span class=p>)</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>gradients</span><span class=p>)</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>optimizer_states</span><span class=p>),</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Activations&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s1>&#39;Model&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;Memory (GB)&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;Memory Usage Breakdown for LLaMA Series (batch size=</span><span class=si>{</span><span class=n>b</span><span class=si>}</span><span class=s1>)&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_xticks</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_xticklabels</span><span class=p>(</span><span class=n>model_names</span><span class=p>,</span> <span class=n>rotation</span><span class=o>=</span><span class=mi>45</span><span class=p>,</span> <span class=n>ha</span><span class=o>=</span><span class=s1>&#39;right&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=s1>&#39;y&#39;</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div></section><footer class=article-footer><section class=article-tags><a href=/tags/transformer/>Transformer</a>
<a href=/tags/training/>Training</a>
<a href=/tags/inference/>Inference</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section><section class=article-lastmod><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>Last updated on January 24, 2026 at 5:06 PM</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".main-article");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/llm-flops-computation/><div class=article-details><h2 class=article-title>LLM FLOPs Computation</h2></div></a></article><article><a href=/p/llm-parameter-computation/><div class=article-details><h2 class=article-title>LLM Parameter Computation</h2></div></a></article><article><a href=/p/megatron-lm/><div class=article-details><h2 class=article-title>megatron-lm</h2></div></a></article><article><a href=/p/moe-tutorial/><div class=article-details><h2 class=article-title>MoE tutorial</h2></div></a></article><article><a href=/p/load-balancing-tutorial/><div class=article-details><h2 class=article-title>Load Balancing tutorial</h2></div></a></article></div></div></aside><script src=https://utteranc.es/client.js repo=MaoSong2022/MaoSong2022.github.io issue-term=pathname crossorigin=anonymous async></script><style>.utterances{max-width:unset}</style><script>let utterancesLoaded=!1;function setUtterancesTheme(e){let t=document.querySelector(".utterances iframe");t&&t.contentWindow.postMessage({type:"set-theme",theme:`github-${e}`},"https://utteranc.es")}addEventListener("message",e=>{if(e.origin!=="https://utteranc.es")return;utterancesLoaded=!0,setUtterancesTheme(document.documentElement.dataset.scheme)}),window.addEventListener("onColorSchemeChange",e=>{if(!utterancesLoaded)return;setUtterancesTheme(e.detail)})</script><footer class=site-footer><section class=copyright>&copy;
2020 -
2026 Mao Song(毛松)'s Homepage</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.31.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>