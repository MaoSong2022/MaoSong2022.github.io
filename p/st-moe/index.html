<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="AllenAI 在 24 年 9 月提出了 olmoe, 一个全开源的基于 MoE 架构的大语言模型，参数量为 7B-A1B，作者详细介绍了模型的设计，数据以及训练策略. 论文获得了ICLR2025 oral"><title>ST-MoE</title>
<link rel=canonical href=https://maosong2022.github.io/p/st-moe/><link rel=stylesheet href=/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css><meta property='og:title' content="ST-MoE"><meta property='og:description' content="AllenAI 在 24 年 9 月提出了 olmoe, 一个全开源的基于 MoE 架构的大语言模型，参数量为 7B-A1B，作者详细介绍了模型的设计，数据以及训练策略. 论文获得了ICLR2025 oral"><meta property='og:url' content='https://maosong2022.github.io/p/st-moe/'><meta property='og:site_name' content="Mao Song(毛松)'s Homepage"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='MoE'><meta property='article:tag' content='Google'><meta property='article:published_time' content='2025-10-29T11:19:37+08:00'><meta property='article:modified_time' content='2025-10-29T11:19:37+08:00'><meta name=twitter:title content="ST-MoE"><meta name=twitter:description content="AllenAI 在 24 年 9 月提出了 olmoe, 一个全开源的基于 MoE 架构的大语言模型，参数量为 7B-A1B，作者详细介绍了模型的设计，数据以及训练策略. 论文获得了ICLR2025 oral"><link rel="shortcut icon" href=/favicon.png></head><script async src=https://scripts.simpleanalyticscdn.com/latest.js></script><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu9788782091159651930.jpg width=300 height=240 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>🍥</span></figure><div class=site-meta><h1 class=site-name><a href=/>Mao Song(毛松)'s Homepage</a></h1><h2 class=site-description>Never stop learning.</h2></div></header><ol class=menu-social><li><a href=https://b23.tv/a0Bb9Z1 target=_blank title=bilibili rel=me><svg role="img" viewBox="0 0 24 24"><title>Bilibili</title><path d="M17.813 4.653h.854c1.51.054 2.769.578 3.773 1.574 1.004.995 1.524 2.249 1.56 3.76v7.36c-.036 1.51-.556 2.769-1.56 3.773s-2.262 1.524-3.773 1.56H5.333c-1.51-.036-2.769-.556-3.773-1.56S.036 18.858.0 17.347v-7.36c.036-1.511.556-2.765 1.56-3.76 1.004-.996 2.262-1.52 3.773-1.574h.774l-1.174-1.12a1.234 1.234.0 01-.373-.906c0-.356.124-.658.373-.907l.027-.027c.267-.249.573-.373.92-.373s.653.124.92.373L9.653 4.44c.071.071.134.142.187.213h4.267a.836.836.0 01.16-.213l2.853-2.747c.267-.249.573-.373.92-.373s.662.151.929.4.391.551.391.907c0 .355-.124.657-.373.906zM5.333 7.24c-.746.018-1.373.276-1.88.773-.506.498-.769 1.13-.786 1.894v7.52c.017.764.28 1.395.786 1.893.507.498 1.134.756 1.88.773h13.334c.746-.017 1.373-.275 1.88-.773.506-.498.769-1.129.786-1.893v-7.52c-.017-.765-.28-1.396-.786-1.894-.507-.497-1.134-.755-1.88-.773zM8 11.107c.373.0.684.124.933.373.25.249.383.569.4.96v1.173c-.017.391-.15.711-.4.96-.249.25-.56.374-.933.374s-.684-.125-.933-.374c-.25-.249-.383-.569-.4-.96V12.44c0-.373.129-.689.386-.947.258-.257.574-.386.947-.386zm8 0c.373.0.684.124.933.373.25.249.383.569.4.96v1.173c-.017.391-.15.711-.4.96-.249.25-.56.374-.933.374s-.684-.125-.933-.374c-.25-.249-.383-.569-.4-.96V12.44c.017-.391.15-.711.4-.96.249-.249.56-.373.933-.373z"/></svg></a></li><li><a href=https://github.com/MaoSong2022 target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href='https://scholar.google.com/citations?user=BaqGkQQAAAAJ' target=_blank title="Google Scholar" rel=me><svg role="img" viewBox="0 0 24 24"><title>Google Scholar</title><path d="M5.242 13.769.0 9.5 12 0l12 9.5-5.242 4.269C17.548 11.249 14.978 9.5 12 9.5c-2.977.0-5.548 1.748-6.758 4.269zM12 10a7 7 0 100 14 7 7 0 000-14z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/about/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>About</span></a></li><li><a href=/tags/><svg class="icon icon-tabler icon-tabler-tag" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11 3l9 9a1.5 1.5.0 010 2l-6 6a1.5 1.5.0 01-2 0L3 11V7a4 4 0 014-4h4"/><circle cx="9" cy="9" r="2"/></svg>
<span>Tags</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#introduction>Introduction</a></li><li><a href=#training-stability>Training Stability</a><ol><li><a href=#multiplicative-components>Multiplicative Components</a></li><li><a href=#adding-noise>Adding Noise</a></li><li><a href=#constraining-activations>Constraining Activations</a></li><li><a href=#numerical-precision>Numerical Precision</a></li></ol></li><li><a href=#fine-tuning>Fine-tuning</a></li><li><a href=#design-sparse-models>Design Sparse Models</a></li><li><a href=#tracing-tokens-through-the-model>Tracing Tokens through the Model</a></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#references>References</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/llm/>LLM</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/st-moe/>ST-MoE</a></h2><h3 class=article-subtitle>AllenAI 在 24 年 9 月提出了 olmoe, 一个全开源的基于 MoE 架构的大语言模型，参数量为 7B-A1B，作者详细介绍了模型的设计，数据以及训练策略. 论文获得了ICLR2025 oral</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Nov 01, 2025</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>4 minute read</time></div></footer></div></header><section class=article-content><h2 id=introduction>Introduction</h2><p>已有的工作如 <a class=link href=https://maosong.website/p/GShard.md target=_blank rel=noopener>GShard</a> 和 <a class=link href=https://maosong.website/p/switch-transformer/ target=_blank rel=noopener>Switch Transformer</a> 均验证了 MoE 模型的有效性，但是他们的问题在于训练不稳定。</p><p>因此，在本文中，作者就提出了 ST-MoE 来解决这个问题。作者的主要贡献如下：</p><ol><li>探究了如何平衡模型的表现与训练稳定性</li><li>提出了 router Z-loss 来解决训练的不稳定性</li><li>探究了如何设定 MoE 模型 fine tuning 时的超参数</li><li>对 MoE 的性质进行了分析</li></ol><h2 id=training-stability>Training Stability</h2><p>作者发现，直接训练 MoE 模型会面临训练不稳定性的问题，实验结果如下图所示</p><p><img src=/p/st-moe/ST-MoE-training-instabilities.png width=1040 height=420 srcset="/p/st-moe/ST-MoE-training-instabilities_hu7326905665646279246.png 480w, /p/st-moe/ST-MoE-training-instabilities_hu5976367835112795528.png 1024w" loading=lazy alt="Training instabilities for sparse models." class=gallery-image data-flex-grow=247 data-flex-basis=594px></p><p>可以看到不同的实验，一次训练崩溃，一次训练正常。</p><p>作者接下来探究了如何提高训练的稳定性，作者有三点发现：</p><ol><li>大多数方法都可以提高训练稳定性，但是也会是的模型表现更差</li><li>router Z-loss 可以在不损失模型表现的情况下提高训练的稳定性</li><li>multiplicative components 如 RMSNorm 等可以提高模型表现，但是训练稳定性更差</li></ol><p>作者构建了一个 baseline 的 MoE 模型，总专家个数为 $N=32$, 激活专家个数为 $K=2$, Transformer block 按照 4 个 block 为一组，每组里包含一个 MoE layer. 为了避免初始化带来的误差，作者使用 6 个随机数种子进行训练。</p><h3 id=multiplicative-components>Multiplicative Components</h3><p>multiplicative components 指的是 GEGLU 和 RMSNorm 这些操作，其实验结果如下所示</p><div class=table-wrapper><table><thead><tr><th>Method</th><th>Fraction Stable</th><th>Quality</th></tr></thead><tbody><tr><td>Baseline</td><td>4/6</td><td>$-1.755 \pm0.02$</td></tr><tr><td>Remove GEGLU</td><td>3/3</td><td>$-1.849 \pm0.02$</td></tr><tr><td>Remove RMS Norm. Scale Param</td><td>3/3</td><td>$-2.020 \pm0.06$</td></tr></tbody></table></div><p>结果发现，移除掉 multiplicative components 之后，模型训练变得稳定，但是效果也变差了。</p><h3 id=adding-noise>Adding Noise</h3><p>接下来作者尝试了 <a class=link href=https://maosong.website/p/switch-transformer/ target=_blank rel=noopener>Switch Transformer</a> 中的 jitter noise 和 dropoout 方法，实验结果如下图所示</p><div class=table-wrapper><table><thead><tr><th>Method</th><th>Fraction Stable</th><th>Quality</th></tr></thead><tbody><tr><td>Baseline</td><td>4/6</td><td>$-1.755 ± 0.02$</td></tr><tr><td>Input jitter ($10^{-2}$)</td><td>3/3</td><td>$-1.777 \pm 0.03$</td></tr><tr><td>Dropout (0.1)</td><td>3/3</td><td>$-1.822 \pm0.11$</td></tr></tbody></table></div><p>实验结果显示，加入噪声对模型的表现存在负面影响。</p><h3 id=constraining-activations>Constraining Activations</h3><p>作者接下来分析了以下 <a class=link href=https://maosong.website/p/switch-transformer/ target=_blank rel=noopener>Switch Transformer</a> 中 router 存在的问题，作者发现尽管在 router 中使用 <code>float32</code> 可以提高训练稳定性，但是这还不够，因此作者使用了如下的 router Z-loss:</p>$$
\mathcal{L}_z(x) = \frac1B\sum_{i=1}^B\left(\log\sum_{j=1}^N e^{x_j^{(i)}}\right)^2
$$<p>其中 $B, N$ 分别是 batch size 和专家个数，$x_j^{(i)}$ 代表了第 $j$ 个专家对 $i$ 个 token 的激活 logits. 通过增加这个 loss, 我们可以让 routing layer 的 logits 都在一个合理的范围内。</p><div class=table-wrapper><table><thead><tr><th>Method</th><th>Fraction Stable</th><th>Quality ($\uparrow$)</th><th></th></tr></thead><tbody><tr><td>Baseline</td><td>$4/6$</td><td>$-1.755 \pm 0.02$</td><td></td></tr><tr><td>Update clipping (clip = 0.1)</td><td>$3/3$</td><td>$-4.206 \pm 0.17$</td><td></td></tr><tr><td>Router Z-Loss</td><td>$3/3$</td><td>$-1.741 \pm 0.02$</td><td></td></tr></tbody></table></div><p>可以看到，加入 router Z-loss 之后，模型的稳定性和表现都有了提升。</p><p>因此，在本文中，作者使用的损失函数为</p>$$
\mathcal{L} = \mathcal{L}_{CE} + \alpha \mathcal{L}_B + \beta \mathcal{L}_z
$$<p>其中 $\mathcal{L}_{CE}, \mathcal{L}_B$ 分别是 cross-entropy loss 和 Load Balancing loss, $\alpha,\beta$ 是对应 loss 的权重。</p><h3 id=numerical-precision>Numerical Precision</h3><p>接下来作者探究了数值精度对训练效率和稳定性的影响，作者发现使用低精度进行训练的优势有：</p><ol><li>通信开销更小</li><li>计算消耗更小</li><li>内存需求更小</li></ol><p>但是低精度进行训练的问题是存在严重的 roundoff error, 其示意图如下所示</p><p><img src=/p/st-moe/ST-MoE-numerical-precision-and-roundoff.png width=925 height=249 srcset="/p/st-moe/ST-MoE-numerical-precision-and-roundoff_hu1402581372307421845.png 480w, /p/st-moe/ST-MoE-numerical-precision-and-roundoff_hu14172820709588307084.png 1024w" loading=lazy alt="Numerical precision formats and roundoff errors." class=gallery-image data-flex-grow=371 data-flex-basis=891px></p><h2 id=fine-tuning>Fine-tuning</h2><p>作者在本节探究了如何 fine-tune 一个 MoE 模型。</p><p>作者首先通过实验给出了 MoE 模型容易过拟合的结论，实验结果如下图所示</p><p><img src=/p/st-moe/ST-MoE-overfitting-of-MoE.png width=1138 height=456 srcset="/p/st-moe/ST-MoE-overfitting-of-MoE_hu15053576913876449027.png 480w, /p/st-moe/ST-MoE-overfitting-of-MoE_hu11759343686158783167.png 1024w" loading=lazy alt="Sparse models are prone to overfit." class=gallery-image data-flex-grow=249 data-flex-basis=598px></p><p>可以看到，在两个人呢误伤，MoE 模型相比于 dense 模型都更容易过拟合。</p><p>接下来作者探究了超参数特别是 batch size 和 learning rate 对模型表现的影响，结果如下图所示</p><p><img src=/p/st-moe/ST-MoE-hyperparameter-sensitivity.png width=1036 height=416 srcset="/p/st-moe/ST-MoE-hyperparameter-sensitivity_hu2550488555205211608.png 480w, /p/st-moe/ST-MoE-hyperparameter-sensitivity_hu9603778225196677401.png 1024w" loading=lazy alt="Batch size and learning rate sensitivity" class=gallery-image data-flex-grow=249 data-flex-basis=597px></p><p>可以看到，与 dense 模型相反，MoE 模型需要更大的 batch size 以及更小的 learning rate.</p><h2 id=design-sparse-models>Design Sparse Models</h2><p>首先作者分析了专家个数 $N$ 的设置，作者认为当专家个数超过 $256$ 之后，带来的收益就微乎其微了。并且，当专家个数增加之后，虽然算力没有变化，但是通信开销以及计算优化会变得更加困难。作者还认为，在 TPU 上，每个 core 应该进放置一个 expert</p><p>作者还对 capacity factor 进行了实验，实验结果显示，提升 capacity factor 可以提高模型的表现。但是同时也会带来计算开销，从而让模型训练更慢，实验结果如下图所示</p><div class=table-wrapper><table><thead><tr><th>Model</th><th>Train CF</th><th>Step Time (s) ($\downarrow$)</th><th></th></tr></thead><tbody><tr><td>ST-MoE-L</td><td>1.25</td><td>$2.397$</td><td></td></tr><tr><td>ST-MoE-L</td><td>2.0</td><td>$2.447 (+7%)$</td><td></td></tr><tr><td>ST-MoE-32B</td><td>1.25</td><td>$4.244$</td><td></td></tr><tr><td>ST-MoE-32B</td><td>2.0</td><td>$4.819 (+14%)$</td><td></td></tr></tbody></table></div><p>最终结论如下：</p><blockquote><ol><li>使用 top-2 routing, 即激活 $K=2$ 个专家，capacity factor 设置为 $1.25$, 每个 core 上最多放置一个专家</li><li>在评估时可以动态调整 capacity factor</li></ol></blockquote><h2 id=tracing-tokens-through-the-model>Tracing Tokens through the Model</h2><p>在这一节里，作者分析了 MoE 模型专家的性质。</p><p>首先，作者发现，每一层里，至少有一个专家对于 sentinel token 比较敏感。并且，encoder 里的专家 specialization 更强，比如某些专家会负责 punctuation, verbs, proper names 等</p><p>接下来，作者发现，专家的 specialization 在 decoder 里几乎没有，作者认为这是因为 target token distribution 不同导致的，即：</p><ol><li>decoding 是只有一小部分 token 被一个专家处理</li><li>decoding 过程中大部分 token 都是 sentinel token</li></ol><p>因此，每个 group 相比于 encoder 来说通常只 cover 一小部分的 semantic space</p><p>encoder 和 decoder 专家的 specialization 实验结果如下</p><div class=table-wrapper><table><thead><tr><th></th><th>Layer 1</th><th>Layer 2</th><th>Layer 3</th><th>Layer 4</th><th>Layer 5</th><th>Layer 6</th><th>Uniform (32-experts)</th></tr></thead><tbody><tr><td>Encoder</td><td>2.2</td><td>1.8</td><td>1.6</td><td>1.7</td><td>1.7</td><td>1.2</td><td>3.5</td></tr><tr><td>Decoder</td><td>3.4</td><td>3.4</td><td>3.4</td><td>3.4</td><td>3.4</td><td>3.4</td><td>3.5</td></tr></tbody></table></div><p>可以看到，decoder 的专家分布和均匀分布差不多，这说明 decoder 里的专家 specialization 更弱。</p><p>作者还探究了不同专家对于不同语言的敏感程度，结果发现，专家对于不同语言并没有出现明显的 specialization 现象，作者分析原因认为，输入一般只含有一种语言，因此各个专家均需要去处理对应语言的 token, 这就导致了专家对于不同语种的数据处理基本上没有太大差别。</p><h2 id=conclusion>Conclusion</h2><p>在本文中，作者提出了 ST-MoE 系列 MoE 大语言模型，作者通过实验优化了 MoE 模型训练的不稳定性问题。</p><h2 id=references>References</h2><ul><li><a class=link href=http://arxiv.org/abs/2202.08906 target=_blank rel=noopener>Arxiv</a></li></ul></section><footer class=article-footer><section class=article-tags><a href=/tags/moe/>MoE</a>
<a href=/tags/google/>Google</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".main-article");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/gshard/><div class=article-details><h2 class=article-title>GShard</h2></div></a></article><article><a href=/p/switch-transformer/><div class=article-details><h2 class=article-title>Switch Transformer</h2></div></a></article><article><a href=/p/mixstral-8x7b/><div class=article-details><h2 class=article-title>Mixstral 8x7B</h2></div></a></article><article><a href=/p/st-moe/><div class=article-details><h2 class=article-title>ST-MoE</h2></div></a></article><article><a href=/p/moe-tutorial/><div class=article-details><h2 class=article-title>MoE tutorial</h2></div></a></article></div></div></aside><script src=https://utteranc.es/client.js repo=MaoSong2022/MaoSong2022.github.io issue-term=pathname crossorigin=anonymous async></script><style>.utterances{max-width:unset}</style><script>let utterancesLoaded=!1;function setUtterancesTheme(e){let t=document.querySelector(".utterances iframe");t&&t.contentWindow.postMessage({type:"set-theme",theme:`github-${e}`},"https://utteranc.es")}addEventListener("message",e=>{if(e.origin!=="https://utteranc.es")return;utterancesLoaded=!0,setUtterancesTheme(document.documentElement.dataset.scheme)}),window.addEventListener("onColorSchemeChange",e=>{if(!utterancesLoaded)return;setUtterancesTheme(e.detail)})</script><footer class=site-footer><section class=copyright>&copy;
2020 -
2025 Mao Song(毛松)'s Homepage</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.31.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>