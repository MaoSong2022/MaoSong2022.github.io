<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><link rel=dns-prefetch href=https://fonts.googleapis.com><link rel=dns-prefetch href=https://cdn.jsdelivr.net><link rel=dns-prefetch href=https://scripts.simpleanalyticscdn.com><link rel=preconnect href=https://fonts.googleapis.com crossorigin><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><link rel=preconnect href=https://scripts.simpleanalyticscdn.com crossorigin><meta http-equiv=x-dns-prefetch-control content="on"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=5,minimum-scale=1"><meta name=format-detection content="telephone=no"><meta name=theme-color content="#4e54c8" media="(prefers-color-scheme: light)"><meta name=theme-color content="#1a1a2e" media="(prefers-color-scheme: dark)"><link rel=apple-touch-icon href><meta name=description content="DeepSeek 在 24 年 11 月发布了 DeepSeek-V3, 一个仅花费 2.8M H800 hours 的大语言模型，且在各个 benchmark 上达到了 SOTA 表现"><meta name=keywords content><meta name=author content="Mao Song"><link rel=canonical href=https://maosong.website/p/notes-on-deepseek-v3/><meta property="og:title" content="Notes on DeepSeek-V3"><meta property="og:description" content="DeepSeek 在 24 年 11 月发布了 DeepSeek-V3, 一个仅花费 2.8M H800 hours 的大语言模型，且在各个 benchmark 上达到了 SOTA 表现"><meta property="og:type" content="article"><meta property="og:url" content="https://maosong.website/p/notes-on-deepseek-v3/"><meta property="og:site_name" content="Mao Song(毛松)'s Homepage"><meta property="og:locale" content="en"><meta property="og:image" content="https://maosong.website/p/notes-on-deepseek-v3/DeepSeek-V2-architecture.png"><meta property="og:image:alt" content="Notes on DeepSeek-V3"><meta property="article:published_time" content="2025-12-08T11:14:45+08:00"><meta property="article:modified_time" content="2026-01-24T17:06:04+08:00"><meta property="article:author" content="Mao Song"><meta property="article:tag" content="deepseek"><meta property="article:tag" content="MoE"><meta property="article:tag" content="Reasoning"><meta name=robots content="index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1"><meta name=googlebot content="index, follow"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"Notes on DeepSeek-V3","description":"DeepSeek 在 24 年 11 月发布了 DeepSeek-V3, 一个仅花费 2.8M H800 hours 的大语言模型，且在各个 benchmark 上达到了 SOTA 表现","author":{"@type":"Person","name":"Mao Song"},"datePublished":"2025-12-08T11:14:45\u002b08:00","dateModified":"2026-01-24T17:06:04\u002b08:00","image":"https:\/\/maosong.website\/p\/notes-on-deepseek-v3\/DeepSeek-V2-architecture.png","publisher":{"@type":"Organization","name":"Mao Song(毛松)\u0027s Homepage","logo":{"@type":"ImageObject","url":"https:\/\/maosong.website\/"}},"mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/maosong.website\/p\/notes-on-deepseek-v3\/"},"keywords":"deepseek, MoE, Reasoning","articleSection":"post","inLanguage":"en"}</script><title>Notes on DeepSeek-V3</title>
<link rel=stylesheet href=/scss/style.min.fb2ef3860c8335f835ed6d55e46d9c435d7a37375d695eed32deb59ecfadc82b.css><script async src=https://scripts.simpleanalyticscdn.com/latest.js></script></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><a href=#main-content class=skip-to-main>Skip to main content</a><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu9788782091159651930.jpg width=300 height=240 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>Mao Song(毛松)'s Homepage</a></h1><h2 class=site-description>Delving into the Latent Unknown.</h2></div></header><ol class=menu-social><li><a href=https://b23.tv/a0Bb9Z1 target=_blank title=bilibili rel=me><svg role="img" viewBox="0 0 24 24"><title>Bilibili</title><path d="M17.813 4.653h.854c1.51.054 2.769.578 3.773 1.574 1.004.995 1.524 2.249 1.56 3.76v7.36c-.036 1.51-.556 2.769-1.56 3.773s-2.262 1.524-3.773 1.56H5.333c-1.51-.036-2.769-.556-3.773-1.56S.036 18.858.0 17.347v-7.36c.036-1.511.556-2.765 1.56-3.76 1.004-.996 2.262-1.52 3.773-1.574h.774l-1.174-1.12a1.234 1.234.0 01-.373-.906c0-.356.124-.658.373-.907l.027-.027c.267-.249.573-.373.92-.373s.653.124.92.373L9.653 4.44c.071.071.134.142.187.213h4.267a.836.836.0 01.16-.213l2.853-2.747c.267-.249.573-.373.92-.373s.662.151.929.4.391.551.391.907c0 .355-.124.657-.373.906zM5.333 7.24c-.746.018-1.373.276-1.88.773-.506.498-.769 1.13-.786 1.894v7.52c.017.764.28 1.395.786 1.893.507.498 1.134.756 1.88.773h13.334c.746-.017 1.373-.275 1.88-.773.506-.498.769-1.129.786-1.893v-7.52c-.017-.765-.28-1.396-.786-1.894-.507-.497-1.134-.755-1.88-.773zM8 11.107c.373.0.684.124.933.373.25.249.383.569.4.96v1.173c-.017.391-.15.711-.4.96-.249.25-.56.374-.933.374s-.684-.125-.933-.374c-.25-.249-.383-.569-.4-.96V12.44c0-.373.129-.689.386-.947.258-.257.574-.386.947-.386zm8 0c.373.0.684.124.933.373.25.249.383.569.4.96v1.173c-.017.391-.15.711-.4.96-.249.25-.56.374-.933.374s-.684-.125-.933-.374c-.25-.249-.383-.569-.4-.96V12.44c.017-.391.15-.711.4-.96.249-.249.56-.373.933-.373z"/></svg></a></li><li><a href=https://github.com/MaoSong2022 target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href='https://scholar.google.com/citations?user=BaqGkQQAAAAJ' target=_blank title="Google Scholar" rel=me><svg role="img" viewBox="0 0 24 24"><title>Google Scholar</title><path d="M5.242 13.769.0 9.5 12 0l12 9.5-5.242 4.269C17.548 11.249 14.978 9.5 12 9.5c-2.977.0-5.548 1.748-6.758 4.269zM12 10a7 7 0 100 14 7 7 0 000-14z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/tags/><svg class="icon icon-tabler icon-tabler-tag" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11 3l9 9a1.5 1.5.0 010 2l-6 6a1.5 1.5.0 01-2 0L3 11V7a4 4 0 014-4h4"/><circle cx="9" cy="9" r="2"/></svg>
<span>Tags</span></a></li><li><a href=/about/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>About</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#introduction>Introduction</a></li><li><a href=#architecture>Architecture</a><ol><li><a href=#basic-architecture>Basic Architecture</a></li><li><a href=#mtp>MTP</a></li></ol></li><li><a href=#infra>Infra</a><ol><li><a href=#training-framework>Training Framework</a><ol><li><a href=#dualpipe>DualPipe</a></li><li><a href=#cross-node-all-to-all-communication>Cross-node All-to-all Communication</a></li><li><a href=#memory-saving>Memory saving</a></li></ol></li><li><a href=#fp8-training>FP8 Training</a><ol><li><a href=#mixed-precision-training>Mixed Precision Training</a></li><li><a href=#enhancing-low-precision-training-accuracy>Enhancing Low-precision Training Accuracy</a></li><li><a href=#low-precision-storage-and-communication>Low Precision Storage and Communication</a></li></ol></li><li><a href=#inference-and-deployment>Inference and Deployment</a><ol><li><a href=#prefilling>Prefilling</a></li><li><a href=#decoding>Decoding</a></li></ol></li><li><a href=#suggestions-on-hardware-design>Suggestions on Hardware Design</a><ol><li><a href=#communication-hardware>Communication Hardware</a></li><li><a href=#computation-hardware>Computation Hardware</a></li></ol></li></ol></li><li><a href=#pre-training>Pre-training</a><ol><li><a href=#pre-training-data>Pre-training Data</a></li><li><a href=#hyper-parameters>Hyper-parameters</a></li><li><a href=#long-context-extension>Long Context Extension</a></li><li><a href=#performance>Performance</a></li><li><a href=#discussion>Discussion</a></li></ol></li><li><a href=#post-training>Post-training</a><ol><li><a href=#sft>SFT</a></li><li><a href=#rl>RL</a></li><li><a href=#post-training-performance>Post-training Performance</a></li><li><a href=#post-training-discussion>Post-training Discussion</a></li></ol></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#references>References</a></li></ol></nav></div></section></aside><main id=main-content class="main full-width" role=main aria-label="Main content"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/llm/>LLM</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/notes-on-deepseek-v3/>Notes on DeepSeek-V3</a></h2><h3 class=article-subtitle>DeepSeek 在 24 年 11 月发布了 DeepSeek-V3, 一个仅花费 2.8M H800 hours 的大语言模型，且在各个 benchmark 上达到了 SOTA 表现</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>December 8, 2025</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>11 minute read</time></div></footer></div></header><section class=article-content><p>DeepSeek 在 24 年 11 月发布了 DeepSeek-V3, 一个仅花费 2.8M H800 hours 的大语言模型，且在各个 benchmark 上达到了 SOTA 表现</p><h2 id=introduction><a href=#introduction class=header-anchor></a>Introduction</h2><p>作者在本文中提出了 DeepSeek-V3, 一个 671B-A37B 的 MoE 大语言模型。</p><p>在训练目标和架构上，作者做了如下改进：</p><ol><li>efficiency inference: 采用了 <a class=link href=https://maosong.website/p/notes-on-deepseek-v2/ target=_blank rel=noopener>DeepSeek-V2</a> 提出的 <a class=link href=https://maosong.website/p/notes-on-mla/ target=_blank rel=noopener>MLA</a></li><li>cost-effective training: 采用了 <a class=link href=https://maosong.website/p/notes-on-deepseekmoe/ target=_blank rel=noopener>DeepSeekMoE</a> 提出了 MoE 架构</li><li>auxiliary-loss-free strategy: 采用了 <a class=link href=https://maosong.website/p/notes-on-loss-free-balancing/ target=_blank rel=noopener>Loss-Free Balancing</a> 提出的 loss balancing 策略</li><li>multi-token prediction: 采用了 MTP 的训练目标来提升模型的表现</li></ol><p>在训练上，作者做了如下改进：</p><ol><li>使用了 FP8 混合精度进行训练并验证了其在大规模模型上的有效性</li><li>作者构建了 DualPipe 算法用于高效的 pipeline parallelism</li><li>构建了 cross-node all-to-all communication kernel 来高效使用 InfiniBand 以及 NVLink bandwidth</li><li>优化了 memory footprint, 来避免使用 tensor parallelism</li></ol><p>预训练阶段，DeepSeek-V3 使用了<strong>14.8T</strong> token. 在 mid-training 阶段，作者将模型的上下文长度由 8K 扩展到 32K, 再扩展到 128K.</p><p>后训练阶段，作者使用了 SFT 和 RL 两个阶段来提高模型的表现，作者还对 <a class=link href=https://maosong.website/p/notes-on-deepseek-r1/ target=_blank rel=noopener>DeepSeek-R1</a> 进行蒸馏来提高模型的 reasoning 能力</p><h2 id=architecture><a href=#architecture class=header-anchor></a>Architecture</h2><h3 id=basic-architecture><a href=#basic-architecture class=header-anchor></a>Basic Architecture</h3><p>DeepSeek-V3 的架构与 <a class=link href=https://maosong.website/p/notes-on-deepseek-v2/ target=_blank rel=noopener>DeepSeek-V2</a> 的架构一致，如下图所示</p><p><img src=/p/notes-on-deepseek-v3/DeepSeek-V2-architecture.png width=1324 height=1064 loading=lazy alt="Architecture of DeepSeek-V2" class=gallery-image data-flex-grow=124 data-flex-basis=298px></p><p>MLA 的介绍见 <a class=link href=https://maosong.website/p/notes-on-mla/ target=_blank rel=noopener>MLA</a>, MoE 架构的介绍见 <a class=link href=https://maosong.website/p/notes-on-deepseekmoe/ target=_blank rel=noopener>DeepSeekMoE</a>. DeepSeek-V3 在 DeepSeekMoE 的基础上做了两点改变：</p><ol><li>受 <a class=link href=https://maosong.website/p/notes-on-loss-free-balancing/ target=_blank rel=noopener>Loss-Free Balancing</a> 启发，作者使用了 sigmoid fu nction 来计算 affinity score</li><li>对于 selected affinity score 应用了 normalization</li></ol><p>在 <a class=link href=https://maosong.website/p/notes-on-deepseekmoe/ target=_blank rel=noopener>DeepSeekMoE</a> 的基础上，作者使用了 <a class=link href=https://maosong.website/p/notes-on-loss-free-balancing/ target=_blank rel=noopener>Loss-Free Balancing</a>. 其表达式如下</p>$$
g_{i,t} = \begin{cases}
s_{i,t}, & s_{i,t}+b_i\in\mathrm{Topk}(\{s_{i,j}+b_j\mid 1\leq j\leq N\}, K)\\
0, &\text{otherwise}
\end{cases}
$$<p>其中 $b_i$ 仅影响 routing, 训练时，如果对应的 expert 负载不均衡，则对 $b_i$ 进行更新，更新方式为增加/减少 $\gamma$, 这里 $\gamma$ 是一个超参数</p><p>为了提高 routing 在 sequence 层面的负载均衡，作者还是用了一个 complementary sequence-wise balance loss:</p>$$
\begin{aligned}
\mathcal{L}_{\mathrm{Bal}} &= \alpha\sum_{i=1}^{N_r} f_iP_i\\
f_i &= \frac{}{}\sum_{t=1}^T\mathbb{1}(s_{i,t}\in\mathrm{TopK}(\{s_{j,t}\mid 1\leq j \leq N_r\}, K_r))\\
s_{i,y}' &= \frac{s_{i,t}}{\sum_{j=1}^{N_r}s_{j,t}}\\
P_i &= \frac1T\sum_{t=1}^T s_{i,t}'
\end{aligned}
$$<p>其中 $\alpha$ 是 balance factor, 为超参数，$T$ 是 sequence length, 加入这个损失后，每个 sequence 上的负载会变得更加均衡</p><p>与 <a class=link href=https://maosong.website/p/notes-on-deepseekmoe/ target=_blank rel=noopener>DeepSeekMoE</a> 一样，作者也在 node 层面实现负载均衡，具体做法就是，根据 node 中所包含 expert 的 affinity score 之和来选取最高的 $M$ 个 nodes, 这样就可以进一步提高 computation 和 communication 之间的 overlap.</p><p>由于 DeepSeek-V3 负载均衡比较好，因此作者使用了 no token-dropping strategy.</p><h3 id=mtp><a href=#mtp class=header-anchor></a>MTP</h3><p>受 MTP 启发，DeepSeek-V3 也构建了 MTP 模块，作者认为 MTP 模块有两个优势：</p><ol><li>MTP objective 提供了更多的学习信号，进而提高了数据使用效率</li><li>MTP 可以让模型更好预测未来的 token</li></ol><p>与 MTP 不同，DeepSeek-V3 【todo】, DeepSeek-V3 所使用的 MTP 模块架构图如下所示</p><p><img src=/p/notes-on-deepseek-v3/DeepSeek-V3-MTP.png width=1203 height=536 loading=lazy alt="Illustration of MTP" class=gallery-image data-flex-grow=224 data-flex-basis=538px></p><p>MTP 模块使用了 $D$ sequential modules 来预测未来的 $D$ 个 token. 其中，第 $k$ 个 MTP 模块包含一个共享的 embedding layer $\mathrm{Emb}(\cdot)$ , 一个共享的 output head $\mathrm{OutHead}(\cdot)$, 一个 transformer block $\mathrm{TRM}_k(\cdot)$ 和一个 projection matrix $M_k\in\mathbb{R}^{d\times 2d}$.</p><p>对于第 $i$ 个 token 以及第 $k$ 个 MTP 模块，作者首先将第 $k-1$ 个 MTP 模块的第 $i$ 个 token 的 hidden states $h_i^{k-1}\in\mathbb{R}^d$ 和第 $i+k$ 个 token 的 embedding $\mathrm{Emb}(t_{i+1})\in\mathbb{R}^d$ 联合在一起</p>$$
h_i'^{k}=M_k[\mathrm{RMSNorm(h_{i}^{k-1});\mathrm{RMSNorm(\mathrm{Emb}(t_{i+k}))}}]
$$<p>其中 $[\cdot;\cdot]$ 代表 concatenation 操作。当 $k=1$ 时，$h_{i}^{k-1}$ 就代表了 main model 的输出。每个 MTP 模块的 embedding layer 和 main model 的 embedding layer 是共享的，接下来， $h_i'^k$ 作为第 $k$ 个 MTP 模块 transformer block 的输入，得到</p>$$
h_{i}^k = \mathrm{TRM}_k(h_{i}'^l)
$$<p>最后，共享的 output head 输出对应的概率分布：</p>$$
P_{i+k+1}^k = \mathrm{OutHead}(h_i^k)
$$<p>这里的 $\mathrm{OutHead}(\cdot)$ 的权重与 main model 也是共享的。作者这里提到，所使用的思想与 EAGLE 是类似的，但是不同的地方在于，EAGLE 主要是用于 speculative decoding, 而 MTP 主要用于提升训练。</p><p>MTP 的训练目标为未来 $T-k-1$ 个 token 的 cross-entropy loss:</p>$$
\mathcal{L}_{\mathrm{MTP}}^k = \mathrm{CrossEntropy}(P_{2+k:T+1}^k,t_{2+k:T+1}) = -\frac1T\sum_{t=k+2}^{T+1}\log P_i^k[t_i],
$$<p>其中 $T$ 是 sequence length, $t_i$ 是第 $i$ 个位置对应的 ground truth token, $P_i^k[t_i]$ 代表低 $k$ 个 MTP 模块给出的 $t_i$ 的预测概率。最后，作者对所有的 MTP loss 进行求和，得到</p>$$
\mathcal{L}_{\mathrm{MTP}} = \frac{\lambda}{D}\sum_{k=1}^D \mathcal{L}_{\mathrm{MTP}}^k.
$$<h2 id=infra><a href=#infra class=header-anchor></a>Infra</h2><p>DeepSeek-V3 训练使用了 2048 张 H800, 每个 node 包含 8 张 GPU, node 内部使用 NVLink 和 NVSwitch 进行连接，node 之间使用 InfiniBand 进行连接</p><p>与之前的 DeepSeek 系列相同，DeepSeek-V3 也是用了 HAI-LLM 框架来支持训练。训练时，DeepSeek-V3 使用了 16-way PP, 64-way EP (spanning 8 nodes, <a class=link href=https://maosong.website/p/gshard/ target=_blank rel=noopener>GShard</a>) 以及 ZeRO-1 DP.</p><p>作者主要进行了三点优化：</p><ol><li>构建了 DualPipe 用于高效 pipeline parallelism</li><li>构建了 cross-node all-to-all communication kernels 来高效利用 IB 以及 NVLink bandwidth</li><li>优化了训练时的 memory footprint, 使得训练时不再依赖 TP</li></ol><h3 id=training-framework><a href=#training-framework class=header-anchor></a>Training Framework</h3><h4 id=dualpipe><a href=#dualpipe class=header-anchor></a>DualPipe</h4><p>DeepSeek-V3 中，由于 cross-node EP, computation-to-communication ratio 近似为 1:1, 为了解决这个问题，作者提出了 DualPipe. DualPipe 的核心思想是将 forward 和 backward 过程中的 computation 以及 communication 进行重叠。与 ZeroBubble 类似，作者将每个 chunk 分为四个部分：attention, all-to-all dispatch, MLP 以及 all-to-all combine. 对于 attention 和 MLP, 作者还进一步将 backward 拆分为 针对权重和输入的 backward. 其示意图如下所示，这里橙色部分代表 forward, 绿色代表了针对输入的 backward, 蓝色代表了针对权重的 backward</p><p><img src=/p/notes-on-deepseek-v3/DeepSeek-V3-overlapping-strategy.png width=1101 height=125 loading=lazy alt="Overlapping stategy of DeepSeek-V3" class=gallery-image data-flex-grow=880 data-flex-basis=2113px></p><p>示意图里包含两个 block, 我们记为 block1, block2, 前向计算过程为</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>dispatch(F, block1) -&gt; MLP(F, block1) -&gt; combine(F, block1) -&gt; attention(F, block2)
</span></span></code></pre></td></tr></table></div></div><p>其中，<code>dispatch(F, block1)</code> 与 MLP 的反向传播 <code>MLP(B, block1)</code> 计算重叠, <code>MLP(F, block1)</code> 与 MLP 反向传播的 dispatch <code>dispatch(B, block1)</code> 通信重叠，<code>combine(F, block1)</code> 与 attention 反向传播的 <code>attention(B, block2)</code> 重叠，<code>attention(F, block2)</code> 与反向传播的 combine <code>combine(block2)</code> 重叠。下面是一个具体的例子</p><p><img src=/p/notes-on-deepseek-v3/DeepSeek-V3-DualPipe-scheduling.png width=1212 height=210 loading=lazy alt="DualPipe scheduling" class=gallery-image data-flex-grow=577 data-flex-basis=1385px></p><p>作者进一步对比了 DualPipe, 1F1B 和 ZeroBubble, 结果如下表所示</p><div class=table-wrapper><table><thead><tr><th>Method</th><th>Bubble</th><th>Parameter</th><th>Activation</th></tr></thead><tbody><tr><td>1F1B</td><td>$(PP-1)(F+B)$</td><td>$1\times$</td><td>$PP$</td></tr><tr><td>ZB1P</td><td>$(PP-1)(F+B-2W)$</td><td>$1\times$</td><td>$PP$</td></tr><tr><td>DualPipe (Ours)</td><td>$(\frac{PP}{2}-1)(F\&B+B-3W)$</td><td>$2\times$</td><td>$PP+1$</td></tr></tbody></table></div><p>这里 $F$ 是 forward chunk 的执行时间，$B$ 是 backward chunk 执行的时间，$W$ 是一个 chunk &ldquo;backward for weights&rdquo; 的执行时间，$F\&B$ 是一个 chunk 前向反向传播重叠的时间。可以看到，DualPipe 只使用了额外的 $1/PP$ 倍的 peak activation memory, 就大幅度降低了 bubble 时间</p><h4 id=cross-node-all-to-all-communication><a href=#cross-node-all-to-all-communication class=header-anchor></a>Cross-node All-to-all Communication</h4><p>作者针对 DualPipe 构建了 cross-node all-to-all communication kernels 来提高通信效率。</p><p>作者提到，跨节点通信使用的是 IB, 节点内部通信使用的是 NVLink， 通信方式如下图所示。</p><p><img src=/p/notes-on-deepseek-v3/GPU-communication-pattern.png width=1360 height=749 loading=lazy alt="Communication of GPU" class=gallery-image data-flex-grow=181 data-flex-basis=435px></p><p>对于 H800 来说,NVLink 的带宽为 160GB/s, IB 的带宽为 50GB/s, 因此 NVLInk 的通信效率是 IB 的 3.2 倍，即节点内部通信效率高于节点之间的通信效率。为了提高通信效率，作者限制每个 token 只能被分发到至多 4 个节点上（<a class=link href=https://maosong.website/p/notes-on-deepseek-v2/ target=_blank rel=noopener>DeepSeek-V2</a> 里提到的 device limited routing）。其具体通信方式为想传输到目标 node index 相同的 rank 上，然后再通过节点内部通信传输到目标的 rank 上。通过这种方式，我们可以让节点间通信与节点内部通信进行重叠，从而每个 token 可以从一个节点上选取 3.2 个专家，这样 DeepSeek-V3 可以在不损失效率的情况下最高选取 $13$ 个专家。</p><p>作者进一步采用了 warp specialization 技巧来将 20 个 SM 划分为 10 个通信 channel. 作者分别针对 dispatching 和 combing 阶段使用了不同数量的 warps.</p><h4 id=memory-saving><a href=#memory-saving class=header-anchor></a>Memory saving</h4><p>作者使用了如下技巧来减少内存访问：</p><ol><li>Recomputation of RMSNorm and MLA Up-Projection. 在 backward 过程中重新计算 RMSNorm operation 以及 MLA up projection 来避免存储器对应的输出</li><li>Exponential Moving Average in CPU. 作者将 EMA 参数保存在 CPU 中，然后进行异步更新来进一步减少内存访问</li><li>Shared Embedding and Output Head for Multi-Token Prediction. 作者将 embedding layer 和 output head 放在一个 PP rank 上，这样就可以提高 MTP 的内存访问效率</li></ol><h3 id=fp8-training><a href=#fp8-training class=header-anchor></a>FP8 Training</h3><p>作者提出了一个基于 FP8 的混合精度训练框架。作者提出了两个改进方案：</p><ol><li>分组量化，将 tensor 按照 tile 分组或者按照 block 分组来分组量化，这样就避免了全局量化的精度损失</li><li>高精度累加，作者在乘法计算时，使用了 FP8 格式，然后在累加阶段，使用了更高精度的格式</li></ol><p>为了进一步减少内存和通信开销，对于 activation 的 cache 以及 dispatch, 作者使用了 FP8 数据格式，然后对于优化器状态，作者使用了 BF16 数据格式。下面是对上面改进的具体说明。</p><h4 id=mixed-precision-training><a href=#mixed-precision-training class=header-anchor></a>Mixed Precision Training</h4><p>作者在本文中提出了使用 FP8 混合精度进行预训练，作者参考了 low precision training 构建 FP8 训练框架，即计算量高的使用 FP8 精度，计算量低的使用原本的数据精度, 框架如下图所示</p><p><img src=/p/notes-on-deepseek-v3/DeepSeek-V3-mixed-precision.png width=1210 height=365 loading=lazy alt="Mix-precision training of DeepSeek-V3" class=gallery-image data-flex-grow=331 data-flex-basis=795px></p><p>其中各个模块使用的精度如下表所示</p><div class=table-wrapper><table><thead><tr><th>Precision</th><th>Modules</th></tr></thead><tbody><tr><td>FP8</td><td>Linear (Fprop, Dgrad, Wgrad)</td></tr><tr><td>higher precision</td><td>- embedding<br>- output head<br>- Moe gating<br>- normalization<br>- attention operator<br>- master weights<br>- weight gradients<br>- optimizer states</td></tr></tbody></table></div><h4 id=enhancing-low-precision-training-accuracy><a href=#enhancing-low-precision-training-accuracy class=header-anchor></a>Enhancing Low-precision Training Accuracy</h4><p>作者介绍了几个策略用于提高 FP8 混合精度训练的表现：</p><p><img src=/p/notes-on-deepseek-v3/DeepSeek-V3-quantization.png width=1162 height=588 loading=lazy alt="Quantization of DeepSeek-V3" class=gallery-image data-flex-grow=197 data-flex-basis=474px></p><ol><li>fine-grained quantization: 对于激活值，作者将 tensor 分割为 $1\times 128$ 大小的 groups, 然后每个 group 内部进行 quantization; 对于权重，作者将其分割为 $128\times 128$ 大小的 groups, 然后进行 quantization, 这样可以降低 quantization error, 如上图左图所示</li><li>increasing accumulation precision: 低精度训练会带来 underflow 的问题，而 FP8 GEMM 累加仅能保留约 14bit 精度，远低于 FP32 的 32bit 精度。为了解决这个问题，作者采用了 Tensor Core 不分累加 +CUDA core 高精度聚合的协同策略。即在 Tensor Core 上执行 MMA 指令时，先按照 14bit 精度对 $N_C$ 个元素进行累加，当达到 $N_C$ 时，作者将结果复制到 CUDA core 的 FP32 寄存器中，在 FP32 精度下完成累加。过程如上图右图所示</li><li>Mantissa over exponents. 与之前的工作不同，作者使用了 E4M3 的数据格式来达到更高的精度</li><li>Online Quantization. 为了降低 quantization 的误差，作者实时计算了 activation block 以及 weight block 的最大绝对值来导出 scaling factor 并量化为 FP8 精度</li></ol><h4 id=low-precision-storage-and-communication><a href=#low-precision-storage-and-communication class=header-anchor></a>Low Precision Storage and Communication</h4><p>作者通过压缩 cached activation 和 optimizer states 来进一步减少内存访问以及通信访问次数</p><ul><li>Low-precision optimizer states: 对于 optimizer states, 作者使用了 BF16 数据格式来保存 <a class=link href=https://maosong.website/p/notes-on-adamw/ target=_blank rel=noopener>AdamW</a> 的优化器的一阶和二阶动量，但是对于 master weight 和 gradients 作者仍然使用了 FP32 来保证训练的数值稳定性</li><li>Low-Precision Activation: 对于 linear operator, 作者将其 cache activation 使用 FP8 格式进行存储，对于 attention 的输出，作者使用了 E5M6 数据格式来存储 activations, 这些 activations 的 tile size 为 $1\times 128$, 作者还是用了 2 的幂次作为 scale factor 来减少 quantizationerror; 对于 SwiGLU, 作者将其输入也保存为 FP8 的数据格式来降低内存消耗</li><li>Low-Precision Communication: 在 MoE up-projection 之前，作者将 attention 的输出量化为 FP8 数据格式再执行 dispatch 操作，这样可以降低通信开销。在反向传播的时候同理，先进行 FP8 量化，然后再进行反向 dispatch. 对于 combine 阶段，为了避免精度损失，作者还是使用了 BF16 数据格式</li></ul><p>作者对比了 FP8 和 BF16 精度训练，结果如下图所示</p><p><img src=/p/notes-on-deepseek-v3/DeepSeek-V3-FP8-performance.png width=1056 height=305 loading=lazy alt="FP8 v.s. BF16" class=gallery-image data-flex-grow=346 data-flex-basis=830px></p><p>实验结果显示，FP8 混合精度训练的损失降低不足 $0.25\%$.</p><h3 id=inference-and-deployment><a href=#inference-and-deployment class=header-anchor></a>Inference and Deployment</h3><p>作者在 H800 集群上部署 DeepSeek-V3, 作者分别针对 prefilling 和 decoding 两个阶段进行了优化</p><h4 id=prefilling><a href=#prefilling class=header-anchor></a>Prefilling</h4><p>prefilling 阶段在 4 节点 32 GPU 上进行，并行策略如下</p><div class=table-wrapper><table><thead><tr><th>模块</th><th>并行策略</th><th>说明</th></tr></thead><tbody><tr><td>Dense MLP</td><td>1-wat TP</td><td>减少 TP 通信</td></tr><tr><td>Attention</td><td>4-way TP, SP, 8-way DP</td><td>SP 用于长文本处理</td></tr><tr><td>MoE</td><td>32-way EP</td><td>每个 GPU 包含 8 个专家</td></tr></tbody></table></div><p>为了实现负载均衡，作者提出了<strong>redundant experts</strong>的策略，具体就是将负载比较高的专家进行复制。模型会周期性进行统计，然后计算出负载比较高的专家，接下来每个 GPU 除了原来的 8 个专家之外，还会 host 一个额外的 redundant expert.</p><p>为了提高 throughput, 作者同时处理两个 micro-batch, 来重叠 attention, MoE 和 dispatch, combine 通信，如下图所示</p><p><img src=/p/notes-on-deepseek-v3/DeepSeek-V3-prefilling-overlap.png width=661 height=316 loading=lazy alt="Prefiling overlap of DeepSeek-V3" class=gallery-image data-flex-grow=209 data-flex-basis=502px></p><p>作者还探索了 dynamic redundancy 策略，即每个 GPU host 更多的专家（比如 16 个）然后 inference 的每一步仅激活其中的 9 个，在进行 all-to-all operation 时，作者首先进行 global optimal routing 来计算最合适的专家。作者认为 prefilling 计算量很大，因此 routing 的计算量可以忽略不计</p><h4 id=decoding><a href=#decoding class=header-anchor></a>Decoding</h4><p>在 decoding 阶段，作者在 40 个节点 320 GPU 上进行部署，并行策略如下</p><div class=table-wrapper><table><thead><tr><th>模块</th><th>并行策略</th><th>说明</th></tr></thead><tbody><tr><td>Attention</td><td>4-way TP, SP, 80-way DP</td><td>SP 用于长文本处理</td></tr><tr><td>MoE</td><td>320-way EP</td><td>每个 GPU 包含 8 个专家</td></tr></tbody></table></div><p>这个阶段，每个 GPU 只 host 一个专家，64 个 GPU 负责 host redundant expert 以及 shared expert. 作者通过在 IB 上直接进行 P2P 的传输来降低通信的开销，作者还是用了 IBGDA 来进一步最小化 latency 以及提高通信效率</p><p>在这个阶段，attention 的计算占据了大部分时间，因此，作者采取了如下的 overlap 策略</p><p><img src=/p/notes-on-deepseek-v3/DeepSeek-V3-decoding-overlap.png width=896 height=308 loading=lazy alt="Decoding overlap of DeepSeek-V3" class=gallery-image data-flex-grow=290 data-flex-basis=698px></p><p>由于 decoding 阶段一般 batch size 比较小，因此整个 decoding 的瓶颈在于 memory access, 因此为了避免影响 attention 的计算效率，作者仅安排一小部分 SM 用于 dispatch-MoE-combine.</p><h3 id=suggestions-on-hardware-design><a href=#suggestions-on-hardware-design class=header-anchor></a>Suggestions on Hardware Design</h3><h4 id=communication-hardware><a href=#communication-hardware class=header-anchor></a>Communication Hardware</h4><p>尽管作者将计算与通信重叠来提高训练效率，但是已有的通信仍然依赖于 SM, 这样限制了计算的效率。作者希望工能够构建专门针对通信的设备，另一方面，作者希望统一 IB 和 NVLink 来降低实现难度</p><h4 id=computation-hardware><a href=#computation-hardware class=header-anchor></a>Computation Hardware</h4><ol><li>提升 FP8 GEMM 的累加精度，当前精度只有 14bit, 作者希望能够在 GPU 设计上进行改进，将这个精度提升到 34-bit</li><li>支持 tile 以及 block 层面的 quantization, 尽管前文作者已经设计出了改进的算法，但是 Tensor Core 以及 CUDA 之前平凡的数据移动降低了计算效率，作者希望未来能够支持细粒度的 quantization</li><li>online quantization, 作者希望将 FP8 cast 以及 TMA 结合在一起，从而 quantization 可以在传输的时候完成计算，减少了内存读写。作者还建议使用 warp-level cast instruction</li><li>Transposed GEMM operations. 本文中，矩阵被切分为不同的 tiles, 在计算的时候需要先加载这些 tiles 然后进行 dequantization, transpose 等操作，作者希望未来能够直接支持 transposed reads of matrices</li></ol><h2 id=pre-training><a href=#pre-training class=header-anchor></a>Pre-training</h2><h3 id=pre-training-data><a href=#pre-training-data class=header-anchor></a>Pre-training Data</h3><p>相比于 <a class=link href=https://maosong.website/p/notes-on-deepseek-v2/ target=_blank rel=noopener>DeepSeek-V2</a>, DeepSeek-V3 提升了数学和代码数据的比例，以及增加了多语种数据。最终训练数据一共包括 <strong>14.8T</strong></p><p>作者还使用了 DeepSeekCoder-V2 里应用的 Fill in the middle 策略来让模型基于上下文越策中间的文本，对应的数据格式如下</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>&lt;|fim_begin|&gt;f_pre&lt;|fim_hole|&gt;f_suf&lt;|fim_hole|&gt;f_middle&lt;|fim_end|&gt;
</span></span></code></pre></td></tr></table></div></div><p>这个结构与 sequence packing 结合在一起。</p><p>Tokenizer 基于 BBPE, 大小为 128K tokens. 在训练时，作者将随机一部分 combine token 进行切分来减少 token boundary bias 问题</p><h3 id=hyper-parameters><a href=#hyper-parameters class=header-anchor></a>Hyper-parameters</h3><p>模型参数如下表所示，最终 DeepSeek-V3 拥有 671B 总参数，激活参数为 37B</p><div class=table-wrapper><table><thead><tr><th>variable</th><th>notation</th><th>value</th></tr></thead><tbody><tr><td>layers</td><td>$\ell$</td><td>61</td></tr><tr><td>dense layers</td><td>-</td><td>3</td></tr><tr><td>hidden dimension</td><td>$d$</td><td>7168</td></tr><tr><td>num of attention heads</td><td>$n_h$</td><td>128</td></tr><tr><td>head dimension</td><td>$d_h$</td><td>128</td></tr><tr><td>KV compression dimension</td><td>$d_c$</td><td>512</td></tr><tr><td>query compression dimension</td><td>$d_c'$</td><td>1536</td></tr><tr><td>decouple query and key dimension</td><td>$d_h^R$</td><td>64</td></tr><tr><td>routed expert</td><td>$N_r$</td><td>256</td></tr><tr><td>shared expert</td><td>$N_s$</td><td>1</td></tr><tr><td>MoE hidden dimension</td><td>$d_{MoE}$</td><td>2048</td></tr><tr><td>activated experts</td><td>$K$</td><td>8</td></tr><tr><td>limited node routing</td><td>$M$</td><td>4</td></tr><tr><td>MTP depth</td><td>$D$</td><td>1</td></tr></tbody></table></div><p>训练时，作者使用了 learning rate scheduling, batch size scheduling 等方法。对于 PP, routed expert 会均匀分布为 8node 对应的 64 个 GPU 上，预训练时模型的上下文长度为 4k</p><h3 id=long-context-extension><a href=#long-context-extension class=header-anchor></a>Long Context Extension</h3><p>作者使用了 <a class=link href=https://maosong.website/p/notes-on-yarn/ target=_blank rel=noopener>YARN</a> 来扩展模型的上下文长度，模型上下文长度经过两个额外的训练阶段从 4K 扩展到 32K 再扩展到 128K, 均训练了 1000 步。</p><p>YARN 配置与 <a class=link href=https://maosong.website/p/notes-on-deepseek-v2/ target=_blank rel=noopener>DeepSeek-V2</a> 基本一致，在额外的 decoupled query and key 上作者没有应用这一点。参数配置如下表</p><div class=table-wrapper><table><thead><tr><th>parameter</th><th>$s$</th><th>$\alpha$</th><th>$\beta$</th><th>$\sqrt{t}$</th></tr></thead><tbody><tr><td>value</td><td>40</td><td>1</td><td>32</td><td>$0.1\ln s+1$</td></tr></tbody></table></div><p>第一阶段的上下文长度为 32K, batch size 为 1920, 第二个阶段的上下文长度为 128K, batch size 为 480.</p><h3 id=performance><a href=#performance class=header-anchor></a>Performance</h3><p>DeepSeek-V3 base 的表现下图所示，作者对比了 <a class=link href=https://maosong.website/p/notes-on-deepseek-v2/ target=_blank rel=noopener>DeepSeek-V2</a>, <a class=link href=https://maosong.website/p/notes-on-qwen2.5/ target=_blank rel=noopener>Qwen2.5</a>, <a class=link href=LLaMA%203.1>LLaMA 3.1</a></p><p><img src=/p/notes-on-deepseek-v3/DeepSeek-V3-base-performance.png width=1065 height=1083 loading=lazy alt="Performance of DeepSeek-V3 base" class=gallery-image data-flex-grow=98 data-flex-basis=236px></p><h3 id=discussion><a href=#discussion class=header-anchor></a>Discussion</h3><p>作者首先验证了 MTP 的有效性，结果如下图所示</p><p><img src=/p/notes-on-deepseek-v3/DeepSeek-V3-ablation-MTP.png width=979 height=451 loading=lazy alt="Ablation study on MTP" class=gallery-image data-flex-grow=217 data-flex-basis=520px></p><p>可以看到，在模型架构相同，训练数据相同的情况下，1-MTP 的效果超过了 baseline 的表现，说明了 MTP 策略的有效性</p><p>接下来，作者还验证了 <a class=link href=https://maosong.website/p/notes-on-loss-free-balancing/ target=_blank rel=noopener>Loss-Free Balancing</a> 的有效性，结果如下图所示</p><p><img src=/p/notes-on-deepseek-v3/DeepSeek-V3-ablation-loss-free-balancing.png width=987 height=458 loading=lazy alt="Ablation on loss-free balancing" class=gallery-image data-flex-grow=215 data-flex-basis=517px></p><p>可以看到，loss-free Balancing 的效果比 loss balancing 的效果更好</p><p>接下来，作者对比了 loss-free balancing 和 sequence-wise auxiliary loss, 即 batch-wise v.s. sequence-wise. 作者认为，前者的约束更灵活，因为其不要求 in-domain balance. 作者在测试集上进行可视化，结果如下图所示</p><p><img src=/p/notes-on-deepseek-v3/DeepSeek-V3-ablation-batch-wise.png width=1106 height=522 loading=lazy alt="Ablation study on batch-wise load balancing" class=gallery-image data-flex-grow=211 data-flex-basis=508px></p><p>实验结果发现，loss-free 策略对应的 expert specialization 更强。作者进一步设计了一个 batch-wise auxiliary loss 来实现 batch-wise load balance, 结果发现，这种策略也能达到和 loss-free balancing 一样的效果，这说明了 batch-wise load balancing 效果更好</p><p>最后，作者提了两点 loss-free 策略的问题：</p><ol><li>在特定的 sequence 或者小 batch 里出现负载不均衡。作者通通过增大 batch size 来解决这个问题</li><li>在推理阶段因为 domain-shift 导致的负载不均衡。作者实现了一个基于 redundant expert 的推理框架来解决这个问题</li></ol><h2 id=post-training><a href=#post-training class=header-anchor></a>Post-training</h2><h3 id=sft><a href=#sft class=header-anchor></a>SFT</h3><p>post-training 包含 1.5M 样本，数据包括 reasoning 数据以及 non-reasoning 数据，前者由 <a class=link href=https://maosong.website/p/notes-on-deepseek-r1/ target=_blank rel=noopener>DeepSeek-R1</a> 合成，后者由 DeepSeek-V2.5 合成</p><p>SFT 时，作者训练了两个 epoch, 使用了 sequence packing 技巧</p><h3 id=rl><a href=#rl class=header-anchor></a>RL</h3><p>Reward model 包含 rule-based reward model 和 model-based reward model.</p><p>RL 训练使用的算法为 GRPO</p><h3 id=post-training-performance><a href=#post-training-performance class=header-anchor></a>Post-training Performance</h3><p><img src=/p/notes-on-deepseek-v3/DeepSeek-V3-instruct-performance.png width=1112 height=813 loading=lazy alt="Performance of DeepSeek-V3" class=gallery-image data-flex-grow=136 data-flex-basis=328px></p><h3 id=post-training-discussion><a href=#post-training-discussion class=header-anchor></a>Post-training Discussion</h3><p>作者首先探究了 Distillation 对模型表现的影响，作者使用 <a class=link href=https://maosong.website/p/notes-on-deepseek-r1/ target=_blank rel=noopener>DeepSeek-R1</a> 来蒸馏 DeepSeek-V2.5, 结果如下图所示</p><div class=table-wrapper><table><thead><tr><th>Model</th><th>LiveCodeBench-CoT</th><th></th><th>MATH-500</th><th></th></tr></thead><tbody><tr><td></td><td>Pass@1</td><td>Length</td><td>Pass@1</td><td>Length</td></tr><tr><td>DeepSeek-V2.5 Baseline</td><td>31.1</td><td>718</td><td>74.6</td><td>769</td></tr><tr><td>DeepSeek-V2.5 +R1 Distill</td><td>37.4</td><td>783</td><td>83.2</td><td>1510</td></tr></tbody></table></div><p>可以看到，distillation 可以显著提高模型的表现。但是其问题在于也会让模型输出的长度增加。作者认为知识蒸馏是 post-training optimization 的一个重要方向。</p><p>接下来，作者讨论了 self-rewarding, 具体做法就是使用 DeepSeek-V3 来对评估结果进行投票，结果发现最终的效果很好，作者认为 LLM 可以很好地将非结构化信息转换为 rewards.</p><p>最后，作者讨论了 MTP. MTP 可以于 speculative decoding 结合，进一步提高 decoding 的速度。通过实验作者发现，second token prediction 的接受率在 $85\%\sim 90\%$ 之间，说明了其有效性。</p><h2 id=conclusion><a href=#conclusion class=header-anchor></a>Conclusion</h2><p>在本文中，作者提出了 DeepSeek-V3, 一个 671B-A37B 的 MoE 大语言模型，训练 token 数为 <strong>14.8T</strong>. 作者使用了 loss-free-balancing strategy 来实现负载均衡。训练时作者使用了 FP8 混合精度。评估发现 DeepSeek-V3 的达到了 SOTA 表现。</p><p>作者认为，DeepSeek-V3 model size 太大，不适合部署。第二，部署策略需要进一步改进。</p><p>最后，作者认为未来工作有以下几点：</p><ol><li>改进模型架构，进一步提高训练以及推理效率，扩展模型的上下文</li><li>提升训练数据的数量和质量</li><li>提高模型的 reasoning 能力</li><li>更详尽的评估</li></ol><h2 id=references><a href=#references class=header-anchor></a>References</h2><ul><li><a class=link href=http://arxiv.org/abs/2412.19437 target=_blank rel=noopener>Arxiv</a></li></ul></section><footer class=article-footer><section class=article-tags><a href=/tags/deepseek/>Deepseek</a>
<a href=/tags/moe/>MoE</a>
<a href=/tags/reasoning/>Reasoning</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section><section class=article-lastmod><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>Last updated on January 24, 2026 at 5:06 PM</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".main-article");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/notes-on-deepseek-v3.2/><div class=article-details><h2 class=article-title>Notes on DeepSeek-V3.2</h2></div></a></article><article><a href=/p/notes-on-deepseek-r1/><div class=article-details><h2 class=article-title>Notes on DeepSeek-R1</h2></div></a></article><article><a href=/p/notes-on-deepseek-v2/><div class=article-details><h2 class=article-title>Notes on DeepSeek-V2</h2></div></a></article><article><a href=/p/notes-on-loss-free-balancing/><div class=article-details><h2 class=article-title>Notes on Loss-free Balancing</h2></div></a></article><article><a href=/p/notes-on-deepseekmoe/><div class=article-details><h2 class=article-title>Notes on DeepSeekMoE</h2></div></a></article></div></div></aside><script src=https://utteranc.es/client.js repo=MaoSong2022/MaoSong2022.github.io issue-term=pathname crossorigin=anonymous async></script><style>.utterances{max-width:unset}</style><script>let utterancesLoaded=!1;function setUtterancesTheme(e){let t=document.querySelector(".utterances iframe");t&&t.contentWindow.postMessage({type:"set-theme",theme:`github-${e}`},"https://utteranc.es")}addEventListener("message",e=>{if(e.origin!=="https://utteranc.es")return;utterancesLoaded=!0,setUtterancesTheme(document.documentElement.dataset.scheme)}),window.addEventListener("onColorSchemeChange",e=>{if(!utterancesLoaded)return;setUtterancesTheme(e.detail)})</script><footer class=site-footer><section class=copyright>&copy;
2020 -
2026 Mao Song(毛松)'s Homepage</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.31.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>