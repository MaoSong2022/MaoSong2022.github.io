<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><link rel=dns-prefetch href=https://fonts.googleapis.com><link rel=dns-prefetch href=https://cdn.jsdelivr.net><link rel=dns-prefetch href=https://scripts.simpleanalyticscdn.com><link rel=preconnect href=https://fonts.googleapis.com crossorigin><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><link rel=preconnect href=https://scripts.simpleanalyticscdn.com crossorigin><meta http-equiv=x-dns-prefetch-control content="on"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=5,minimum-scale=1"><meta name=format-detection content="telephone=no"><meta name=theme-color content="#4e54c8" media="(prefers-color-scheme: light)"><meta name=theme-color content="#1a1a2e" media="(prefers-color-scheme: dark)"><link rel=apple-touch-icon href><meta name=description content="An brief introduction to Llama3"><meta name=keywords content><meta name=author content="Mao Song"><link rel=canonical href=https://maosong.website/p/notes-on-llama3/><meta property="og:title" content="Notes on Llama3"><meta property="og:description" content="An brief introduction to Llama3"><meta property="og:type" content="article"><meta property="og:url" content="https://maosong.website/p/notes-on-llama3/"><meta property="og:site_name" content="Mao Song(毛松)'s Homepage"><meta property="og:locale" content="en"><meta property="og:image" content="https://maosong.website/img/avatar.jpg"><meta property="og:image:alt" content="Notes on Llama3"><meta property="article:published_time" content="2024-04-22T16:22:19+08:00"><meta property="article:modified_time" content="2025-12-09T11:14:41+08:00"><meta property="article:author" content="Mao Song"><meta property="article:tag" content="Llama"><meta name=robots content="index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1"><meta name=googlebot content="index, follow"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"Notes on Llama3","description":"An brief introduction to Llama3","author":{"@type":"Person","name":"Mao Song"},"datePublished":"2024-04-22T16:22:19\u002b08:00","dateModified":"2025-12-09T11:14:41\u002b08:00","image":"https:\/\/maosong.website\/img\/avatar.jpg","publisher":{"@type":"Organization","name":"Mao Song(毛松)\u0027s Homepage","logo":{"@type":"ImageObject","url":"https:\/\/maosong.website\/"}},"mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/maosong.website\/p\/notes-on-llama3\/"},"keywords":"Llama","articleSection":"post","inLanguage":"en"}</script><title>Notes on Llama3</title>
<link rel=stylesheet href=/scss/style.min.fb2ef3860c8335f835ed6d55e46d9c435d7a37375d695eed32deb59ecfadc82b.css><script async src=https://scripts.simpleanalyticscdn.com/latest.js></script></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><a href=#main-content class=skip-to-main>Skip to main content</a><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu9788782091159651930.jpg width=300 height=240 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>Mao Song(毛松)'s Homepage</a></h1><h2 class=site-description>Delving into the Latent Unknown.</h2></div></header><ol class=menu-social><li><a href=https://b23.tv/a0Bb9Z1 target=_blank title=bilibili rel=me><svg role="img" viewBox="0 0 24 24"><title>Bilibili</title><path d="M17.813 4.653h.854c1.51.054 2.769.578 3.773 1.574 1.004.995 1.524 2.249 1.56 3.76v7.36c-.036 1.51-.556 2.769-1.56 3.773s-2.262 1.524-3.773 1.56H5.333c-1.51-.036-2.769-.556-3.773-1.56S.036 18.858.0 17.347v-7.36c.036-1.511.556-2.765 1.56-3.76 1.004-.996 2.262-1.52 3.773-1.574h.774l-1.174-1.12a1.234 1.234.0 01-.373-.906c0-.356.124-.658.373-.907l.027-.027c.267-.249.573-.373.92-.373s.653.124.92.373L9.653 4.44c.071.071.134.142.187.213h4.267a.836.836.0 01.16-.213l2.853-2.747c.267-.249.573-.373.92-.373s.662.151.929.4.391.551.391.907c0 .355-.124.657-.373.906zM5.333 7.24c-.746.018-1.373.276-1.88.773-.506.498-.769 1.13-.786 1.894v7.52c.017.764.28 1.395.786 1.893.507.498 1.134.756 1.88.773h13.334c.746-.017 1.373-.275 1.88-.773.506-.498.769-1.129.786-1.893v-7.52c-.017-.765-.28-1.396-.786-1.894-.507-.497-1.134-.755-1.88-.773zM8 11.107c.373.0.684.124.933.373.25.249.383.569.4.96v1.173c-.017.391-.15.711-.4.96-.249.25-.56.374-.933.374s-.684-.125-.933-.374c-.25-.249-.383-.569-.4-.96V12.44c0-.373.129-.689.386-.947.258-.257.574-.386.947-.386zm8 0c.373.0.684.124.933.373.25.249.383.569.4.96v1.173c-.017.391-.15.711-.4.96-.249.25-.56.374-.933.374s-.684-.125-.933-.374c-.25-.249-.383-.569-.4-.96V12.44c.017-.391.15-.711.4-.96.249-.249.56-.373.933-.373z"/></svg></a></li><li><a href=https://github.com/MaoSong2022 target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href='https://scholar.google.com/citations?user=BaqGkQQAAAAJ' target=_blank title="Google Scholar" rel=me><svg role="img" viewBox="0 0 24 24"><title>Google Scholar</title><path d="M5.242 13.769.0 9.5 12 0l12 9.5-5.242 4.269C17.548 11.249 14.978 9.5 12 9.5c-2.977.0-5.548 1.748-6.758 4.269zM12 10a7 7 0 100 14 7 7 0 000-14z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/tags/><svg class="icon icon-tabler icon-tabler-tag" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11 3l9 9a1.5 1.5.0 010 2l-6 6a1.5 1.5.0 01-2 0L3 11V7a4 4 0 014-4h4"/><circle cx="9" cy="9" r="2"/></svg>
<span>Tags</span></a></li><li><a href=/about/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>About</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#instruct-model-performance>Instruct model performance</a></li><li><a href=#pre-trained-model-performance>Pre-trained model performance</a></li></ol></nav></div></section></aside><main id=main-content class="main full-width" role=main aria-label="Main content"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/llm/>LLM</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/notes-on-llama3/>Notes on Llama3</a></h2><h3 class=article-subtitle>An brief introduction to Llama3</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>April 22, 2024</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>3 minute read</time></div></footer></div></header><section class=article-content><p>Meta released Llama3 at April 18, which is evaluated on several benchmarks and achieves the SOTA on open-sourced LLMs.</p><h1 id=introduction><a href=#introduction class=header-anchor></a>Introduction</h1><h2 id=instruct-model-performance><a href=#instruct-model-performance class=header-anchor></a>Instruct model performance</h2><p>The performance of Llama3 8B compared with Gemma and Mistral:</p><div class=table-wrapper><table><thead><tr><th>Model</th><th>Llama3 8B</th><th>Gemma 7B - It</th><th>Mistral &amp;B Instruct</th></tr></thead><tbody><tr><td><em>MMLU</em> (5 shot)</td><td><strong>68.4</strong></td><td>53.3</td><td>58.4</td></tr><tr><td><em>GPQA</em> (0 shot)</td><td><strong>34.2</strong></td><td>21.4</td><td>26.3</td></tr><tr><td><em>HumanEval</em> (0 shot)</td><td><strong>62.2</strong></td><td>30.5</td><td>36.6</td></tr><tr><td><em>GSM-8K</em> (8 shot, CoT)</td><td><strong>79.6</strong></td><td>30.6</td><td>39.9</td></tr><tr><td><em>MATH</em> (4 shot, CoT)</td><td><strong>30.0</strong></td><td>12.2</td><td>11.0</td></tr></tbody></table></div><p>performance of Llama3 70B compared with Gemini Pro 1.5 and Claude Sonnet:</p><div class=table-wrapper><table><thead><tr><th>Model</th><th>Llama3 70B</th><th>Gemini Pro 1.5 (Published)</th><th>Claude 3 Sonnet (Published)</th></tr></thead><tbody><tr><td><em>MMLU</em> (5 shot)</td><td><strong>82.0</strong></td><td>81.9</td><td>79.0</td></tr><tr><td><em>GPQA</em> (0 shot)</td><td>39.5</td><td><strong>41.5</strong> (CoT)</td><td>38.5 (CoT)</td></tr><tr><td><em>HumanEval</em> (0 shot)</td><td><strong>81.7</strong></td><td>71.9</td><td>73.0</td></tr><tr><td><em>GSM-8K</em> (8 shot, CoT)</td><td><strong>93.0</strong></td><td>91.7 (11 shot)</td><td>92.3 (0 shot)</td></tr><tr><td><em>MATH</em> (4 shot, CoT)</td><td>50.4</td><td><strong>58.5</strong> (Minerva prompt)</td><td>40.5</td></tr></tbody></table></div><h2 id=pre-trained-model-performance><a href=#pre-trained-model-performance class=header-anchor></a>Pre-trained model performance</h2><p>The performance of Llama3 8B compared with Gemma and Mistral:</p><div class=table-wrapper><table><thead><tr><th>Model</th><th>Llama3 8B</th><th>Gemma 7B (Published)</th><th>Gemma 7B (Measured)</th><th>Mistral 7B (Published)</th><th>Mistral 7B (Measured)</th></tr></thead><tbody><tr><td><em>MMLU</em> (5 shot)</td><td><strong>66.6</strong></td><td>64.3</td><td>64.4</td><td>62.5</td><td>63.9</td></tr><tr><td><em>AGIEval English</em> (3-5 shot)</td><td><strong>45.9</strong></td><td>41.7</td><td>44.9</td><td>-</td><td>44.0</td></tr><tr><td><em>BIG-Bench Hard</em> (3 shot, CoT)</td><td><strong>61.1</strong></td><td>55.1</td><td>59.0</td><td>-</td><td>56.0</td></tr><tr><td><em>ARC-Challenge</em> (25 shot)</td><td>78.6</td><td>53.2(0 shot)</td><td><strong>79.1</strong></td><td>78.1</td><td>78.7</td></tr><tr><td><em>DROP</em> (3 shot, F1)</td><td><strong>58.4</strong></td><td>-</td><td>56.3</td><td>-</td><td>54.4</td></tr></tbody></table></div><p>performance of Llama3 70B compared with Gemini Pro 1.5 and Claude Sonnet:</p><div class=table-wrapper><table><thead><tr><th>Model</th><th>Llama3 70B</th><th>Gemini Pro 1.0 (Published)</th><th>Mixtral 8 $\times$ 22B (Measured)</th></tr></thead><tbody><tr><td><em>MMLU</em> (5-shot)</td><td><strong>79.5</strong></td><td>71.8</td><td>77.7</td></tr><tr><td><em>AGIEval English</em> (3-5 shot)</td><td><strong>63.0</strong></td><td>-</td><td>61.2</td></tr><tr><td><em>BIG-Bench Hard</em> (3 shot, CoT)</td><td><strong>81.3</strong></td><td>75.0</td><td>79.2</td></tr><tr><td><em>ARC-Challenge</em> (25 shot)</td><td><strong>93.0</strong></td><td>-</td><td>90.7</td></tr><tr><td><em>DROP</em> (3 shot, F1)</td><td><strong>79.7</strong></td><td>74.1 (variable shot)</td><td>77.6</td></tr></tbody></table></div><h1 id=model-architecture><a href=#model-architecture class=header-anchor></a>Model Architecture</h1><p>Several improvements are made on Llama3 compared to llama2:</p><ol><li>Llama3 uses a tokenizer with a vocabulary of 128K tokens.</li><li>Llama3 adopts <a class=link href=https://maosong.website/p/notes-on-gqa/ target=_blank rel=noopener>grouped query attention (GQA)</a> across both the 8B and 70B sizes.</li><li>Llama3 uses to context window of size 8192 tokens</li></ol><h1 id=traning><a href=#traning class=header-anchor></a>Traning</h1><p>Llama3 uses 15T tokens for pre-training. Compares to Llama2, it is seven times larger and includes four times more code.</p><p>5% data of the training dataset are non-English to support multi-lingual use cases.</p><p>Data processing includes:</p><ol><li>Heuristic filters</li><li>NSFW filters</li><li>Semantic deduplication approaches</li><li>Text classifiers to predict data quality. Llama2 is used to generate training data for the text classifiers.</li></ol><p>Data mixing strategy is explored to improve the performance of Llama3.</p><h1 id=scaling-up-pretraining><a href=#scaling-up-pretraining class=header-anchor></a>Scaling up pretraining</h1><p>Llama3 developed a series of scaling laws for downstream benchmark evaluations.</p><p>Scaling laws help:</p><ol><li>Select an optimal data mix and to make informed decisions on how to best use training compute.</li><li>Scaling laws allow Llama3 to predict the performance of the largest models on key tasks without training the models.</li></ol><p>The authors finds our that the performance of the model continues to improve log-linearly as the training tokens increase. It is seen that Larger models can match the performance of these smaller models with less training compute, but smaller models are generally preferred because they are much more efficient during inference.</p><p>The authors combine three types of parallelization:</p><ol><li>Data parallelization</li><li>Model parallelization</li><li>Pipeline parallelization</li></ol><h1 id=instruction-fine-tuning><a href=#instruction-fine-tuning class=header-anchor></a>Instruction fine-tuning</h1><p>The fine-tuning of Llama3 contains:</p><ol><li>Supervised fine-tuning</li><li>Rejection sampling</li><li>Proximal Policy Optimization</li><li>Direct Preference Optimization</li></ol><p>Learning from perference rankings via PPO and <a class=link href=https://maosong.website/p/notes-on-dpo/ target=_blank rel=noopener>DPO</a> also greatly improved the performance of LLma3 on reasoning and coding tasks. Since perference ranking helps the model to select answer when it is in a dilemma.</p><h1 id=reference><a href=#reference class=header-anchor></a>Reference</h1><ul><li><a class=link href=https://ai.meta.com/blog/meta-llama-3/ target=_blank rel=noopener>Llam3 blog</a></li><li><a class=link href=https://github.com/meta-llama/llama3/blob/main/eval_details.md target=_blank rel=noopener>Evaluation details</a></li><li><a class=link href=https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md target=_blank rel=noopener>Model Card</a></li></ul></section><footer class=article-footer><section class=article-tags><a href=/tags/llama/>LLaMA</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section><section class=article-lastmod><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>Last updated on December 9, 2025 at 11:14 AM</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".main-article");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/notes-on-kimi-k2.5/><div class=article-details><h2 class=article-title>Notes on Kimi-k2.5</h2></div></a></article><article><a href=/p/notes-on-qwen3-next/><div class=article-details><h2 class=article-title>Notes on Qwen3-Next</h2></div></a></article><article><a href=/p/megatron-lm/><div class=article-details><h2 class=article-title>megatron-lm</h2></div></a></article><article><a href=/p/notes-on-gated-attention/><div class=article-details><h2 class=article-title>Notes on Gated Attention</h2></div></a></article><article><a href=/p/llm-memory-computation/><div class=article-details><h2 class=article-title>LLM Memory Computation</h2></div></a></article></div></div></aside><script src=https://utteranc.es/client.js repo=MaoSong2022/MaoSong2022.github.io issue-term=pathname crossorigin=anonymous async></script><style>.utterances{max-width:unset}</style><script>let utterancesLoaded=!1;function setUtterancesTheme(e){let t=document.querySelector(".utterances iframe");t&&t.contentWindow.postMessage({type:"set-theme",theme:`github-${e}`},"https://utteranc.es")}addEventListener("message",e=>{if(e.origin!=="https://utteranc.es")return;utterancesLoaded=!0,setUtterancesTheme(document.documentElement.dataset.scheme)}),window.addEventListener("onColorSchemeChange",e=>{if(!utterancesLoaded)return;setUtterancesTheme(e.detail)})</script><footer class=site-footer><section class=copyright>&copy;
2020 -
2026 Mao Song(毛松)'s Homepage</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.31.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>