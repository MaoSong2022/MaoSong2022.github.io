<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="An brief introduction to Llama3"><title>Notes on Llama3</title>
<link rel=canonical href=https://maosong2022.github.io/p/notes-on-llama3/><link rel=stylesheet href=/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css><meta property='og:title' content="Notes on Llama3"><meta property='og:description' content="An brief introduction to Llama3"><meta property='og:url' content='https://maosong2022.github.io/p/notes-on-llama3/'><meta property='og:site_name' content="Mao Song(ÊØõÊùæ)'s Homepage"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='Llama'><meta property='article:published_time' content='2024-04-22T16:22:19+08:00'><meta property='article:modified_time' content='2024-04-22T16:22:19+08:00'><meta name=twitter:title content="Notes on Llama3"><meta name=twitter:description content="An brief introduction to Llama3"><link rel="shortcut icon" href=/favicon.png></head><script async src=https://scripts.simpleanalyticscdn.com/latest.js></script><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu9788782091159651930.jpg width=300 height=240 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>üç•</span></figure><div class=site-meta><h1 class=site-name><a href=/>Mao Song(ÊØõÊùæ)'s Homepage</a></h1><h2 class=site-description>Never stop learning.</h2></div></header><ol class=menu-social><li><a href=https://b23.tv/a0Bb9Z1 target=_blank title=bilibili rel=me><svg role="img" viewBox="0 0 24 24"><title>Bilibili</title><path d="M17.813 4.653h.854c1.51.054 2.769.578 3.773 1.574 1.004.995 1.524 2.249 1.56 3.76v7.36c-.036 1.51-.556 2.769-1.56 3.773s-2.262 1.524-3.773 1.56H5.333c-1.51-.036-2.769-.556-3.773-1.56S.036 18.858.0 17.347v-7.36c.036-1.511.556-2.765 1.56-3.76 1.004-.996 2.262-1.52 3.773-1.574h.774l-1.174-1.12a1.234 1.234.0 01-.373-.906c0-.356.124-.658.373-.907l.027-.027c.267-.249.573-.373.92-.373s.653.124.92.373L9.653 4.44c.071.071.134.142.187.213h4.267a.836.836.0 01.16-.213l2.853-2.747c.267-.249.573-.373.92-.373s.662.151.929.4.391.551.391.907c0 .355-.124.657-.373.906zM5.333 7.24c-.746.018-1.373.276-1.88.773-.506.498-.769 1.13-.786 1.894v7.52c.017.764.28 1.395.786 1.893.507.498 1.134.756 1.88.773h13.334c.746-.017 1.373-.275 1.88-.773.506-.498.769-1.129.786-1.893v-7.52c-.017-.765-.28-1.396-.786-1.894-.507-.497-1.134-.755-1.88-.773zM8 11.107c.373.0.684.124.933.373.25.249.383.569.4.96v1.173c-.017.391-.15.711-.4.96-.249.25-.56.374-.933.374s-.684-.125-.933-.374c-.25-.249-.383-.569-.4-.96V12.44c0-.373.129-.689.386-.947.258-.257.574-.386.947-.386zm8 0c.373.0.684.124.933.373.25.249.383.569.4.96v1.173c-.017.391-.15.711-.4.96-.249.25-.56.374-.933.374s-.684-.125-.933-.374c-.25-.249-.383-.569-.4-.96V12.44c.017-.391.15-.711.4-.96.249-.249.56-.373.933-.373z"/></svg></a></li><li><a href=https://github.com/MaoSong2022 target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href='https://scholar.google.com/citations?user=BaqGkQQAAAAJ' target=_blank title="Google Scholar" rel=me><svg role="img" viewBox="0 0 24 24"><title>Google Scholar</title><path d="M5.242 13.769.0 9.5 12 0l12 9.5-5.242 4.269C17.548 11.249 14.978 9.5 12 9.5c-2.977.0-5.548 1.748-6.758 4.269zM12 10a7 7 0 100 14 7 7 0 000-14z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/about/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>About</span></a></li><li><a href=/tags/><svg class="icon icon-tabler icon-tabler-tag" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11 3l9 9a1.5 1.5.0 010 2l-6 6a1.5 1.5.0 01-2 0L3 11V7a4 4 0 014-4h4"/><circle cx="9" cy="9" r="2"/></svg>
<span>Tags</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#instruct-model-performance>Instruct model performance</a></li><li><a href=#pre-trained-model-performance>Pre-trained model performance</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/llm/>LLM</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/notes-on-llama3/>Notes on Llama3</a></h2><h3 class=article-subtitle>An brief introduction to Llama3</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Apr 22, 2024</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>3 minute read</time></div></footer></div></header><section class=article-content><p>Meta released Llama3 at April 18, which is evaluated on several benchmarks and achieves the SOTA on open-sourced LLMs.</p><h1 id=introduction>Introduction</h1><h2 id=instruct-model-performance>Instruct model performance</h2><p>The performance of Llama3 8B compared with Gemma and Mistral:</p><div class=table-wrapper><table><thead><tr><th>Model</th><th>Llama3 8B</th><th>Gemma 7B - It</th><th>Mistral &amp;B Instruct</th></tr></thead><tbody><tr><td><em>MMLU</em> (5 shot)</td><td><strong>68.4</strong></td><td>53.3</td><td>58.4</td></tr><tr><td><em>GPQA</em> (0 shot)</td><td><strong>34.2</strong></td><td>21.4</td><td>26.3</td></tr><tr><td><em>HumanEval</em> (0 shot)</td><td><strong>62.2</strong></td><td>30.5</td><td>36.6</td></tr><tr><td><em>GSM-8K</em> (8 shot, CoT)</td><td><strong>79.6</strong></td><td>30.6</td><td>39.9</td></tr><tr><td><em>MATH</em> (4 shot, CoT)</td><td><strong>30.0</strong></td><td>12.2</td><td>11.0</td></tr></tbody></table></div><p>performance of Llama3 70B compared with Gemini Pro 1.5 and Claude Sonnet:</p><div class=table-wrapper><table><thead><tr><th>Model</th><th>Llama3 70B</th><th>Gemini Pro 1.5 (Published)</th><th>Claude 3 Sonnet (Published)</th></tr></thead><tbody><tr><td><em>MMLU</em> (5 shot)</td><td><strong>82.0</strong></td><td>81.9</td><td>79.0</td></tr><tr><td><em>GPQA</em> (0 shot)</td><td>39.5</td><td><strong>41.5</strong> (CoT)</td><td>38.5 (CoT)</td></tr><tr><td><em>HumanEval</em> (0 shot)</td><td><strong>81.7</strong></td><td>71.9</td><td>73.0</td></tr><tr><td><em>GSM-8K</em> (8 shot, CoT)</td><td><strong>93.0</strong></td><td>91.7 (11 shot)</td><td>92.3 (0 shot)</td></tr><tr><td><em>MATH</em> (4 shot, CoT)</td><td>50.4</td><td><strong>58.5</strong> (Minerva prompt)</td><td>40.5</td></tr></tbody></table></div><h2 id=pre-trained-model-performance>Pre-trained model performance</h2><p>The performance of Llama3 8B compared with Gemma and Mistral:</p><div class=table-wrapper><table><thead><tr><th>Model</th><th>Llama3 8B</th><th>Gemma 7B (Published)</th><th>Gemma 7B (Measured)</th><th>Mistral 7B (Published)</th><th>Mistral 7B (Measured)</th></tr></thead><tbody><tr><td><em>MMLU</em> (5 shot)</td><td><strong>66.6</strong></td><td>64.3</td><td>64.4</td><td>62.5</td><td>63.9</td></tr><tr><td><em>AGIEval English</em> (3-5 shot)</td><td><strong>45.9</strong></td><td>41.7</td><td>44.9</td><td>-</td><td>44.0</td></tr><tr><td><em>BIG-Bench Hard</em> (3 shot, CoT)</td><td><strong>61.1</strong></td><td>55.1</td><td>59.0</td><td>-</td><td>56.0</td></tr><tr><td><em>ARC-Challenge</em> (25 shot)</td><td>78.6</td><td>53.2(0 shot)</td><td><strong>79.1</strong></td><td>78.1</td><td>78.7</td></tr><tr><td><em>DROP</em> (3 shot, F1)</td><td><strong>58.4</strong></td><td>-</td><td>56.3</td><td>-</td><td>54.4</td></tr></tbody></table></div><p>performance of Llama3 70B compared with Gemini Pro 1.5 and Claude Sonnet:</p><div class=table-wrapper><table><thead><tr><th>Model</th><th>Llama3 70B</th><th>Gemini Pro 1.0 (Published)</th><th>Mixtral 8 $\times$ 22B (Measured)</th></tr></thead><tbody><tr><td><em>MMLU</em> (5-shot)</td><td><strong>79.5</strong></td><td>71.8</td><td>77.7</td></tr><tr><td><em>AGIEval English</em> (3-5 shot)</td><td><strong>63.0</strong></td><td>-</td><td>61.2</td></tr><tr><td><em>BIG-Bench Hard</em> (3 shot, CoT)</td><td><strong>81.3</strong></td><td>75.0</td><td>79.2</td></tr><tr><td><em>ARC-Challenge</em> (25 shot)</td><td><strong>93.0</strong></td><td>-</td><td>90.7</td></tr><tr><td><em>DROP</em> (3 shot, F1)</td><td><strong>79.7</strong></td><td>74.1 (variable shot)</td><td>77.6</td></tr></tbody></table></div><h1 id=model-architecture>Model Architecture</h1><p>Several improvements are made on Llama3 compared to llama2:</p><ol><li>Llama3 uses a tokenizer with a vocabulary of 128K tokens.</li><li>Llama3 adopts <a class=link href=https://maosong.website/p/notes-on-gqa/ target=_blank rel=noopener>grouped query attention (GQA)</a> across both the 8B and 70B sizes.</li><li>Llama3 uses to context window of size 8192 tokens</li></ol><h1 id=traning>Traning</h1><p>Llama3 uses 15T tokens for pre-training. Compares to Llama2, it is seven times larger and includes four times more code.</p><p>5% data of the training dataset are non-English to support multi-lingual use cases.</p><p>Data processing includes:</p><ol><li>Heuristic filters</li><li>NSFW filters</li><li>Semantic deduplication approaches</li><li>Text classifiers to predict data quality. Llama2 is used to generate training data for the text classifiers.</li></ol><p>Data mixing strategy is explored to improve the performance of Llama3.</p><h1 id=scaling-up-pretraining>Scaling up pretraining</h1><p>Llama3 developed a series of scaling laws for downstream benchmark evaluations.</p><p>Scaling laws help:</p><ol><li>Select an optimal data mix and to make informed decisions on how to best use training compute.</li><li>Scaling laws allow Llama3 to predict the performance of the largest models on key tasks without training the models.</li></ol><p>The authors finds our that the performance of the model continues to improve log-linearly as the training tokens increase. It is seen that Larger models can match the performance of these smaller models with less training compute, but smaller models are generally preferred because they are much more efficient during inference.</p><p>The authors combine three types of parallelization:</p><ol><li>Data parallelization</li><li>Model parallelization</li><li>Pipeline parallelization</li></ol><h1 id=instruction-fine-tuning>Instruction fine-tuning</h1><p>The fine-tuning of Llama3 contains:</p><ol><li>Supervised fine-tuning</li><li>Rejection sampling</li><li>Proximal Policy Optimization</li><li>Direct Preference Optimization</li></ol><p>Learning from perference rankings via PPO and DPO also greatly improved the performance of LLma3 on reasoning and coding tasks. Since perference ranking helps the model to select answer when it is in a dilemma.</p><h1 id=reference>Reference</h1><ul><li><a class=link href=https://ai.meta.com/blog/meta-llama-3/ target=_blank rel=noopener>Llam3 blog</a></li><li><a class=link href=https://github.com/meta-llama/llama3/blob/main/eval_details.md target=_blank rel=noopener>Evaluation details</a></li><li><a class=link href=https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md target=_blank rel=noopener>Model Card</a></li></ul></section><footer class=article-footer><section class=article-tags><a href=/tags/llama/>LLaMA</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/mixstral-8x7b/><div class=article-details><h2 class=article-title>Mixstral 8x7B</h2></div></a></article><article><a href=/p/mixstral-7b/><div class=article-details><h2 class=article-title>Mixstral 7B</h2></div></a></article><article><a href=/p/st-moe/><div class=article-details><h2 class=article-title>ST-MoE</h2></div></a></article><article><a href=/p/gshard/><div class=article-details><h2 class=article-title>GShard</h2></div></a></article><article><a href=/p/st-moe/><div class=article-details><h2 class=article-title>ST-MoE</h2></div></a></article></div></div></aside><script src=https://utteranc.es/client.js repo=MaoSong2022/MaoSong2022.github.io issue-term=pathname crossorigin=anonymous async></script><style>.utterances{max-width:unset}</style><script>let utterancesLoaded=!1;function setUtterancesTheme(e){let t=document.querySelector(".utterances iframe");t&&t.contentWindow.postMessage({type:"set-theme",theme:`github-${e}`},"https://utteranc.es")}addEventListener("message",e=>{if(e.origin!=="https://utteranc.es")return;utterancesLoaded=!0,setUtterancesTheme(document.documentElement.dataset.scheme)}),window.addEventListener("onColorSchemeChange",e=>{if(!utterancesLoaded)return;setUtterancesTheme(e.detail)})</script><footer class=site-footer><section class=copyright>&copy;
2020 -
2025 Mao Song(ÊØõÊùæ)'s Homepage</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.31.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>