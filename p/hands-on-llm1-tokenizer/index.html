<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="Tokenizer总结与BPE的高效实现"><title>Hands on LLM(1) Tokenizer</title>
<link rel=canonical href=https://maosong2022.github.io/p/hands-on-llm1-tokenizer/><link rel=stylesheet href=/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css><meta property='og:title' content="Hands on LLM(1) Tokenizer"><meta property='og:description' content="Tokenizer总结与BPE的高效实现"><meta property='og:url' content='https://maosong2022.github.io/p/hands-on-llm1-tokenizer/'><meta property='og:site_name' content="Mao Song(毛松)'s Homepage"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='transformer'><meta property='article:published_time' content='2025-05-24T19:56:34+08:00'><meta property='article:modified_time' content='2025-06-29T11:47:22+08:00'><meta name=twitter:title content="Hands on LLM(1) Tokenizer"><meta name=twitter:description content="Tokenizer总结与BPE的高效实现"><link rel="shortcut icon" href=/favicon.png></head><script async src=https://scripts.simpleanalyticscdn.com/latest.js></script><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu9788782091159651930.jpg width=300 height=240 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>🍥</span></figure><div class=site-meta><h1 class=site-name><a href=/>Mao Song(毛松)'s Homepage</a></h1><h2 class=site-description>Never stop learning.</h2></div></header><ol class=menu-social><li><a href=https://b23.tv/a0Bb9Z1 target=_blank title=bilibili rel=me><svg role="img" viewBox="0 0 24 24"><title>Bilibili</title><path d="M17.813 4.653h.854c1.51.054 2.769.578 3.773 1.574 1.004.995 1.524 2.249 1.56 3.76v7.36c-.036 1.51-.556 2.769-1.56 3.773s-2.262 1.524-3.773 1.56H5.333c-1.51-.036-2.769-.556-3.773-1.56S.036 18.858.0 17.347v-7.36c.036-1.511.556-2.765 1.56-3.76 1.004-.996 2.262-1.52 3.773-1.574h.774l-1.174-1.12a1.234 1.234.0 01-.373-.906c0-.356.124-.658.373-.907l.027-.027c.267-.249.573-.373.92-.373s.653.124.92.373L9.653 4.44c.071.071.134.142.187.213h4.267a.836.836.0 01.16-.213l2.853-2.747c.267-.249.573-.373.92-.373s.662.151.929.4.391.551.391.907c0 .355-.124.657-.373.906zM5.333 7.24c-.746.018-1.373.276-1.88.773-.506.498-.769 1.13-.786 1.894v7.52c.017.764.28 1.395.786 1.893.507.498 1.134.756 1.88.773h13.334c.746-.017 1.373-.275 1.88-.773.506-.498.769-1.129.786-1.893v-7.52c-.017-.765-.28-1.396-.786-1.894-.507-.497-1.134-.755-1.88-.773zM8 11.107c.373.0.684.124.933.373.25.249.383.569.4.96v1.173c-.017.391-.15.711-.4.96-.249.25-.56.374-.933.374s-.684-.125-.933-.374c-.25-.249-.383-.569-.4-.96V12.44c0-.373.129-.689.386-.947.258-.257.574-.386.947-.386zm8 0c.373.0.684.124.933.373.25.249.383.569.4.96v1.173c-.017.391-.15.711-.4.96-.249.25-.56.374-.933.374s-.684-.125-.933-.374c-.25-.249-.383-.569-.4-.96V12.44c.017-.391.15-.711.4-.96.249-.249.56-.373.933-.373z"/></svg></a></li><li><a href=https://github.com/MaoSong2022 target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href='https://scholar.google.com/citations?user=BaqGkQQAAAAJ' target=_blank title="Google Scholar" rel=me><svg role="img" viewBox="0 0 24 24"><title>Google Scholar</title><path d="M5.242 13.769.0 9.5 12 0l12 9.5-5.242 4.269C17.548 11.249 14.978 9.5 12 9.5c-2.977.0-5.548 1.748-6.758 4.269zM12 10a7 7 0 100 14 7 7 0 000-14z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/about/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>About</span></a></li><li><a href=/tags/><svg class="icon icon-tabler icon-tabler-tag" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11 3l9 9a1.5 1.5.0 010 2l-6 6a1.5 1.5.0 01-2 0L3 11V7a4 4 0 014-4h4"/><circle cx="9" cy="9" r="2"/></svg>
<span>Tags</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#introduction>Introduction</a></li><li><a href=#training-free-tokenizer>Training-free tokenizer</a><ol><li><a href=#word-tokenizer>Word tokenizer</a></li><li><a href=#character-tokenizer>Character tokenizer</a></li><li><a href=#byte-tokenizer>Byte tokenizer</a></li><li><a href=#总结>总结</a></li></ol></li><li><a href=#bpe>BPE</a><ol><li><a href=#基本原理与实现>基本原理与实现</a></li><li><a href=#高效实现>高效实现</a></li></ol></li><li><a href=#other-subword-tokenizers>Other subword tokenizers</a><ol><li><a href=#wordpiece>WordPiece</a></li><li><a href=#unigram>Unigram</a></li><li><a href=#subword-tokenizer总结>Subword tokenizer总结</a></li></ol></li><li><a href=#实践>实践</a><ol><li><a href=#tiktoken>tiktoken</a></li><li><a href=#sentencepiece>SentencePiece</a></li><li><a href=#tokenizer>Tokenizer</a></li><li><a href=#总结-1>总结</a></li></ol></li><li><a href=#结论>结论</a></li><li><a href=#参考文献>参考文献</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/llm/>LLM</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/hands-on-llm1-tokenizer/>Hands on LLM(1) Tokenizer</a></h2><h3 class=article-subtitle>Tokenizer总结与BPE的高效实现</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>May 24, 2025</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>12 minute read</time></div></footer></div></header><section class=article-content><h1 id=tokenizer总结>Tokenizer总结</h1><h2 id=introduction>Introduction</h2><p>在自然语言处理中，tokenizer的作用是将一个文本序列通过一个字典转化为一个token id的序列。我们回顾图片分类任务，模型在预测的时候，实际上预测的是类别对应的id，而不是类别本身。tokenizer做的事情就是提供一个类似于从类别到对应id的字典。</p><p>一般来说，一个tokenizer处理文本序列的过程有两步：</p><ol><li>pre-tokenize，也就是预处理，我们需要将文本序列分割成合适大小的chunks (words)</li><li>tokenize，构建chunks (words)到token id的映射</li></ol><p>实际上, huggingface的tokenizer包括<a class=link href=https://huggingface.co/docs/tokenizers/pipeline target=_blank rel=noopener>四个步骤</a>, 其中第二第三个步骤与上述一致. 在pre-tokenize之前, 我们有一个normalization过程, 该过程会对文本序列进行处理, 如将文本序列变为小写, 删掉声调符号等, 如下面例子所示：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tokenizers</span> <span class=kn>import</span> <span class=n>normalizers</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tokenizers.normalizers</span> <span class=kn>import</span> <span class=n>NFD</span><span class=p>,</span> <span class=n>StripAccents</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>normalizer</span> <span class=o>=</span> <span class=n>normalizers</span><span class=o>.</span><span class=n>Sequence</span><span class=p>([</span><span class=n>NFD</span><span class=p>(),</span> <span class=n>StripAccents</span><span class=p>()])</span>
</span></span><span class=line><span class=cl><span class=n>normalizer</span><span class=o>.</span><span class=n>normalize_str</span><span class=p>(</span><span class=s2>&#34;Héllò hôw are ü?&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># &#34;Hello how are u?&#34;</span>
</span></span></code></pre></td></tr></table></div></div><p>在tokenize之后, 我们会有一个post-processing过程, 比如BERT会在生成的token系列前后加入 <code>[CLS]</code> token 和 <code>[SEP]</code> token, 例子如下:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoTokenizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;bert-base-cased&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>token_ids</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&#34;I love NLP.&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>token_ids</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># [101, 146, 1567, 21239, 2101, 119, 102]</span>
</span></span><span class=line><span class=cl><span class=c1># represents [[CLS], &#34;I&#34;, &#34;love&#34;, &#34;NL&#34;, &#34;##P&#34;, &#34;.&#34;, [SEP]]</span>
</span></span></code></pre></td></tr></table></div></div><p>其完整流程如下图所示 (图源: <a class=link href="https://huggingface.co/learn/llm-course/chapter6/4?fw=pt" target=_blank rel=noopener>huggingface llm-course</a>)</p><p><img src=/p/hands-on-llm1-tokenizer/tokenization_pipeline.png width=1702 height=1234 srcset="/p/hands-on-llm1-tokenizer/tokenization_pipeline_hu10346641896860173084.png 480w, /p/hands-on-llm1-tokenizer/tokenization_pipeline_hu472026015685023953.png 1024w" loading=lazy alt="tokenization pipeline" class=gallery-image data-flex-grow=137 data-flex-basis=331px></p><p>构建好tokenizer之后, 我们还要保证tokenizer提供两个接口：</p><ol><li>encoding, 给定文本序列, 将其映射到字典中去得到token id序列</li><li>decoding, 给定token id序列, 将其解码成文本序列</li></ol><p>接下来, 我们将简单介绍一下word tokenizer, character tokenizer以及byte tokenizer, 并分析它们各自的不足。
然后, 我们介绍现代大语言模型中使用最多的BPE tokenizer。最后, 我们介绍一些sub-word tokenizer。</p><h2 id=training-free-tokenizer>Training-free tokenizer</h2><p>本节我们将要介绍word tokenizer, character tokenizer以及byte tokenizer, 它们的特点就是简单易懂, 不需要额外的规则和学习.</p><h3 id=word-tokenizer>Word tokenizer</h3><p>给定一个文本序列, 我们现在需要将其转化为一个token序列。一个比较自然的想法是, 我们按照空格将序列拆分成若干个单词, 这样每个单词的语义都能比较好地保留。下面是一个例子：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>tiktoken</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>tiktoken</span><span class=o>.</span><span class=n>get_encoding</span><span class=p>(</span><span class=s2>&#34;gpt2&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>indices</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&#34;hello world&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># indices = [31373, 995]</span>
</span></span><span class=line><span class=cl><span class=c1># decode = [&#34;hello&#34;, &#34; world&#34;]</span>
</span></span></code></pre></td></tr></table></div></div><p>接下来我们基于一个预定义好的词典, 将其转化为一个token id的序列。</p><p>word tokenizer的优点是能够保留语义信息，且压缩率比较高（每个token包含的bytes数），其问题是不能处理预定义好的词典之外的词 (out of vocabulary, OOV)。现有的处理方法是使用 <code>&lt;UNK></code> token来表示这些OOV的词。
但这样显然会丢失语义信息, 因为我们编码成 <code>&lt;UNK></code> token之后, 就没办法再解码回原有的语义信息了。</p><p>word tokenizer的缺点为：</p><ol><li>单词数量很大, 很多罕见单词的出现频率很低, 降低了tokenizer的利用率</li><li>对于不在词典内的单词只能用<code>&lt;UNK></code> token表示, 损害了语义信息</li></ol><p>既然基于word的tokenizer有OOV的问题，我们能否想办法解决这个问题呢？答案是可以的, 我们可以使用 character tokenizer。</p><h3 id=character-tokenizer>Character tokenizer</h3><p>Character tokenizer的基本思想是使用字符而不是单词来编码文本序列。其实现方式如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>CharacterTokenizer</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>encode</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>s</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>list</span><span class=p>[</span><span class=nb>int</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=nb>list</span><span class=p>(</span><span class=nb>ord</span><span class=p>(</span><span class=n>c</span><span class=p>)</span> <span class=k>for</span> <span class=n>c</span> <span class=ow>in</span> <span class=n>s</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>decode</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>token_ids</span><span class=p>:</span> <span class=nb>list</span><span class=p>[</span><span class=nb>int</span><span class=p>])</span> <span class=o>-&gt;</span> <span class=nb>str</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=s2>&#34;&#34;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=nb>chr</span><span class=p>(</span><span class=n>token_id</span><span class=p>)</span> <span class=k>for</span> <span class=n>token_id</span> <span class=ow>in</span> <span class=n>token_ids</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>character tokenizer的词表大小取决于我们的编码方式，UTF-8的编码大概有<a class=link href=https://en.wikipedia.org/wiki/UTF-8 target=_blank rel=noopener>110K code points</a>。character tokenizer的缺点总结如下：</p><ol><li>character tokenizer会导致我们的词表非常大</li><li>和word tokenizer一样, 很多character非常罕见, 会降低词表的利用率</li><li>token序列的上下文语义信息较差</li></ol><h3 id=byte-tokenizer>Byte tokenizer</h3><p>我们发现, character tokenizer和word tokenizer的词表都很大, 我们能否想办法降低词表大小, 提升每个token的利用率呢？答案是使用Byte tokenizer.</p><p>Byte tokenizer的基本思想是, 所有的字符(character)都是由byte组成的, 比如对于UTF-8编码来说, 每个字符由1-4个byte组成。
因此, 所有满足UTF-8编码的文本, 我们都可以将它们转换为基于byte的token序列。
由于现在的大部分文本都是基于UTF-8的, 因此, 我们只讨论UTF-8编码的文本。</p><p>Byte tokenizer的实现如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>ByteTokenizer</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>encode</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>s</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>list</span><span class=p>[</span><span class=nb>int</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=nb>list</span><span class=p>(</span><span class=n>s</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&#34;utf-8&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>decode</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>token_ids</span><span class=p>:</span> <span class=nb>list</span><span class=p>[</span><span class=nb>int</span><span class=p>])</span> <span class=o>-&gt;</span> <span class=nb>str</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=nb>bytes</span><span class=p>(</span><span class=n>token_ids</span><span class=p>)</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=s2>&#34;utf-8&#34;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>byte tokenizer的词表很小, 其词表大小为 <code>256</code>, 这是因为一个byte可以有256中可能的值.</p><p>尽管byte tokenizer实现简单，并且词表也很小，可以说byte tokenizer解决了character tokenizer和word tokenizer的问题。
但是，byte tokenizer的问题在于，其encode的到的token序列可能会非常长！我们知道，transformer计算量与token序列的长度是平方级关系的，也就是说token序列长度增加10倍，整体的计算量就会增加100倍，因此我们势必需要考虑token序列的长度。</p><p>总之，byte tokenizer的问题为：</p><ol><li>产生的token序列过长, 增加了transformer的计算量</li><li>没有上下文语义信息</li></ol><h3 id=总结>总结</h3><p>我们总结一下word tokenizer, character tokenizer以及byte tokenizer三者各自的特点:</p><div class=table-wrapper><table><thead><tr><th>Feature</th><th>Word Tokenizer</th><th>Character Tokenizer</th><th>Byte Tokenizer</th></tr></thead><tbody><tr><td>Granularity</td><td>Coarse</td><td>Medium</td><td>Fine</td></tr><tr><td>Vocabulary</td><td>Yes</td><td>No</td><td>No</td></tr><tr><td>Support OOV</td><td>Bad</td><td>Good</td><td>Best</td></tr><tr><td>#Tokens</td><td>Small</td><td>Large</td><td>Very Large</td></tr><tr><td>Chinese</td><td>Yes</td><td>Yes</td><td>Yes</td></tr><tr><td>Support Spell Error</td><td>Bad</td><td>Yes</td><td>Yes</td></tr><tr><td>Context</td><td>Good</td><td>Bad</td><td>Worst</td></tr></tbody></table></div><p>因此, 这三种tokenizer尽管实现起来很简单, 但是其都有各自的问题. 为了解决这些问题, 我们的做法就是折衷, 使用sub-word tokenizer, 也就是介于word tokenizer和byte tokenizer之间的方法.</p><h2 id=bpe>BPE</h2><h3 id=基本原理与实现>基本原理与实现</h3><p>实际生活中，对于出现频率比较高的词，我们会有一个简写的方式，也就是我们使用一个新的单词来表示这个词。比如在英语中，我们会使用<code>plz</code> 来代替 <code>please</code> 以及使用<code>how r u</code> 来代替<code>how are you</code>。
BPE，即byte pair tokenizer的原理也是类似的，对于出现频率比较高的byte pair或者character pair, 我们会使用一个新的token来表示这个pair，这样就压缩了sequence的长度。</p><p>BPE算法包括以下几个步骤:</p><ol><li>对文本序列进行pre-tokenize, 分割成不同的单词</li><li>当<code>len(vocab)&lt;vocab_size</code>时, 重复以下步骤:<ol><li>对所有单词, 统计其相邻character或者byte pair的频率</li><li>计算出现频率最高的pair, 使用一个新的token来表示这个pair</li><li>将新的token和其对应的<code>token_id</code>加入到<code>vocab</code>中, 并更新单词的分割表示</li></ol></li></ol><p>算法如下图所示 (参考文献2)</p><p><img src=/p/hands-on-llm1-tokenizer/bpe_algorithm.png width=724 height=828 srcset="/p/hands-on-llm1-tokenizer/bpe_algorithm_hu8572890471772692345.png 480w, /p/hands-on-llm1-tokenizer/bpe_algorithm_hu4951491798023889503.png 1024w" loading=lazy alt="BPE algorithm" class=gallery-image data-flex-grow=87 data-flex-basis=209px></p><blockquote><p>注意：实际上，我们实现的是BBPE (byte BPE算法)，BBPE与BPE的区别在于我们的最小单元是character还是bytes。本质上原理是一致的</p></blockquote><p>实现代码见 <a class=link href=https://github.com/MaoSong2022/assignment1-basics/blob/main/cs336_basics/naive_bpe.py target=_blank rel=noopener>Github naive BPE</a></p><h3 id=高效实现>高效实现</h3><p>BPE的原理很简单, 我们也实现了其naive版本, 但是naive版本的问题是太慢了。因此我们将要优化naive版本的效率。</p><p>首先我们发现, 我们不需要遍历所有的word, 只有含有<code>best_pair</code>的word我们才会进行处理, 因此, 我们的第一个改进就是使用 <code>pair_to_word</code> 来记录每个pair的来源, 比如：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>pair_to_word</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=sa>b</span><span class=s1>&#39; &#39;</span><span class=p>,</span> <span class=sa>b</span><span class=s1>&#39;t&#39;</span><span class=p>):</span> <span class=p>[</span><span class=sa>b</span><span class=s1>&#39; the&#39;</span><span class=p>,</span> <span class=sa>b</span><span class=s1>&#39; it&#39;</span><span class=p>],</span> 
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=sa>b</span><span class=s1>&#39;t&#39;</span><span class=p>,</span> <span class=sa>b</span><span class=s1>&#39;h&#39;</span><span class=p>):</span> <span class=p>[</span><span class=sa>b</span><span class=s1>&#39;the&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>这样, 我们在merge的时候, 直接使用 <code>pair_to_word[best_pair]</code> 来获取需要被更新的token序列就可以了。</p><p>其次, 注意到每次merge之后, 我们都需要重新计算一次 <code>pair_freq</code>, 而实际上, 只有被merge的token序列才需要被重新计数, 其他大部分token序列都是不需要重新计数的。
因此, 一个改进点就是我们在merge的过程中就更新 <code>pair_freq</code>, 而不是重新计算。为了达到这个目标, 我们其实只需要两个操作。
我们用<code>(b'x', b'a', b'b', b'y')</code> 和 <code>best_pair=(b'a', b'b')</code>来说明, merge之前, 这个序列贡献的<code>pair_freq</code>为：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=sa>b</span><span class=s1>&#39;x&#39;</span><span class=p>,</span> <span class=sa>b</span><span class=s1>&#39;a&#39;</span><span class=p>):</span> <span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=sa>b</span><span class=s1>&#39;a&#39;</span><span class=p>,</span> <span class=sa>b</span><span class=s1>&#39;b&#39;</span><span class=p>):</span> <span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=sa>b</span><span class=s1>&#39;b&#39;</span><span class=p>,</span> <span class=sa>b</span><span class=s1>&#39;y&#39;</span><span class=p>):</span> <span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>merge之后, token序列变成了<code>(b'x', b'z', b'y')</code> (假设<code>best_pair</code>对应的新的token为<code>b'z'</code>), 这时候的计数为:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=sa>b</span><span class=s1>&#39;x&#39;</span><span class=p>,</span> <span class=sa>b</span><span class=s1>&#39;a&#39;</span><span class=p>):</span> <span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=sa>b</span><span class=s1>&#39;a&#39;</span><span class=p>,</span> <span class=sa>b</span><span class=s1>&#39;b&#39;</span><span class=p>):</span> <span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=sa>b</span><span class=s1>&#39;b&#39;</span><span class=p>,</span> <span class=sa>b</span><span class=s1>&#39;y&#39;</span><span class=p>):</span> <span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=sa>b</span><span class=s1>&#39;x&#39;</span><span class=p>,</span> <span class=sa>b</span><span class=s1>&#39;z&#39;</span><span class=p>):</span> <span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=sa>b</span><span class=s1>&#39;z&#39;</span><span class=p>,</span> <span class=sa>b</span><span class=s1>&#39;y&#39;</span><span class=p>):</span> <span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>也就是说, merge之后, 三个pair的计数减少了1, 分别是<code>(token_seq[i-1], merge_pair[0])</code>,<code>merge_pair</code> 和 <code>(merge_pair[1], token_seq[i+2])</code>。两个pair的个数增加了1, 分别是 <code>(token_seq[i-1], new_token)</code>和<code>(new_token, token_seq[i+2])</code> (这里我们假设<code>merge_pair=(token_seq[i], token_seq[i+1])</code>)。</p><p>基于这个结论，我们就可以优化BPE算法了，具体逻辑就是：</p><ol><li>pretokenize, 将 text 切分为若干个 word</li><li>计算<code>word_count</code>, <code>pair_freq</code>, <code>pair_to_word</code>, 使用<code>splits</code>记录每个word对应的token分布</li><li>重复以下过程：<ol><li>挑选频率最高的pair将其merge为一个新的token, 基于<code>pair_to_words</code>更新对应的<code>pair_freq</code></li><li>对每个<code>split</code>, 按照上述方式更新<code>pair_freq</code>和<code>split</code></li></ol></li></ol><p>其具体实现见 <a class=link href=https://github.com/MaoSong2022/assignment1-basics/blob/main/cs336_basics/efficient_bpe.py target=_blank rel=noopener>Github</a></p><h2 id=other-subword-tokenizers>Other subword tokenizers</h2><h3 id=wordpiece>WordPiece</h3><p>WordPiece是Google在预训练BERT时采用的tokenizer，WordPiece的基本思想和BPE差不多，都是从一个较小的vocab开始的。</p><p>首先，WordPiece会通过加上prefix <code>##</code>来把单词进行切分，比如 <code>word</code> 会被拆分为：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=p>[</span><span class=s1>&#39;w&#39;</span><span class=p>,</span> <span class=s1>&#39;##o&#39;</span><span class=p>,</span> <span class=s1>&#39;##r&#39;</span><span class=p>,</span> <span class=s1>&#39;##d&#39;</span><span class=p>]</span>
</span></span></code></pre></td></tr></table></div></div><p>接下来，对于pair $(a, b)$, WordPiece定义了merge pair的规则如下：</p>$$
\mathrm{score}((a, b)) = \frac{\#(a, b)}{\#a \times \#b}
$$<p>其中 $#(a, b)$, $#a$, $#b$ 分别代表 pair $(a, b)$, 元素 $a$ 和元素 $b$ 的频率。
通过这个方式，我们会给予包含元素出现频率较低的pair更高的优先级。通过这个方式，我们选取score最高的pair，然后将其用一个新的token表示，然后和BPE算法一样，继续这一过程直到我们的vocab size达到指定大小。</p><p>在tokenize的时候，WordPiece会找出现在vocab中的最长的subword, 比如对于<code>'hugs'</code>, 假设从左向右在词典中的最长subword是<code>'hug'</code>, 那么<code>'hugs'</code> 就会被拆分为 <code>['hug', '##s']</code>。如果我们在词表中找不到对应的subword，这个时候我们就会使用<code>'[UNK]'</code>来表示。</p><p>具体实现见 <a class=link href=https://github.com/MaoSong2022/assignment1-basics/blob/main/cs336_basics/word_piece.py target=_blank rel=noopener>Github wordpiece</a> (基于<a class=link href="https://huggingface.co/learn/llm-course/chapter6/4?fw=pt" target=_blank rel=noopener>huggingface llm course</a>)。 代码实现除了选择最优pair的方式不同之外，和BPE基本一致。</p><h3 id=unigram>Unigram</h3><p>Unigram也是由Google提出来的tokenizer，与BPE和wordpiece不同，unigram从一个非常大的vocab开始，然后merge token来降低vocab的size，直到达到指定大小。初始的vocab可以基于BPE算法或者使用prefix subword来构建。并且，初始vocab还包含所有的base characters来保证所有的word都可以被tokenize。</p><p>算法的描述如下:</p><p><img src=/p/hands-on-llm1-tokenizer/unigram.png width=619 height=669 srcset="/p/hands-on-llm1-tokenizer/unigram_hu9742228212950594435.png 480w, /p/hands-on-llm1-tokenizer/unigram_hu16226380729442000185.png 1024w" loading=lazy alt=unigram class=gallery-image data-flex-grow=92 data-flex-basis=222px></p><p>我们来看一下算法的细节, 首先对于一个word, 我们有多种切割方式, 比如<code>'bug'</code>可以被切分为如下三种形式:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=p>[[</span><span class=s1>&#39;b&#39;</span><span class=p>,</span> <span class=s1>&#39;u&#39;</span><span class=p>,</span> <span class=s1>&#39;g&#39;</span><span class=p>],</span> <span class=p>[</span><span class=s1>&#39;b&#39;</span><span class=p>,</span> <span class=s1>&#39;ug&#39;</span><span class=p>],</span> <span class=p>[</span><span class=s1>&#39;bu&#39;</span><span class=p>,</span> <span class=s1>&#39;g&#39;</span><span class=p>]]</span>
</span></span></code></pre></td></tr></table></div></div><p>unigram 假设每个 word 出现的概率是其 subword 出现概率的乘积, 即对于包含 $n$个subword的单词 $\bm{x}=(x_1,\dots,x_n)$, 我们有:</p>$$
p(\bm{x}) = \prod_{i=1}^n p(x_i)
$$<p>其中，对于给定的vocab $\mathcal{V}$, 我们有：</p>$$\sum_{v\in\mathcal{V}} p(x)=1$$<p>unigram的目的就是选择合适的切分 $\bm{x}\in S(\bf{x})$ (这里我们用 $\bf{x}$ 表示单词本身, 用 $\bm{x}$ 表示 $\bf{x}$ 的一个切分), 使得 $p(\bm{x})$的概率最大. 这样我们就可以写出unigram的损失函数了:</p>$$
\mathcal{L} = \sum_{i=1}^{N} \log\left(\sum_{\bm{x}\in S(\bf{x})}p(\bm{x})\right)
$$<p>其本质就是: 我们希望对每个单词找到一种合适的切分, 切分得到的subword的概率分布满足其求和为1, 并且使得每个单词的概率最大.</p><p>但是直接对上面概率最大化的问题就是我们每个subword的概率是未知的, unigram的做法是使用EM算法求解这个问题.</p><p>当我们求解完成之后, 对每个subword, 我们都尝试将其从 $\mathcal{V}$中移除, 然后计算移除后的损失 $loss_i$, 我们依照$loss_i$对subword进行排序, 然后我们去掉 $\eta %$ 比例的subword.</p><p>unigram的伪代码逻辑如下:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>while</span> <span class=nb>len</span><span class=p>(</span><span class=n>model</span><span class=p>)</span> <span class=o>&gt;</span> <span class=n>vocab_size</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>scores</span> <span class=o>=</span> <span class=n>compute_scores</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>sorted_scores</span> <span class=o>=</span> <span class=nb>sorted</span><span class=p>(</span><span class=n>scores</span><span class=o>.</span><span class=n>items</span><span class=p>(),</span> <span class=n>key</span><span class=o>=</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>x</span><span class=p>[</span><span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=c1># Remove percent_to_remove tokens with the lowest scores.</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>int</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>model</span><span class=p>)</span> <span class=o>*</span> <span class=n>percent_to_remove</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>        <span class=n>_</span> <span class=o>=</span> <span class=n>token_freqs</span><span class=o>.</span><span class=n>pop</span><span class=p>(</span><span class=n>sorted_scores</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=mi>0</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>total_sum</span> <span class=o>=</span> <span class=nb>sum</span><span class=p>([</span><span class=n>freq</span> <span class=k>for</span> <span class=n>token</span><span class=p>,</span> <span class=n>freq</span> <span class=ow>in</span> <span class=n>token_freqs</span><span class=o>.</span><span class=n>items</span><span class=p>()])</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=p>{</span><span class=n>token</span><span class=p>:</span> <span class=o>-</span><span class=n>log</span><span class=p>(</span><span class=n>freq</span> <span class=o>/</span> <span class=n>total_sum</span><span class=p>)</span> <span class=k>for</span> <span class=n>token</span><span class=p>,</span> <span class=n>freq</span> <span class=ow>in</span> <span class=n>token_freqs</span><span class=o>.</span><span class=n>items</span><span class=p>()}</span>
</span></span></code></pre></td></tr></table></div></div><p>其中 <code>compute_scores</code> 用于计算最优分割以及从<code>model</code>中去掉每个token之后的loss.</p><p>具体实现见 <a class=link href=https://github.com/MaoSong2022/assignment1-basics/blob/main/cs336_basics/unigram.py target=_blank rel=noopener>Github wordpiece</a> (基于<a class=link href="https://huggingface.co/learn/llm-course/chapter6/4?fw=pt" target=_blank rel=noopener>huggingface llm course</a>)。代码实现的关键在于为每个word选取最优分割，huggingface是采取了动态规划的方法，也就是我们使用 <code>dp[i]</code> 来表示 <code>word[:i]</code> 的最优score，这样我们有：</p>$$
dp[i] = \max_{0 \leq j < i} dp[j]* p(word[j:i]),\quad \mathrm{s.t.}\ word[j:i]\in \mathcal{V}
$$<p>这里的乘法代表 $p(\bm{x}) = \prod_{i=1}^n p(x_i)$, 在实现的时候我们会取log变成加法，然后概率会由频率来代替。</p><h3 id=subword-tokenizer总结>Subword tokenizer总结</h3><p>sub-word tokenizer的对比 (来自<a class=link href="https://huggingface.co/learn/llm-course/chapter6/4?fw=pt" target=_blank rel=noopener>huggingface llm course</a>)：</p><div class=table-wrapper><table><thead><tr><th>Method</th><th>BPE</th><th>WordPiece</th><th>Unigram</th></tr></thead><tbody><tr><td>Start Vocabulary</td><td>Small</td><td>Small</td><td>Large</td></tr><tr><td>Train</td><td>Merge tokens</td><td>Merge tokens</td><td>Remove tokens</td></tr><tr><td>Training Step</td><td>Merge with most frequent pair</td><td>Merge with best score</td><td>Remove all tokens minimized the loss</td></tr><tr><td>Learns</td><td>Merge rules and a vocab</td><td>A vocab</td><td>A vocab with a score for each token</td></tr><tr><td>Encoding</td><td>Splits into words and applies merge rules</td><td>Find the longest subword from the beginning that is in the vocab</td><td>Finds the most likely split into tokens with learned scores</td></tr><tr><td>Model</td><td>GPT</td><td>BERT</td><td>T5</td></tr></tbody></table></div><h2 id=实践>实践</h2><h3 id=tiktoken>tiktoken</h3><p><a class=link href=https://github.com/openai/tiktoken target=_blank rel=noopener>tiktoken</a>是openAI提出来的一个BPE tokenizer, openAI的模型都基于这个tokenizer, 其主要用于调用GPT系列模型是对token进行计数, 我们可以在<a class=link href=https://platform.openai.com/tokenizer target=_blank rel=noopener>tokenizer</a> 这个网站查看其分词情况.</p><h3 id=sentencepiece>SentencePiece</h3><p><a class=link href=https://github.com/google/sentencepiece target=_blank rel=noopener>SentencePiece</a>是google开源的一个无监督的text tokenizer，其实现了BPE和unigram两种算法，SentencePiece还是一个语言无关的tokenizer，使其更适合多语种大语言模型的开发。</p><h3 id=tokenizer>Tokenizer</h3><p><a class=link href=https://github.com/huggingface/tokenizers target=_blank rel=noopener>tokenizer</a> 是huggingface推出的为基于transformer服务的tokenizer库, 其支持BPE, wordpiece和unigram等分词算法, 使用简便. 并且, huggingface的tokenizer包括两种:</p><ol><li>fast tokenizer, 即<a class=link href=https://github.com/huggingface/tokenizers target=_blank rel=noopener>Tokenizer库</a>, 这个库是基于Rust开发的</li><li>slow tokenizer, 这个是transformer库里模型自带的, 比如ChatGLM就有自己开发的tokenizer</li></ol><p>huggingface比较了并行处理时两者的区别:</p><div class=table-wrapper><table><thead><tr><th>Setting</th><th>Fast Tokenizer</th><th>Slow Tokenizer</th></tr></thead><tbody><tr><td><code>batched=True</code></td><td>10.8s</td><td>4min41s</td></tr><tr><td><code>batched=False</code></td><td>59.2s</td><td>5min3s</td></tr></tbody></table></div><p>huggingface提供的tokenizer库已经非常齐全了, 如果我们要训练新的基于transformer的模型的话，建议直接使用Huggingface的<code>AutoTokenizer</code>。</p><h3 id=总结-1>总结</h3><div class=table-wrapper><table><thead><tr><th>特性</th><th>SentencePiece</th><th>Tokenizer</th><th>tiktoken</th></tr></thead><tbody><tr><td>是否适合中文</td><td>√</td><td>√</td><td>×</td></tr><tr><td>是否适合英文</td><td>√</td><td>√</td><td>√</td></tr><tr><td>是否适合训练</td><td>√</td><td>√</td><td>×</td></tr><tr><td>是否快速</td><td>√</td><td>√</td><td>fast</td></tr><tr><td>是否用于 GPT 系列</td><td>×</td><td>×</td><td>√</td></tr><tr><td>是否可解码</td><td>√</td><td>√</td><td>√</td></tr><tr><td>是否支持多语言</td><td>√</td><td>√</td><td>×</td></tr></tbody></table></div><h2 id=结论>结论</h2><p>本文中, 我们介绍了大语言模型中的tokenizer, 我们从byte level, word level到sub-word level, 再到现代大语言模型最常使用的BPE tokenizer, 并给出了其（高效版本）实现。最后, 我们介绍了一下tokenizer-free的大语言模型和huggingface的tokenizer库。在未来, 我们将继续深入了解大语言模型的基本原理和实现细节。</p><h2 id=参考文献>参考文献</h2><ul><li><a class=link href=https://stanford-cs336.github.io/spring2025/ target=_blank rel=noopener>cs336 Lecture1</a></li><li><a class=link href=https://arxiv.org/abs/1508.07909 target=_blank rel=noopener>Neural Machine Translation of Rare Words with Subword Units</a></li><li><a class=link href=https://arxiv.org/pdf/1808.06226 target=_blank rel=noopener>SentencePiece</a></li><li><a class=link href=https://arxiv.org/pdf/1804.10959 target=_blank rel=noopener>Unigram</a></li><li><a class=link href=https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf target=_blank rel=noopener>WordPiece</a></li><li><a class=link href=https://huggingface.co/learn/llm-course/chapter6/1 target=_blank rel=noopener>Huggingface LLM Course</a></li></ul></section><footer class=article-footer><section class=article-tags><a href=/tags/transformer/>Transformer</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section><section class=article-lastmod><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>Last updated on Jun 29, 2025 11:47 +0800</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".main-article");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/hands-on-llm2-transformer/><div class=article-details><h2 class=article-title>Hands on LLM(2) Transformer</h2></div></a></article><article><a href=/p/notes-on-attention-bias/><div class=article-details><h2 class=article-title>Notes on attention bias</h2></div></a></article><article><a href=/p/formal-algorithms-for-transformer/><div class=article-details><h2 class=article-title>Formal Algorithms for Transformer</h2></div></a></article><article><a href=/p/mixstral-8x7b/><div class=article-details><h2 class=article-title>Mixstral 8x7B</h2></div></a></article><article><a href=/p/mixstral-7b/><div class=article-details><h2 class=article-title>Mixstral 7B</h2></div></a></article></div></div></aside><script src=https://utteranc.es/client.js repo=MaoSong2022/MaoSong2022.github.io issue-term=pathname crossorigin=anonymous async></script><style>.utterances{max-width:unset}</style><script>let utterancesLoaded=!1;function setUtterancesTheme(e){let t=document.querySelector(".utterances iframe");t&&t.contentWindow.postMessage({type:"set-theme",theme:`github-${e}`},"https://utteranc.es")}addEventListener("message",e=>{if(e.origin!=="https://utteranc.es")return;utterancesLoaded=!0,setUtterancesTheme(document.documentElement.dataset.scheme)}),window.addEventListener("onColorSchemeChange",e=>{if(!utterancesLoaded)return;setUtterancesTheme(e.detail)})</script><footer class=site-footer><section class=copyright>&copy;
2020 -
2025 Mao Song(毛松)'s Homepage</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.31.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>