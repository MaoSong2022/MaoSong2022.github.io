<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Machine Learning on Mao Song(毛松)'s Homepage</title><link>https://maosong2022.github.io/categories/machine-learning/</link><description>Recent content in Machine Learning on Mao Song(毛松)'s Homepage</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 27 Jun 2025 11:35:33 +0800</lastBuildDate><atom:link href="https://maosong2022.github.io/categories/machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Relationship between MLE and KL divergence</title><link>https://maosong2022.github.io/p/relationship-between-mle-and-kl-divergence/</link><pubDate>Fri, 27 Jun 2025 11:35:33 +0800</pubDate><guid>https://maosong2022.github.io/p/relationship-between-mle-and-kl-divergence/</guid><description>&lt;h1 id="mle">MLE
&lt;/h1>&lt;p>最大似然估计，即MLE (maximum likelihood estimation), 是一个估计参数分布的方法，其核心思想是：模型的参数，应该让观察样本出现的概率最大。&lt;/p>
&lt;p>假设我们有一个参数分布 $p(x\mid \theta)$, 其中 $\theta$ 是参数，如正态分布中的均值和方差。我们从$p(x\mid \theta)$进行采样得到 $i.i.d.$ 的数据 $X={x_1,\dots,x_n}$.&lt;/p>
&lt;p>似然函数 (likelihood function) 定义为给定数据 $X$ 的联合分布，即：&lt;/p>
$$
\mathcal{L}(\theta\mid X) = P(X\mid \theta)
$$&lt;p>由于 $X={x_1,\dots,x_n}$ 是 $i.i.d.$, 因此，我们可以将上式改写为：&lt;/p>
$$
\mathcal{L}(\theta\mid X) = \prod_{i=1}^n p(x_i\mid \theta)
$$&lt;p>这样我们的优化目标就是&lt;/p>
$$
\begin{aligned}
\theta_{MLE}^* &amp;= \arg\max_{\theta} \mathcal{L}(\theta\mid X)\\
&amp;= \arg\max_{\theta} \prod_{i=1}^n p(x_i\mid \theta)\\
&amp;= \arg\max_{\theta} \log\prod_{i=1}^n p(x_i\mid \theta)\\
&amp;=\arg\max_{\theta} \sum_{i=1}^n \log p(x_i\mid \theta)\\
\end{aligned}
$$&lt;p>即&lt;/p>
$$
\theta_{MLE}^* = \arg\max_{\theta} \sum_{i=1}^n \log p(x_i\mid \theta)
$$&lt;h1 id="kl-divergence">KL divergence
&lt;/h1>&lt;p>KL divergence 用于衡量概率分布 $Q(x)$ 到概率分布 $P(x)$ 的不同程度，我们可以将其理解为：如果我们用 $Q(x)$来替换 $P(x)$, 会有多大的信息损失？&lt;/p>
&lt;p>连续概率分布的KL divergence的定义如下&lt;/p>
$$
D_{KL}(P\mid\mid Q) =\int P(x)\log\left(\frac{P(x)}{Q(x)}\right)dx
$$&lt;p>
离散概率分布的KL divergence定义如下&lt;/p>
$$
D_{KL}(P\mid\mid Q) = \sum_{x} P(x)\log\left(\frac{P(x)}{Q(x)}\right)
$$&lt;p>KL divergence有两个关键性质：&lt;/p>
&lt;ol>
&lt;li>非负性：$D_{KL}(P\mid\mid Q)\geq0$, 且 $D_{KL}(P\mid\mid Q)=0$ 当且仅当 $P(x)=Q(x)$ 对任意 $x$成立&lt;/li>
&lt;li>非对称性： 一般情况下，$D_{KL}(P\mid\mid Q)\neq D_{KL}(Q\mid\mid P)$.&lt;/li>
&lt;/ol>
&lt;h1 id="mle和kl-divergence的等价性">MLE和KL Divergence的等价性
&lt;/h1>&lt;p>我们假设 $p_{data}(x)$ 是数据$X$的真实分布， 我们现在需要找到合适的参数 $\theta$ 以及其对应的分布 $p(x\mid \theta)$ 来近似 $p_{data}(x)$, 此时我们可以用KL Divergence作为我们的目标函数，即&lt;/p>
$$
\theta_{KL} = \arg\min_{\theta}D_{KL}(p_{data}(x)\mid\mid p(x\mid \theta))
$$&lt;p>我们将上面的式子进行展开得到&lt;/p>
$$
\begin{aligned}
\theta_{KL}^* &amp;= \arg\min_{\theta}D_{KL}(p_{data}(x)\mid\mid p(x\mid \theta))\\
&amp;= \arg\min_{\theta} \int p_{data}(x)\frac{p_{data}(x)}{p(x\mid \theta)} dx\\
&amp;= \arg\min_{\theta}\int p_{data}(x)\log p_{data}(x) dx - \int p_{data}(x)\log p(x\mid \theta)dx \\
&amp;= \arg\min_{\theta} - \int p_{data}(x)\log p(x\mid \theta)dx \\
&amp;= \arg\max_{\theta} \int p_{data}(x)\log p(x\mid \theta)dx
\end{aligned}
$$&lt;p>实际上，真实的数据分布 $p_{data}(x)$ 是未知的，我们只有从 $p_{data}(x)$ 采样得到的一批数据 $X={x_1,\dots,x_n}\sim p_{data}(x)$.&lt;/p>
&lt;p>基于大数定律，我们有&lt;/p>
$$
\frac{1}{n}\sum_{i=1}^n\log p(\theta_i\mid \theta)=\mathbb{E}_{x\sim p_{data}}[\log p(x\mid \theta)] = \int p_{data}(x)\log p(x\mid \theta)dx, n\to \infty
$$&lt;p>这样，最大似然估计就与最小化KL divergence构建起了联系：&lt;/p>
$$
\begin{aligned}
\theta_{MLE}^*&amp;=\arg\max_{\theta} \sum_{i=1}^n \log p(x_i\mid \theta)\\
&amp;= \arg\max_{\theta} \int p_{data}(x)\log p(x\mid \theta)dx\\
&amp;= \theta_{KL}^*, n\to\infty.
\end{aligned}
$$&lt;p>也就是说，当采样样本足够多的时候，最大似然估计和最小KL divergence是等价的。&lt;/p></description></item><item><title>Notes on t-SNE</title><link>https://maosong2022.github.io/p/notes-on-t-sne/</link><pubDate>Thu, 02 May 2024 13:13:12 +0800</pubDate><guid>https://maosong2022.github.io/p/notes-on-t-sne/</guid><description>&lt;p>This post introduces how to understand t-SNE.&lt;/p>
&lt;h1 id="introduction">Introduction
&lt;/h1>&lt;p>t-SNE an dimension reduction algorithm, which projects high-dimensional data into low-dimensional space. Thus the algorithm can be used to visualize the data distribution.&lt;/p>
&lt;p>To understand how t-SNE works, we first review the SNE algorithms, then we introduce the t-SNE algorithm.&lt;/p>
&lt;h1 id="sne">SNE
&lt;/h1>&lt;h2 id="method">Method
&lt;/h2>&lt;p>Stochastic Neighbor Embedding, or SNE, is the previous version of t-SNE.&lt;/p>
&lt;p>The basic idea behind SNE is that: &lt;em>data points that are close in high-dimensional space should be close in lower-dimensional space too&lt;/em>.&lt;/p>
&lt;p>Formally speaking, given a data set $X\in\mathbb{R}^{D\times N}$ consisting of $N$ data points, with each data point lies in $D$ dimensional space. Our goal is to reduce the data points into $d&amp;laquo; D$ dimensional space $Y\in\mathbb{R}^{d\times N}$, that is, we seek to find a map $f:\mathbb{R}^{D\times N}\to \mathbb{R}^{d\times N}$ such that $f(X)=Y$. Usually, $d=2$ or $d=3$ for visualization use.&lt;/p>
&lt;p>SNE measures &amp;ldquo;close&amp;rdquo; in a probabilistic way. The similarity is represented by converting Euclidean distance between data points to condition probabilities:&lt;/p>
$$ p_{j\mid i} = \frac{\exp\left(-\Vert\bm{x}_i-\bm{x}_j\Vert^2/(2\sigma_i^2)\right)}{\sum_{k\neq i}\exp\left(-\Vert\bm{x}_i-\bm{x}_k\Vert^2/(2\sigma_i^2)\right)} $$&lt;p>the above equation can be interpreted as &lt;em>the probability of point $\bm{x}_j$ being a neighbor of point $\bm{i}$ is proportional to the distance between them&lt;/em>. $\sigma_i$ is the variance of the Gaussian distribution that is centered on data point $\bm{x}_i$. We introduce the method for determining $\sigma_i$ later.&lt;/p>
&lt;p>Similarly, we can construct a probability distribution $q$ based on $Y$.&lt;/p>
$$ q_{j\mid i} = \frac{\exp\left(-\Vert\bm{x}_i-\bm{x}_j\Vert^2\right)}{\sum_{k\neq i}\exp\left(-\Vert\bm{x}_i-\bm{x}_k\Vert^2\right)} $$&lt;p>where we set the variance as $1/\sqrt{2}$ following the original paper.&lt;/p>
&lt;p>$p_{i\mid i}$ and $q_{i\mid i}$ are set $0$ since we are only interested in modeling pairwise similarities.&lt;/p>
&lt;p>Now we want $q_{j\mid i}$ are as close as $p_{j\mid i}$, that is, we want two distributions are as close as to each other. This can be measured by &lt;strong>Kullback- Leibler divergence&lt;/strong>, which is written as:&lt;/p>
$$ C(P, Q) = \sum_{i=1}^N\mathrm{KL}(P_i\Vert Q_i)=\sum_{i=1}^N\sum_{j=1}^N p_{j\mid i}\log \frac{p_{i\mid j}}{q_{i\mid j}} $$&lt;p>where $P_i=[p_{1\mid i},\dots,p_{N\mid i}]\in\mathbb{R}^N$ and $Q_i=[q_{1\mid i},\dots,q_{N\mid i}]\in\mathbb{R}^N$.&lt;/p>
&lt;h2 id="choosing-sigma">Choosing $\sigma$
&lt;/h2>&lt;p>Now we introduce how to choose $\sigma$. Note that $\sigma$ determines the distribution of data points, larger $\sigma$ indicates sparser distribution of data points. The original paper uses &lt;em>perplexity&lt;/em> to measure such sparsity. It is defined as&lt;/p>
$$ \mathrm{Perp}(P_i) = 2^{H(P_i)} $$&lt;p>where $H(P_i)$ is the &lt;em>Shannon entropy&lt;/em> of $P_i$ measured in bits:&lt;/p>
$$ H(P_i) = -\sum_{i=1}^N p_{j\mid i}\log p_{j\mid i} $$&lt;p>The perplexity can be interpreted as a smooth measure of the effective number of neighbors. The performance of SNE is fairly robust to changes in the perplexity, and typical values are between 5 and 50.&lt;/p>
&lt;p>Notice that $p_{j\mid i}$, by setting different value on $\mathrm{Perp}(P_i)$, we can obtain different $\sigma_i$ via binary search.&lt;/p>
&lt;h2 id="optimization">Optimization
&lt;/h2>&lt;p>Our goal now becomes minimizing $C(p, q)$ over variables $\bm{y}_1,\dots,\bm{y}_N\in\mathbb{R}^d$, given $p$ and hyperparameter $\sigma_i, i=1,\dots,N$. This can be done via gradient descent methods. The gradient is now given by&lt;/p>
$$ \frac{d C}{d\bm{y}_i}=2\sum_{j=1}^N\left(p_{j\mid i} - q_{j\mid i} + p_{i\mid j}- q_{i\mid j} \right)(\bm{y}_i-\bm{y}_j)\in\mathbb{R}^d, \ i=1,\dots,N $$&lt;p>We can write it in matrix form and add a momentum term:&lt;/p>
$$ Y^{t+1} = Y^t + \beta\frac{dC}{dY} + \alpha_t\left(Y^{t-1}-Y^{t-2}\right) $$&lt;p>where $\beta$ is the step size and $\alpha_t$ is momentum parameter,&lt;/p>
$$ Y^t = [\bm{y}_1^t,\dots, \bm{y}_N^t]\in\mathbb{R}^{d\times N} ,\ \frac{dC}{dY} = \left[\frac{d C}{d\bm{y}_1},\dots,\frac{d C}{d\bm{y}_N}\right]\in\mathbb{R}^{d\times N} $$&lt;h1 id="t-sne">t-SNE
&lt;/h1>&lt;h2 id="symmetric-sne">Symmetric SNE
&lt;/h2>&lt;p>The first difference between t-SNE and SNE is the probability, t-SNE uses symmetric version of SNE to simplify computations.&lt;/p>
&lt;p>Different from SNE, symmetric SNE uses a joint probability instead of a condition probability:&lt;/p>
$$ C(P, Q) = \sum_{i=1}^N\mathrm{KL}(P\Vert Q)=\sum_{i=1}^N\sum_{j=1}^N p_{ij}\log \frac{p_{ij}}{q_{ij}} $$&lt;p>where $p_{ij}$ and $q_{ij}$ are defined as&lt;/p>
$$ p_{ij} = \frac{\exp\left(-\Vert\bm{x}_i-\bm{x}_j\Vert^2/(2\sigma^2)\right)}{\sum_{k\neq r}\exp\left(-\Vert\bm{x}_r-\bm{x}_k\Vert^2/(2\sigma^2)\right)},q_{ij} = \frac{\exp\left(-\Vert\bm{x}_i-\bm{x}_j\Vert^2\right)}{\sum_{k\neq r}\exp\left(-\Vert\bm{x}_r-\bm{x}_k\Vert^2\right)} $$&lt;p>the problem if joint probability $p_{ij}$ is that if there is an outlier $\bm{x}&lt;em>i$, then $p&lt;/em>{ij}$ will be extremely small for all $j$. This problem can be solved by defining $p_{ij}$ from the conditional probability $p_{i\mid j}$ and $p_{j\mid i}$&lt;/p>
$$ p_{ij} = \frac{p_{j\mid i}+p_{i\mid j}}{2N} $$&lt;p>This ensures that&lt;/p>
$$ \sum_{j=1}^N p_{ij} > \frac{1}{2N} $$&lt;p>for all $\bm{x}_i$, in result, each data point makes a significant contribution to the cost function.&lt;/p>
&lt;p>In this case, the gradient of the cost function is now given by&lt;/p>
$$ \frac{d C}{d\bm{y}_i}=4\sum_{j=1}^N\left(p_{ij} - q_{ij}\right)(\bm{y}_i-\bm{y}_j)\in\mathbb{R}^d, \ i=1,\dots,N $$&lt;h2 id="t-sne-1">t-SNE
&lt;/h2>&lt;p>Experiments show that symmetric SNE seems to produce maps that are just as good as asymmetric SNE, and sometimes even a little better.&lt;/p>
&lt;p>However, there is a problem with SNE, that is, the &lt;em>crowding problem&lt;/em>, which manifests as a tendency for points in the low-dimensional space to be clustered too closely together, particularly in high-density regions of the data.&lt;/p>
&lt;p>The causes of the crowding problems are:&lt;/p>
&lt;ol>
&lt;li>Data points in high dimensional space tend to far from each other, which makes the distance information less useful.&lt;/li>
&lt;li>SNE aims to preserve the local structure of the data points, but it can struggle with non-linear relationships. The projected data points will be closed to each other due to this reason.&lt;/li>
&lt;li>The optimization algorithm used by SNE can get stuck in local minimum.&lt;/li>
&lt;/ol>
&lt;p>To alleviate the crowding problem, t-SNE is introduced in the following way:
&lt;em>In the high-dimensional space, we convert distances into probabilities using a Gaussian distribution. In the low-dimensional map, we can use a probability distribution that has much heavier tails than a Gaussian to convert distances into probabilities.&lt;/em>
This allows a moderate distance in the high-dimensional space to be faithfully modeled by a much larger distance in the map and, as a result, it eliminates the unwanted attractive forces between map points that represent moderately dissimilar data points.&lt;/p>
&lt;p>t-SNE uses student t-distribution in low-dimensional map:&lt;/p>
$$ q_{ij} = \frac{\left(1+\Vert\bm{y}_i-\bm{y}_j\Vert^2\right)^{-1}}{\sum_{k\neq r}\left(1+\Vert\bm{y}_i-\bm{y}_j\Vert^2\right)^{-1}} $$&lt;p>A Student t-distribution with a single degree of freedom is used, because it has the particularly nice property that $\left(1+\Vert\bm{y}_i-\bm{y}_j\Vert^2\right)^{-1}$ approaches an inverse square law for large pairwise distances $\Vert\bm{y}_i-\bm{y}_j\Vert$ in the low-dimensional map.&lt;/p>
&lt;p>Compared to Gaussian distribution, t-distribution is heavily tailed。&lt;/p>
&lt;p>A computationally convenient property of t-SNE is that it is much faster to evaluate the density of a point under a Student t-distribution than under a Gaussian because it does not involve an exponential, even though the Student t-distribution is equivalent to an infinite mixture of Gaussians with different variances.&lt;/p>
&lt;p>The gradient of t-SNE is given by&lt;/p>
$$ \frac{d C}{d\bm{y}_i}=4\sum_{j=1}^N\left(p_{ij} - q_{ij}\right)(\bm{y}_i-\bm{y}_j)\left(1+\Vert\bm{y}_i-\bm{y}_j\Vert^2\right)^{-1}\in\mathbb{R}^d, \ i=1,\dots,N $$&lt;p>The advantages of t-SNE gradients over SNE are given by:&lt;/p>
&lt;ol>
&lt;li>The t-SNE gradient strongly repels dissimilar data points that are modeled by a small pairwise distance in the low-dimensional representation.&lt;/li>
&lt;li>Second, although t-SNE introduces strong repulsions between dissimilar data points that are modeled by small pairwise distances, these repulsions do not go to infinity.&lt;/li>
&lt;/ol>
&lt;p>The algorithm is given as follows:&lt;/p>
&lt;p>&lt;img src="https://maosong2022.github.io/p/notes-on-t-sne/t-SNE-algorithm.png"
width="1910"
height="1096"
srcset="https://maosong2022.github.io/p/notes-on-t-sne/t-SNE-algorithm_hu528927631688914678.png 480w, https://maosong2022.github.io/p/notes-on-t-sne/t-SNE-algorithm_hu15284604004524732623.png 1024w"
loading="lazy"
alt="algorithm"
class="gallery-image"
data-flex-grow="174"
data-flex-basis="418px"
>&lt;/p>
&lt;h2 id="optimization-1">Optimization
&lt;/h2>&lt;p>There are some optimizations that can be used to improve performance of t-SNE:&lt;/p>
&lt;ol>
&lt;li>Early compression, which is used to force the map points to stay close together at the start of the optimization&lt;/li>
&lt;li>Early exaggeration, which is used to multiply all of the pi j’s by, for example, 4, in the initial stages of the optimization&lt;/li>
&lt;/ol>
&lt;h1 id="implementation">Implementation
&lt;/h1>&lt;h1 id="reference">Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="http://jmlr.org/papers/v9/vandermaaten08a.html" target="_blank" rel="noopener"
>Visualizing Data using t-SNE&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Practical advice for analysis of large, complex data sets</title><link>https://maosong2022.github.io/p/practical-advice-for-analysis-of-large-complex-data-sets/</link><pubDate>Wed, 17 Apr 2024 22:40:11 +0800</pubDate><guid>https://maosong2022.github.io/p/practical-advice-for-analysis-of-large-complex-data-sets/</guid><description>&lt;h1 id="introduction">Introduction
&lt;/h1>&lt;p>These advises are given by Patrick Riley in 2016, though it has been years util now, I think some of them are still useful.&lt;/p>
&lt;p>The advice is organized into three general areas:&lt;/p>
&lt;ul>
&lt;li>Technical: Ideas and techniques for how to manipulate and examine your data.&lt;/li>
&lt;li>Process: Recommendation on how you approach your data, what questions to ask, and what things to check.&lt;/li>
&lt;li>Social: How to work with others and communicate about your data and insights.&lt;/li>
&lt;/ul>
&lt;h1 id="technical">Technical
&lt;/h1>&lt;h2 id="look-at-your-distribution">Look at your distribution
&lt;/h2>&lt;p>Besides the typically used summary metrics, we should looking at a much richer representation of the distribution, such as histograms, CDFs, Q-Q plots, etc. This allows us to see some interesting features.&lt;/p>
&lt;h2 id="consider-the-outliers">Consider the outliers
&lt;/h2>&lt;p>We should look at the outliers in our data. It&amp;rsquo;s fine to exclude them from our data or to lump them together into an unusual category, but we should make sure we know why.&lt;/p>
&lt;h2 id="report-noise-confidence">Report noise/ confidence
&lt;/h2>&lt;p>Every estimator that you produce should have a notion of your confidence in this estimate attached to it.&lt;/p>
&lt;h2 id="look-at-examples">Look at examples
&lt;/h2>&lt;p>Anytime you are producing new analysis code, you need to look at examples of the underlying data and how your code is interpreting those examples.&lt;/p>
&lt;h2 id="slice-your-data">Slice your data
&lt;/h2>&lt;p>slicing help us obtain underlying features of the data subgroups easier. However, when we use slicing, we need to care about the mix shift.&lt;/p>
&lt;h2 id="consider-practical-significance">Consider practical significance
&lt;/h2>&lt;p>Don&amp;rsquo;t be blind by statistics, watch out those may have an impact on deploying or ethical problems.&lt;/p>
&lt;h2 id="check-for-consistency-over-time">Check for consistency over time
&lt;/h2>&lt;p>One particular slicing you should almost always employ is to slice by units of time.
This is because many disturbances to underlying data happen as our systems evolve over time.&lt;/p>
&lt;h1 id="process">Process
&lt;/h1>&lt;h2 id="separate-validation-description-and-evaluation">Separate Validation, description, and evaluation
&lt;/h2>&lt;ul>
&lt;li>Description should be things that everyone can agree on from the data.&lt;/li>
&lt;li>Evaluation is likely to have much more debate because you imbuing meaning and value to the data.&lt;/li>
&lt;/ul>
&lt;h2 id="confirm-exptdata-collection-setup">Confirm expt/data collection setup
&lt;/h2>&lt;p>Before looking at any data, make sure you understand the experiment and data collection setup&lt;/p>
&lt;h2 id="check-vital-signs">Check vital signs
&lt;/h2>&lt;p>Before actually answering the question you are interested in you need to check for a lot of other things that may not be related to what you are interested in but may be useful in later analysis or indicate problems in the data&lt;/p>
&lt;h2 id="standard-first-custom-second">Standard first, custom second
&lt;/h2>&lt;p>When we use metric, we should always look at standard metrics first, even if we expect them to change.&lt;/p>
&lt;h2 id="measure-twice-or-more">Measure twice, or more
&lt;/h2>&lt;p>If you are trying to capture a new phenomenon, try to measure the same underlying thing in multiple ways.
Then, check to see if these multiple measurements are consistent&lt;/p>
&lt;h2 id="check-for-reproducibility">Check for reproducibility
&lt;/h2>&lt;p>Both slicing and consistency over time are particular examples of checking for reproducibility.
If a phenomenon is important and meaningful, you should see it across different user populations and time.&lt;/p>
&lt;h2 id="check-for-consistency-with-past-measurements">Check for consistency with past measurements
&lt;/h2>&lt;p>You should compare your metrics to metrics reported in the past, even if these measurements are on different user populations.&lt;/p>
&lt;p>New metrics should be applied to old data/features first&lt;/p>
&lt;h2 id="make-hypothesis-and-look-for-evidence">Make hypothesis and look for evidence
&lt;/h2>&lt;p>Typically, exploratory data analysis for a complex problem is iterative. You will discover anomalies, trends, or other features of the data. Naturally, you will make hypotheses to explain this data. It’s essential that you don’t just make a hypothesis and proclaim it to be true. Look for evidence (inside or outside the data) to confirm/deny this theory.&lt;/p>
&lt;h2 id="exploratory-analysis-benefits-from-end-to-end-iteration">Exploratory analysis benefits from end to end iteration
&lt;/h2>&lt;p>When doing exploratory analysis, you should strive to get as many iterations of the whole analysis as possible.&lt;/p>
&lt;h1 id="social">Social
&lt;/h1>&lt;h2 id="data-analysis-starts-with-questions-not-data-or-a-technique">Data analysis starts with questions, not data or a technique
&lt;/h2>&lt;p>Ask question first and use tools to answer the questions.&lt;/p>
&lt;h2 id="acknowledge-and-count-your-filtering">Acknowledge and count your filtering
&lt;/h2>&lt;ul>
&lt;li>Acknowledge and clearly specify what filtering you are doing&lt;/li>
&lt;li>Count how much is being filtered at each of your steps
the best way to do the latter is to actually compute all your metrics even for the population you are excluding&lt;/li>
&lt;/ul>
&lt;h2 id="ratios-should-have-clear-numerator-and-denominators">Ratios should have clear numerator and denominators
&lt;/h2>&lt;p>When you communicate results containing ratios, you must be clear about the numerator and denominator.&lt;/p>
&lt;h2 id="educate-your-consumers">Educate your consumers
&lt;/h2>&lt;p>You are responsible for providing the context and a full picture of the data and not just the number a consumer asked for.&lt;/p>
&lt;h2 id="be-both-skeptic-and-champion">Be both skeptic and champion
&lt;/h2>&lt;p>As you work with data, you must be both the champion of the insights you are gaining as well as a skeptic.&lt;/p>
&lt;h2 id="share-with-peers-first-external-consumers-second">Share with peers first, external consumers second
&lt;/h2>&lt;p>A skilled peer reviewer can provide qualitatively different feedback and sanity-checking than the consumers of your data can, especially since consumers generally have an outcome they want to get&lt;/p>
&lt;h1 id="reference">Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://www.unofficialgoogledatascience.com/2016/10/practical-advice-for-analysis-of-large.html" target="_blank" rel="noopener"
>Practical advice for analysis of large, complex data sets&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Rules of Machine Learning</title><link>https://maosong2022.github.io/p/rules-of-machine-learning/</link><pubDate>Sat, 13 Apr 2024 19:57:47 +0800</pubDate><guid>https://maosong2022.github.io/p/rules-of-machine-learning/</guid><description>&lt;p>Best practice for machine learning.&lt;/p>
&lt;h1 id="introduction">Introduction
&lt;/h1>&lt;p>Google posts a guide on how to uses machine learning in practice.
It represents a style for machine learning, similar to Google C++ Style Guide.&lt;/p>
&lt;p>In overview, to make great products:&lt;/p>
&lt;blockquote>
&lt;p>Do machine learning like the great engineer your are, not like the great machine learning expert you are.&lt;/p>
&lt;/blockquote>
&lt;p>Most algorithms we are facing are engineering problems instead of machine learning algorithms. A basic approach Google recommends is:&lt;/p>
&lt;ol>
&lt;li>Make sure your pipeline is solid end to end&lt;/li>
&lt;li>Start with a reasonable objective&lt;/li>
&lt;li>Add a common-sense features in a simple way&lt;/li>
&lt;li>Make sure that your pipeline stays solid.&lt;/li>
&lt;/ol>
&lt;p>Google separates rules with respect to different stages.&lt;/p>
&lt;h1 id="before-machine-learning">Before machine learning
&lt;/h1>&lt;p>These rules help us understand whether the time is right for building a machine learning system.&lt;/p>
&lt;blockquote>
&lt;p>Rule #1: Do not be afraid of lunch a product without machine learning.&lt;/p>
&lt;/blockquote>
&lt;p>This rule tells us that is not absolutely necessary, if rule-based methods work well, there is no need to develop a machine learning algorithm.&lt;/p>
&lt;blockquote>
&lt;p>Rule #2: First, design and implement metrics.&lt;/p>
&lt;/blockquote>
&lt;p>This rule tells us that tracking as much as possible before we formalize what our machine learning system will do. This step helps us construct the goal of our system, that is, a metric.&lt;/p>
&lt;blockquote>
&lt;p>Rule #3: Choose machine learning over a complex heuristic.&lt;/p>
&lt;/blockquote>
&lt;p>Considering using machine learning algorithms only if the heuristic algorithm doesn&amp;rsquo;t work well, since a complex heuristic is not maintainable. Meanwhile, machine-learned models are easier to update and maintain.&lt;/p>
&lt;h1 id="ml-phase-1-your-first-pipeline">ML phase 1: Your First Pipeline
&lt;/h1>&lt;p>When creating our first pipeline, we should focus on our system infrastructure&lt;/p>
&lt;blockquote>
&lt;p>Rule #4: Keep the first model simple and get the infrastructure right.&lt;/p>
&lt;/blockquote>
&lt;p>Remember infrastructure issues are many more than model problems when we creating the first machine learning model. We have to determine:&lt;/p>
&lt;ol>
&lt;li>How to obtain data for our model&lt;/li>
&lt;li>How to evaluate the performance of our model&lt;/li>
&lt;li>How to integrate our model into our application&lt;/li>
&lt;/ol>
&lt;p>Moreover, we should choose simple features to ensure that:&lt;/p>
&lt;ul>
&lt;li>The features reach our learning algorithm correctly&lt;/li>
&lt;li>The model learns reasonably weights&lt;/li>
&lt;li>The features reach our model in the sever correctly&lt;/li>
&lt;/ul>
&lt;p>Once we have a system that does above things reliably, we have done most of the work.&lt;/p>
&lt;blockquote>
&lt;p>Rule #5: Test the infrastructure independently from the machine learning.&lt;/p>
&lt;/blockquote>
&lt;p>This rule tells us split the infrastructure and the machine learning model and test them separately to avoid dependency issue occurs. In this way, our model can be developed without worrying about environment. Specifically:&lt;/p>
&lt;ol>
&lt;li>Test getting data into algorithm. Check the data that are feed into the models and do some statistics before using the data&lt;/li>
&lt;li>Testing getting models out of the training algorithm. Make sure our algorithms work in the same way in our serving environment as the training environment.&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>Rule #6: Be careful about dropped data.&lt;/p>
&lt;/blockquote>
&lt;p>Do not drop data without thoroughly test, this may data loss.&lt;/p>
&lt;blockquote>
&lt;p>Rule #7: Turn heuristics into features, or handle them externally.&lt;/p>
&lt;/blockquote>
&lt;p>Before using machine learning models, if we have tried some heuristic algorithms, then these algorithms may help us to improve the overall performance. Some ways we can use an existing heuristic algorithm:&lt;/p>
&lt;ol>
&lt;li>Preprocessing using the heuristic. If the feature is incredibly awesome, then do not try to relearn the feature. Just use the heuristic way to pre-process the data.&lt;/li>
&lt;li>Create a feature. We can use the heuristic way to create a new feature to help improve the machine learning performance.&lt;/li>
&lt;li>Mine the raw inputs of the heuristic. We can use the inputs of heuristic as features to learn the heuristic implicitly.&lt;/li>
&lt;li>Modify the label.&lt;/li>
&lt;/ol>
&lt;h2 id="monitoring">Monitoring
&lt;/h2>&lt;p>In general, such as making alerts and having a dashboard page.&lt;/p>
&lt;blockquote>
&lt;p>Rule #8: Know the freshness requirements of our system.&lt;/p>
&lt;/blockquote>
&lt;p>It is important for us to know the freshness of our model, for example, how much does performance degrade if we have a model that is a day old. The freshness helps us monitor and improve the performance.&lt;/p>
&lt;blockquote>
&lt;p>Rule #9: Detect problems before exporting models.&lt;/p>
&lt;/blockquote>
&lt;p>This rule tells us that evaluating the performance of the model before serving. The evaluation includes the testing on hold-out data, check AUC metric.&lt;/p>
&lt;blockquote>
&lt;p>Rule #10: Watch for silent failures.&lt;/p>
&lt;/blockquote>
&lt;p>Since the continuos change of data, silent failures may occur, so keep tracking statistics of the data as well as manually inspect the data on occasion help us reduce these kind of issues.&lt;/p>
&lt;blockquote>
&lt;p>Rule #11: Give feature owners and documentation.&lt;/p>
&lt;/blockquote>
&lt;p>Knowing who created the feature helps us gain information about data. A detailed documentation helps user understand how it works.&lt;/p>
&lt;h2 id="your-first-objective">Your first objective
&lt;/h2>&lt;blockquote>
&lt;p>Rule #12: Don&amp;rsquo;t overthink which objective you choose to optimize.&lt;/p>
&lt;/blockquote>
&lt;p>There are many metrics to optimize according to Rule #2. However, it turns out in early stage, some metrics are optimized even though we not directly optimizing them.&lt;/p>
&lt;blockquote>
&lt;p>Rule #13: Choose simple, observable and attributable metric for your first objective.&lt;/p>
&lt;/blockquote>
&lt;p>This rule tells us the strategy of choosing metric in the beginning. In principal, The ML objective should be something that is easy to measure and is a proxy for the &amp;ldquo;true&amp;rdquo; objective. In fact however, there is no such &amp;ldquo;true&amp;rdquo; objective, so we should keep the objective as simple as possible, it&amp;rsquo;s better if the objective is observable. Then, we can modify the objective based on the performance of the model.&lt;/p>
&lt;blockquote>
&lt;p>Rule #14: Starts with an interpretable model makes debugging easier.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Rule #15: Separate spam filtering and quality ranking in a policy layer.&lt;/p>
&lt;/blockquote>
&lt;p>Sometimes, spam filtering confuses quality ranking, when we do quality ranking, we should clean the data.&lt;/p>
&lt;h1 id="ml-phase-2-feature-engineering">ML phase 2: Feature engineering
&lt;/h1>&lt;p>After we have a working end to end system with unit and system tests instrumented, Phase II begins.
In this phase, we should make use of features.&lt;/p>
&lt;blockquote>
&lt;p>Rule #16: Plan to lunch and iterate.&lt;/p>
&lt;/blockquote>
&lt;p>There are three reasons to lunch new models:&lt;/p>
&lt;ol>
&lt;li>You are coming up with new features&lt;/li>
&lt;li>You are tuning regularization and combining old features in new ways&lt;/li>
&lt;li>You are tuning the objectives&lt;/li>
&lt;/ol>
&lt;p>When lunch a new model, we should think about:&lt;/p>
&lt;ol>
&lt;li>How easy is it to add or remove or recombine features&lt;/li>
&lt;li>How easy is it to create a fresh copy of the pipeline and verify its correctness.&lt;/li>
&lt;li>Is it possible to have two or three copies running in parallel.&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>Rule #17: Start with directly observed and reported features as opposed to learned features.&lt;/p>
&lt;/blockquote>
&lt;p>a learned feature is a feature generated by an external system or by the learner itself.
If the learned feature comes from an external system, then bias or being out-of-date may affect the model. If the learned feature comes from the learner itself, then it is hard to tell the impact of the feature.&lt;/p>
&lt;blockquote>
&lt;p>Rule #18: Explore with features of content that generalize across contexts.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Rule #19: Use very specific features when you can.&lt;/p>
&lt;/blockquote>
&lt;p>It is simpler to learn millions of simple features than a few complex features.&lt;/p>
&lt;blockquote>
&lt;p>Rule #20: Combine and modify existing features to create new features in human-understanding ways.&lt;/p>
&lt;/blockquote>
&lt;p>Combine features may causing overfitting problems&lt;/p>
&lt;blockquote>
&lt;p>Rule #21: The number of feature weights we can learn in a linear model is roughly proportional to the number of data you have.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Rule #22: Clean up features you are no longer using.&lt;/p>
&lt;/blockquote>
&lt;p>If you find that you are not using a feature, and that combining it with other features is not working, then drop it out of your infrastructure.&lt;/p>
&lt;h2 id="human-analysis-of-the-system">Human analysis of the system
&lt;/h2>&lt;p>This subsection teaches us how to look at an existing model and improve it.&lt;/p>
&lt;blockquote>
&lt;p>Rule #23: You are not a typical end user.&lt;/p>
&lt;/blockquote>
&lt;p>Check carefully before we deploying the model.&lt;/p>
&lt;blockquote>
&lt;p>Rule #24: Measure the delta between models&lt;/p>
&lt;/blockquote>
&lt;p>Make sure the system is stable when making small changes. Make sure that a model when compared with itself has a low (ideally zero) symmetric difference.&lt;/p>
&lt;blockquote>
&lt;p>Rule #25: When choosing models, utilitarian performance trumps predictive power.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Rule #26: Look for patterns in the measured errors, and create new features.&lt;/p>
&lt;/blockquote>
&lt;p>Once you have examples that the model got wrong, look for trends that are outside your current feature set.&lt;/p>
&lt;blockquote>
&lt;p>Rule #27: Try to quantify observed undesired behavior&lt;/p>
&lt;/blockquote>
&lt;p>If your issues are measurable, then you can start using them as features, objectives, or metrics. The general rule is &amp;ldquo;&lt;strong>measure first, optimize second&lt;/strong>&amp;rdquo;.&lt;/p>
&lt;blockquote>
&lt;p>Rule #28: Be aware that identical short-term behavior does not imply identical long-term behavior.&lt;/p>
&lt;/blockquote>
&lt;h2 id="training-serving-skew">Training-Serving skew
&lt;/h2>&lt;p>Training-serving skew is a difference between performance during training and performance during serving.
This skew can be caused by:&lt;/p>
&lt;ol>
&lt;li>A discrepancy between how you handle data in the training and serving pipelines&lt;/li>
&lt;li>A change in the data between when you train and when you serve&lt;/li>
&lt;li>A feedback loop between your model and your algorithm.&lt;/li>
&lt;/ol>
&lt;p>The best solution is to explicitly monitor it so that system and data changes don&amp;rsquo;t introduce skew unnoticed.&lt;/p>
&lt;blockquote>
&lt;p>Rule #29: The best way to make sure that you train like you serve is to save the set of features used at the serving time, and then pipe those features to a log to use them at training time.&lt;/p>
&lt;/blockquote>
&lt;p>This can help verify the consistency between the training and serving.&lt;/p>
&lt;blockquote>
&lt;p>Rule #30: Importance-weight sampled data, don&amp;rsquo;t arbitrarily drop it.&lt;/p>
&lt;/blockquote>
&lt;p>Importance weighting means that if you decide that you are going to sample example X with a 30% probability, then give it a weight of 10/3. With importance weighting, all of the calibration properties discussed in Rule #14 still hold.&lt;/p>
&lt;blockquote>
&lt;p>Rule #31: Beware that if your join data from a table at training and serving time, the data in the table may change.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Rule #32: Re-use code between your training pipeline and your serving pipeline whenever possible.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Rule #33: If you produce a model based on the data until January 5th, test the model on the data from January 6th and after.&lt;/p>
&lt;/blockquote>
&lt;p>In general, measure performance of a model on the data gathered after the data you trained the model on, as this better reflects what your system will do in production&lt;/p>
&lt;blockquote>
&lt;p>Rule #37: Measure Training-Serving Skew.&lt;/p>
&lt;/blockquote>
&lt;p>We can divide causes of Training-Serving Skew into several parts:&lt;/p>
&lt;ol>
&lt;li>The difference between the performance on the training data and the holdout data. In general, this will always exist, and it is not always bad.&lt;/li>
&lt;li>The difference between the performance on the holdout data and the &amp;ldquo;next­day&amp;rdquo; data. Again, this will always exist&lt;/li>
&lt;li>The difference between the performance on the &amp;ldquo;next-day&amp;rdquo; data and the live data.&lt;/li>
&lt;/ol>
&lt;h1 id="ml-phase-3-slowed-growth-optimization-refinement-and-complex-models">ML phase 3: Slowed growth, Optimization refinement, and complex models
&lt;/h1>&lt;blockquote>
&lt;p>Rule #38: Don’t waste time on new features if unaligned objectives have become the issue.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Rule #39: Launch decisions are a proxy for long-term product goals.&lt;/p>
&lt;/blockquote>
&lt;p>The only easy launch decisions are when all metrics get better (or at least do not get worse).&lt;/p>
&lt;p>Individuals, on the other hand, tend to favor one objective that they can directly optimize.&lt;/p>
&lt;blockquote>
&lt;p>Rule #40: Keep ensembles simple.&lt;/p>
&lt;/blockquote>
&lt;p>To keep things simple, each model should either be an ensemble only taking the input of other models, or a base model taking many features, but not both.&lt;/p>
&lt;blockquote>
&lt;p>Rule #41: When performance plateaus, look for qualitatively new sources of information to add rather than refining existing signals.&lt;/p>
&lt;/blockquote>
&lt;p>As in any engineering project, you have to weigh the benefit of adding new features against the cost of increased complexity.&lt;/p>
&lt;blockquote>
&lt;p>Rule #42: Don’t expect diversity, personalization, or relevance to be as correlated with popularity as you think they are.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Rule #43: Your friends tend to be the same across different products. Your interests tend not to be.&lt;/p>
&lt;/blockquote>
&lt;h1 id="conclusion">Conclusion
&lt;/h1>&lt;p>In early stage, make sure that the infrastructure is well constructed, the used model can be simple.&lt;/p>
&lt;p>In main stage, focusing on the utilitarian performance and the gap between training data and test data.&lt;/p>
&lt;p>When utilizing features, use simple, observable features.&lt;/p>
&lt;p>When deploying models, watch out training-serving skew.&lt;/p>
&lt;h1 id="reference">Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://developers.google.com/machine-learning/guides/rules-of-ml" target="_blank" rel="noopener"
>Rules of Machine Learning&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>