<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>MLLM on Mao Song(毛松)'s Homepage</title><link>https://maosong.website/categories/mllm/</link><description>Recent content in MLLM on Mao Song(毛松)'s Homepage</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sat, 14 Feb 2026 10:00:05 +0800</lastBuildDate><atom:link href="https://maosong.website/categories/mllm/index.xml" rel="self" type="application/rss+xml"/><item><title>Notes on Step3-VL 10B</title><link>https://maosong.website/p/notes-on-step3-vl-10b/</link><pubDate>Fri, 13 Feb 2026 18:05:47 +0800</pubDate><guid>https://maosong.website/p/notes-on-step3-vl-10b/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>Step3-VL-10B 基于 &lt;a class="link" href="Perception%20Encoder.md" >Perception Encoder&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a> 开发得到，模型在预训练阶段使用了 1.2T 多模态 token, 在 post training 阶段使用了 &lt;a class="link" href="PaCoRe.md" >PaCoRe&lt;/a> 来提高模型的表现。作者强调关键改进在于高质量的预训练数据以及 RL 阶段的提升。&lt;/p>
&lt;p>总的来说，感觉这是阶跃在多模态大模型领域的一次初步尝试，使用的技术路线都比较成熟。&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;p>Step3-VL-10B 包含 3 个模块：&lt;/p>
&lt;ul>
&lt;li>ViT: 基于 &lt;a class="link" href="Perception%20Encoder.md" >Perception Encoder&lt;/a>, 大小为 1.8B&lt;/li>
&lt;li>MLP: 基于 &lt;a class="link" href="DeepSeek-OCR.md" >DeepSeek-OCR&lt;/a> 构建了一个两层的卷积层，将视觉 token 个数压缩为原来的 16 倍，对应代码如下&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># vision_encoder.py StepRoboticsVisionEncoder&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">vit_downsampler1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Conv2d&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">kernel_size&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">stride&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">padding&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">vit_downsampler2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Conv2d&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">4&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">kernel_size&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">stride&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">padding&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># modeling_step_vl.py _process_image_features&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">image_features&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">vision_model&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">vit_downsampler1&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">image_features&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">image_features&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">vision_model&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">vit_downsampler2&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">image_features&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>LLM: 基于 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a> 的 Qwen3-8B&lt;/li>
&lt;/ul>
&lt;p>对于输入的图片，Step3-VL-10B 采用了 LLaVA-OneVision 的做法，即将图片分为 $728\times 728$ 的 global thumnail 和 $504\times 504$ 的 local crop.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># modeling_step_vl.py _process_image_input&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">image_features&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_process_image_features&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">image_features&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">patch_image_features&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_process_image_features&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">patch_image_features&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="n">patch_image_features&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="kc">None&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="kc">None&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h3>&lt;p>预训练的数据包括：&lt;/p>
&lt;ul>
&lt;li>knowledge: knowledge 数据又包括图文交错数据，image-text pairs 数据&lt;/li>
&lt;li>education: 15M K12, university, adult education 数据&lt;/li>
&lt;li>OCR: 又分为以下几类
&lt;ul>
&lt;li>image to text: 10M (real-world) + 30M (synthetic) 数据&lt;/li>
&lt;li>image to code: 10M markup-based code (latex, matplotlib 等) 数据，15M 合成的 infographics 数据，5M reconstruction (tikz) 数据&lt;/li>
&lt;li>document to text: 80M full-page 数据&lt;/li>
&lt;li>document to code: HTML, markdown, latex 等数据，共 4M tables 和 100M formulas&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>grounding and counting: 400M 数据&lt;/li>
&lt;li>VQA: 10M 数据&lt;/li>
&lt;li>GUI: 23M 数据&lt;/li>
&lt;/ul>
&lt;p>训练使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-adamw/" target="_blank" rel="noopener"
>AdamW&lt;/a> 优化器，一共训练了 1.2T token, batch size 为 8192, 上下文长度为 4096.&lt;/p>
&lt;h3 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h3>&lt;p>post-training 包括 SFT 和 RL 两个阶段&lt;/p>
&lt;p>SFT 阶段又包括了两个小的 stage, 第一个 stage 用于提高模型的纯文本推理能力，纯文本数据和多模态数据的比例为 9:1,训练使用了 190B token; 第二个 stage 用于提高模型的多模态推理能力，纯文本数据和多模态数据的比例为 1:1. 训练使用了 36B token. SFT 训练时 batch size 为 32, 上下文长度为 128K&lt;/p>
&lt;p>RL 阶段作者使用了 &lt;a class="link" href="PPO.md" >PPO&lt;/a> 算法进行训练。reward function 也是分为 rule-based 和 model-based&lt;/p>
&lt;p>在 RLVR 之后，作者还进行了 RLHF 来提高模型的对齐能力。&lt;/p>
&lt;p>作者进一步使用了 &lt;a class="link" href="PaCoRe.md" >PaCoRe&lt;/a> 来提高模型的并行推理能力。&lt;/p>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>Step3-VL-10B 的表现如下所示，可以看到在 10B 模型下面，Step3-VL-10B 的表现达到了 SOTA.&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-step3-vl-10b/Step3-VL-performance.png"
width="1210"
height="691"
loading="lazy"
alt="Performance of Step3-VL"
class="gallery-image"
data-flex-grow="175"
data-flex-basis="420px"
>&lt;/p>
&lt;h3 id="ablation-study">&lt;a href="#ablation-study" class="header-anchor">&lt;/a>Ablation Study
&lt;/h3>&lt;p>作者首先对比了 perception encoder 和 &lt;a class="link" href="DINOv3.md" >DINOv3&lt;/a> 作为 vision encoder 的表现，结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Vision Encoder&lt;/th>
&lt;th>Perception BLINK&lt;/th>
&lt;th>Perception Omni.&lt;/th>
&lt;th>Perception MMVP&lt;/th>
&lt;th>Perception OCRBench&lt;/th>
&lt;th>General MMStar&lt;/th>
&lt;th>General SVQA&lt;/th>
&lt;th>General CCBench&lt;/th>
&lt;th>General V*&lt;/th>
&lt;th>General MMMU&lt;/th>
&lt;th>General ReMI&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>DINOv3&lt;/td>
&lt;td>42.35&lt;/td>
&lt;td>43.31&lt;/td>
&lt;td>28.00&lt;/td>
&lt;td>57.60&lt;/td>
&lt;td>41.43&lt;/td>
&lt;td>22.18&lt;/td>
&lt;td>56.32&lt;/td>
&lt;td>34.55&lt;/td>
&lt;td>46.56&lt;/td>
&lt;td>24.50&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>PE-lang (Ours)&lt;/td>
&lt;td>41.19&lt;/td>
&lt;td>43.57&lt;/td>
&lt;td>32.00&lt;/td>
&lt;td>70.10&lt;/td>
&lt;td>42.10&lt;/td>
&lt;td>21.15&lt;/td>
&lt;td>59.39&lt;/td>
&lt;td>37.17&lt;/td>
&lt;td>47.67&lt;/td>
&lt;td>26.08&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Δ&lt;/td>
&lt;td>-1.16&lt;/td>
&lt;td>+0.26&lt;/td>
&lt;td>+4.00&lt;/td>
&lt;td>+12.50&lt;/td>
&lt;td>+0.67&lt;/td>
&lt;td>-1.03&lt;/td>
&lt;td>+3.07&lt;/td>
&lt;td>+2.62&lt;/td>
&lt;td>+1.11&lt;/td>
&lt;td>+1.58&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>作者分析认为由于 DINOv3 在纯视觉任务上进行训练，其表现不如使用语言监督信号的 perception encoder&lt;/p>
&lt;p>作者还对比了 &lt;a class="link" href="https://maosong.website/p/notes-on-adamw/" target="_blank" rel="noopener"
>AdamW&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/notes-on-moonlight/" target="_blank" rel="noopener"
>Muon&lt;/a> 两种优化器，结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Optimizer&lt;/th>
&lt;th>Perception BLINK&lt;/th>
&lt;th>Perception Omni.&lt;/th>
&lt;th>Perception MMVP&lt;/th>
&lt;th>Perception OCRBench&lt;/th>
&lt;th>General MMStar&lt;/th>
&lt;th>General SVQA&lt;/th>
&lt;th>General CCBench&lt;/th>
&lt;th>General V*&lt;/th>
&lt;th>General MMMU&lt;/th>
&lt;th>General ReMI&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Muon&lt;/td>
&lt;td>41.14&lt;/td>
&lt;td>42.73&lt;/td>
&lt;td>32.00&lt;/td>
&lt;td>67.70&lt;/td>
&lt;td>44.58&lt;/td>
&lt;td>27.08&lt;/td>
&lt;td>60.72&lt;/td>
&lt;td>36.65&lt;/td>
&lt;td>47.56&lt;/td>
&lt;td>22.23&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Adam (Ours)&lt;/td>
&lt;td>40.72&lt;/td>
&lt;td>44.94&lt;/td>
&lt;td>29.33&lt;/td>
&lt;td>71.10&lt;/td>
&lt;td>41.77&lt;/td>
&lt;td>20.60&lt;/td>
&lt;td>60.13&lt;/td>
&lt;td>39.27&lt;/td>
&lt;td>46.11&lt;/td>
&lt;td>25.00&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Δ&lt;/td>
&lt;td>-0.42&lt;/td>
&lt;td>+2.21&lt;/td>
&lt;td>-2.67&lt;/td>
&lt;td>+3.40&lt;/td>
&lt;td>-2.81&lt;/td>
&lt;td>-6.48&lt;/td>
&lt;td>-0.59&lt;/td>
&lt;td>+2.62&lt;/td>
&lt;td>-1.45&lt;/td>
&lt;td>+2.77&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>结果发现 Muon 可以解决大规模数据的噪声和不平衡问题，但是作者没有使用 Muon, 原因是 Muon 对参数的初始化比较敏感&lt;/p>
&lt;p>作者还探究了 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3-vl/" target="_blank" rel="noopener"
>Qwen3-VL&lt;/a> 使用的 Deepstack 的有效性，结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Technique&lt;/th>
&lt;th>Perception BLINK&lt;/th>
&lt;th>Perception Omni.&lt;/th>
&lt;th>Perception MMVP&lt;/th>
&lt;th>Perception OCRBench&lt;/th>
&lt;th>General MMStar&lt;/th>
&lt;th>General SVQA&lt;/th>
&lt;th>General CCBench&lt;/th>
&lt;th>General V*&lt;/th>
&lt;th>General MMMU&lt;/th>
&lt;th>General ReMI&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>w/ DeepStack&lt;/td>
&lt;td>40.72&lt;/td>
&lt;td>42.92&lt;/td>
&lt;td>26.00&lt;/td>
&lt;td>71.20&lt;/td>
&lt;td>43.31&lt;/td>
&lt;td>28.66&lt;/td>
&lt;td>63.94&lt;/td>
&lt;td>36.65&lt;/td>
&lt;td>47.44&lt;/td>
&lt;td>26.96&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>w/o DeepStack (Ours)&lt;/td>
&lt;td>40.61&lt;/td>
&lt;td>43.57&lt;/td>
&lt;td>31.33&lt;/td>
&lt;td>69.30&lt;/td>
&lt;td>42.44&lt;/td>
&lt;td>25.20&lt;/td>
&lt;td>62.80&lt;/td>
&lt;td>38.22&lt;/td>
&lt;td>47.78&lt;/td>
&lt;td>26.96&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Δ&lt;/td>
&lt;td>-0.11&lt;/td>
&lt;td>+0.65&lt;/td>
&lt;td>+5.33&lt;/td>
&lt;td>-1.90&lt;/td>
&lt;td>-0.87&lt;/td>
&lt;td>-3.46&lt;/td>
&lt;td>-1.14&lt;/td>
&lt;td>+1.57&lt;/td>
&lt;td>+0.34&lt;/td>
&lt;td>+0.00&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>结果发现，尽管 DeepStack 可以加速训练，但是对于下游任务的提升非常有限，因此作者没有使用这个策略。&lt;/p>
&lt;p>对于 RL 训练，作者发现随着训练进行，模型的 reward 稳步提升。但是其输出长度并不是单调提升的，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-step3-vl-10b/Step3-VL-RL-dynamics.png"
width="1271"
height="407"
loading="lazy"
alt="RLVR dynamics"
class="gallery-image"
data-flex-grow="312"
data-flex-basis="749px"
>&lt;/p>
&lt;p>作者分析认为这是由于模型在 reasoning 任务和 perception 任务上模型使用的模式不同导致的，reasoning 任务上模型使用更长的输出长度来解决问题，而对于 perception 任务，由于答案唯一且确定，因此模型通过多次探索后会逐渐收敛到唯一的确定性模式，直接给出对应的答案。因此，其输出长度会越来越短。作者针对这种现象给出了一个假设，即针对 perception 任务，我们的训练数据并不包含思考的过程，而这种数据则让模型只能选择直接回答或者瞎猜，而不是先思考再回答。为了解决这个问题，作者使用了 &lt;a class="link" href="PaCoRe.md" >PaCoRe&lt;/a> 来让模型通过 proposal-then-refinement 策略来提高模型的思考长度&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者介绍了 Step3-VL，一个 10B 的多模态大模型，作者详细介绍了模型的架构，训练以及数据。&lt;/p>
&lt;p>作者认为后续工作有：&lt;/p>
&lt;ol>
&lt;li>通过 universal RL Scaling 提高 token efficiency:
&lt;ol>
&lt;li>将算力重心从 pre-training 迁移到 RL, 这一点与 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v3.2/" target="_blank" rel="noopener"
>DeepSeek-V3.2&lt;/a> 一致&lt;/li>
&lt;li>消除过度思考，提高模型的 reasoning efficiency&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>提高模型理解物理世界的能力
&lt;ol>
&lt;li>构建 world model 帮助模型理解世界&lt;/li>
&lt;li>使用 high-fidelity environment 来提高模型对于物理定律的理解能力&lt;/li>
&lt;li>具身智能&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2601.09668" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Kimi-k2.5</title><link>https://maosong.website/p/notes-on-kimi-k2.5/</link><pubDate>Thu, 12 Feb 2026 11:13:13 +0800</pubDate><guid>https://maosong.website/p/notes-on-kimi-k2.5/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>Kimi-K2.5 的核心有亮点：&lt;/p>
&lt;ol>
&lt;li>native multi-modal: 通过在预训练，SFT, RL 阶段使用多模态数据来提高模型的多模态能力&lt;/li>
&lt;li>agent: 通过并行 multi-agent 的方式来提高模型解决复杂问题的效率和能力&lt;/li>
&lt;/ol>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;p>Kimi K2.5 是一个标准的 ViT-MLP-LLM 架构，其中&lt;/p>
&lt;ol>
&lt;li>ViT, 基于 &lt;a class="link" href="Kimi-VL.md" >Kimi-VL&lt;/a> 提出的 MoonViT, 并进行了改进, 参数量为 400M&lt;/li>
&lt;li>MLP, 基于 patch merger,&lt;/li>
&lt;li>LLM, 基于 &lt;a class="link" href="https://maosong.website/p/notes-on-kimi-k2/" target="_blank" rel="noopener"
>Kimi-k2&lt;/a>, 参数量为 1.02T-A32B&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>ViT&lt;/strong>
作者使用了 &lt;a class="link" href="Kimi-VL.md" >Kimi-VL&lt;/a> 提出的 MoonViT, MoonViT 基于 &lt;a class="link" href="SigLIP.md" >SigLIP&lt;/a> 提出的 SigLIP-SO-400M 开发得到，MoonViT 使用了 &lt;a class="link" href="NaViT.md" >NaViT&lt;/a> 来避免切分图片和使用不同精度图片进行训练。&lt;/p>
&lt;p>在 MoonViT 的基础上，Kimi-K2.5 还进一步提出了 MoonViT-3D, 将 &lt;a class="link" href="NaViT.md" >NaViT&lt;/a> 的思想扩展到了 3D 用于提高模型的视频理解能力，具体做法为将连续 4 帧的视频展开为 1D sequence, 这样在图像上的注意力机制就可以无缝衔接到视频上了。并且，通过这种方式，我们可以让模型关注跨帧的信息（注意力在 4 帧的 token 之间进行），简化代码如下所示&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># config.json temporal_merge_kernel_size&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># kimi_k25_vision_processing.py split_video_chunks&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">video_chunk&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">frames&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mi">4&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">patches&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">frame&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">video_chunk&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">patches&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">extend&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">split_into_patches&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">frame&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">tokens&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">patches&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># modeling_kimi_k25.py Learnable2DInterpPosEmbDivided_fixed&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">positions&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">spatial_embedding&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">temporal_embedding&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># modeling_kimi_k25.py MoonViT3dEncoder&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">transformer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tokens&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">positions&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>最后，在进入 MLP 之前，作者还对每个 temporal chunk 内的特征进行 pooling 操作，将时序长度压缩到了原来的 1/4, 进而提高模型可处理的视频长度。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># modeling_kimi_k25.py tpool_patch_merger&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">tpool_patch_merger&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">grid_thws&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">merge_kernel_size&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">tuple&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="nb">list&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">d_model&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">outputs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pre_sum&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">t&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">h&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">w&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">grid_thws&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tolist&lt;/span>&lt;span class="p">():&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Get the current sequence&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">seq&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">pre_sum&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">pre_sum&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">t&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">h&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">w&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Reshape along self.merge_kernel_size and concat to the last dimension&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">kernel_height&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">kernel_width&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">merge_kernel_size&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">new_height&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">new_width&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">h&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="n">kernel_height&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">w&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="n">kernel_width&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">reshaped_seq&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">seq&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">new_height&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">kernel_height&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">new_width&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">kernel_width&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d_model&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">reshaped_seq&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">reshaped_seq&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">permute&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">4&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">contiguous&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mean&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># temporal pooling&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">padded_seq&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">reshaped_seq&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">new_height&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">new_width&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">kernel_height&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">kernel_width&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">outputs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">padded_seq&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pre_sum&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">t&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">h&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">w&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">outputs&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;strong>MLP&lt;/strong>
MLP 使用了 PatchMerger, 用于减少视觉 token 个数，这个方案在之前的 Qwen-VL 系列里已经得到了应用。&lt;/p>
&lt;p>&lt;strong>LLM&lt;/strong>
LLM 基于 &lt;a class="link" href="https://maosong.website/p/notes-on-kimi-k2/" target="_blank" rel="noopener"
>Kimi-k2&lt;/a> 的 MoE 模型，总参数为 1T, 激活参数为 32B&lt;/p>
&lt;h3 id="data">&lt;a href="#data" class="header-anchor">&lt;/a>Data
&lt;/h3>&lt;h3 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h3>&lt;p>预训练阶段一共使用了 15T token, 分为了三个阶段：&lt;/p>
&lt;ol>
&lt;li>ViT-training: 单独训练 ViT, 实际用了 image caption, grounding, ocr, video 等数据进行训练，训练方式采用了类似 InternVL 的方式，即通过 cross entropy loss 来与一个清凉话的 LLM 进行对齐，这个阶段训练使用了 1T token, 然后作者使用了一个非常短的 stage 来更新 MLP 用于对齐 ViT 和 Kimi-K2&lt;/li>
&lt;li>Joint pre-training: 训练所有参数，长下文长度为 4K, 使用了 15T token. 这里主要强调了提升代码数据的比例&lt;/li>
&lt;li>Long context mid-training: 使用 &lt;a class="link" href="https://maosong.website/p/notes-on-yarn/" target="_blank" rel="noopener"
>YARN&lt;/a> 来提高模型的上下文长度&lt;/li>
&lt;/ol>
&lt;p>最终预训练阶段 recipe 如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2.5/Kimi-k2.5-pre-training.png"
width="1430"
height="415"
loading="lazy"
alt="Pre-training recipe of Kimi-K2.5"
class="gallery-image"
data-flex-grow="344"
data-flex-basis="826px"
>&lt;/p>
&lt;p>&lt;strong>Native Multimodal pre-training&lt;/strong>
在 Joint pre-training stage, Kimi-K2.5 还采用了一个与 &lt;a class="link" href="InternVL3.md" >InternVL3&lt;/a> 类似的策略，即在预训练一开始直接使用多模态数据进行预训练。&lt;/p>
&lt;p>传统的多模态大模型往往基于一个比较成熟的 LLM backbone 来完成多模态大模型的训练，但是其问题在于成熟的 LLM 其表示空间会收敛到语言模态上，多模态信息的迁移能力比较差。&lt;a class="link" href="InternVL3.md" >InternVL3&lt;/a> 虽然也是 native multimodal pre-training, 但是其仍然依赖于成熟的 LLM. Kimi K2.5 则是使用预训练阶段的 Kimi K2 作为 backbone 来避免表示空间的塌缩，在训练一开始即直接加入少量多模态数据来保持模型的多模态能力。&lt;/p>
&lt;p>作者探究了预训练阶段不同的数据对比，试验结果如下图所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Vision Injection Timing&lt;/th>
&lt;th>Vision-Text Ratio&lt;/th>
&lt;th>Vision Knowledge&lt;/th>
&lt;th>Vision Reasoning&lt;/th>
&lt;th>OCR&lt;/th>
&lt;th>Text Knowledge&lt;/th>
&lt;th>Text Reasoning&lt;/th>
&lt;th>Code&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Early&lt;/td>
&lt;td>0%&lt;/td>
&lt;td>10%:90%&lt;/td>
&lt;td>25.8&lt;/td>
&lt;td>43.8&lt;/td>
&lt;td>65.7&lt;/td>
&lt;td>45.5&lt;/td>
&lt;td>58.5&lt;/td>
&lt;td>24.8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Mid&lt;/td>
&lt;td>50%&lt;/td>
&lt;td>20%:80%&lt;/td>
&lt;td>25.0&lt;/td>
&lt;td>40.7&lt;/td>
&lt;td>64.1&lt;/td>
&lt;td>43.9&lt;/td>
&lt;td>58.6&lt;/td>
&lt;td>24.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Late&lt;/td>
&lt;td>80%&lt;/td>
&lt;td>50%:50%&lt;/td>
&lt;td>24.2&lt;/td>
&lt;td>39.0&lt;/td>
&lt;td>61.5&lt;/td>
&lt;td>43.1&lt;/td>
&lt;td>57.8&lt;/td>
&lt;td>24.0&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>结果显示，在训练早期加入少部分的多模态数据可以有效提高模型的表现。&lt;/p>
&lt;h3 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h3>&lt;p>Post-training 分为了 SFT 和 RL, SFT 阶段作者使用了合成的高质量数据，主要提升模型的交互式推理能力以及工具调用能力。为了解决传统 VLM 工具调用能力比较差且扩展性差的问题，Kimi-k2.5 提出了 Zero-Vision SFT, 其核心思想模型在预训练阶段已经完成了多模态对齐，因此我们可以仅使用纯文本 SFT 数据来激活 VLM 的视觉 agent 能力，具体做法就是将所有图像操作通过 IPython 的代码进行代理操作，这样视觉工具的调用就编程了程序化的图像处理指令。&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2.5/Kimi-k2.5-zero-vision-sft.png"
width="1340"
height="796"
loading="lazy"
alt="Performance of Vision RL on zero-vision SFT"
class="gallery-image"
data-flex-grow="168"
data-flex-basis="404px"
>&lt;/p>
&lt;p>在 RL 阶段，作者基于 &lt;a class="link" href="https://maosong.website/p/notes-on-kimi-k1.5/" target="_blank" rel="noopener"
>Kimi-k1.5&lt;/a> 提出的策略优化算法加入了一个 token-level clipping 机制来减少 off-policy divergence, 目标函数如下所示&lt;/p>
$$
\mathcal{L}(\theta)=\mathbb{E}_{x\sim\mathcal{D}}\left[\frac{1}{N}\sum_{j=1}^k\sum_{i=1}^{|y_i|}\mathrm{clip}\left(\log\frac{\pi_{\theta}(y_j^i\mid x, y_{j}^{0:i})}{\pi_{\mathrm{old}}(y_j^i\mid x, y_{j}^{0:i})},\alpha,\beta\right)(r(x, y_j) - \bar{r}(x)) - \tau\left(\log\frac{\pi_{\theta}(y_j^i\mid x, y_{j}^{0:i})}{\pi_{\mathrm{old}}(y_j^i\mid x, y_{j}^{0:i})}\right)^2\right]
$$&lt;p>其中 $k$ 是针对每个回答 $x$ 的采样次数，$N=\sum_{j=1}^k|y_j|$ 是一个 batch 里总的 token 个数， $\alpha,\beta,\tau$ 为超参数，$\bar{r}(x)$ 是对 normalization 的估计，这里采用了 Kimi-K1.5 的 mean reward, 即 $\bar{r}(x)=1/k\sum_{j=1}^Kr(x,y_j)$. 这里的 clipping 机制与 PPO 不同的地方在于针对 log-ratio 进行 clipping, 而不依赖于 advantage 的计算。最终训练时使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-moonlight/" target="_blank" rel="noopener"
>Moonlight&lt;/a> 的 MuonClip 算法&lt;/p>
&lt;p>对于 reward 的设计，Kimi-k2.5 也使用了基于规则和基于 reward model 的方式，前者针对答案可验证的任务，后者针对开放式的任务。&lt;/p>
&lt;p>作者还构建了 length penalty 来提高模型的推理效率，作者发现 &lt;a class="link" href="https://maosong.website/p/notes-on-kimi-k1.5/" target="_blank" rel="noopener"
>Kimi-k1.5&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/notes-on-kimi-k2/" target="_blank" rel="noopener"
>Kimi-k2&lt;/a> 中的 length penalty 虽然可以生成更准确的 reasoning chain, 但是其很难泛化到更高的算力. 为了解决这个问题，作者提出了 &lt;strong>Toggle&lt;/strong> 策略，即在 inference-time scaling 和 budget-constrained optimization 两种模式之间进行切换优化，对应的 reward 定义为&lt;/p>
$$
\tilde{r}(x,y) = \begin{cases}
r(x,y)*\mathbb{I}\{1/k\sum_{j=1}^kr(x,y_i&lt;\lambda \text{ or }|y_j|\leq \text{budget}(x)\},&amp;\text{if }\lfloor t/m\rfloor\mod 2 == 0\ (\text{Phase }0)\\
r(x,y),&amp;\text{otherwise } (\text{Phase }1)
\end{cases}
$$&lt;p>其中 $\lambda, m$ 都是超参数。budget 基于正确回答的长度的 p 分位得到：&lt;/p>
$$
\text{budget}(x) = \text{Percentile}(\{|y_j| \mid r(x,y_j)=1, j\in[k]\},\rho)
$$&lt;p>两种模式每隔 $m$ 个 iteration 切换一次：&lt;/p>
&lt;ul>
&lt;li>phase 0: budget limited phase, 训练模型在给定 token budget 下解决问题，减少 reasoning chain 长度&lt;/li>
&lt;li>phase 1: scaling phase, 训练模型使用更多的算力来解决更复杂的问题，提高模型的智能程度&lt;/li>
&lt;/ul>
&lt;p>作者评估 Toogle 策略得到的结果如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2.5/Kimi-k2.5-toggle.png"
width="1200"
height="689"
loading="lazy"
alt="Performance of toggle strategy"
class="gallery-image"
data-flex-grow="174"
data-flex-basis="417px"
>&lt;/p>
&lt;p>结果现实，使用 toggle 策略之后，模型的输出长度减少了 30% 左右，且模型的表现并没有明显下降。作者还发现，一些重复的 pattern 也随之降低，且 toggle 策略的泛化程度更高。&lt;/p>
&lt;p>在 Zero-Vision SFT 的基础上，Kimi-k2.5 使用了 Joint multimodal RL 训练策略。现有的多模态 RL 存在的问题为：模型很容易忽略视觉输入而过度依赖于纯文本进行推理。为了解决这个问题，作者构建了需要视觉理解才能得到答案的任务来提高模型对于视觉信息的利用程度，这些任务覆盖三个 domain:&lt;/p>
&lt;ol>
&lt;li>visual grounding and counting: 定位和计数&lt;/li>
&lt;li>chart and document understanding: 图表文档理解&lt;/li>
&lt;li>vision-critical STEM problems: 需要图片来完成求解的数学物理问题&lt;/li>
&lt;/ol>
&lt;p>作者在 visual RL 之后评估了模型的表现，发现模型在 MMLU-Pro, GPQA-Diamond 等任务上的表现都有了提升，作者认为 visual RL 可以在不损害模型纯文本能力的情况下提高模型跨模态的泛化性&lt;/p>
&lt;h3 id="agent-swarm">&lt;a href="#agent-swarm" class="header-anchor">&lt;/a>Agent Swarm
&lt;/h3>&lt;p>Kimi-k2.5 的另一个重大改进为使用并行机制来提高模型的 agent 能力。传统的 agent 往往序列执行 reasoning, tool-use, 这限制了模型处理复杂任务的能力，Kimi-k2.5 通过 Agent Swarm 和 Parallel Agent Reinforcement Learning (PARL) 来解决这个问题，其核心思想就是并行，框架图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2.5/Kimi-k2.5-agent-swarm.png"
width="1354"
height="729"
loading="lazy"
alt="Framework of Agent Swarm"
class="gallery-image"
data-flex-grow="185"
data-flex-basis="445px"
>&lt;/p>
&lt;p>agent swarm 架构包含了一个 orchestrator 和若干个 subagent, 为了解决 agent swarm 的 reward 比较难以设置的问题，PARL 构建了三个不同 level 的 reward&lt;/p>
$$
r_{PARL}(x,y) = \lambda_1 r_{parallel} + \lambda_2r_{finish} + \lambda_3r_{perf}(x,y)
$$&lt;p>其中 $r_{perf}$ 评估了 solution $y$ 的质量， $r_{parallel}$ 则是避免并行模式崩塌，从 multi-agent 崩塌为 single agent, $r_{finish}$ 则是评估模型的完成性。超参数 $\lambda_1,\lambda_2$ 随训练逐渐降为 0 来提高模型整体的表现。&lt;/p>
&lt;p>作者还提出了使用 critical steps 来评估 parallel agent 的计算时间消耗，其计算公式如下&lt;/p>
$$
CriticalSteps = \sum_{i=1}^T\left(S_{main}^{(t)}+\max_iS_{sub,i}^{(t)}\right)
$$&lt;p>其中 $T$ 为一个 episode 的时间，$S_{main}^{(t)}$ 为 orchestrator 在第 $t$ 步的运行时间， $S_{sub,i}^{(t)}$ 为第 i 个 subagent 的运行时间。&lt;/p>
&lt;p>为了提高模型的并行能力，作者构建了一批广度优先搜索和深度优先搜索的数据，通过这些数据的构建，我们可以提高 orchestrator 的并行调用能力。&lt;/p>
&lt;p>最终，PARL 的表现如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2.5/Kimi-k2.5-PARL.png"
width="1501"
height="441"
loading="lazy"
alt="Performance of PARL"
class="gallery-image"
data-flex-grow="340"
data-flex-basis="816px"
>&lt;/p>
&lt;h3 id="infra">&lt;a href="#infra" class="header-anchor">&lt;/a>Infra
&lt;/h3>&lt;p>Kimi-k2.5 的 infra 基于 &lt;a class="link" href="https://maosong.website/p/notes-on-kimi-k2/" target="_blank" rel="noopener"
>Kimi-k2&lt;/a>, 作者主要强调了 decouple encoder process (DEP) 这一改进。之前的工作将 vision encoder 和 text embedding 都做为 PP 的第一个 stage, 但是由于 vision encoder 对不同输入的处理时间不同，这个 stage 的算历和内存分配随输入变化比较大。为了解决这个问题，作者提出了 DEP, 包含三个 stage 来提高训练效率:&lt;/p>
&lt;ol>
&lt;li>balanced vision forward: 由于 vision encoder 比较小 (400M), 因此作者将 vision encoder 复制到所有 GPU 上，然后根据负载来将 visual data 分配到不同的 GPU 上进行处理，这个阶段不保存中间激活值，处理完毕之后所有的结果作为 PP Stage0 的输入&lt;/li>
&lt;li>backbone training: 正常进行训练，与 LLM 的训练优化一致&lt;/li>
&lt;li>vision recomputation &amp;amp; backward: 这个阶段，我们重新计算 vision encoder 的 forward pass, 然后再对 vision encoder 进行反向传播&lt;/li>
&lt;/ol>
&lt;p>通过 DEP, Kimi-k2.5 的训练效率达到了 Kimi-k2 的 90%.&lt;/p>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>首先是 Kimi-k2.5 在 general &amp;amp; reasoning 类任务上的表现，可以看到，Kimi-k2.5 超过了 DeeoSeek-V3.2 的表现，&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Benchmark&lt;/th>
&lt;th>Kimi K2.5&lt;/th>
&lt;th>Claude Opus 4.5&lt;/th>
&lt;th>GPT-5.2 (xhigh)&lt;/th>
&lt;th>Gemini 3 Pro&lt;/th>
&lt;th>DeepSeek-V3.2&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>HLE-Full&lt;/td>
&lt;td>30.1&lt;/td>
&lt;td>30.8&lt;/td>
&lt;td>34.5&lt;/td>
&lt;td>37.5&lt;/td>
&lt;td>25.1†&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>HLE-Full w/ tools&lt;/td>
&lt;td>50.2&lt;/td>
&lt;td>43.2&lt;/td>
&lt;td>45.5&lt;/td>
&lt;td>45.8&lt;/td>
&lt;td>40.8†&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>AIME 2025&lt;/td>
&lt;td>96.1&lt;/td>
&lt;td>92.8&lt;/td>
&lt;td>100&lt;/td>
&lt;td>95.0&lt;/td>
&lt;td>93.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>HMMT 2025 (Feb)&lt;/td>
&lt;td>95.4&lt;/td>
&lt;td>92.9*&lt;/td>
&lt;td>99.4&lt;/td>
&lt;td>97.3*&lt;/td>
&lt;td>92.5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>IMO-AnswerBench&lt;/td>
&lt;td>81.8&lt;/td>
&lt;td>78.5*&lt;/td>
&lt;td>86.3&lt;/td>
&lt;td>83.1*&lt;/td>
&lt;td>78.3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GPQA-Diamond&lt;/td>
&lt;td>87.6&lt;/td>
&lt;td>87.0&lt;/td>
&lt;td>92.4&lt;/td>
&lt;td>91.9&lt;/td>
&lt;td>82.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MMLU-Pro&lt;/td>
&lt;td>87.1&lt;/td>
&lt;td>89.3*&lt;/td>
&lt;td>86.7*&lt;/td>
&lt;td>90.1&lt;/td>
&lt;td>85.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SimpleQA Verified&lt;/td>
&lt;td>36.9&lt;/td>
&lt;td>44.1&lt;/td>
&lt;td>38.9&lt;/td>
&lt;td>72.1&lt;/td>
&lt;td>27.5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>AdvancedIF&lt;/td>
&lt;td>75.6&lt;/td>
&lt;td>63.1&lt;/td>
&lt;td>81.1&lt;/td>
&lt;td>74.7&lt;/td>
&lt;td>58.8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LongBench v2&lt;/td>
&lt;td>61.0&lt;/td>
&lt;td>64.4*&lt;/td>
&lt;td>54.5*&lt;/td>
&lt;td>68.2*&lt;/td>
&lt;td>59.8*&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>接下来是模型在 coding 任务上的表现&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Benchmark&lt;/th>
&lt;th>Kimi K2.5&lt;/th>
&lt;th>Claude Opus 4.5&lt;/th>
&lt;th>GPT-5.2 (xhigh)&lt;/th>
&lt;th>Gemini 3 Pro&lt;/th>
&lt;th>DeepSeek-V3.2&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>SWE-Bench Verified&lt;/td>
&lt;td>76.8&lt;/td>
&lt;td>80.9&lt;/td>
&lt;td>80.0&lt;/td>
&lt;td>76.2&lt;/td>
&lt;td>73.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SWE-Bench Pro (public)&lt;/td>
&lt;td>50.7&lt;/td>
&lt;td>55.4*&lt;/td>
&lt;td>55.6&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SWE-Bench Multilingual&lt;/td>
&lt;td>73.0&lt;/td>
&lt;td>77.5&lt;/td>
&lt;td>72.0&lt;/td>
&lt;td>65.0&lt;/td>
&lt;td>70.2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Terminal Bench 2.0&lt;/td>
&lt;td>50.8&lt;/td>
&lt;td>59.3&lt;/td>
&lt;td>54.0&lt;/td>
&lt;td>54.2&lt;/td>
&lt;td>46.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>PaperBench (CodeDev)&lt;/td>
&lt;td>63.5&lt;/td>
&lt;td>72.9*&lt;/td>
&lt;td>63.7*&lt;/td>
&lt;td>-&lt;/td>
&lt;td>47.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CyberGym&lt;/td>
&lt;td>41.3&lt;/td>
&lt;td>50.6&lt;/td>
&lt;td>-&lt;/td>
&lt;td>39.9*&lt;/td>
&lt;td>17.3*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SciCode&lt;/td>
&lt;td>48.7&lt;/td>
&lt;td>49.5&lt;/td>
&lt;td>52.1&lt;/td>
&lt;td>56.1&lt;/td>
&lt;td>38.9&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OIBench (cpp)&lt;/td>
&lt;td>57.4&lt;/td>
&lt;td>54.6*&lt;/td>
&lt;td>-&lt;/td>
&lt;td>68.5*&lt;/td>
&lt;td>54.7*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LiveCodeBench (v6)&lt;/td>
&lt;td>85.0&lt;/td>
&lt;td>82.2*&lt;/td>
&lt;td>-&lt;/td>
&lt;td>87.4*&lt;/td>
&lt;td>83.3&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>在 agent 任务上的表现&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Benchmark&lt;/th>
&lt;th>Kimi K2.5&lt;/th>
&lt;th>Claude Opus 4.5&lt;/th>
&lt;th>GPT-5.2 (xhigh)&lt;/th>
&lt;th>Gemini 3 Pro&lt;/th>
&lt;th>DeepSeek-V3.2&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>BrowseComp&lt;/td>
&lt;td>60.6&lt;/td>
&lt;td>37.0&lt;/td>
&lt;td>65.8&lt;/td>
&lt;td>37.8&lt;/td>
&lt;td>51.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>BrowseComp (w/ ctx manage)&lt;/td>
&lt;td>74.9&lt;/td>
&lt;td>57.8&lt;/td>
&lt;td>-&lt;/td>
&lt;td>59.2&lt;/td>
&lt;td>67.6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>BrowseComp (Agent Swarm)&lt;/td>
&lt;td>78.4&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>WideSearch&lt;/td>
&lt;td>72.7&lt;/td>
&lt;td>76.2*&lt;/td>
&lt;td>-&lt;/td>
&lt;td>57.0&lt;/td>
&lt;td>32.5*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>WideSearch (Agent Swarm)&lt;/td>
&lt;td>79.0&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DeepSearchQA&lt;/td>
&lt;td>77.1&lt;/td>
&lt;td>76.1*&lt;/td>
&lt;td>71.3*&lt;/td>
&lt;td>63.2*&lt;/td>
&lt;td>60.9*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>FinSearchCompT2&amp;amp;T3&lt;/td>
&lt;td>67.8&lt;/td>
&lt;td>66.2*&lt;/td>
&lt;td>-&lt;/td>
&lt;td>49.9&lt;/td>
&lt;td>59.1*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Seal-0&lt;/td>
&lt;td>57.4&lt;/td>
&lt;td>47.7*&lt;/td>
&lt;td>45.0&lt;/td>
&lt;td>45.5*&lt;/td>
&lt;td>49.5*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GDPVal-AA&lt;/td>
&lt;td>41.0&lt;/td>
&lt;td>45.0&lt;/td>
&lt;td>48.0&lt;/td>
&lt;td>35.0&lt;/td>
&lt;td>34.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OSWorld-Verified&lt;/td>
&lt;td>63.3&lt;/td>
&lt;td>66.3&lt;/td>
&lt;td>8.6&lt;/td>
&lt;td>20.7&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>WebArena&lt;/td>
&lt;td>58.9&lt;/td>
&lt;td>63.4&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>多模态表现&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Benchmark&lt;/th>
&lt;th>Kimi K2.5&lt;/th>
&lt;th>Claude Opus 4.5&lt;/th>
&lt;th>GPT-5.2 (xhigh)&lt;/th>
&lt;th>Gemini 3 Pro&lt;/th>
&lt;th>Qwen3-VL-235B-A22B&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;strong>Image&lt;/strong>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MMMU-Pro&lt;/td>
&lt;td>78.5&lt;/td>
&lt;td>74.0&lt;/td>
&lt;td>79.5*&lt;/td>
&lt;td>81.0&lt;/td>
&lt;td>69.3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MMMU (val)&lt;/td>
&lt;td>84.3&lt;/td>
&lt;td>80.7&lt;/td>
&lt;td>86.7*&lt;/td>
&lt;td>87.5*&lt;/td>
&lt;td>80.6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CharXiv (RQ)&lt;/td>
&lt;td>77.5&lt;/td>
&lt;td>67.2*&lt;/td>
&lt;td>82.1&lt;/td>
&lt;td>81.4&lt;/td>
&lt;td>66.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MathVision&lt;/td>
&lt;td>84.2&lt;/td>
&lt;td>77.1*&lt;/td>
&lt;td>83.0&lt;/td>
&lt;td>86.1*&lt;/td>
&lt;td>74.6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MathVista (mini)&lt;/td>
&lt;td>90.1&lt;/td>
&lt;td>80.2*&lt;/td>
&lt;td>82.8*&lt;/td>
&lt;td>89.8*&lt;/td>
&lt;td>85.8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SimpleVQA&lt;/td>
&lt;td>71.2&lt;/td>
&lt;td>69.7*&lt;/td>
&lt;td>55.8*&lt;/td>
&lt;td>69.7*&lt;/td>
&lt;td>56.8*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>WorldVQA&lt;/td>
&lt;td>46.3&lt;/td>
&lt;td>36.8&lt;/td>
&lt;td>28.0&lt;/td>
&lt;td>47.4&lt;/td>
&lt;td>23.5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ZeroBench&lt;/td>
&lt;td>9&lt;/td>
&lt;td>3*&lt;/td>
&lt;td>9*&lt;/td>
&lt;td>8*&lt;/td>
&lt;td>4*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ZeroBench w/ tools&lt;/td>
&lt;td>11&lt;/td>
&lt;td>9*&lt;/td>
&lt;td>7*&lt;/td>
&lt;td>12*&lt;/td>
&lt;td>3*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>BabyVision&lt;/td>
&lt;td>36.5&lt;/td>
&lt;td>14.2&lt;/td>
&lt;td>34.4&lt;/td>
&lt;td>49.7&lt;/td>
&lt;td>22.2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>BLINK&lt;/td>
&lt;td>78.9&lt;/td>
&lt;td>68.8*&lt;/td>
&lt;td>-&lt;/td>
&lt;td>78.7*&lt;/td>
&lt;td>68.9&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MMVP&lt;/td>
&lt;td>87.0&lt;/td>
&lt;td>80.0*&lt;/td>
&lt;td>83.0*&lt;/td>
&lt;td>90.0*&lt;/td>
&lt;td>84.3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OmniDocBench 1.5&lt;/td>
&lt;td>88.8&lt;/td>
&lt;td>87.7*&lt;/td>
&lt;td>85.7&lt;/td>
&lt;td>88.5&lt;/td>
&lt;td>82.0*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OCRBench&lt;/td>
&lt;td>92.3&lt;/td>
&lt;td>86.5*&lt;/td>
&lt;td>80.7*&lt;/td>
&lt;td>90.3*&lt;/td>
&lt;td>87.5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>InfoVQA (test)&lt;/td>
&lt;td>92.6&lt;/td>
&lt;td>76.9*&lt;/td>
&lt;td>84*&lt;/td>
&lt;td>57.2*&lt;/td>
&lt;td>89.5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>Video&lt;/strong>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>VideoMMMU&lt;/td>
&lt;td>86.6&lt;/td>
&lt;td>84.4*&lt;/td>
&lt;td>85.9&lt;/td>
&lt;td>87.6&lt;/td>
&lt;td>80.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MMVU&lt;/td>
&lt;td>80.4&lt;/td>
&lt;td>77.3*&lt;/td>
&lt;td>80.8*&lt;/td>
&lt;td>77.5*&lt;/td>
&lt;td>71.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MotionBench&lt;/td>
&lt;td>70.4&lt;/td>
&lt;td>60.3&lt;/td>
&lt;td>64.8&lt;/td>
&lt;td>70.3&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Video-MME&lt;/td>
&lt;td>87.4&lt;/td>
&lt;td>66.0*&lt;/td>
&lt;td>86.0*&lt;/td>
&lt;td>88.4*&lt;/td>
&lt;td>79.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LongVideoBench&lt;/td>
&lt;td>79.8&lt;/td>
&lt;td>67.2*&lt;/td>
&lt;td>76.5*&lt;/td>
&lt;td>77.7*&lt;/td>
&lt;td>65.6*&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LVBench&lt;/td>
&lt;td>75.9&lt;/td>
&lt;td>57.3&lt;/td>
&lt;td>-&lt;/td>
&lt;td>73.5*&lt;/td>
&lt;td>63.6&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>我们这里基于模型在不同类别任务上的排名来进行可视化，结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2.5/Kimi-k2.5-rank-frequency.png"
width="1590"
height="790"
loading="lazy"
alt="Rank performance of difference models"
class="gallery-image"
data-flex-grow="201"
data-flex-basis="483px"
>&lt;/p>
&lt;p>从结果可以看出，Kimi-K2.5 的 agent 能力达到了 SOTA 级别，其多模态能力也比较强。&lt;/p>
&lt;p>与 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-v3.2/" target="_blank" rel="noopener"
>DeepSeek-V3.2&lt;/a> 一样，作者也对比了不同模型的推理效率，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k2.5/Kimi-k2.5-reasoning-efficiency.png"
width="783"
height="262"
loading="lazy"
alt="Reasoning efficiency of Kimi K2.5"
class="gallery-image"
data-flex-grow="298"
data-flex-basis="717px"
>&lt;/p>
&lt;p>可以看到，相比与 Kimi-K2, Kimi-K2.5 通过在 RL 层面进行优化，降低了输出长度，但是相比与 DeepSeel-V3.2 和 Gemini3.0 Pro 之间还存在一定差距。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>在本文中，作者提出了 Kimi-k2.5， 一个多模态的 agent model, Kimi-k2.5 集成了 Kimi-k2 和 Kimi-VL 的能力，扩展了模型的 agent 能力。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://huggingface.co/moonshotai/Kimi-K2.5" target="_blank" rel="noopener"
>huggingface&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2602.02276" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on MiniMax-01</title><link>https://maosong.website/p/notes-on-minimax-01/</link><pubDate>Tue, 06 Jan 2026 17:38:01 +0800</pubDate><guid>https://maosong.website/p/notes-on-minimax-01/</guid><description>&lt;p>MiniMax-01 是一个基于 hybrid attention 架构的大模型系列，包含 MiniMax-Text-01 和 MiniMax-VL-01 两个模型，其中 MiniMax-Text-01 推理时支持 4M 的上下文长度，MiniMax-VL-01 支持 512B 的上下文长度&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>现有大部分模型的上下文长度为 32K-256K, 但实际上我们对于长上下文的需求已经超过了这个范围。已有的模型主要基于 self-attention, 这是一个平方复杂度的算法。&lt;/p>
&lt;p>为了解决 self-attention 的问题，相关工作如 sparse attention, linear attention, state space models 等都提出了对应的解决办法。但是已有的这些解决方法的问题就是表现不是很强劲。&lt;/p>
&lt;p>因此，作者在本文中就提出了一个基于 hybrid attention 架构的大语言模型系列 MiniMax-01. 作者主要从架构，数据和 infra 三个方面进行了改进。&lt;/p>
&lt;p>在架构上，作者使用了基于 Lightning Attention 的混合架构。作者还基于实际部署来决定模型的参数。为了最大化模型的表现，作者使用了 MoE 架构。&lt;/p>
&lt;p>已有的 infra 主要是针对基于 softmax 的 attention 进行优化的，MiniMax-01 包含 softmax attention, linear attention 和 MoE, 架构比较复杂，因此作者实现了 expert parallel 和 expert tensor parallel 来提高整体的计算效率和 GPU 之间的通信效率。作者还实现了 varlen ring attention 来减少计算冗余。最终，作者发现模型在 NVIDIA-H20 上的 MFU 超过了 75%.&lt;/p>
&lt;p>基于前面提到的架构设计，作者训练得到了 MiniMax-Text-01, 模型总参数为 456B, 激活参数为 45.9B, 专家个数为 32 个，激活专家个数为 2 个。 作者首先构建了高质量的数据集，然后构建了一个三阶段的训练 pipeline。&lt;/p>
&lt;p>基于 MiniMax-Text-01, 作者扩展得到了 MiniMax-VL-01,训练使用了&lt;strong>512B&lt;/strong> token&lt;/p>
&lt;p>作者总结本文的贡献如下：&lt;/p>
&lt;ol>
&lt;li>作者构建了一个领先的大模型系列，支持超过 4M 上下文&lt;/li>
&lt;li>作者构建了第一个大规模的基于 linear attention 的大语言模型系列&lt;/li>
&lt;li>作者详细介绍了使用的数据，模型和训练策略&lt;/li>
&lt;li>作者开源了模型&lt;/li>
&lt;/ol>
&lt;h2 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h2>&lt;p>模型架构如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-minimax-01/minimax_text_01_architecture.png"
width="737"
height="908"
loading="lazy"
alt="minimax_text_01_architecture"
class="gallery-image"
data-flex-grow="81"
data-flex-basis="194px"
>&lt;/p>
&lt;p>相比于原始的 transformer, MiniMax-01 做出了以下改进：&lt;/p>
&lt;ol>
&lt;li>将 attention block 按照 8 个 block 为一组，每组里只有最后 1 个 block 使用 softmax attention, 其余 7 个 block 使用 lightning attention&lt;/li>
&lt;li>使用 MoE 替换 FFN, MoE 总专家个数为 32 个，激活专家个数为 2 个&lt;/li>
&lt;li>softmax attention 使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-gqa/" target="_blank" rel="noopener"
>GQA&lt;/a>, 来提高内存加载效率, group size 为 8&lt;/li>
&lt;li>使用了 RoPE 作为 position embedding&lt;/li>
&lt;li>使用了 RMSNorm 替换了 LayerNorm&lt;/li>
&lt;/ol>
&lt;p>FFN: MoE (32 个专家，激活 2 个专家)&lt;/p>
&lt;p>transformer: 8 个 block 为一组，一组里前 7 个使用 Lightning attention，第 8 个使用 softmax attention，80 layers&lt;/p>
&lt;p>Attention ： GQA，group size=8，64 heads，&lt;/p>
&lt;h3 id="moe">&lt;a href="#moe" class="header-anchor">&lt;/a>MoE
&lt;/h3>&lt;p>在训练基于 MoE 的 LLM 时，有两种策略，分别是 token-drop 和 dropless, 前者保证每个专家处理的 token 个数差不多，可以提高效率，缺点是某些 token 不会被任何专家处理，某些 token 会被多个专家处理；后者是保证每个 token 都会被处理。&lt;/p>
&lt;p>在本文中，作者采取了 token-drop 的方式，作者为每个专家设置一个 capacity limit，超过这个 limit 之后该专家就不再处理新的 token.&lt;/p>
&lt;p>为了评估 MoE 模型的有效性，作者对比了 MoE 和 dense 模型的表现，结果如下图所示：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-minimax-01/MiniMax-01-dense-vs-MoE.png"
width="1402"
height="480"
loading="lazy"
alt="Performance of MoE v.s. Dense"
class="gallery-image"
data-flex-grow="292"
data-flex-basis="701px"
>&lt;/p>
&lt;p>结果发现，相同的算力下，MoE 模型比 dense 模型好。&lt;/p>
&lt;p>但是，当 scaling up 到更大的模型是，作者发现训练产生了 routing collapse 的情况，这是因为 routing 的分布过于集中。为了解决这个问题，作者使用了 auxiliary loss 以及 global router 两个方法&lt;/p>
&lt;p>&lt;strong>Auxiliary loss&lt;/strong>
作者首先构建了 auxiliary loss 来提高负载均衡, 也就是&lt;/p>
$$
\mathcal{L}_{B} = K\sum_{i=1}^K f_i P_i
$$&lt;p>这里 $f_i$ 是分配给第 $i$ 个专家的 token 比例， $P_i$ 是第 $i$ 个专家的平均 routing 概率。&lt;/p>
&lt;p>&lt;strong>Global router&lt;/strong>
作者还基于 &lt;a class="link" href="https://maosong.website/p/gshard/" target="_blank" rel="noopener"
>GShard&lt;/a> 构建了一个 global routing 策略，由于 GPU memory 限制，对于每一个 micro batch size, token 分布不仅在一个 EP group 内分布不平衡，在不同 EP group 之间可能也不平衡。因此，作者实现了一个 global token dispatching 策略。具体来说，在 token 分发到不同的 EP group 之前，作者使用 &lt;code>allgather&lt;/code> 来计算每个专家需要处理的 token. 这样就可以基于全局信息智能分配 token, 避免某些专家过载。&lt;/p>
&lt;h3 id="linear-attention">&lt;a href="#linear-attention" class="header-anchor">&lt;/a>Linear Attention
&lt;/h3>&lt;p>本文中使用的 linear attention 由 Lightning Attention 提出，其表达式为&lt;/p>
$$
O = \mathrm{Norm}((QK^T)V)
$$&lt;p>这里 $Q,K,V\in\mathbb{R}^{n\times n}$ 分别是 query, key 和 value, $n$ 和 $d$ 分别是序列长度和 hidden size.上式可以改变运算顺序，得到&lt;/p>
$$
O = \mathrm{Norm}(Q(K^TV))
$$&lt;p>这样，attention 的计算复杂毒就从 $O(n^2d)$ 变成了 $O(nd^2)$,&lt;/p>
&lt;h4 id="lightning-attention">&lt;a href="#lightning-attention" class="header-anchor">&lt;/a>Lightning Attention
&lt;/h4>&lt;p>当我们不考虑 attention mask 的时候，我们很轻松可以降低 attention 计算的复杂度。但实际上，LLM 会使用 causal mask, 也就是每个 token 智能看到其前面 token 的信息，这样我们的 attention 计算实际上是&lt;/p>
$$
O = \mathrm{Norm}[QK^T\odot M]V)
$$&lt;p>这里 $M_{ij}=\mathbb{1}(i\geq j)$ 是 attention mask.&lt;/p>
&lt;p>见 Lightning Attention&lt;/p>
&lt;h4 id="effectiveness-of-lightning-attention">&lt;a href="#effectiveness-of-lightning-attention" class="header-anchor">&lt;/a>Effectiveness of Lightning Attention
&lt;/h4>&lt;p>作者接下来分析了一下 softmax attention, lightning attention 和 hybrid attention 之间的效率&lt;/p>
&lt;p>首先，作者计算了一下三种架构的参数量以及 FLOPS. 作者分别使用 $l, d, h, b, n$ 来代表 Layer 数，hidden dimension, number of attention heads, batch size 和 sequence length.&lt;/p>
&lt;p>最终计算结果如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Architecture&lt;/th>
&lt;th>Parameter count&lt;/th>
&lt;th>FLOPs count&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Softmax&lt;/td>
&lt;td>$12ld^2$&lt;/td>
&lt;td>$72bnld^2 (1 + \frac{n}{6d} + \frac{5}{18d} )$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Lightning&lt;/td>
&lt;td>$12ld^2+2ld^2/h$&lt;/td>
&lt;td>$72bnld^2(1+\frac{1}{2h}+\frac{5}{18d})$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Hybrid&lt;/td>
&lt;td>$12ld^2+7ld^2/4h$&lt;/td>
&lt;td>$72bnld^2(1+\frac{n}{48d}+\frac{7}{16h}+\frac{5}{18d})$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;blockquote>
&lt;p>[!todo] TODO
compute these results&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Scaling law&lt;/strong>
作者接下来分别针对三个架构设计了 70M, 160M, 410M, 1B, 3B, 7B 系列模型，模型使用 300B token 进行训练，上下文长度为 8192, 对于每种架构，作者将 batch size 设置为 4M tokens. 与 &lt;a class="link" href="https://maosong.website/p/chinchilla-scaling-law/" target="_blank" rel="noopener"
>Chinchilla scaling law&lt;/a> 类似， 作者探究了以下模型的 scaling law, 结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-minimax-01/MiniMax-01-scaling-law.png"
width="1403"
height="584"
loading="lazy"
alt="Summary of Scaling law"
class="gallery-image"
data-flex-grow="240"
data-flex-basis="576px"
>&lt;/p>
&lt;p>从实验结果可以看到，在相同的算力下，lighting attention 倾向于使用更多的参数和 token, 但是其表现相比于 softmax attention 更好。&lt;/p>
&lt;p>&lt;strong>Performance&lt;/strong>
作者还对比了一下三种 attention 在 public benchmark 上的表现，结果如下图&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-minimax-01/MiniMax-01-performance-attention.png"
width="1394"
height="765"
loading="lazy"
alt="Performance of different attention architectures"
class="gallery-image"
data-flex-grow="182"
data-flex-basis="437px"
>&lt;/p>
&lt;p>是检验结果发现，lightning attention 和 softmax attention 除了在 retrieval 任务（Needle in a Haystack）上之外，表现都差不多。与之相对的是，hybrid attention 弥补了这一问题，大幅度提升了模型在 retrieval 任务上的表现&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Observation
Lightning attention 与 softmax attention 的效果差不多，但是其在长上下文任务上的表现比较差。Hybrid attention 可以解决这个问题。&lt;/p>
&lt;/blockquote>
&lt;p>作者还评估了一下三种 attention 的速度，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-minimax-01/Minimax-01-attention-speed-comparison.png"
width="692"
height="373"
loading="lazy"
alt="Training speed of various attention mechanisms"
class="gallery-image"
data-flex-grow="185"
data-flex-basis="445px"
>&lt;/p>
&lt;p>实验结果显示，softmax attention 随序列长度上升其速度急剧下降，Lightning attention 的速度基本没有太大变化。而 Hybrid attention 的速度介于两者之间。&lt;/p>
&lt;p>作者进一步对比了以下两种不同的变体：hybrid-cosformer2 以及 hybrid-hgrn2.这两个模型替换的逻辑与 minimax-01 的结果一致，都是 8 个 block 为一组，每组中前面 7 个 block 将 softmax attention 更换为对应的模块，最后一层不做改动。三种 hybrid attention 机制的表现如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-minimax-01/Minimax-01-hybrid-attention-comparison.png"
width="1377"
height="190"
loading="lazy"
alt="Performance of various hybrid-linear models"
class="gallery-image"
data-flex-grow="724"
data-flex-basis="1739px"
>&lt;/p>
&lt;p>实验结果显示，hybrid-lightning 的表现最好。&lt;/p>
&lt;p>作者最后对比了以下 hybrid-lightning 和 hybrid-window, hybrid window 在每个 group 的前 7 个 block 使用了 window attention, window size 为别为 256, 512, 1024.实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-minimax-01/MiniMax-01-hybrid-window-comparison.png"
width="1392"
height="369"
loading="lazy"
alt="Comparison between hybrid lighting and hybrid window"
class="gallery-image"
data-flex-grow="377"
data-flex-basis="905px"
>&lt;/p>
&lt;p>&lt;strong>Discussion&lt;/strong>
作者最后总结认为，虽然 linear attention 的效率很高，但是它们在 retrieval 相关的任务表现很差，而 retrieval 对于 In-context learning 来说是至关重要的。因此，作者采取了 hybrid 架构，来兼顾模型的效率以及表现。&lt;/p>
&lt;p>作者给出了一个解释。&lt;/p>
&lt;h3 id="ablation-study">&lt;a href="#ablation-study" class="header-anchor">&lt;/a>Ablation Study
&lt;/h3>&lt;p>作者主要进行了两个消融实验：&lt;/p>
&lt;ol>
&lt;li>Hybrid-lightning 与 softmax attention 的对比：作者训练了一个总参数 28B, 激活参数 5B 的 MoE 模型，然后作者使用 MiniMax-01 的方式替换每个 group 的 softmax attention, 并使用了 1T 的 token 进行训练&lt;/li>
&lt;li>pre-layer normalization 与 Post-layer normalization 的对比： 现有的 LLaMA 和 Qwen 等系列模型采用的都是 PreNorm 的方式。&lt;strong>PreNorm 可以让 gradient 通过 residual connection 传播更加直接，但是这也减少了模型的有效深度&lt;/strong>。反之，PostNorm 则可以保留模型的有效深度，但是其问题是会导致梯度消失或爆炸。作者构建了一个总参数为 60B, 激活参数为 9.3B 的 MoE 模型，包含 48 个 block, 模型训练使用 500B token. 模型有两个变体，一个使用 PreNorm, 另一个使用 PostNorm, 对于 PostNorm, 作者使用的是 DeepNorm.&lt;/li>
&lt;/ol>
&lt;p>最终表现如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-minimax-01/MiniMax-01-module-ablation.png"
width="1392"
height="238"
loading="lazy"
alt="Module ablations"
class="gallery-image"
data-flex-grow="584"
data-flex-basis="1403px"
>&lt;/p>
&lt;p>实验结果显示，hybrid lightning 的表现与 softmax 的表现相当，甚至超过了 softmax 的表现。&lt;/p>
&lt;p>另一方面，PostNorm 的表现也超过了 PreNorm 的表现。&lt;/p>
&lt;h3 id="model-spec">&lt;a href="#model-spec" class="header-anchor">&lt;/a>Model Spec
&lt;/h3>&lt;p>基于已有的模型设计，作者探究了如何决定模型的参数。作者的目标&lt;strong>在 performance 和 inference efficiency 之间达到一个平衡&lt;/strong>。&lt;/p>
&lt;p>作者将模型的参数限制在 500B 以下，要求能够在 $8\times 80G$ 的服务器上和 8-bit 的量化下面，支持 1M 的上下文长度。建模的问题如下：&lt;/p>
$$
\min_{P_{all}, P_{act}}\mathcal{L}(P_{all}, P_{act}, T), \quad \mathrm{s.t.}\ C_{compute}(P_{all}, P_{act}, T)&lt;C \text{ and } P_{all} &lt; 500B
$$&lt;p>其中 $\mathcal{L}$, $P_{all}$, $P_{act}$, $T$, $C_{compute}$, $C$ 分别代表损失，总参数，激活参数，训练的 token 数，算力消耗 y 以及算力的 budget.&lt;/p>
&lt;p>作者首先确定了几个关键的因素：&lt;/p>
&lt;ol>
&lt;li>softmax 和 lightning 的混合比例&lt;/li>
&lt;li>depth-to-width 的比例&lt;/li>
&lt;li>linear attention memory size 和 hidden size 的比例&lt;/li>
&lt;li>FFN 的 hidden size 大小&lt;/li>
&lt;li>RoPE 的 base frequency&lt;/li>
&lt;/ol>
&lt;p>作者通过实验发现，hybrid 架构需要更多的 layer 才能带来更好地表现。对于 layer 比较少的模型，其需要更多的 softmax attention layers 来达到相似的表现。&lt;/p>
&lt;p>作者还发现，提升 linear attention memory size 可以有效提高模型的表现。并且 RoPE 的 dimension 最好设置为 softmax attention dimension 的一半。&lt;/p>
&lt;p>基于这些发现，作者构建了 scaling law 来决定最优模型配置。作者训练了不同大小的模型，然后拟合 scaling law 的曲线，最后，作者将模型的总参数确定为 456B, 激活参数确定为 45.9B.&lt;/p>
&lt;h2 id="infra">&lt;a href="#infra" class="header-anchor">&lt;/a>Infra
&lt;/h2>&lt;p>本节介绍了一下 MiniMax-01 的 infra, 作者主要解决了以下几个关键问题：&lt;/p>
&lt;ol>
&lt;li>减少训练时 MoE 中的 all-to-all 通信开销，如何在内存使用，计算效率和 all-to-all 通信开销之间达到一个平衡&lt;/li>
&lt;li>在处理长上下文时，不同 GPU 之间的信息需要共享，这会导致额外的通信开销。因此，如何减少长上下文情形下的通信开销也是一个挑战&lt;/li>
&lt;li>如何提高 lightning attention 在 inference 阶段的效率。当前只在训练阶段做了优化，但是推理阶段也需要提高模型的效率&lt;/li>
&lt;/ol>
&lt;h3 id="moe-optimization">&lt;a href="#moe-optimization" class="header-anchor">&lt;/a>MoE Optimization
&lt;/h3>&lt;p>优化 MoE 架构的主要目的是&lt;strong>最小化通信开销&lt;/strong>，特别是 all-to-all 的通信开销。为了解决这个问题，作者构建了一个 token-grouping-based overlap scheme, 来让通信和计算尽可能并行执行，也就是在计算的同时进行通信。为了保证通信的正确性，每个 ProcessGroup 之间的通信操作必须串行执行，这就导致了不同 group 之间的通信无法 overlap, 也就造成了 idle time 的产生，示意图如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-minimax-01/MiniMax-01-EP-overlap.png"
width="702"
height="451"
loading="lazy"
alt="Expert parallel overlap illustration"
class="gallery-image"
data-flex-grow="155"
data-flex-basis="373px"
>&lt;/p>
&lt;p>但是，在 MiniMax-01 上应用这种方法时，作者发现如果使用 TP 来切分 expert 的参数的话，会导致计算密度过低。&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Explanation
针对 MoE 模块使用 TP 导致计算密度过低的原因是 MoE 模块是稀疏的，计算式大量 GPU 都处于空闲状态，因此计算密度低。&lt;/p>
&lt;/blockquote>
&lt;p>如果不使用 TP 的话，就必须使用更大的 PP 配置，也就是使用更多的 GPU, 但是使用 PP 并不会减少 activation 的内存占用，这在长上下文训练时，会增加内存消耗，而且不会提升整体的训练速度。因此，我们需要一种新的策略，使其能够：&lt;/p>
&lt;ol>
&lt;li>平衡内存使用与计算密度&lt;/li>
&lt;li>优化特定模型和任务的训练过程&lt;/li>
&lt;/ol>
&lt;p>基于这个目标，在本文中作者提出了 ETP, 也就是 Expert Tensor Parallel, 用于管理 expert 的参数划分。同时，作者还提出了 EDP, 也就是 Expert Data Parallel, 来将每个 expert 的数据并行封装在一起。&lt;code>world_size&lt;/code> 满足两个条件：&lt;/p>
$$
\texttt{world\_size} = \texttt{size}_{PP}\times \texttt{size}_{DP}\times \texttt{size}_{CP}\times \texttt{size}_{TP}
$$&lt;p>以及&lt;/p>
$$
\texttt{world\_size} = \texttt{size}_{PP}\times \texttt{size}_{EDP}\times \texttt{size}_{ETP}\times \texttt{size}_{EP}
$$&lt;p>首先，作者构建了 EP-ETP 策略，用于平衡内存使用以及计算密度，这个过程包括了四个步骤：&lt;/p>
&lt;ol>
&lt;li>all-to-all dispatch: （EP）将每个 token 分发到 expert 所在 GPU 上&lt;/li>
&lt;li>allgather: （TP）将输入的 token 进行切分，分发给对应的 TP GPU 上&lt;/li>
&lt;li>Expert:（TP）执行 expert 的计算过程&lt;/li>
&lt;li>ReduceScatter: （TP）将 TP 输出的结果进行汇总然后再分发给不同的 TP&lt;/li>
&lt;li>all-to-all combine: 将不同 expert 的输出结果进行汇总，传递给下一层&lt;/li>
&lt;/ol>
&lt;p>但是，由于同一个 ProcessGroup 内部的通信必须串行执行，如果计算效率比较高的话，就会产生 bubble, 作者的改进方法是提高计算量，让计算消耗的时间与通信消耗的时间大致相当。&lt;/p>
&lt;p>作者还尝试降低 ProcessGroup 的 size, 这样可以进一步提高计算和通信之间的 overlap, 但是问题是降低 group size 之后，group 的数量会增加，而这会导致 scheduling 以及让通信变为 CPU-bound. 作者认为这需要根据具体场景来进行设置。最终，通过优化，作者将 MoE 模块的通信开销降低了 50%.&lt;/p>
&lt;p>改进的示意图如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-minimax-01/MiniMax-01-EP-ETP-overlap.png"
width="1136"
height="660"
loading="lazy"
alt="EP-ETP overlap"
class="gallery-image"
data-flex-grow="172"
data-flex-basis="413px"
>&lt;/p>
&lt;h3 id="long-context-optimization">&lt;a href="#long-context-optimization" class="header-anchor">&lt;/a>Long Context Optimization
&lt;/h3>&lt;p>为了处理不同长度的 sequence, 作者使用了 sequence packing 的技巧。&lt;/p>
&lt;p>&lt;strong>Varlen Ring Attention&lt;/strong>
当前主要是使用 ring attention 来划分数据，但是 ring-attention 与 sequence packing 是冲突的。已有工作如 &lt;a class="link" href="https://maosong.website/p/chinchilla-scaling-law/" target="_blank" rel="noopener"
>flash attention&lt;/a> 支持 varlen data packing, 但是不支持 ring attention; 而 TransformerEngine 实现了 ring attention, 但不支持真正的 varlen. 因此作者的目的就是解决这个问题。&lt;/p>
&lt;p>TransformerEngine 的问题在于每个 sequence 必须是 $2\times \texttt{size}_{CP}$ 的整数倍，当 CP 很大且样本长度分布未知的时候，padding 会很严重，导致算力和内存消耗严重。&lt;/p>
&lt;p>作者的解决方法为使用 varlen ring attention. 具体做法就是在每一步通信中，都基于 attention mask 的 offset 来处理不同的 sequence. 示意图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-minimax-01/MiniMax-01-varlen-ring-attention.png"
width="969"
height="374"
loading="lazy"
alt="Varlen Ring Attention"
class="gallery-image"
data-flex-grow="259"
data-flex-basis="621px"
>&lt;/p>
&lt;p>&lt;strong>LASP+&lt;/strong>
对于 lightning attention, 之前的做法是使用 LASP 算法来实现 linear attention. 但是 LASP 的问题在于其是一个串行依赖，这样导致训练效率变低。&lt;/p>
&lt;p>为了解决这个问题，作者提出了 LASP+, 包括以下几个步骤&lt;/p>
&lt;ol>
&lt;li>Local Prefix Sum Calculation: 每个 GPU 先计算自己局部 prefix sum, 作者将其记为 KVL&lt;/li>
&lt;li>Global Synchronization via AllGather: 然后，作者通过 AllGather 将自己的 KVL 发送给其他 GPU. 这样就可以一次完成所有通信&lt;/li>
&lt;li>Prefix Sum Computation: 最后，每个 GPU 利用所有 KVL, 分别计算自己需要的 prefix sum.&lt;/li>
&lt;/ol>
&lt;p>通过这样一个优化，我们可以将 LASP 的延迟降低 $N_{pcn}$ 倍，这里 $N_{pcn}$ 是并行计算节点的个数。&lt;/p>
&lt;p>作者进一步结合了前面的 Varlen ring attention, 主要包括 padding to block size 和 sequential concatenation 两个步骤。&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Remark
LISP+ 说明了现在大模型分布式训练的一个重要趋势，也就是用通信换并行。特别是 GPU 通信带宽特别高的情况。因此，算法设计不仅要考虑理论复杂度还要考虑实际中硬件并行的利用率。&lt;/p>
&lt;/blockquote>
&lt;h3 id="lightning-attention-inference-optimization">&lt;a href="#lightning-attention-inference-optimization" class="header-anchor">&lt;/a>Lightning Attention Inference Optimization
&lt;/h3>&lt;p>已有的 lightning attention 并没有考虑实际的部署需求。作者提出了四个策略来优化 lightning attention 的 inference efficiency:&lt;/p>
&lt;p>&lt;strong>Batched Kernel Fusion&lt;/strong>
作者首先对 memory-bound kernels 进行融合。在 prefill phase, 作者将多步操作，比如 Q, K, V projection, padding, partitioning 等放在一个 kernel 里，来减少内存 I/O. 在 decoding phase, 作者将 KV 的计算也放在一个 kernel 里，计算完之后就直接写入 KV cache 而不经过缓冲区&lt;/p>
&lt;p>&lt;strong>PD separation&lt;/strong>
inference 的 decoding 阶段模型每次只生成 1 个 token, 此时任务是 memory-bound, 也就是瓶颈在显存读写而不是算力，我们只需要很少的 GPU SMs 就可以跑起来。因此，作者设计了两个不同的 CUDA streams 来分别处理 length 为 1 和 length&amp;gt;1 的情况，这样就可以有效提高整体的计算效率和平衡 GPU 的利用率。&lt;/p>
&lt;p>&lt;strong>Multi-level padding&lt;/strong>
已有的 padding 技巧主要是将输入的多个 sequence 都 resize 到固定长度方便处理。但是在推理的时候，使用预设的固定长度 (block size) 会导致无效计算量增加。因此，作者提出了 multi-level padding, 也就是提供不同的 block size, 比如 32, 64, 128. 这样可以有效避免模型因为 padding 而产生的额外计算开销。&lt;/p>
&lt;p>&lt;strong>StridedBatchedMatmul Extension&lt;/strong>
这一点主要是针对 Hopper GPU 进行的优化。Hopper 架构相比于 Ampere 架构有一些新的特性，作者基于这些特性来优化整体的性能。&lt;/p>
&lt;p>作者使用 WGMMA 来优化计算，用 TMA 来优化内存访问效率，实现计算与内存访问重叠，减少等待时间。最终，作者的目的是希望基于不同的 GPU 架构来动态调整 Pipeline stages, 以达到更好的性能。&lt;/p>
&lt;p>通过以上这些优化，作者在 H20 上达到了 75% 的 MFU. 作者发现，由于使用了 lightning attention, attention 不分计算从传统的 95% 以上降低到了 12% 左右，不再是算力的瓶颈部分。&lt;/p>
&lt;h2 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h2>&lt;p>作者首先介绍了预训练的数据构成，然后作者介绍了如何选取高质量的训练数据，最后，作者介绍了实验的参数。&lt;/p>
&lt;h3 id="data">&lt;a href="#data" class="header-anchor">&lt;/a>Data
&lt;/h3>&lt;p>数据来源包括学术文献，数据，互联网语料以及代码。作者构建了如下的数据清洗策略：&lt;/p>
&lt;ol>
&lt;li>data quality enhancement: 作者使用了 rule-based cleaning 以及去重。然后，作者构建了一个 reward labeler，来从 knowledge depth, practical helpfulness 以及 categorical distribution 三个维度进行打分。&lt;/li>
&lt;li>data formatting optimization. 作者发现，类似 markdown 的格式会降低数据整体的多样性和质量。因此，作者构建了一个嵌套的文档格式，来将对话和 QA 数据组织起来，进而平衡自然数据和结构化数据之间的平衡性。&lt;/li>
&lt;li>data mixture investigation. 作者发现完全去除掉低质量的数据也会影响模型在下游任务上的表现。因此作者平衡了高质量数据和低质量数据&lt;/li>
&lt;/ol>
&lt;p>作者基于 BPE 算法来构建 tokenizer,最终 tokenizer 的大小为 &lt;strong>200K&lt;/strong>&lt;/p>
&lt;p>接下来，作者通过实验来确定最优的数据配置，给定数据集 $\mathcal{D}$, 作者作如下备择假设&lt;/p>
$$
H_1: \mu_{T_{\mathcal{D}}}> \mu_{T_{baseline}}
$$&lt;p>这里 $\mu$ 代表模型表现的加权平均， $T$ 代表模型的表现分布。也就是说，加入新的数据集之后，其平均表现大于 baseline 的表现，则我们认为这个数据集是不被拒绝的。&lt;/p>
&lt;p>作者通过多项选择题来进行评估，评估时作者计算每个选项 completion 的概率，最终的分布计算方式为&lt;/p>
$$
\log \mathrm{acc}_{\mathrm{norm}^2}(x) = \log\mathrm{softmax}_{p'(c\in C_x)}\left\{(p'(c^*))\right\}
$$&lt;p>其中 $p_i'(c)=\frac{p_i(c)}{\mathrm{bytes(c)}}$ 是 normalized 之后的选项概率。作者在训练的时候，通过实验来保证这个 metric 的稳定性，除此之外，作者还加入了一个 discriminator, 也就是 $\Delta_{obvious}/\sigma_{seed}$, 其中 $\Delta_{obvious}$ 表示不同模型表现的差距， $\sigma_{seed}$ 表示不同随机种子的方差。&lt;/p>
&lt;h3 id="training-strategy">&lt;a href="#training-strategy" class="header-anchor">&lt;/a>Training Strategy
&lt;/h3>&lt;p>作者使用 Xavier 对模型参数进行初始化，然后使用了 DeepNorm 作为 Normalization 模块。作者使用 AdamW 作为优化器，其中 $\beta_1=0.9$, $\beta_2=0.95$, weight decay 为 $0.1$.&lt;/p>
&lt;p>训练的 sequence length 为 8192，batch size 分别为 16M, 32M (69B tokens), 64M (790B tokens), 128M (4.7T tokens).&lt;/p>
&lt;p>作者基于 training loss 和 critical batch size 来设计 schedule. 作者基于小模型的实验结果来拟合一个 power-law 关系，结果如下图所示：&lt;/p>
&lt;p>基于实验结果，作者&lt;/p>
&lt;h2 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h2>&lt;h3 id="prompt-collection">&lt;a href="#prompt-collection" class="header-anchor">&lt;/a>Prompt Collection
&lt;/h3>&lt;p>作者从多个 domain 收集到了数百万 diverse 和高质量的 query,作者基于任务类型，domain 和难度对 prompt 进行分类。接下来作者对 prompt 进行去重以及平衡难度问题。prompt 覆盖 long context, math, reasoning 等 domain&lt;/p>
&lt;h3 id="reward-model">&lt;a href="#reward-model" class="header-anchor">&lt;/a>Reward Model
&lt;/h3>&lt;p>作者基于以下几个原则来设计 reward:&lt;/p>
&lt;ul>
&lt;li>Correctness: 对于 math 任务，作者使用 MiniMax-Text-01 来生成 binary reward; 对于 code 任务，作者使用沙盒环境运行代码，根据通过率来给出奖励&lt;/li>
&lt;li>Truthfulness: 作者使用先进的语言模型来评估 response 的 factual accuracy&lt;/li>
&lt;li>Helpfulness: 作者实现了基于规则的验证系统，来评估回答的 coherence, depth, contextual relevance 和 stylistic appropriateness.&lt;/li>
&lt;li>Harmlessness: 作者基于 Constitutional AI principles, 构建了一系列协议来保证模型输出的安全性。&lt;/li>
&lt;/ul>
&lt;h3 id="sft">&lt;a href="#sft" class="header-anchor">&lt;/a>SFT
&lt;/h3>&lt;p>作者通过使用多个 expert model 来生成多样化的回答。然后，作者通过 n-gram 以及 semantic similarity 来提高数据的多样性和质量。&lt;/p>
&lt;h3 id="rl">&lt;a href="#rl" class="header-anchor">&lt;/a>RL
&lt;/h3>&lt;p>RL 训练包括 offline stage 以及 online stage.&lt;/p>
&lt;p>&lt;strong>Offline RL&lt;/strong>
在 offline stage, 作者使用 DPO 来进行训练。数据构造过程也比较简单，使用模型来进行采样，评估，选取最好和最差的作为正样本和负样本。&lt;/p>
&lt;p>&lt;strong>Online RL&lt;/strong>
这个阶段使用 GRPO 进行训练，作者发现使用 SFT 阶段的 prompt 会导致训练饱和，因此，作者并没有采用 SFT 阶段的 prompts&lt;/p>
&lt;p>作者主要针对 GRPO 进行了一下改进：&lt;/p>
&lt;ol>
&lt;li>Important Sampling Weight Clipping. 作者发现，PPO 以及 GRPO 只使用了单侧的 clipping, 这会导致训练的不稳定性。因此作者构建了额外的 clipping 策略。&lt;/li>
&lt;li>KL divergence optimization. 作者发现 KL divergence 也会导致训练不稳定，因此作者对 KL divergence 进行了 reformulate 来稳定梯度&lt;/li>
&lt;li>Balanced Advantage Estimation. 作者确保正样本和负样本的对 reward 的贡献差不多，来提高训练的稳定性。&lt;/li>
&lt;/ol>
&lt;h3 id="safety-alignment">&lt;a href="#safety-alignment" class="header-anchor">&lt;/a>Safety Alignment
&lt;/h3>&lt;p>作者还加入了一个 safety alignment 阶段，用于提升模型的安全性。&lt;/p>
&lt;p>数据包括三类：&lt;/p>
&lt;ol>
&lt;li>safety-category specific prompts.&lt;/li>
&lt;li>Real-world user data collection.&lt;/li>
&lt;li>prompt augmentation.&lt;/li>
&lt;/ol>
&lt;p>接下来，作者使用一个无害的 reward model 来对模型的输出进行打分。&lt;/p>
&lt;h3 id="training-recipe">&lt;a href="#training-recipe" class="header-anchor">&lt;/a>Training Recipe
&lt;/h3>&lt;p>最终，post-training 一共包括 5 个 stage，训练时，作者将 RoPE 的 base frequency 保持在 10M.&lt;/p>
&lt;ol>
&lt;li>Initial short-context training: 模型的上下文长为 8192，然后进行 SFT&lt;/li>
&lt;li>Extended context training: 模型的上下文长度为 1,032,192. 训练数据包括 50% 长文本和 50% 短文本&lt;/li>
&lt;li>Short-context Preference Optimization: 模型的上下文长度为 8192, 然后进行 DPO 的训练&lt;/li>
&lt;li>Long-context Preference Optimization: 模型的上下文长度为 1,032,192, 然后进行 DPO 的训练&lt;/li>
&lt;li>Online Reinforcement Learning: RL 的训练，与上文一致。&lt;/li>
&lt;/ol>
&lt;p>训练的配置如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Stage I&lt;/th>
&lt;th>Stage II&lt;/th>
&lt;th>Stage III&lt;/th>
&lt;th>Stage IV&lt;/th>
&lt;th>Stage V&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Sequence Length&lt;/td>
&lt;td>8192&lt;/td>
&lt;td>1032192&lt;/td>
&lt;td>8192&lt;/td>
&lt;td>1032192&lt;/td>
&lt;td>8192&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Epoch&lt;/td>
&lt;td>2&lt;/td>
&lt;td>2&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Batch Size&lt;/td>
&lt;td>128&lt;/td>
&lt;td>80&lt;/td>
&lt;td>64&lt;/td>
&lt;td>64&lt;/td>
&lt;td>512&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Max LR&lt;/td>
&lt;td>1e-5&lt;/td>
&lt;td>3e-6&lt;/td>
&lt;td>5e-7&lt;/td>
&lt;td>5e-7&lt;/td>
&lt;td>1e-6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Min LR&lt;/td>
&lt;td>1e-6&lt;/td>
&lt;td>3e-6&lt;/td>
&lt;td>5e-8&lt;/td>
&lt;td>5e-7&lt;/td>
&lt;td>1e-7&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LR Decay&lt;/td>
&lt;td>Cosine&lt;/td>
&lt;td>Constant&lt;/td>
&lt;td>Cosine&lt;/td>
&lt;td>Constant&lt;/td>
&lt;td>Cosine&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="llm-evaluation">&lt;a href="#llm-evaluation" class="header-anchor">&lt;/a>LLM Evaluation
&lt;/h2>&lt;p>作者对比了 GPT-4o, Claude-3.5-Sonnet, Gemini-1.5-Pro, Qwen2.5-72B, DeepSeek-V3 以及 LLaMA3.1-405B, 实验结果如下图&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-minimax-01/MiniMax-01-text-performance.png"
width="1393"
height="953"
loading="lazy"
alt="Performance of MiniMax-Text-01"
class="gallery-image"
data-flex-grow="146"
data-flex-basis="350px"
>&lt;/p>
&lt;p>作者还评估了以下 MiniMax-Text-01 在长上下文情境下表现，我们这里列出 Ruler 上的结果，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-minimax-01/MiniMax-01-text-Ruler-performance.png"
width="1334"
height="250"
loading="lazy"
alt="MiniMax-Text-01 performance on Ruler benchmark"
class="gallery-image"
data-flex-grow="533"
data-flex-basis="1280px"
>&lt;/p>
&lt;p>可以看到在 1M 上下文时，模型的表现超过了 Gemini-1.5-Pro 的表现。&lt;/p>
&lt;hr>
&lt;h2 id="vlm-architecture">&lt;a href="#vlm-architecture" class="header-anchor">&lt;/a>VLM Architecture
&lt;/h2>&lt;p>作者基于 MiniMax-Text-01 开发了 MiniMax-VL-01, 来扩展模型的多模态理解能力。MiniMax-VL-01 是一个标准的 ViT-MLP-LLM 架构，其中：&lt;/p>
&lt;ul>
&lt;li>ViT: 从零开始训练的 ViT, 参数为 303M&lt;/li>
&lt;li>MLP: 2 层的 MLP, 激活函数为 GeLU&lt;/li>
&lt;li>LLM: MiniMax-Text-01&lt;/li>
&lt;/ul>
&lt;p>作者实现了动态分辨率策略，用于处理不同大小的图片。每张图片都被切分为 $336\times 336$ 的大小， 然后作者还加入了一个 thumbnail image 用于提取全局信息。&lt;/p>
&lt;h2 id="data-1">&lt;a href="#data-1" class="header-anchor">&lt;/a>Data
&lt;/h2>&lt;p>为了训练 vision encoder, 作者使用了 &lt;strong>694M&lt;/strong> 的 Image-caption pair 数据，为了提高数据质量，作者对其中 180M 的数据进行的 caption 的改写，训练的时候原始数据和重写数据的采样概率都是 $50\%$.&lt;/p>
&lt;p>作者还从公开数据收集到了 100M 的图片，每个图片都有详细的描述，这些描述都由一个 caption model 生成，然后再由人类进行修正。每条描述大概包含 300 个 text token&lt;/p>
&lt;p>作者还构建了一个高质量的 instruction dataset, 这个数据集是通过预定义一系列任务，然后对于每个任务分别生成对应的 QA pair 得到的。在训练的时候，作者平衡了不同任务的采样概率。&lt;/p>
&lt;p>最后，作者对收集收集数据的类别分布进行了可视化，结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-minimax-01/MiniMax-01-VL-data-distribution.png"
width="692"
height="707"
loading="lazy"
alt="Visualization of top rags of sampled instruction data"
class="gallery-image"
data-flex-grow="97"
data-flex-basis="234px"
>&lt;/p>
&lt;h2 id="training">&lt;a href="#training" class="header-anchor">&lt;/a>Training
&lt;/h2>&lt;p>训练一共包含 5 个 stage, 第一个 stage 用于单独训练 ViT, 后面三个 stage 分别用于对齐和提升模型的多模态能力。&lt;/p>
&lt;p>&lt;strong>Stage 0: ViT training&lt;/strong>
作者基于 ViT-L/14 来训练 vision encoder, 作者使用了基于对比学习的方法，参考了 CoCa 里的方法。训练过程中，作者首先在 224 的图片精度下训练了 &lt;strong>37B&lt;/strong> 的 image-caption pairs. 接下来，作者在 336 的精度下使用了 &lt;strong>1.2B&lt;/strong> 的 pairs 进行微调。最终，模型在 ImageNet-1K 的表现达到了 80.55%.&lt;/p>
&lt;p>&lt;strong>Stage 1: Modality alignment&lt;/strong>
这个阶段的目的是对齐视觉模态和文本模态，作者冻结 LLM, 只训练 ViT 和 MLP. 这个阶段一共使用了&lt;strong>80B&lt;/strong> 的 image description data. 作者发现，在这个阶段，提升图片精度对模型表现提升影响不大。&lt;/p>
&lt;p>&lt;strong>Stage 2: Enhancement of Vision Understanding&lt;/strong>
这个阶段其实就是 instruction tuning, 作者解冻所有参数，然后使用了 &lt;strong>420B&lt;/strong> 多模态 token 以及 &lt;strong>21B&lt;/strong> MiniMax-Text-01 的 post-training token 来提高模型的指令跟随能力。&lt;/p>
&lt;p>&lt;strong>Stage 3: Enhancement of User Experience&lt;/strong>
这个阶段的目的是进一步提高模型在真实场景下模型的避险。作者构建了高质量的多模态数据，包括 &lt;strong>44.8B&lt;/strong> 的多模态 token, 这个阶段模型训练了一个 epoch.&lt;/p>
&lt;p>&lt;strong>Stage 4: Enhancement of Preference&lt;/strong>
这个阶段的目的是对齐，作者使用 &lt;a class="link" href="https://maosong.website/p/notes-on-dpo/" target="_blank" rel="noopener"
>DPO&lt;/a> 来训练模型，数据集包括 &lt;strong>40,000&lt;/strong> 条数据。数据构建过程如下：&lt;/p>
&lt;ol>
&lt;li>prompt selection: 作者基于真实用户交互数据构建了高质量的 prompt&lt;/li>
&lt;li>Response generation: 作者通过多次采样，然后使用 MiniMax-Text-01 来生成多样化的数据&lt;/li>
&lt;li>Reward Assignment: 接下来作者使用 MiniMax-Text- 来评估最终的结果&lt;/li>
&lt;li>Pair Construction: 最后，基于评估结果来构建 preference pairs.&lt;/li>
&lt;/ol>
&lt;p>作者还加入了一些纯文本的 preference data.&lt;/p>
&lt;blockquote>
&lt;p>Observation
作者发现，如果 foundation model 比较强，使用 DPO 的话可能会导致过拟合，因此作者的做法是在一个 epoch 之前就停止训练。&lt;/p>
&lt;/blockquote>
&lt;h2 id="evaluation">&lt;a href="#evaluation" class="header-anchor">&lt;/a>Evaluation
&lt;/h2>&lt;p>作者对比了 GPT-4o, Claude-3.5-Sonnet, Gemini-1.5-Pro, Qwen2-VL-72B, InternVL2.5-78B 和 LLaMA-3.2-90B, 实验结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-minimax-01/MiniMax-01-VL-performance.png"
width="1342"
height="937"
loading="lazy"
alt="Performance of MiniMax-VL-01"
class="gallery-image"
data-flex-grow="143"
data-flex-basis="343px"
>&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>在本文中，作者提出了 MiniMax-01 大模型系列，包括 MiniMax-Text-01 和 MiniMax-VL-01 两个模型，前者是一个总参数为 456B, 激活参数为 45.9B 的基于 MoE 和 hybrid attention 的大语言模型，后者是基于 MiniMax-Text-01 的多模态大模型。作者详细介绍了模型的架构，数据和训练。&lt;/p>
&lt;p>作者认为，未来有以下探索方向：&lt;/p>
&lt;ol>
&lt;li>Long-context Evaluation: 如何更好评估模型的长上下文能力。现有的 long-context benchmark 的任务都比较简单，很难反应模型的真实能力&lt;/li>
&lt;li>Model Architecture: 当前的模型还有 1/8 的模块包含 softmax attention, 如何构建更高效的架构来去掉 softmax attention 是一个值得探究的方向。&lt;/li>
&lt;li>Complex Programming Tasks: 如何提高模型的 coding 能力也是一个值得探索的方向。&lt;/li>
&lt;/ol></description></item><item><title>Notes on Gemini3.0</title><link>https://maosong.website/p/notes-on-gemini3.0/</link><pubDate>Tue, 06 Jan 2026 10:26:39 +0800</pubDate><guid>https://maosong.website/p/notes-on-gemini3.0/</guid><description>&lt;p>Gemini 3.0 是是 Google 新一代最强模型，model card 介绍了 Gemini 3.0 系列的评估结果以及基本能力&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>Gemini 3.0 系列包含&lt;/p>
&lt;ul>
&lt;li>Gemini 3.0 Pro&lt;/li>
&lt;li>Gemini 3.0 Flash&lt;/li>
&lt;li>Gemini 3.0 Pro Image
三个模型&lt;/li>
&lt;/ul>
&lt;p>Gemini 3.0 Pro 拥有原生多模态以及 reasoning 能力，可以处理 text, audio, images, video 以及 code repositories 等模态。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>modalities&lt;/th>
&lt;th>context&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>input&lt;/td>
&lt;td>text, images, audio, video&lt;/td>
&lt;td>1M&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>output&lt;/td>
&lt;td>text&lt;/td>
&lt;td>64K&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Gemini 3.0 Flash 与 Gemini 3.0 Pro 基本一致，与 &lt;a class="link" href="https://maosong.website/p/notes-on-gemini2.5/" target="_blank" rel="noopener"
>Gemini2.5&lt;/a> 相同，应该是采取了蒸馏的方式来实现更高的吞吐速度以及效率&lt;/p>
&lt;p>Gemini 3.0 Pro Image 基于 Gemini 3.0 Pro 开发，是一个支持 text, image prompt 的图片生成模型&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;p>模型从零开始训练，使用了 MoE 架构和 Transformer 架构&lt;/p>
&lt;p>模型使用 TPU 进行训练，训练架构为 JAX 和 ML Pathways.&lt;/p>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>Gemini 3.0 Pro 对比了 &lt;a class="link" href="https://maosong.website/p/notes-on-gemini2.5/" target="_blank" rel="noopener"
>Gemini2.5&lt;/a> , Claude Sonnet 4.5 和 GPT-5.1&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gemini3.0/Gemini-3-0-pro-performance.png"
width="1081"
height="963"
loading="lazy"
alt="Performance of Gemini 3.0 Pro"
class="gallery-image"
data-flex-grow="112"
data-flex-basis="269px"
>&lt;/p>
&lt;p>Gemini 3.0 Flash 对比了 Gemini 3.0 Pro, Gemini 2.5 Flash, Gemini 2.5 Pro, Claude Sonnet 4.5, GPT-5.2 和 Grok 4.1 Fast.&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gemini3.0/Gemini-3-0-Flash-performance.png"
width="1082"
height="1073"
loading="lazy"
alt="Performance of Gemini 3.0 Flash"
class="gallery-image"
data-flex-grow="100"
data-flex-basis="242px"
>&lt;/p>
&lt;p>Gemini 3.0 Pro Image 对比了 Gemini 2.5 Flash Image, GPT-Image 1, Seedream v4, Flux Pro Kontext Max&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gemini3.0/Gemini-3-0-Pro-Image-performance.png"
width="1372"
height="795"
loading="lazy"
alt="Performance of Gemini 3.0 Pro Image on existing capabilities"
class="gallery-image"
data-flex-grow="172"
data-flex-basis="414px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gemini3.0/Gemini-3-0-Pro-Image-performance-new.png"
width="1332"
height="721"
loading="lazy"
alt="Performance of Gemini 3.0 Pro Image on new capabilities"
class="gallery-image"
data-flex-grow="184"
data-flex-basis="443px"
>&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf" target="_blank" rel="noopener"
>Gemini 3.0 Pro Model Card&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Flash-Model-Card.pdf" target="_blank" rel="noopener"
>Gemini 3.0 Flash Model Card&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Image-Model-Card.pdf" target="_blank" rel="noopener"
>Gemini 3.0 Pro Image Model Card&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Gemini2.5</title><link>https://maosong.website/p/notes-on-gemini2.5/</link><pubDate>Sat, 06 Dec 2025 18:14:15 +0800</pubDate><guid>https://maosong.website/p/notes-on-gemini2.5/</guid><description>&lt;h2 id="介绍">&lt;a href="#%e4%bb%8b%e7%bb%8d" class="header-anchor">&lt;/a>介绍
&lt;/h2>&lt;p>DeepMind 在 6 月 17 号发布了 Gemini2.x 系列的技术报告，包括&lt;/p>
&lt;ul>
&lt;li>Gemini 2.5 Pro&lt;/li>
&lt;li>Gemini 2.5 Flash&lt;/li>
&lt;li>Gemini 2.0 Flash (earlier)&lt;/li>
&lt;li>Gemini 2.0 Flash-Lite (earlier)&lt;/li>
&lt;/ul>
&lt;p>技术报告简单说了一些技术细节，主要还是模型的评估&lt;/p>
&lt;blockquote>
&lt;p>注：Gemini 2.0 Flash 和 Gemini 2.0 Flash-Lite 将要被 Gemini 2.5 Flash 和 Gemini 2.5 Flash-Lite 取缔，见最新的 blog&lt;/p>
&lt;/blockquote>
&lt;p>Gemini2.x 系列亮点：&lt;/p>
&lt;ol>
&lt;li>领先的 coding 和 reasoning 能力&lt;/li>
&lt;li>超过 1M 的上下文，可以处理超过 3 个小时的 video&lt;/li>
&lt;li>集成 long context, multimodal 和 reasoning 三种能力的 agentic workflow 能力&lt;/li>
&lt;/ol>
&lt;p>模型能力对比&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Gemini 1.5 Flash&lt;/th>
&lt;th>Gemini 1.5 Pro&lt;/th>
&lt;th>Gemini 2.0 Flash-Lite&lt;/th>
&lt;th>Gemini 2.0 Flash&lt;/th>
&lt;th>Gemini 2.5 Flash&lt;/th>
&lt;th>Gemini 2.5 Pro&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Input Modalities&lt;/td>
&lt;td>Text, Image, Video, Audio&lt;/td>
&lt;td>Text, Image, Video, Audio&lt;/td>
&lt;td>Text, Image, Video, Audio&lt;/td>
&lt;td>Text, Image, Video, Audio&lt;/td>
&lt;td>Text, Image, Video, Audio&lt;/td>
&lt;td>Text, Image, Video, Audio&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Input length&lt;/td>
&lt;td>1M&lt;/td>
&lt;td>2M&lt;/td>
&lt;td>1M&lt;/td>
&lt;td>1M&lt;/td>
&lt;td>1M&lt;/td>
&lt;td>1M&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Output modalities&lt;/td>
&lt;td>Text&lt;/td>
&lt;td>Text&lt;/td>
&lt;td>Text&lt;/td>
&lt;td>Text, Image*&lt;/td>
&lt;td>Text, Audio*&lt;/td>
&lt;td>Text, Audio&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Output length&lt;/td>
&lt;td>8K&lt;/td>
&lt;td>8K&lt;/td>
&lt;td>8K&lt;/td>
&lt;td>8K&lt;/td>
&lt;td>64K&lt;/td>
&lt;td>64K&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Thinking&lt;/td>
&lt;td>No&lt;/td>
&lt;td>No&lt;/td>
&lt;td>No&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>Dynamic&lt;/td>
&lt;td>Dynamic&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Supports tool use?&lt;/td>
&lt;td>No&lt;/td>
&lt;td>No&lt;/td>
&lt;td>No&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Knowledge cutoff&lt;/td>
&lt;td>November 2023&lt;/td>
&lt;td>November 2023&lt;/td>
&lt;td>June 2024&lt;/td>
&lt;td>June 2024&lt;/td>
&lt;td>January 2025&lt;/td>
&lt;td>January 2025&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>模型场景使用对比&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Gemini 2.5 Flash-Lite&lt;/th>
&lt;th>Gemini 2.5 Flash&lt;/th>
&lt;th>Gemini 2.5&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Thinking&lt;/td>
&lt;td>No&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>使用场景&lt;/td>
&lt;td>大规模调用&lt;/td>
&lt;td>日常使用&lt;/td>
&lt;td>coding 或者 reasoning 人物&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>速度&lt;/td>
&lt;td>非常快&lt;/td>
&lt;td>快&lt;/td>
&lt;td>一半&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>表现&lt;/td>
&lt;td>一半&lt;/td>
&lt;td>强&lt;/td>
&lt;td>非常强&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>输入价格&lt;/td>
&lt;td>0.1&lt;/td>
&lt;td>0.3&lt;/td>
&lt;td>1.25&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>输出价格&lt;/td>
&lt;td>0.4&lt;/td>
&lt;td>2.5&lt;/td>
&lt;td>10&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>模型表现&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gemini2.5/Gemini_2_5_performance.png"
width="3840"
height="2160"
loading="lazy"
alt="Gemini_2_5_performance"
class="gallery-image"
data-flex-grow="177"
data-flex-basis="426px"
>&lt;/p>
&lt;p>模型吞吐量对比&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gemini2.5/Gemini_2_5_throughput.png"
width="2156"
height="960"
loading="lazy"
alt="Gemini_2_5_throughput"
class="gallery-image"
data-flex-grow="224"
data-flex-basis="539px"
>&lt;/p>
&lt;h2 id="架构数据与训练">&lt;a href="#%e6%9e%b6%e6%9e%84%e6%95%b0%e6%8d%ae%e4%b8%8e%e8%ae%ad%e7%bb%83" class="header-anchor">&lt;/a>架构，数据与训练
&lt;/h2>&lt;h3 id="架构">&lt;a href="#%e6%9e%b6%e6%9e%84" class="header-anchor">&lt;/a>架构
&lt;/h3>&lt;p>Gemini2.5 是一个&lt;strong>基于 MoE 的 transformer 架构&lt;/strong>，支持 text, vision, audio 模态&lt;/p>
&lt;p>Flash 系列使用的是知识蒸馏的方法训练得到的，训练时使用了 $k$-sparse 的策略，也就是只保留教师模型输出概率最高的 $k$ 的词以及对应的概率。作者认为知识蒸馏可以有效提高小模型的能力。&lt;/p>
&lt;h3 id="infra">&lt;a href="#infra" class="header-anchor">&lt;/a>Infra
&lt;/h3>&lt;p>Gemini 系列在 TPUv5p 的架构上进行训练。作者主要提了两点：&lt;/p>
&lt;ol>
&lt;li>Slice-Granularity Elasticity：可以在部分 TPU 出现故障时快速切换并继续训练&lt;/li>
&lt;li>Split-Phase SDC detection：通过轻量级重放和校验机制，在几分钟内就能识别出有问题的硬件设备&lt;/li>
&lt;/ol>
&lt;h3 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h3>&lt;p>post-training 包含 SFT，reward model 以及 RL 的训练。&lt;/p>
&lt;p>在 RL 阶段，奖励来自 verifiable rewards 和 model-based generative rewards&lt;/p>
&lt;h3 id="能力提升">&lt;a href="#%e8%83%bd%e5%8a%9b%e6%8f%90%e5%8d%87" class="header-anchor">&lt;/a>能力提升
&lt;/h3>&lt;p>技术报告提到了几个方面能力的提升&lt;/p>
&lt;p>&lt;strong>code&lt;/strong>
pre-training 阶段，加入了大量的代码数据，作者还评估了代码数据的质量
post-training 阶段，作者基于 reasoning 能力构建了一系列的工程任务，来提高模型解决问题的能力&lt;/p>
&lt;p>&lt;strong>Factuality&lt;/strong>
通过 search 和 tool use，reason about output 以及 issue follow-up queries 来验证 factual accuracy&lt;/p>
&lt;p>&lt;strong>Multilinguality&lt;/strong>
预训练时使用了 400 多种语言的语料进行训练&lt;/p>
&lt;p>&lt;strong>Audio&lt;/strong>
训练模型完成 audio generation 任务，生成的时候使用了&lt;strong>causal audio representation&lt;/strong>，训练数据覆盖了 200 多种语言&lt;/p>
&lt;p>&lt;strong>Video&lt;/strong>
通过降低每帧视频对应的 visual token 个数（258-&amp;gt; 66），来让模型可以处理 3 个小时的视频&lt;/p>
&lt;h2 id="evaluation">&lt;a href="#evaluation" class="header-anchor">&lt;/a>Evaluation
&lt;/h2>&lt;p>对比了 Claude_4, o3, &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-r1/" target="_blank" rel="noopener"
>DeepSeek-R1&lt;/a> 和 Grok-1&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gemini2.5/Gemini_2_5_evaluation.png"
width="1754"
height="1174"
loading="lazy"
alt="Gemini_2_5_evaluation"
class="gallery-image"
data-flex-grow="149"
data-flex-basis="358px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gemini2.5/Gemini_2_5_video_understanding_performance.png"
width="1724"
height="988"
loading="lazy"
alt="Gemini_2_5_video_understanding_performance"
class="gallery-image"
data-flex-grow="174"
data-flex-basis="418px"
>&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>结论里作者主要提到了两点&lt;/p>
&lt;ol>
&lt;li>模型能力的提升已经超过了 benchmark 的构建速度和成本&lt;/li>
&lt;li>未来如何设计经济的，覆盖广的，能动态调整难度的 benchmark 是一个关键问题&lt;/li>
&lt;/ol>
&lt;p>技术报告中作者还提到了 Gemini Plays Pokemon 的 case study，作者提到了两点问题：&lt;/p>
&lt;ol>
&lt;li>作者发模型对视觉信息的依赖程度并不是很高&lt;/li>
&lt;li>尽管模型上下文长度超过了 1M，但是对于这种复杂的 long horizon 问题，当输入超过了 100K token 之后，模型倾向于重复过去的行为，而不是生成新的计划
因此，未来如何解决 multi-turn, long-horizon 的 agentic task 也是一个值得探究的方向。&lt;/li>
&lt;/ol>
&lt;h2 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://blog.google/products/gemini/gemini-2-5-model-family-expands/" target="_blank" rel="noopener"
>Gemini 2.5 Flash/Flash Lite&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://blog.google/products/gemini/gemini-2-5-model-family-expands/" target="_blank" rel="noopener"
>Gemini 2.5 Technical Report&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Qwen3 VL</title><link>https://maosong.website/p/notes-on-qwen3-vl/</link><pubDate>Fri, 05 Dec 2025 10:12:01 +0800</pubDate><guid>https://maosong.website/p/notes-on-qwen3-vl/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者在本文中提出了 Qwen3-VL 系列多模态大模型，包括 4 个 dense 模型和两个 MoE 模型，模型的上下文长度为 256K, 通过数据和训练上的优化，作者保持了模型的纯文本能力。最终 Qwen3-VL 包括 non-thinking 和 thinking variants.&lt;/p>
&lt;p>在架构上，Qwen3-VL 进行了三点改进：&lt;/p>
&lt;ol>
&lt;li>Interleaved MRoPE: 作者解决了 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5-vl/" target="_blank" rel="noopener"
>Qwen2.5-VL&lt;/a> 提出的 MRoPE 在长视频理解场景下的频谱不平衡问题&lt;/li>
&lt;li>DeepStack: 作者使用了 DeepStack 来提取 &lt;a class="link" href="https://maosong.website/p/notes-on-vit/" target="_blank" rel="noopener"
>ViT&lt;/a> 不同 layer 的视觉特征&lt;/li>
&lt;li>Explicit Video timestamps: 作者使用了绝对时间来标记 frame 来提供更直接的时间信息&lt;/li>
&lt;/ol>
&lt;p>在数据上，作者使用了 image caption, OCR, grounding, spatial reasoning, code, long documents 以及 temporally grounded video 等数据，作者 还是用了 GUI-agent interaction 数据来提高模型的 action 能力&lt;/p>
&lt;p>在训练上，Qwen3-VL 包含两个大的阶段：pre-training 和 post-traing, pre-training 包含 4 个小阶段，post-training 包含 3 个阶段。&lt;/p>
&lt;h2 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h2>&lt;p>Qwen3-VL 的架构如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen3-vl/Qwen3-VL-Architecture.png"
width="5908"
height="3413"
loading="lazy"
alt="Architecture of Qwen3-VL"
class="gallery-image"
data-flex-grow="173"
data-flex-basis="415px"
>&lt;/p>
&lt;p>其中，&lt;/p>
&lt;ul>
&lt;li>LLM: LLM 使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a> 系列大语言模型，包括 2B, 4B, 8B, 32B 四个 dense model 以及 30B-A3B, 235B-A22B 两个 moe 模型&lt;/li>
&lt;li>Vision Encoder: encoder 基于 [[SigLip-2]] 初始化，然后使用了 dynamic input resolutions 进行 continue training, 作者使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-comp/" target="_blank" rel="noopener"
>CoMP&lt;/a> 提出的 2D-RoPE 以及 interpolate absolute position embedding, 最终包括 SigLip2-SO-400M 和 SigLip-Large (300M) 两个 size, 后者用于 2B 和 4B 两个 size&lt;/li>
&lt;li>Patch Merger: 一个 2 层的 MLP, 将四个 visual token 压缩为 1 个&lt;/li>
&lt;/ul>
&lt;h3 id="interleaved-mrope">&lt;a href="#interleaved-mrope" class="header-anchor">&lt;/a>Interleaved MRoPE
&lt;/h3>&lt;p>这部分介绍见 [[MRoPE-Interleave]]&lt;/p>
&lt;h3 id="deepstack">&lt;a href="#deepstack" class="header-anchor">&lt;/a>DeepStack
&lt;/h3>&lt;p>受 Deepstack 启发，作者从 vision encoder 的中间层（具体来说是第 8， 16， 24 层）提取对应的视觉特征，然后经过 MLP 与 LLM 对应 layer 的视觉 token 直接进行相加。&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen3-vl/Qwen3-VL-DeepStack.png"
width="1723"
height="1024"
loading="lazy"
alt="architecture of DeepStack"
class="gallery-image"
data-flex-grow="168"
data-flex-basis="403px"
>&lt;/p>
&lt;h3 id="video-timestamp">&lt;a href="#video-timestamp" class="header-anchor">&lt;/a>Video Timestamp
&lt;/h3>&lt;p>作者发现，&lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5-vl/" target="_blank" rel="noopener"
>Qwen2.5-VL&lt;/a> 中使用的 MRoPE 存在如下问题：&lt;/p>
&lt;ol>
&lt;li>将 temporal position 与绝对时间绑定之后，对于长视频会产生非常大且稀疏的 temporal position ids&lt;/li>
&lt;li>需要使用不同的 FPS 进行采样来提高模型的泛化性&lt;/li>
&lt;/ol>
&lt;p>为了解决这个问题，作者使用了一个 textual token-based time encoding strategy, 其中每个 video temporal patch 对应的 timestamp 表示为 &lt;code>&amp;lt;3.0 seconds&amp;gt;&lt;/code>, 这样视频会被处理为以下格式&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;lt;0.0 seconds&amp;gt; &amp;lt;video token&amp;gt; &amp;lt;video token&amp;gt; ... &amp;lt;4.0 seconds&amp;gt; &amp;lt;video token&amp;gt; &amp;lt;video token&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>在训练时，作者还使用了 seconds 以及 HMS 两种格式来提高模型对于不同格式的泛化能力。作者认为，虽然这种表示会提高上下文长度，但是也能够提高模型 video grounding 或者 dense captioning 等时序信息敏感任务的表现&lt;/p>
&lt;h2 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h2>&lt;h3 id="training-recipe">&lt;a href="#training-recipe" class="header-anchor">&lt;/a>Training Recipe
&lt;/h3>&lt;p>预训练阶段包含 4 个阶段，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen3-vl/Qwen3-VL-pre-training-recipe.png"
width="1209"
height="216"
loading="lazy"
alt="Qwen3-VL pretraining recipe"
class="gallery-image"
data-flex-grow="559"
data-flex-basis="1343px"
>&lt;/p>
&lt;ul>
&lt;li>Stage 0: 这一阶段的目的是对齐视觉特征和文本特征，只训练 Patch merger, 训练使用了 67B token, 覆盖 image-caption, knowledge, OCR 数据，上下文长度为 8192&lt;/li>
&lt;li>Stage 1: 这一阶段所有参数都参加训练，训练使用了 1Ttoken, 作者在训练是加入了纯文本数据，最终数据包含 interleaved image-text, visual grounding, VQA, STEM, video 数据，上下文长度为 8192&lt;/li>
&lt;li>Stage 2: 这一阶段的目的是扩展模型的上下文长度到 32K, 训练使用了 1T token, 数据包括长视频以及 agent-oriented instruction-following 数据&lt;/li>
&lt;li>Stage 3: 这一阶段的目的是将模型的上下文长度进一步扩展到 262K, 训练使用了 100B token. 数据包括长视频以及长文本&lt;/li>
&lt;/ul>
&lt;h3 id="data">&lt;a href="#data" class="header-anchor">&lt;/a>Data
&lt;/h3>&lt;ul>
&lt;li>Image Caption Data: 作者使用了 Qwen2.5-VL 32B 来进行 re-captioning, 然后进行了 de-duplication 以及 clustering 来提高数据的质量和多样性&lt;/li>
&lt;li>Interleaved Text-Image Data: 作者对文档进行分裂，然后使用微调的 Qwen2.5-VL 7B 来进行解析，对于长文本，作者将连续页面拼接在一起。作者使用了对齐以及页数来保证数据的质量&lt;/li>
&lt;li>Knowledge Data: 作者构建了多个类别的数据，然后对这些数据进行 refine&lt;/li>
&lt;li>OCR: 作者构造了 30M 的数据以及 1M 的多语种数据&lt;/li>
&lt;li>Document Parsing Data: 作者从 CC 上收集了 3M PDF 以及处理了自有的 4M 数据，最终数据集里包含合成数据和真实数据；对于长文档理解数据，作者通过将 single-page 数据 merge 在一起得到，然后作者构造了 long document VQA 数据&lt;/li>
&lt;li>Grounding and counting Data: grounding 数据包括 box-based 和 Point-based 两种形式，均从开源数据集收集得到，前者包括 RefCOCO, Object365, 后者包括 PixMo; 对于 Counting, 作者基于 grounding 数据构造了 direct counting, box-based counting 以及 point-based counting 三种形式&lt;/li>
&lt;li>Spatial Understanding: 数据包括 spatial understanding 和 3D grounding 两类数据，前者的数据使用了相对位置关系来提高 spatial reasoning 的 robustness; 后者使用了 Omni3D 来统一数据格式&lt;/li>
&lt;li>Code: 包括 Qwen3, Qwen3-Coder 的纯文本 coding 数据，以及多模态 coding 数据，覆盖了将 UI 截图转换为 HTML/CSS 以及从图片生成 SVG 等任务&lt;/li>
&lt;li>Video: 包括 Dense Caption Synthesis 以及 Spatial-Temporal Video Grounding 两个任务。作者还对不同来源不同长度的数据进行了平衡&lt;/li>
&lt;li>STEM: 作者构造了一个合成数据 pipeline, 合成了 1M point-grounding samples, 2M perception-oriented VQA 数据，最终数据集包含 6M 标注图表数据，覆盖了 STEM 相关学科；对于多模态推理数据，作者收集了 60M 的 K12 以及本科生级别的练习题，作者还合成了 12M 的多模态推理数据。除了多模态推理数据，作者还加入了纯文本推理数据&lt;/li>
&lt;li>Agent: 这部分数据包括 GUI, function calling 以及 Search 三部分， GUI 数据通过数据合成得到，Function calling 数据通过强模型生成轨迹得到，search 数据通过收集执行搜索轨迹得到&lt;/li>
&lt;/ul>
&lt;h2 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h2>&lt;p>Post-training 包含三个阶段：&lt;/p>
&lt;ol>
&lt;li>SFT: 提高模型的指令跟随能力，SFT 又分为了两个小阶段，上下文长度分别为 32K 和 256K, 对于 instruct 和 reasoning 版本，作者设计了不同的数据格式，后者包含 CoT reasoning trace&lt;/li>
&lt;li>Strong-to-Weak Distillation: 提高小模型的能力，这里应该是和 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a> 一样，将大模型的能力蒸馏到小模型里&lt;/li>
&lt;li>RL: 提高模型的 reasoning 能力以及人类偏好对齐。这里包含了 Reasoning RL 以及 General RL 两个阶段，覆盖了 math, OCR, grounding, instruction following 等 domain&lt;/li>
&lt;/ol>
&lt;p>整体的训练 pipeline 我猜测应该是这样：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen3-vl/Qwen3-VL-post-training-pipeline.png"
width="1282"
height="279"
loading="lazy"
alt="Post-training pipeline of Qwen3-VL (guessed)"
class="gallery-image"
data-flex-grow="459"
data-flex-basis="1102px"
>&lt;/p>
&lt;h3 id="code-start-data">&lt;a href="#code-start-data" class="header-anchor">&lt;/a>Code-start Data
&lt;/h3>&lt;p>Code-start Data 分为 SFT 数据和 Long CoT SFT 数据，前者用于训练 instruct 版模型，后者用于训练 reasoning 版模型&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Data&lt;/th>
&lt;th>tasks&lt;/th>
&lt;th>samples&lt;/th>
&lt;th>training&lt;/th>
&lt;th>filtering&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>SFT&lt;/td>
&lt;td>spatial reasoning&lt;br>image-grounded reasoning&lt;br>spatio-temporal grounding&lt;br>long document understanding&lt;/td>
&lt;td>1.2M (1/3 are text-only)&lt;/td>
&lt;td>- stage 1: 32K&lt;br>- stage 2: 256K&lt;/td>
&lt;td>- query &lt;br>- rule-based&lt;br>- model-based&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Long CoT SFT&lt;/td>
&lt;td>VQA, OCR, 2D/3D grounding, &lt;br>video analysis, STEM, agent&lt;/td>
&lt;td>text:multimodal = 1:1&lt;/td>
&lt;td>&lt;/td>
&lt;td>- difficulty&lt;br>- multi-modal&lt;br>- response quality&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="strong-to-weak-distillation">&lt;a href="#strong-to-weak-distillation" class="header-anchor">&lt;/a>Strong-to-Weak Distillation
&lt;/h3>&lt;p>蒸馏过程包括两个阶段：&lt;/p>
&lt;ul>
&lt;li>off-policy Distillation: 使用教师模型的输出进行训练提高模型基本的 reasoning 能力&lt;/li>
&lt;li>On-policy Distillation: 使用教师模型输出的 logit 作为蒸馏信号提高模型的 reasoning 能力&lt;/li>
&lt;/ul>
&lt;h3 id="rl">&lt;a href="#rl" class="header-anchor">&lt;/a>RL
&lt;/h3>&lt;h4 id="reasoning-rl">&lt;a href="#reasoning-rl" class="header-anchor">&lt;/a>Reasoning RL
&lt;/h4>&lt;p>作者收集了 30K 的 RL 数据，然后对通过率超过 90% 的数据进行过滤 (16 responses per query), 对于 reward, 作者构建了一个 unified reward framework 来提供奖励&lt;/p>
&lt;p>训练时，作者使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-sapo/" target="_blank" rel="noopener"
>SAPO&lt;/a> 算法进行训练&lt;/p>
&lt;h4 id="general-rl">&lt;a href="#general-rl" class="header-anchor">&lt;/a>General RL
&lt;/h4>&lt;p>作者采用了一个 multi-task RL 的范式来提高模型在不同任务上的表现，reward 主要包含两个方面：&lt;/p>
&lt;ol>
&lt;li>instruction following: 评估模型遵循用户指令的能力，包括内容，格式，长度等&lt;/li>
&lt;li>preference alignment: 对于开放式问题，评估模型帮助性，事实准确性等方面的表现&lt;/li>
&lt;/ol>
&lt;p>基于这两个方面 reward 有两个部分组成：&lt;/p>
&lt;ol>
&lt;li>rule-based reward: 基于规则的 reward, 比如格式要求等&lt;/li>
&lt;li>model-based reward: 使用 Qwen2.5-VL 72B 和 Qwen3 作为 judge model 来提供奖励&lt;/li>
&lt;/ol>
&lt;p>为了解决模型的重复性实处，中英文混杂等问题，作者构造了一个数据集来故意触发模型这些问题然后加以改正。&lt;/p>
&lt;h3 id="thinking-with-images">&lt;a href="#thinking-with-images" class="header-anchor">&lt;/a>Thinking with Images
&lt;/h3>&lt;p>作者还够在了数据提高模型的 &amp;ldquo;thinking with images&amp;rdquo; 的能力，训练包含两个阶段：&lt;/p>
&lt;ol>
&lt;li>Stage 1: 作者构造了 10K Grounding 数据，然后对 Qwen2.5-VL 32B 进行 SFT 来模仿 agent 的行为: think -&amp;gt; act -&amp;gt; analyze feedback -&amp;gt; answer, 然后作者使用 multi-turn, tool-integrated RL 来进一步提高模型的 reasoning 能力&lt;/li>
&lt;li>Stage 2: 作者从 Qwen2.5-VL 32B 蒸馏得到 120K multi-turn agentic interactions 数据集， 然后作者使用了相似的 cold-start SFT 以及 tool-integrated RL pipeline 来训练 Qwen3-VL&lt;/li>
&lt;/ol>
&lt;p>这里 RL 训练的 reward 包含以下几部分：&lt;/p>
&lt;ol>
&lt;li>answer accuracy reward&lt;/li>
&lt;li>multi-turn reasoning reward&lt;/li>
&lt;li>tool-calling reward&lt;/li>
&lt;/ol>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;h3 id="performance">&lt;a href="#performance" class="header-anchor">&lt;/a>Performance
&lt;/h3>&lt;p>Qwen3-VL 235B-A22B 的表现如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen3-vl/Qwen3-VL-performance.png"
width="721"
height="1060"
loading="lazy"
alt="Performance of Qwen3-VL 235B-A22B"
class="gallery-image"
data-flex-grow="68"
data-flex-basis="163px"
>&lt;/p>
&lt;h3 id="ablation-study">&lt;a href="#ablation-study" class="header-anchor">&lt;/a>Ablation Study
&lt;/h3>&lt;p>作者对比了以下 Qwen3-ViT 和 SigLIP-2 的表现，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen3-vl/Qwen3-VL-ablation-on-ViT.png"
width="1349"
height="136"
loading="lazy"
alt="Ablation on Qwen3-ViT"
class="gallery-image"
data-flex-grow="991"
data-flex-basis="2380px"
>&lt;/p>
&lt;p>实验结果显示，使用 1.7B 的 Qwen3 和 1.5T tokens 进行训练之后，Qwen3-ViT 的表现超过了 SigLIP2 的表现，验证了 Qwen3-ViT 的有效性&lt;/p>
&lt;p>作者对比了 Deepseek 和 baseline 的表现，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen3-vl/Qwen3-VL-Ablation-on-Deepstack.png"
width="1355"
height="130"
loading="lazy"
alt="Ablation on DeepStack"
class="gallery-image"
data-flex-grow="1042"
data-flex-basis="2501px"
>&lt;/p>
&lt;p>可以看到，相比于 baseline, DeepStack 的表现更好，说明了 DeepStack 可以提供更丰富的视觉信息。&lt;/p>
&lt;p>作者还评估了以下 Qwen3-VL 在视频版大海捞针任务上的表现，实验结果发现，对于 30 分钟的视频，Qwen3-VL 的准确率为 $100\%$, 通过 &lt;a class="link" href="https://maosong.website/p/notes-on-yarn/" target="_blank" rel="noopener"
>YARN&lt;/a> 上下文扩展策略，模型在 2 个小时视频上的准确率为 $99.5\%$.&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 Qwen3-VL 系列多模态大模型，在架构上，作者使用了 interleaved-MRoPE, DeepStack 等改进策略，在数据上，作者扩展了训练数据的多样性，在训练上，作者分别训练了 instruct 版本和 reasoning 版本。最终评估发现，Qwen3-VL 达到了 SOTA 表现。&lt;/p>
&lt;p>作者认为，未来的工作在于&lt;/p>
&lt;ol>
&lt;li>基于 Qwen3-VL 构建具身智能 agent&lt;/li>
&lt;li>提高模型的可交互感知，tool-augmented reasoning 以及 real-time multimodal control 能力&lt;/li>
&lt;li>提高模型与人类学习，合作的能力&lt;/li>
&lt;li>统一理解与生成多模态大模型&lt;/li>
&lt;/ol>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://qwen.ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&amp;amp;from=research.latest-advancements-list" target="_blank" rel="noopener"
>blog&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2511.21631" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on DeepStack</title><link>https://maosong.website/p/notes-on-deepstack/</link><pubDate>Thu, 04 Dec 2025 17:32:41 +0800</pubDate><guid>https://maosong.website/p/notes-on-deepstack/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>已有的 MLLM 将视觉 token 作为一个 1d sequence, 输入给 LLM. 在本文中，作者将 visual token 注入到 LLM 的不同 layer 中来提高视觉信息的利用率&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;p>&lt;img src="https://maosong.website/p/notes-on-deepstack/DeepStack-architecture.png"
width="1149"
height="427"
loading="lazy"
alt="Architecture of DeepStack"
class="gallery-image"
data-flex-grow="269"
data-flex-basis="645px"
>&lt;/p>
&lt;p>首先，对于输入的图片 $I$, 我们将其分为高精度图片版本 $I_{high}$ 和低精度图片版本 $I_{low}$, $I_{low}$ 通过 vision encoder 和 MLP 得到对应的视觉 token $X_v$ 作为 LLM 的输入，然后在 LLM transformer block 的第 $i$ 层，其对应的视觉 token $X_{i,v}$ 会与 stack feature $X_{v}^i$ 相加，这里 $X_v^i$ 是对高精度图片输入的一个采样，即&lt;/p>
$$
X_v^i = \mathrm{Sampling2D}(\mathrm{MLP}(\mathrm{ViT}(I_{high})))
$$&lt;p>算法伪代码如下所示&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># H0: Input embeddings for LLM (Original inputs args for traditional LMM); # vis_pos: the location of visual tokens; &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># X, Xstack: Original visual tokens, Extra high-resolution visual token list; &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># lstart, n: Index of starting layer, and layer interval for stacking.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">H0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Xstack&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">lstart&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vis_pos&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">H&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">H0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">idx&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">TransformerLayer&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">enumerate&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">layers&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># DeepStack: &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">idx&lt;/span> &lt;span class="o">&amp;gt;=&lt;/span> &lt;span class="n">lstart&lt;/span> &lt;span class="o">&amp;amp;&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">idx&lt;/span> &lt;span class="err">−&lt;/span> &lt;span class="n">lstart&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">H&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">vis_pos&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">Xstack&lt;/span>&lt;span class="p">[(&lt;/span>&lt;span class="n">idx&lt;/span> &lt;span class="err">−&lt;/span> &lt;span class="n">lstart&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">//&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Original Transformer: &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">H&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">TransformerLayer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">H&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;h3 id="ablation-study">&lt;a href="#ablation-study" class="header-anchor">&lt;/a>Ablation Study
&lt;/h3>&lt;p>作者进一步验证了不同实验配置，结果发现在 early layer 进行 deepstack 效果最好，越往后效果越差&lt;/p>
&lt;p>作者还在 ViT 上应用了 DeepStack 策略，结果发现 ViT 的效果也有所提升&lt;/p>
&lt;p>作者还发现，模型表现提升是因为加入了 high-reoslution image token 信息&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 DeepStack, 一个提高 MLLM 中视觉信息利用率的方法，作者验证了这个方法的有效性。&lt;/p>
&lt;h2 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://openreview.net/forum?id=fXDpDzHTDV" target="_blank" rel="noopener"
>paper&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on ViT</title><link>https://maosong.website/p/notes-on-vit/</link><pubDate>Thu, 04 Dec 2025 11:00:44 +0800</pubDate><guid>https://maosong.website/p/notes-on-vit/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>Transformer 架构在 NLP 领域已经成为了事实上的标准。而在 CV 领域，目前还是 CNN 架构占据了主导。&lt;/p>
&lt;p>在本文中，作者就探究是否可以应用 Transformer 架构来完成图像识别的任务。&lt;/p>
&lt;p>作者发现，当在小规模数据集上进行训练时，Transformer 架构的表现是不如 CNN 架构的，作者认为原因是 Transformer 架构缺乏如 translation equivariance 和 locality 等 inductive biases. 但是当数据集规模上去之后，作者发现这种 inductive bias 可以通过大规模训练抵消掉。其中，最好的模型在 ImageNet 上达到了 $88.55\%$ 的准确率。&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;p>ViT 的架构如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-vit/ViT-architecture.png"
width="949"
height="483"
loading="lazy"
alt="Architecture of ViT"
class="gallery-image"
data-flex-grow="196"
data-flex-basis="471px"
>&lt;/p>
&lt;p>为了能够处理图片，对于输入的图片 $x\in\mathbb{R}^{H\times W\times C}$, 其中 $(H,W)$ 是图片大小，$C$ 是 channel 的个数。作者将其展开为一个 2D patch 序列, $x_p\in\mathbb{R}^{N\times (P^2\times C)}$, 其中 $(P, P)$ 是 image patch 的大小，$N=HW/P^2$ 是 image patch 的的个数，也是 Transformer 输入 token 的个数，Transformer 的 hidden size 为 $D$, 作者使用了一个 linear layer 来将 image patch 转换为 Transformer 的输入。&lt;/p>
&lt;p>与 BERT 一致，作者使用了一个 &lt;code>[class]&lt;/code> token 来作为输入 patch 序列的 embedding, 即 $z_0^0=x_{class}$, 其对应的输出 $z_L^0$ 作为该图片的表示。作者还使用了 1D 的 position encoding. 最终，ViT 表达式如下所示&lt;/p>
$$
\begin{aligned}
z_0 &amp;= [x_{class};x_p^1\mathbf{E};x_p^2\mathbf{E};\cdots;x_p^N\mathbf{E};]+\mathbf{E}_{pos}, &amp;\mathbf{E}\in\mathbb{R}^{(P^2\cdot C)\times D},\mathbf{E}_{pos}\in\mathbb{R}^{(N+1)\times D}\\
z_{\ell}'&amp;=\mathrm{MultiHeadAttention}(\mathrm{LayerNorm}(z_{\ell-1}))+z_{\ell-1},&amp;\ell=1,\dots,L\\
z_{\ell} &amp;= \mathrm{MLP}(\mathrm{LayerNorm}(z_{\ell}'))+z_{\ell}',&amp;\ell=1,\dots,L\\
y&amp;=\mathrm{LayerNorm}(z_L^0)
\end{aligned}
$$&lt;p>作者分析认为，相比于 CNN 架构，Transformer 架构缺乏 inductive bias, 这是因为每个 patch 对其周围的 2D 临近信息用的非常少，ViT 架构必须从零开始学习这些位置以及空间关系。&lt;/p>
&lt;p>作者还介绍了混合架构，即 patch embedding 由一个 linear layer 替换为一个 CNN 架构&lt;/p>
&lt;p>在 finetune 时，作者移除了 pre-trained 的 prediction head, 然后加入了一个 $D\times K$ 的 feed forward layer, 其中 $K$ 是分类的类别数。&lt;/p>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>作者使用的数据集如下所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Dataset&lt;/th>
&lt;th>classes&lt;/th>
&lt;th>images&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>ImageNet&lt;/td>
&lt;td>1K&lt;/td>
&lt;td>1.3M&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ImageNet-21K&lt;/td>
&lt;td>21K&lt;/td>
&lt;td>14M&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>JFT&lt;/td>
&lt;td>18K&lt;/td>
&lt;td>303M&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>作者在 ImageNet, CIFAR-10/100, Oxford-IIIT Pets, Oxford Flowers-102 这些数据集上进行评测。&lt;/p>
&lt;p>模型配置如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Layers&lt;/th>
&lt;th>$D$&lt;/th>
&lt;th>$D_{FFN}$&lt;/th>
&lt;th># heads&lt;/th>
&lt;th># params&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>ViT-Base&lt;/td>
&lt;td>12&lt;/td>
&lt;td>768&lt;/td>
&lt;td>3072&lt;/td>
&lt;td>12&lt;/td>
&lt;td>86M&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ViT-Large&lt;/td>
&lt;td>24&lt;/td>
&lt;td>1024&lt;/td>
&lt;td>4096&lt;/td>
&lt;td>16&lt;/td>
&lt;td>307M&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ViT-Huge&lt;/td>
&lt;td>32&lt;/td>
&lt;td>1280&lt;/td>
&lt;td>5120&lt;/td>
&lt;td>16&lt;/td>
&lt;td>632M&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>为了简便，作者在后续实验过程中，对模型名字进行了改写，比如 ViT-L/16 就代表了 ViT-Large model, 其对应的 patch size 为 16.&lt;/p>
&lt;p>ViT 的实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-vit/ViT-performance.png"
width="1159"
height="357"
loading="lazy"
alt="Performance of ViT"
class="gallery-image"
data-flex-grow="324"
data-flex-basis="779px"
>&lt;/p>
&lt;p>可以看到相比于 CNN 架构，ViT 所需要的训练时间更少，且效果更好。&lt;/p>
&lt;p>为了验证 ViT 模型具有更高的训练效率，作者在不同的数据集上进行的实验，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-vit/ViT-transfer-performance.png"
width="545"
height="387"
loading="lazy"
alt="Transfer to ImageNet"
class="gallery-image"
data-flex-grow="140"
data-flex-basis="337px"
>&lt;/p>
&lt;p>可以看到，当数据集比较小时，大部分 ViT 模型表现都比 ResNet 差，但是当数据集规模增加之后，ViT 的表现逐渐超过了 ResNet. 并且，随着数据集规模的增加，大模型的表现也比小模型好。&lt;/p>
&lt;p>作者进一步在四个不同大小的随机数据集上进行了验证，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-vit/ViT-linear-few-shot-performance.png"
width="551"
height="384"
loading="lazy"
alt="Linear few-shot evaluation on ImageNet v.s. pre-training size"
class="gallery-image"
data-flex-grow="143"
data-flex-basis="344px"
>&lt;/p>
&lt;p>实验结果显示，inductive bias 在小规模数据集上比较有效，但是当数据集规模上去之后，直接从数据集中学习相关的 pattern 更加高效和有效。&lt;/p>
&lt;p>由于不同的模型参数可能不太一致，为了更加公平地对比，作者使用算力来进行对比，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-vit/ViT-cost-vs-performance.png"
width="1050"
height="457"
loading="lazy"
alt="Performance v.s. cost for different architectures"
class="gallery-image"
data-flex-grow="229"
data-flex-basis="551px"
>&lt;/p>
&lt;p>实验结果显示，ViT 的训练效率比 ResNet 更高，其次，hybrid model 在算力比较小的时候，其表现比 ViT 更好，但是当算力提升之后，两者表现差不多。最后，ViT 随着算力的提升，还有进一步提升的空间。&lt;/p>
&lt;p>作者还对 ViT 的 attention 进行了进一步的分析，结果有以下几点：&lt;/p>
&lt;ol>
&lt;li>row-column structure 很明显，同一行同一列的 patches 有比较相似的 embedding&lt;/li>
&lt;li>ViT 学习到的 position embedding 对于比较近的 patches 其相似度也更高&lt;/li>
&lt;li>对于 attention 来说，某些 heads 在 early layer 就已经开始关注全局信息，并且随着 layer 的提升，attention distance 也在提升，说明模型关注全局信息的能力逐渐增强&lt;/li>
&lt;/ol>
&lt;p>作者尝试了对 Transformer 进行 scaling up, 结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-vit/ViT-scaling.png"
width="1117"
height="385"
loading="lazy"
alt="Scaling of ViT"
class="gallery-image"
data-flex-grow="290"
data-flex-basis="696px"
>&lt;/p>
&lt;p>实验结果显示，使用更多的 layer 可以有效提高模型的表现，但是当 layer 数大于 16 之后，其提升有限。另一方面，提高宽度对模型表现影响不大。作者发现，降低 patch size 也可以提高模型的表现。&lt;/p>
&lt;p>作者还对比了不同类型的 position encoding, 结果发现，使用/不使用 position encoding 对模型表现影响非常大，但是不同的 position encoding 总体来说表现差别不是很大。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 ViT, 一个基于 Transformer 架构的图像识别模型，作者通过在大规模数据集上进行训练验证了 ViT 架构的有效性。&lt;/p>
&lt;p>作者发现目前还存在如下挑战：&lt;/p>
&lt;ol>
&lt;li>如何使用自监督的方式进行训练，比如类似 BERT 的 masked patch prediction&lt;/li>
&lt;li>将 ViT 应用于其他的视觉任务，比如检测和分割&lt;/li>
&lt;li>进一步 scaling ViT&lt;/li>
&lt;/ol>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://openreview.net/forum?id=YicbFdNTTy" target="_blank" rel="noopener"
>openreview&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on CoMP</title><link>https://maosong.website/p/notes-on-comp/</link><pubDate>Thu, 04 Dec 2025 10:58:30 +0800</pubDate><guid>https://maosong.website/p/notes-on-comp/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者首先提到已有的 vision foundation model (VFM) 存在两个问题：&lt;/p>
&lt;ol>
&lt;li>不能处理动态分辨率图片输入，尽管我们可以使用 Bilinear interpolation 和 multi-resolution training 等方法，但是模型对于动态分辨率图片输入处理能力仍然不足&lt;/li>
&lt;li>VFM 和 LLM 之间存在 representation gap&lt;/li>
&lt;/ol>
&lt;p>针对这两个问题，作者提出了 CoMP, 一个 continual pre-training pipeline, CoMP 包含两个模块：&lt;/p>
&lt;ol>
&lt;li>C-RoPE, 一个针对 VFM 的 continual RoPE, 用于帮助 VFM 处理动态分辨率图片输入&lt;/li>
&lt;li>Alignment Loss, 用于对齐 VFM 和 LLM 的 representation&lt;/li>
&lt;/ol>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;p>CoMP 整体的架构图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-comp/CoMP-overall-architecture.png"
width="868"
height="454"
loading="lazy"
alt="Overview of CoMP"
class="gallery-image"
data-flex-grow="191"
data-flex-basis="458px"
>&lt;/p>
&lt;p>可以看到，CoMP 本质上就是一个多模态大模型，知识我们训练的目标为 VFM&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-comp/CoMP-modules-architecture.png"
width="977"
height="461"
loading="lazy"
alt="C-RoPE and Alignment Loss"
class="gallery-image"
data-flex-grow="211"
data-flex-basis="508px"
>&lt;/p>
&lt;h3 id="c-rope">&lt;a href="#c-rope" class="header-anchor">&lt;/a>C-RoPE
&lt;/h3>&lt;p>C-RopE 的核心思想是结合绝对位置编码以及相对位置编码来使得 pre-trained ViT 可以接受任意精度图片输入，对于输入图片 $X_V\in\mathbb{R}^{H\times W}$, 首先经过 patchify 得到 $N=HW/P^2$ 个 patch, 这里 $P$ 是 patch size, 每个 patch 大小为 $x_p\in\mathbb{R}^{N\times (P^2\cdot C)}$, $C$ 是 channels. 然后 VFM 每一个 layer 的计算过程为&lt;/p>
$$
\begin{aligned}
z_0 &amp;= [x_p^1E;\dots;x_p^NE] + \mathrm{Int}(E_{pos})\\
q_i,k_i,v_i &amp;= \mathrm{Proj}_q(z_i), \mathrm{Proj}_k(z_i), \mathrm{Proj}_v(z_i)\\
y_i &amp;= z_i + \mathrm{Proj}_o(\mathrm{Softmax}\left((Rq_i)^T(Rk_i) / D_v\right)v_i)\\
z_{i+1} &amp;= y_i + \mathrm{FFN}(y_i)
\end{aligned}
$$&lt;p>这里 $E\in\mathbb{E}^{P^2\cdot C\times D_v}$, $E_{pos}\in\mathbb{R}^{N\times D_V}$ 分别是 patch embedding 和 learnable position embedding, $\mathrm{Int}(\cdot)$ 是 bilinear interpolation.&lt;/p>
&lt;p>相比于 &lt;a class="link" href="https://maosong.website/p/notes-on-vit/" target="_blank" rel="noopener"
>ViT&lt;/a>, C-RoPE 做出了两点改动：&lt;/p>
&lt;ol>
&lt;li>使用了 Interpolation 来支持动态分辨率图片输入&lt;/li>
&lt;li>使用了 RoPE 来高效利用位置信息&lt;/li>
&lt;/ol>
&lt;h3 id="text-supervised-generative-pre-training">&lt;a href="#text-supervised-generative-pre-training" class="header-anchor">&lt;/a>Text-supervised Generative Pre-training
&lt;/h3>&lt;p>作者还是用了 LLM 的 cross-entropy loss 来进行对齐，text decoding loss 定义为&lt;/p>
$$
\mathcal{L}_{dec} = -\frac1T\sum_{i=V+1}^{V+T}\log P(X_i\mid X_{&lt;i}, H_v)
$$&lt;p>其中 $H_v=\mathcal{F}_{\psi}(\mathcal{V}(X_V))$ 是视觉特征，$T, V$ 分别代表了文本 token 和视觉 token 个数&lt;/p>
&lt;h3 id="vision-language-representation-alignment">&lt;a href="#vision-language-representation-alignment" class="header-anchor">&lt;/a>Vision-language Representation Alignment
&lt;/h3>&lt;p>text-decoding loss 可以对其视觉特征和文本特征，但是对于使用自监督训练方式的 VLM, 比如 DINOv2, 其预训练目标与 text-decoding loss 之间存在较大 gap, 为了解决这个问题，作者提出了 alignment loss.&lt;/p>
&lt;p>具体做法就是先计算出文本和视觉特征：&lt;/p>
$$
F_v = \mathrm{Pool}(H_v), F_t = \mathrm{Pool}(P_{\theta}(X_t))
$$&lt;p>然后作者将 $F_v, F_t$ 映射到语言空间，&lt;/p>
$$
C_v = W^TF_v, C_t=W^TF_t
$$&lt;p>这里 $W\in\mathbb{R}^{D_t\times K}$, $D_t, K$ 分别为 LLM 的 hidden size 以及 vocabulary size.&lt;/p>
&lt;p>接下来作者使用了 iterative Sinkhorn-Knopp 算法来归一化 $C_t$&lt;/p>
$$
p_t=\mathrm{Diag}(u_W)\exp(\frac{C_t}{\epsilon})\mathrm{Diag}(v)
$$&lt;p>这里 $u_W\in\mathbb{R}^K$ 是 words 的 prior marginal distribution,$v\in\mathbb{R}^B$ 是 renormalization vector&lt;/p>
&lt;p>最终 alignment loss 定义为&lt;/p>
$$
\mathcal{L}_{align} = -p_t\log p_v
$$&lt;p>这里 $p_v=\mathrm{softmax}(C_v)$, 作者对 LLM 使用了 stop gradient 操作，仅训练 VFM&lt;/p>
&lt;h3 id="training">&lt;a href="#training" class="header-anchor">&lt;/a>Training
&lt;/h3>&lt;p>CoMP 训练分为三个阶段：&lt;/p>
&lt;ul>
&lt;li>Stage 1: warming up, 仅训练 adapter&lt;/li>
&lt;li>Stage 2: 所有模型参数参与训练，使用了 RoPE 2D 和高精度图片输入&lt;/li>
&lt;li>Stage 3: instruction tuning, 使用 RoPE 2D 和动态分辨率图片输入&lt;/li>
&lt;/ul>
&lt;p>训练的损失为&lt;/p>
$$
\mathcal{L} = \begin{cases}
\mathcal{L}_{dec} + \alpha \mathcal{L}_{align}, &amp;\text{Stage 1 and Stage 2}\\
\mathcal{L}_{dec}, &amp;\text{Stage 3}
\end{cases}
$$&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>作者基于 CLIP, SigLIP, SigLip-2, DINOv2, AIMv2 等 VFM 进行了实验，主要结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-comp/CoMP-performance.png"
width="1153"
height="499"
loading="lazy"
alt="Performance of CoMP on multimodal understanding tasks"
class="gallery-image"
data-flex-grow="231"
data-flex-basis="554px"
>&lt;/p>
&lt;p>作者还对 training recipe 进行的消融实验，结果如下，可以看到，RoPE-2D 对模型表现提升最大，数据量和动态分辨率图片输入次之&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-comp/CoMP-trianing-ablation.png"
width="1124"
height="400"
loading="lazy"
class="gallery-image"
data-flex-grow="281"
data-flex-basis="674px"
>&lt;/p>
&lt;p>作者还对 C-RoPE 以及 alignment loss 进行的消融实验，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-comp/CoMP-ablation-performance.png"
width="1164"
height="365"
loading="lazy"
alt="Ablation on C-RoPE and aligment loss"
class="gallery-image"
data-flex-grow="318"
data-flex-basis="765px"
>&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 CoMP，一个 continual pre-training VFM 的方法，通过 C-RoPE 以及 alignment loss, 作者提高了 VFM 在 MLLM 中的表现&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2503.18931" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Keye-VL 1.5</title><link>https://maosong.website/p/notes-on-keye-vl-1.5/</link><pubDate>Thu, 11 Sep 2025 11:33:31 +0800</pubDate><guid>https://maosong.website/p/notes-on-keye-vl-1.5/</guid><description>&lt;p>快手提出了 Keye-VL 1.5, 一个强调 reasoning, video understanding 的 8B 多模态大模型。作者提出了 slow-fast video encoding strategy 来提高模型的视频理解能力，作者通过在预训练和后训练提高了模型的长上下文能力和 reasoning 能力&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者回顾了多模态大模型的进展，然后提到视频理解能力仍然是一个挑战。为了解决这个问题，作者提出了 Keye-VL-1.5, 一个 8B 的视频理解多模态大模型。&lt;/p>
&lt;p>Keye-VL-1.5 主要做了三点改进：&lt;/p>
&lt;ol>
&lt;li>在架构上，使用了 Slow-Fast Video Encoding&lt;/li>
&lt;li>在预训练阶段，使用多个 stage 来提升模型的长上下文能力&lt;/li>
&lt;li>在 post-training 阶段，提高模型的 reasoning 能力和 alignment 表现&lt;/li>
&lt;/ol>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;p>Keye-VL 1.5 的架构与 &lt;a class="link" href="https://maosong.website/p/notes-on-keye-vl/" target="_blank" rel="noopener"
>Keye-VL&lt;/a> 一致。&lt;/p>
&lt;p>Keye-VL 1.5 主要做出的改进点为针对视频的 encoding 方式&lt;/p>
&lt;p>作者回顾了已有 MLLM 处理视频的方式，比如 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5-vl/" target="_blank" rel="noopener"
>Qwen2.5-VL&lt;/a> 使用 3D convolution 来 merge 相邻的两帧，&lt;a class="link" href="https://maosong.website/p/notes-on-seed1.5-vl/" target="_blank" rel="noopener"
>Seed1.5-VL&lt;/a> 采用了 Dynamic Frame-Resolution Sampling 技巧，来根据 budget 和处理的任务来动态调整采样率 (frame) 和每一帧的图片精度 (resolution)。&lt;/p>
&lt;p>但是这些方法很难进行泛化。因此，作者在本文中就提出了 &lt;strong>SlowFast video encoding stratrgy&lt;/strong>:&lt;/p>
&lt;ol>
&lt;li>Slow Pathway: 空间信息丰富（high resolution），时间信息简略 (low number of frames)&lt;/li>
&lt;li>Fast Pathway: 时间信息丰富（high number of frames），空间信息简略 (low resolution)&lt;/li>
&lt;/ol>
&lt;p>为了区分 slow/fast frames, 作者提出了一个基于 patch similarity 的 metric:&lt;/p>
&lt;ol>
&lt;li>第一帧始终定义为 slow frame&lt;/li>
&lt;li>接下来的每一帧，如果其和上一帧的相似度超过 $95\%$, 则定义为 fast frame; 反之则定义为 slow frame.&lt;/li>
&lt;/ol>
&lt;p>得到 slow/fast frames 之后，作者将 fast-frame 的 token budget 限制为 slow frame token budget 的 $30\%$ 来平衡时间信息以及空间信息。接下来，作者使用二分搜索来决定 slow frame 的 token budget. 为了区分 slow frame 和 fast frame 的 token, 作者使用了特殊的 token 来进行分离。&lt;/p>
&lt;p>最终的处理结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl-1.5/Keye-VL-1-5-video-encoding.png"
width="1364"
height="335"
loading="lazy"
alt="SlowFast Video Encoding"
class="gallery-image"
data-flex-grow="407"
data-flex-basis="977px"
>&lt;/p>
&lt;h3 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h3>&lt;p>预训练的数据和 Keye-VL 基本一致，我们主要介绍改进的点&lt;/p>
&lt;p>对于 Image caption 数据，作者认为这批数据可能会损害模型的指令跟随和 reasoning 能力，因此作者对数据进行了增广，主要是调整了数据的格式：&lt;/p>
&lt;ol>
&lt;li>QA, 数据格式为 &lt;code>&amp;lt;image, caption, [eos], question, answer&amp;gt;&lt;/code>&lt;/li>
&lt;li>reverse QA, 数据格式为 &lt;code>&amp;lt;image, question, answer, [eos], caption&amp;gt;&lt;/code>&lt;/li>
&lt;li>instruction following: 随机给一批数据作为输入，然后让模型基于特定 image 输出 caption&lt;/li>
&lt;/ol>
&lt;p>作者还构建了一个 trap question 来提高模型的 robustness 以及 faithfulness.&lt;/p>
&lt;p>OCR 数据在 Keye-VL 的基础上加入了两点：&lt;/p>
&lt;ol>
&lt;li>Structured Document and Code Understanding: 基于 markdown 和 HTML 等数据来获取 code OCR 数据&lt;/li>
&lt;li>Instruction Following OCR: 基于特定指令进行 OCR&lt;/li>
&lt;/ol>
&lt;p>对于 grounding 数据，作者进一步加入了 temporal grounding 数据，作者首先使用 TEMPURA&lt;/p>
&lt;p>来将短视频分割成若干个 video clips. 然后作者使用 SOTA MLLM 来过滤数据，最后作者基于 Gemini2.5 来生成对应的 QA.&lt;/p>
&lt;p>预训练和 Keye-VL 一样，包含 3 个 stage&lt;/p>
&lt;p>前两个 stage，作者将模型的上下文限制为 8K, 使用了 DP 和 Zero-2 来减少内存开销。在 stage 3, 作者将模型的上下文从 8K 扩展到 128K, 对应的 base frequency 从 1M 提升到 8M. 训练数据包括长视频，长文本和大规模图片。作者将优化策略调整为 Zero-1, CP 和 PP 来支持 long-context 的训练。训练时数据分布为 video:images:text=24:50:26.&lt;/p>
&lt;h3 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h3>&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl-1.5/Keye-VL-1-5-post-training-pipeline.png"
width="1360"
height="449"
loading="lazy"
alt="Post-Training Pipeline"
class="gallery-image"
data-flex-grow="302"
data-flex-basis="726px"
>&lt;/p>
&lt;p>SFT 阶段使用了 &lt;strong>7.5M&lt;/strong> 多模态 QA 样本进行训练。&lt;/p>
&lt;p>MPO 阶段的数据相比 Keye-VL 有所减少，包含：&lt;/p>
&lt;ol>
&lt;li>250K 开源样本&lt;/li>
&lt;li>150K 纯文本数据&lt;/li>
&lt;li>26K 人类标注数据&lt;/li>
&lt;/ol>
&lt;p>对于 reward model 的训练，作者使用了 SFT 和 RL 两个阶段。SFT 阶段的数据包括 R1-Reward 和 MMPR, 训练之后作者还是用比较短的 good response 来避免产生较长的回答&lt;/p>
&lt;p>在 SFT 和 MPO 阶段之后，作者使用 LongCoT code-start 初步激活模型的 reasoning 能力。&lt;/p>
&lt;p>作者构建了一个 5 部的自动化数据生成 pipeline, 如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl-1.5/Keye-VL-1-5-LongCoT-data-generation-pipeline.png"
width="1362"
height="389"
loading="lazy"
alt="LongCoT data generation pipeline"
class="gallery-image"
data-flex-grow="350"
data-flex-basis="840px"
>&lt;/p>
&lt;p>步骤如下：&lt;/p>
&lt;ol>
&lt;li>Multi-Source Data Collection and Enhancement：收集数据&lt;/li>
&lt;li>Multi-Path Reasoning Generation with Confidence Quantification: 基于 confidence 来挑选数据&lt;/li>
&lt;li>Comprehensive Two-Level Quality Assessment: 基于答案和过程的正确性来提高数据质量&lt;/li>
&lt;li>Human-in-the-Loop Quality Enhancement: 对于中等质量的数据请人类进一步进行标注&lt;/li>
&lt;li>Dynamic Quality Scoring and Data Utilization Strategy: 对数据进行打分，高质量数据进行上采样&lt;/li>
&lt;/ol>
&lt;p>接下来就是 General RL 过程。&lt;/p>
&lt;p>对于通用的 RLVR 训练，作者使用了 &lt;a class="link" href="https://maosong.website/p/notes-on-gspo/" target="_blank" rel="noopener"
>GSPO&lt;/a> 算法来进行训练。&lt;/p>
&lt;p>在训练过程中，作者采取了 progressive hint sampling 方式，也就是提供不同程度的 hint 来提高模型的训练效率。作者将 hint 分为五个等级：&lt;/p>
&lt;ol>
&lt;li>Level 1 (Concept / Observation)&lt;/li>
&lt;li>Level 2 (Strategy / Method)&lt;/li>
&lt;li>Level 3 (Tools / Formula)&lt;/li>
&lt;li>Level 4 (Steps / Calculation)&lt;/li>
&lt;li>Level 5 (Complete Solution)&lt;/li>
&lt;/ol>
&lt;p>来提供不同程度的辅助，作者使用 Keye-VL 1.5 来确定最小的 hint level, 然后基于这个 level 提供信息进行 RL 的训练&lt;/p>
&lt;p>为了进一步提高模型的表现，作者采用了一个和 &lt;a class="link" href="https://maosong.website/p/notes-on-seed1.5-vl/" target="_blank" rel="noopener"
>Seed1.5-VL&lt;/a> 一样的迭代式训练策略，即反复进行 SFT 和 RL 来降低训练成本，提高训练效率。&lt;/p>
&lt;p>最后，再通用 RL 阶段之后，作者加入了 alignment RL 的训练来进行对齐，reward 包括三个方面：&lt;/p>
&lt;ol>
&lt;li>rule-based reward&lt;/li>
&lt;li>generative reward&lt;/li>
&lt;li>model-based reward&lt;/li>
&lt;/ol>
&lt;p>任务主要包括三个方面：&lt;/p>
&lt;ol>
&lt;li>instruction following&lt;/li>
&lt;li>format adherence&lt;/li>
&lt;li>preference alignment&lt;/li>
&lt;/ol>
&lt;p>数据介绍如下：&lt;/p>
&lt;ol>
&lt;li>instruction following:25 类硬约束，20 类软约束，数据包括 17K 多模态数据和 23K 纯文本数据，奖励包括 rule-based reward 和 generative reward&lt;/li>
&lt;li>reasoning: 12K 数学和逻辑推理数据&lt;/li>
&lt;li>RAG: 提高模型的搜索能力，作者使用 GSPO 算法进行训练&lt;/li>
&lt;/ol>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>模型表现如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl-1.5/Keye-VL-1-5-Performance.png"
width="1366"
height="988"
loading="lazy"
alt="Performance of Keye-VL 1.5"
class="gallery-image"
data-flex-grow="138"
data-flex-basis="331px"
>&lt;/p>
&lt;p>接下来作者进行了消融实验。&lt;/p>
&lt;p>首先是不同训练阶段对模型表现的影响，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-training-stage.png"
width="1369"
height="458"
loading="lazy"
alt="Ablation study on training strategy"
class="gallery-image"
data-flex-grow="298"
data-flex-basis="717px"
>&lt;/p>
&lt;p>实验结果显示，提高 SFT 训练数据可以有效提高模型在数学推理，逻辑推理和 OCR 任务上的表现。MPO 可以进一步提高模型的表现，Long CoT cold start 可以有效提高模型的 reasoning 表现。&lt;/p>
&lt;p>作者还探究了 model merging 对模型表现的影响，作者首先基于 base model 和 OCR 数据训练得到 OCR expert, 然后进行 merge, 实验结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-model-merging.png"
width="1244"
height="340"
loading="lazy"
alt="Ablation study on model merging"
class="gallery-image"
data-flex-grow="365"
data-flex-basis="878px"
>&lt;/p>
&lt;p>实验结果显示，model merging 可以有效提高模型在 special domain 上的表现，并且还可以维持模型的通用能力&lt;/p>
&lt;p>作者还发现：&lt;/p>
&lt;ol>
&lt;li>expert model 训练时间过长会影响最终 merge model 的表现&lt;/li>
&lt;li>expert mode 训练的学习率应该要设置比较小&lt;/li>
&lt;/ol>
&lt;p>接下来，作者探究了 alignment RL 对模型表现的影响，&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-alignment-RL.png"
width="1385"
height="275"
loading="lazy"
alt="Ablation on alignment RL"
class="gallery-image"
data-flex-grow="503"
data-flex-basis="1208px"
>&lt;/p>
&lt;p>实验结果说明，alignment RL 可以在保持模型 reasoning 能力的同时提高模型的指令跟随能力&lt;/p>
&lt;p>作者还探究了 hint 对模型表现的影响，使用不同 level hint 进行训练对模型表现影响的结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-hint.png"
width="1209"
height="310"
loading="lazy"
alt="Ablation on hint level"
class="gallery-image"
data-flex-grow="390"
data-flex-basis="936px"
>&lt;/p>
&lt;p>实验结果显示，使用 high level 的 hint 可以有效提高模型输出的正确率。&lt;/p>
&lt;p>最后作者探究了 rejection sampling 对模型表现的影响，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl-1.5/Keye-VL-1-5-ablation-rejection-sampling.png"
width="1384"
height="340"
loading="lazy"
alt="Ablation on rejection sampling"
class="gallery-image"
data-flex-grow="407"
data-flex-basis="976px"
>&lt;/p>
&lt;p>结果发现，通过 rejection sampling，模型的表现有了进一步的提升。因此作者采用了 ST-RL-(RFT-SFT)-(RFT-RL) 的训练方式来进行训练&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 Keye-VL1.5, 一个强调 video understanding 和 reasoning 的 MLLM. Keye-VL 1.5 使用了 SlowFast video encoding strategy 来提高模型的效率和视频理解能力。作者详细介绍了模型的数据，训练和评估。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2509.01563" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on RNoPE-SWA</title><link>https://maosong.website/p/notes-on-rnope-swa/</link><pubDate>Tue, 02 Sep 2025 11:24:10 +0800</pubDate><guid>https://maosong.website/p/notes-on-rnope-swa/</guid><description>&lt;p>作者系统性分析了已有的 attention 机制，然后作者提出了混合的 attention 机制，来提高模型在长上下文的表现以及维持模型在短上下文场景下的表现。&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者首先强调了提升 LLM 上下文长度面临的问题：&lt;/p>
&lt;ol>
&lt;li>如何有效处理长上下文输入&lt;/li>
&lt;li>如何训练长上下文 LLM&lt;/li>
&lt;li>如何降低长上下文 LLM 在 inference 时的 latency 以及 memory usage&lt;/li>
&lt;/ol>
&lt;p>对于建模长上下文输入，我们可以从 attention 机制或者位置编码来入手。前者类似的工作有 Landmark Attention 和 Focused Transformer, 但是这些方法的问题在于训练不稳定。 &lt;a class="link" href="https://maosong.website/p/notes-on-qk-norm/" target="_blank" rel="noopener"
>QK norm&lt;/a> 可以比较好解决 softmax 分布过于极端的问题，但是其问题在于训练时的数值不稳定性，并且可能会影响模型的长上下文能力。&lt;/p>
&lt;p>另一方面，对于位置编码，已有的工作如 APE, AliBi, &lt;a class="link" href="https://maosong.website/p/notes-on-position-encoding/" target="_blank" rel="noopener"
>RoPE&lt;/a> 等都可以提供位置信息。但是，这些方法很有可能回影响模型最终的 attention score 分布。另外，&lt;a class="link" href="https://maosong.website/p/notes-on-nope/" target="_blank" rel="noopener"
>NoPE&lt;/a> 探究了移除 position encoding 的可能性。&lt;/p>
&lt;p>还有一些工作目的是降低 softmax attention 的时间复杂度和空间复杂度。比如 sliding window attention, sparse attention, &lt;a class="link" href="https://maosong.website/p/notes-on-streamingllm/" target="_blank" rel="noopener"
>attention sink&lt;/a> 等都可以降低整体的时间/空间复杂度。但是这些方法最终的表现都有所下降。&lt;/p>
&lt;h2 id="observation">&lt;a href="#observation" class="header-anchor">&lt;/a>Observation
&lt;/h2>&lt;p>作者首先对比了以下不同方法对模型长上下文能力的影响。&lt;/p>
&lt;p>作者训练了一个 8B 的模型，然后分别对比了三种方法：&lt;/p>
&lt;ol>
&lt;li>RoPE: base frequency 设置为 10,000, SFT 阶段扩展到 2M&lt;/li>
&lt;li>QK-Norm: 在 RoPE 的基础上，对 query 和 key 先进行 normalization 再进行 RoPE&lt;/li>
&lt;li>NoPE: 移除 attention 中的位置编码信息&lt;/li>
&lt;/ol>
&lt;p>作者分别评估了三种方法的表现，实验结果如下表&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Val Loss&lt;/th>
&lt;th>MMLU&lt;/th>
&lt;th>HellaSwag&lt;/th>
&lt;th>CommonsenseQA&lt;/th>
&lt;th>ARC-E&lt;/th>
&lt;th>ARC-C&lt;/th>
&lt;th>Needles 65k&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>RoPE&lt;/td>
&lt;td>1.52&lt;/td>
&lt;td>48.55&lt;/td>
&lt;td>73.74&lt;/td>
&lt;td>68.30&lt;/td>
&lt;td>81.05&lt;/td>
&lt;td>39.13&lt;/td>
&lt;td>9.82&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>QK-Norm&lt;/td>
&lt;td>1.53&lt;/td>
&lt;td>48.21&lt;/td>
&lt;td>73.68&lt;/td>
&lt;td>68.23&lt;/td>
&lt;td>80.54&lt;/td>
&lt;td>38.98&lt;/td>
&lt;td>7.93&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>NoPE&lt;/td>
&lt;td>1.58&lt;/td>
&lt;td>47.61&lt;/td>
&lt;td>72.16&lt;/td>
&lt;td>66.42&lt;/td>
&lt;td>76.94&lt;/td>
&lt;td>37.12&lt;/td>
&lt;td>9.03&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>实验结果显示，对于通用任务，RoPE 和 QK-Norm 的表现差不多，而 NoPE 的表现较差。对于长上下文任务，QK-Norm 的表现最差。&lt;/p>
&lt;p>接下来，作者分析了三种方法的 attention 分布情况。作者将上下文分为四个 segments：&lt;/p>
&lt;ul>
&lt;li>begin: 开始的 10 个 token&lt;/li>
&lt;li>needle: 与 needle 相关的 tokens&lt;/li>
&lt;li>context: 通用的上下文 token&lt;/li>
&lt;li>qc: question/completion token, 语文题答案相关的 token&lt;/li>
&lt;/ul>
&lt;p>作者将 needle 放置在 50% 深度的位置。评测的实验结果如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Context Length&lt;/th>
&lt;th>Model Variants&lt;/th>
&lt;th>begin&lt;/th>
&lt;th>needle&lt;/th>
&lt;th>context&lt;/th>
&lt;th>qc&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>8k&lt;/td>
&lt;td>RoPE&lt;/td>
&lt;td>0.3863&lt;/td>
&lt;td>0.0328&lt;/td>
&lt;td>0.3809&lt;/td>
&lt;td>0.2000&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>QK-Norm&lt;/td>
&lt;td>0.0242&lt;/td>
&lt;td>0.0173&lt;/td>
&lt;td>0.8020&lt;/td>
&lt;td>0.1565&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>NoPE&lt;/td>
&lt;td>0.3058&lt;/td>
&lt;td>0.0454&lt;/td>
&lt;td>0.4501&lt;/td>
&lt;td>0.1987&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>32k&lt;/td>
&lt;td>RoPE&lt;/td>
&lt;td>0.3541&lt;/td>
&lt;td>0.0201&lt;/td>
&lt;td>0.4343&lt;/td>
&lt;td>0.1915&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>QK-Norm&lt;/td>
&lt;td>0.0064&lt;/td>
&lt;td>0.0056&lt;/td>
&lt;td>0.8517&lt;/td>
&lt;td>0.1364&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>NoPE&lt;/td>
&lt;td>0.2807&lt;/td>
&lt;td>0.0325&lt;/td>
&lt;td>0.4981&lt;/td>
&lt;td>0.1886&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>128k&lt;/td>
&lt;td>RoPE&lt;/td>
&lt;td>0.3463&lt;/td>
&lt;td>0.0010&lt;/td>
&lt;td>0.4751&lt;/td>
&lt;td>0.1776&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>QK-Norm&lt;/td>
&lt;td>0.0010&lt;/td>
&lt;td>0.0004&lt;/td>
&lt;td>0.8993&lt;/td>
&lt;td>0.0994&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>NoPE&lt;/td>
&lt;td>0.0846&lt;/td>
&lt;td>0.0073&lt;/td>
&lt;td>0.8156&lt;/td>
&lt;td>0.0925&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>实验结果显示：&lt;/p>
&lt;ol>
&lt;li>NoPE 是最关注 needle token 信息的，RoPE 次之，QK-Norm 最差&lt;/li>
&lt;li>QK-Norm 更关注上下文信息，对其他的信息关注度较少&lt;/li>
&lt;/ol>
&lt;p>作者发现 QK-Norm 对初始的 token 信息关注度较少，作者认为这是因为 normalization 会让模型更关注邻近的 token 信息。为了验证这个观点，作者在不同的上下文长度下，分别计算了不同方法的 attention 分布情况，为了避免噪声，作者将开始的 10 个 token 以及最后的 $3\%$ token 排除在外，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-rnope-swa/RNoPE-SWA-8k-attention.png"
width="700"
height="496"
loading="lazy"
alt="Attention distribution on 8K context length"
class="gallery-image"
data-flex-grow="141"
data-flex-basis="338px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-rnope-swa/RNoPE-SWA-32k-attention.png"
width="692"
height="482"
loading="lazy"
alt="Attention distribution on 32K context length"
class="gallery-image"
data-flex-grow="143"
data-flex-basis="344px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-rnope-swa/RNoPE-SWA-128k-attention.png"
width="693"
height="564"
loading="lazy"
alt="Attention distribution on 128K context length"
class="gallery-image"
data-flex-grow="122"
data-flex-basis="294px"
>&lt;/p>
&lt;p>实验结果说明，相比于 softmax attention, QK-Norm 的 attention score 分布更平滑，其对于 need token 的注意力不如 RoPE, 这也说明了为什么 QK-Norm 的长上下文能力比较差。并且，QK-Norm 的 recency bias 更严重。&lt;/p>
&lt;p>作者通过计算 RoPE 和 QK-Norm 的 attention distribution 的 entropy 来进一步说明这一点，结果如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>8k&lt;/th>
&lt;th>32k&lt;/th>
&lt;th>128k&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>RoPE&lt;/td>
&lt;td>6.02&lt;/td>
&lt;td>6.95&lt;/td>
&lt;td>7.62&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>QK-Norm&lt;/td>
&lt;td>10.71&lt;/td>
&lt;td>12.46&lt;/td>
&lt;td>14.14&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>结果显示 QK-Norm 的熵更高，这意味着 QK-Norm 的 attention score 分布更分散，也就证明了 Qk-Norm retrieval 能力比较差。&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;p>考虑到 &lt;a class="link" href="https://maosong.website/p/notes-on-nope/" target="_blank" rel="noopener"
>NoPE&lt;/a> 和 RopE 各自的优点，作者提出了一个混合架构，来结合 NoPE 与 RoPE. 具体做法就是 NoPE layer 和 RoPE layer 交替进行。作者将这个模型架构记为 RNoPE.&lt;/p>
&lt;p>RNoPE 不同 layer 与不同 base frequency 产生的 attention score 分布如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>NoPE Layers - begin&lt;/th>
&lt;th>NoPE Layers - needle&lt;/th>
&lt;th>NoPE Layers - context&lt;/th>
&lt;th>NoPE Layers - qc&lt;/th>
&lt;th>RoPE Layers - begin&lt;/th>
&lt;th>RoPE Layers - needle&lt;/th>
&lt;th>RoPE Layers - context&lt;/th>
&lt;th>RoPE Layers - qc&lt;/th>
&lt;th>needles-128k&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>RoPE&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>0.3541&lt;/td>
&lt;td>0.0201&lt;/td>
&lt;td>0.4343&lt;/td>
&lt;td>0.1915&lt;/td>
&lt;td>7.395&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RNoPE-10k&lt;/td>
&lt;td>0.3275&lt;/td>
&lt;td>0.0765&lt;/td>
&lt;td>0.5672&lt;/td>
&lt;td>0.0287&lt;/td>
&lt;td>0.0049&lt;/td>
&lt;td>0.0004&lt;/td>
&lt;td>0.6805&lt;/td>
&lt;td>0.3142&lt;/td>
&lt;td>8.036&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RNoPE-100k&lt;/td>
&lt;td>0.3263&lt;/td>
&lt;td>0.0778&lt;/td>
&lt;td>0.5633&lt;/td>
&lt;td>0.0327&lt;/td>
&lt;td>0.0241&lt;/td>
&lt;td>0.0005&lt;/td>
&lt;td>0.6782&lt;/td>
&lt;td>0.2972&lt;/td>
&lt;td>7.461&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RNoPE-2M&lt;/td>
&lt;td>0.3250&lt;/td>
&lt;td>0.0712&lt;/td>
&lt;td>0.5735&lt;/td>
&lt;td>0.0303&lt;/td>
&lt;td>0.1111&lt;/td>
&lt;td>0.0046&lt;/td>
&lt;td>0.6233&lt;/td>
&lt;td>0.2611&lt;/td>
&lt;td>7.022&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RNoPE-4M&lt;/td>
&lt;td>0.3486&lt;/td>
&lt;td>0.0369&lt;/td>
&lt;td>0.5981&lt;/td>
&lt;td>0.0165&lt;/td>
&lt;td>0.0960&lt;/td>
&lt;td>0.0039&lt;/td>
&lt;td>0.6774&lt;/td>
&lt;td>0.2227&lt;/td>
&lt;td>6.203&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RNoPE-10k-swa&lt;/td>
&lt;td>0.3303&lt;/td>
&lt;td>0.0742&lt;/td>
&lt;td>0.5634&lt;/td>
&lt;td>0.0321&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>-&lt;/td>
&lt;td>9.562&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>实验结果显示，在 RNoPE 架构中，&lt;/p>
&lt;ol>
&lt;li>提升 base frequency 带来的增益逐渐递减&lt;/li>
&lt;li>NoPE layer 的 retrieval 能力比较强，表现在 attention sink 现象以及对于 needle token 的 spike. 但是其 recency bias 比较弱&lt;/li>
&lt;li>RoPE 的 retrieval 能力比较弱，但是其 attention sink 现象比较小&lt;/li>
&lt;li>当 base frequency 增加止呕，RoPE 的 recency bias 会降低，表现在 qc token 的权重降低&lt;/li>
&lt;/ol>
&lt;p>作者发现，RoPE 的 receptive field 会影响 NoPE layer 的 retrieval 能力。作者总结得到两个 insight&lt;/p>
&lt;ol>
&lt;li>NoPE layer 对于 retrieval 任务比较擅长，而 RoPE layer 对于处理 local Information 比较擅长&lt;/li>
&lt;li>限制 RoPE 的 receptive field 可以保证 NoPE layer 的 retrieval 能力&lt;/li>
&lt;/ol>
&lt;p>基于这两个 insight, 作者构建了 RNoPE-SWA 架构，RNoPE-SWA 相比于 RNoPE, 将 full attention 变成了 sliding window attention, 来避免 RoPE layer 对下游的 NoPE layer 产生影响。&lt;/p>
&lt;p>最终，作者基于 Command R+ 构建了模型，作者去除了 QK-Norm, 对于 NoPE layer, 作者使用了 full attention, 对于 RoPE layer, 作者使用了 window size 为 4096 的 sliding window attention. 作者通过实验验证了 NoPE layer 和 RoPElayer 的比例，实验结果发现 $1:3$ 的比例是最优的。最终每 4 个 layer 为一组，前三组为 SWA, 最后一层为 full attention.&lt;/p>
&lt;p>最终，在通用任务上评测的结果如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>MMLU&lt;/th>
&lt;th>HellaSwag&lt;/th>
&lt;th>ARC-E&lt;/th>
&lt;th>ARC-C&lt;/th>
&lt;th>SATEn&lt;/th>
&lt;th>SATMath&lt;/th>
&lt;th>GSM8K&lt;/th>
&lt;th>Winogrande&lt;/th>
&lt;th>MBPP&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Baseline&lt;/td>
&lt;td>57.5&lt;/td>
&lt;td>75.8&lt;/td>
&lt;td>84.6&lt;/td>
&lt;td>48.5&lt;/td>
&lt;td>70.0&lt;/td>
&lt;td>30.9&lt;/td>
&lt;td>40.9&lt;/td>
&lt;td>68.5&lt;/td>
&lt;td>39.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RNope-SWA&lt;/td>
&lt;td>59.5&lt;/td>
&lt;td>76.2&lt;/td>
&lt;td>82.5&lt;/td>
&lt;td>48.8&lt;/td>
&lt;td>71.9&lt;/td>
&lt;td>30.5&lt;/td>
&lt;td>42.7&lt;/td>
&lt;td>69.5&lt;/td>
&lt;td>39.3&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>在 Ruler retrieval 上评测的结果如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>8k&lt;/th>
&lt;th>16k&lt;/th>
&lt;th>32k&lt;/th>
&lt;th>64k&lt;/th>
&lt;th>128k&lt;/th>
&lt;th>256k&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Baseline&lt;/td>
&lt;td>96.6&lt;/td>
&lt;td>94.4&lt;/td>
&lt;td>95.1&lt;/td>
&lt;td>89.1&lt;/td>
&lt;td>83.0&lt;/td>
&lt;td>57.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RNope-SWA&lt;/td>
&lt;td>96.1&lt;/td>
&lt;td>96.1&lt;/td>
&lt;td>94.9&lt;/td>
&lt;td>92.0&lt;/td>
&lt;td>90.0&lt;/td>
&lt;td>74.8&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>在 Ruler QA 上评测的结果如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>8k&lt;/th>
&lt;th>16k&lt;/th>
&lt;th>32k&lt;/th>
&lt;th>64k&lt;/th>
&lt;th>128k&lt;/th>
&lt;th>256k&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Baseline&lt;/td>
&lt;td>53.5&lt;/td>
&lt;td>50.0&lt;/td>
&lt;td>52.5&lt;/td>
&lt;td>45.5&lt;/td>
&lt;td>36.0&lt;/td>
&lt;td>30.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RNope-SWA&lt;/td>
&lt;td>55.5&lt;/td>
&lt;td>52.5&lt;/td>
&lt;td>55.5&lt;/td>
&lt;td>49.0&lt;/td>
&lt;td>46.0&lt;/td>
&lt;td>42.5&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>实验结果说明，模型在通用任务任务上与 baseline 模型表现差不多，且长上下文能力更强&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者提出了 RNope-SWA, 一个混合 NoPE, RoPE position embedding 和 sliding window attention 的 attention 机制。RNope-SWA 可以在保持模型表现的同时提高计算效率和降低 KV cache 占用。&lt;/p>
&lt;p>尽管本文和已有的工作如 YoCo, Jamba-1.5 和 &lt;a class="link" href="https://maosong.website/p/notes-on-minimax-01/" target="_blank" rel="noopener"
>MiniMax-01&lt;/a> 均证明了混合 attention 架构的有效性。但是，目前对于其工作机制尚缺乏一个比较系统性的理解。作者认为这是一个需要探究的方向。另外，作者还认为如何更好的汇总上下文信息与位置信息也是一个可以探究的方向。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2501.18795" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on InternVL3.5</title><link>https://maosong.website/p/notes-on-internvl3.5/</link><pubDate>Mon, 01 Sep 2025 11:30:50 +0800</pubDate><guid>https://maosong.website/p/notes-on-internvl3.5/</guid><description>&lt;p>上海 AI LAB 提出了 InternVL 3.5 系列多模态大模型，InternVL 3.5 主要强调了模型的 reasoning 能力以及 inference 效率。&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者首先介绍了已有多模态大模型的进展，然后提到有两个未解决的问题：&lt;/p>
&lt;ol>
&lt;li>如何构建一个 stable, effective, scalable 的 RL 框架来训练 MLLM&lt;/li>
&lt;li>如何降低 MLLM 在长上下文场景下的计算开销过高的问题&lt;/li>
&lt;/ol>
&lt;p>为了解决这两个问题，作者提出了 InternVL3.5 多模态大模型系列，相比于 InternVL3, 模型在 reasoning 能力和 efficiency 上均进行了提升。作者主要通过 Cascade RL 框架来提高模型的 reasoning 表现，以及使用 Visual Resolution Router (ViR) 和 Decoupled Vision Language Deployment (DvD) 来提高模型的推理效率。&lt;/p>
&lt;p>总结起来，InternVL3.5 模型的贡献如下：&lt;/p>
&lt;ol>
&lt;li>开源了 InternVL3.5 系列多模态大模型，模型拥有先进的 reasoning 能力以及推理效率&lt;/li>
&lt;li>提出了 Cascade RL, ViR 以及 DvD 等模块来提高模型的表现/推理效率&lt;/li>
&lt;li>系统性评估了模型的表现&lt;/li>
&lt;/ol>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;p>InternVL3.5 的架构与 InternVL3 的架构基本一致，包括一个 ViT, 一个 MLP 和一个 LLM, 模型的架构如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-internvl3.5/InternVL-3-5-architecture.png"
width="1295"
height="419"
loading="lazy"
alt="Architecture of InternVl3.5"
class="gallery-image"
data-flex-grow="309"
data-flex-basis="741px"
>&lt;/p>
&lt;p>模型配置如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-internvl3.5/InternVL-3-5-model-configuration.png"
width="1227"
height="484"
loading="lazy"
alt="Configuration of InternVL3.5"
class="gallery-image"
data-flex-grow="253"
data-flex-basis="608px"
>&lt;/p>
&lt;p>InternVl3.5 延续了 InternVL 系列的做法，即 20B 以下的模型使用 InternViT-300M, 20B 以上的模型使用 InternViT-6B 作为 Visual encoder. 大语言模型基于 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a> 和 &lt;a class="link" href="https://maosong.website/p/notes-on-gpt-oss/" target="_blank" rel="noopener"
>gpt-oss&lt;/a>.&lt;/p>
&lt;p>在 InternVL3.5 的基础上，作者还构建了 InternVL3.5-Flash 模型，InternVL3.5-Flash 不同的地方在于，其在 MLP 之前加入了一个 Visual Resolution Router (ViR) 模块，来处理不同分辨率的图片。在 InternVL3.5 中，每个 image patch 由 1024 个视觉 token 来表示，然后通过 pixel shuffle 压缩至 256 个。 在 InternVL3.5-Flash 中，ViR 首先会判断每个 patch 的 semantic richness, 然后基于 semantic richness 来将对应的 token 路由到不同的 pixel shuffle 模块中，最高可以将视觉 token 压缩到 64 个，这样就可以大幅度提高模型的推理效率。&lt;/p>
&lt;h3 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h3>&lt;p>Pre-training 使用 next-token-prediction loss 来更新模型权重，给定多模态 sequence $x=(x_1,\dots,x_L)$, 损失函数定义为&lt;/p>
$$
\mathcal{L}(\theta) = -\sum_{x_i\text{ is text token}} \log p_{\theta}(x_i
\mid x_1,\dots,x_{i-1})
$$&lt;p>与 InternVL2.5 一样，作者还对不同长度的 sequence 进行了加权，避免模型产生 bias, 加权后的 loss 为&lt;/p>
$$
\mathcal{L}(\theta) = -\sum_{x_i\text{ is text token}}\frac{w_i}{\sum_j w_j} \log p_{\theta}(x_i
\mid x_1,\dots,x_{i-1}),\quad w_i = \frac{1}{N^{0.5}}
$$&lt;p>其中 $N$ 是训练样本中需要计算损失的 token 个数。&lt;/p>
&lt;p>训练数据蛀牙包含两部分：&lt;/p>
&lt;ol>
&lt;li>多模态数据，这部分数据基于 InternVL3&lt;/li>
&lt;li>纯文本数据，基于 InternLM 系列和开源的数据集&lt;/li>
&lt;/ol>
&lt;p>最终，预训练数据一共包含&lt;strong>116M&lt;/strong> 样本，&lt;strong>250B&lt;/strong> token. 纯文本数据和多模态数据的比例为 $1:2.5$. 模型上下文长度为 $32K$.&lt;/p>
&lt;h3 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h3>&lt;p>Post-training 包含三个阶段：&lt;/p>
&lt;ol>
&lt;li>SFT： 使用高质量对话数据提高模型表现&lt;/li>
&lt;li>Cascade RL: 提高模型的 reasoning 能力&lt;/li>
&lt;li>Visual Consistency Learning: 训练 ViR 模块，提高模型处理动态分辨率图片的能力&lt;/li>
&lt;/ol>
&lt;p>训练 pipeline 如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-internvl3.5/InternVL-3-5-post-training-pipeline.png"
width="1290"
height="335"
loading="lazy"
alt="Post-training pipeline of InternVL3.5"
class="gallery-image"
data-flex-grow="385"
data-flex-basis="924px"
>&lt;/p>
&lt;p>SFT 阶段的训练数据包括三个方面：&lt;/p>
&lt;ol>
&lt;li>InternVL3 的指令跟随数据&lt;/li>
&lt;li>多模态 reasoning 数据&lt;/li>
&lt;li>能力扩展数据，包括 GUI 交互，具身交互以及 SVG 理解与生成的数据&lt;/li>
&lt;/ol>
&lt;p>Cascade RL 阶段结合了 offline RL 和 online RL 的优点。基本思想是先使用 offline RL 来提高模型的能力，降低训练成本。然后再使用 online RL 进一步提高模型的 reasoning 表现。&lt;/p>
&lt;p>offline RL 阶段使用的是 InternVL-MPO 算法，其损失函数为&lt;/p>
$$
\mathcal{L}_{MPO} = w_p\mathcal{L}_p+w_q\mathcal{L}_q+w_g\mathcal{L}_g
$$&lt;p>其中，$\mathcal{L}_p$ 为 &lt;a class="link" href="https://maosong.website/p/notes-on-dpo/" target="_blank" rel="noopener"
>DPO&lt;/a> 的损失函数，$\mathcal{L}_q$ 为 Quality loss, $\mathcal{L}_g$ 为 SFT 使用的 next-token-prediction loss.&lt;/p>
&lt;p>online RL 阶段使用的是 &lt;a class="link" href="https://maosong.website/p/notes-on-gspo/" target="_blank" rel="noopener"
>GSPO&lt;/a> 算法，核心思想是在 sample 层面做归一化。这个阶段的目的是进一步提高模型的表现。&lt;/p>
&lt;p>Cascade RL 的优势在于：&lt;/p>
&lt;ol>
&lt;li>训练更稳定：offline RL 可以避免模型产生 reward hacking 现象，online RL 阶段可以进一步提高模型的表现&lt;/li>
&lt;li>训练效率更高：offline RL 可以有效提高采样效率&lt;/li>
&lt;li>表现更好：先 MPO 再 RL 可以达到更好的表现&lt;/li>
&lt;/ol>
&lt;p>Visual Consistency Learning (ViCO) 的目的是降低 InternVL3.5 的推理开销。这个阶段主要训练 ViR 模块&lt;/p>
&lt;p>ViCO 包括两个 stage:&lt;/p>
&lt;p>Stage 1: Consistency training&lt;/p>
&lt;p>这一阶段的目的是使得不同压缩率处理的视觉 token 对应的输出与 ground truth 尽可能一致，作者使用另外一个 InternVL3.5 模型作为 reference model, 然后损失函数为&lt;/p>
$$
\mathcal{L}_{ViCO} = \mathbb{E}_{\xi\sim\mathcal{R}}\left[\frac{1}{N}\sum_{i=1}^N\mathrm{KL}\left(\pi_{\mathrm{ref}}(y_i\mid y_{&lt;i}, I)\ \Vert\ \pi_{\theta}(y_i\mid y_{&lt;i}, I_{\xi})\right)\right]
$$&lt;p>$\xi\in\{1/4,1/16\}$ 为 compression rate, 训练是随机采样。$\xi=1/4$ 代表最终会有 256 个视觉 token, $\xi=1/16$ 代表最终会有 64 个 token.&lt;/p>
&lt;p>Stage 2: Router training&lt;/p>
&lt;p>这个阶段的目的是训练 ViR 来选取合适的精度/压缩率。ViR 此时作为一个 binary classifier 来进行训练，训练的损失函数为 cross-entropy loss. 模型其他部分参数冻结，仅训练 ViR 模块，首先，我们将计算压缩 16 倍后的损失与压缩 4 倍的损失的比例&lt;/p>
$$
r_i = \frac{\mathcal{L}_{ViCO}(y_i\mid I_{1/16})}{\mathcal{L}_{ViCO}(y_i\mid I_{1/4})}
$$&lt;p>该比例衡量了压缩 token 之后带来的性能下降程度，接下来，作者设定了一个阈值 $\tau$, 当性能下降超过阈值 $\tau$, 则认为应该使用高精度，也就是 $\xi=1/4$, 反之则说明不需要过度视觉 token, 可以使用低精度，也就是 $\xi=1/16$. 总结得到&lt;/p>
$$
y_i^{router} = \begin{cases}
0, &amp;r_i&lt;\tau\\
1, &amp;r_i>\tau
\end{cases}
$$&lt;p>训练时，作者基于 sliding window 中的 $r_i$ 来动态调整 $\tau$ 的值。&lt;/p>
&lt;p>post-training 的训练数据如下：&lt;/p>
&lt;ol>
&lt;li>SFT 训练数据包括&lt;strong>56M&lt;/strong> 样本，&lt;strong>130B&lt;/strong> token, 纯文本数据与多模态数据的比例为 $1:3.5$.&lt;/li>
&lt;li>Cascade RL 的训练数据主要基于 MMPR, 包含 200K 左右的样本，通过过滤最终得到&lt;strong>70K&lt;/strong>左右的样本。online RL 同样使用这批数据进行训练&lt;/li>
&lt;li>ViCO 的训练数据与 SFT 阶段的训练数据一致。主要是 OCR 以及 VQA 数据&lt;/li>
&lt;/ol>
&lt;h3 id="test-time-scaling">&lt;a href="#test-time-scaling" class="header-anchor">&lt;/a>Test-time Scaling
&lt;/h3>&lt;p>test time scaling 主要基于两个方面：&lt;/p>
&lt;ol>
&lt;li>deep thinking: 就是 reasoning mode&lt;/li>
&lt;li>parallel thinking: 基于 VisualPRM 进行多次采样，然后基于 BoN 策略得到最终的输出。&lt;/li>
&lt;/ol>
&lt;h3 id="infra">&lt;a href="#infra" class="header-anchor">&lt;/a>Infra
&lt;/h3>&lt;p>模型使用 XTuner 框架进行训练，使用了包括 FSDP, data packing, FP8, flashattention3 等策略。&lt;/p>
&lt;p>作者还提出了 DvD 的策略来提高推理效率，核心思想就是将 ViT-MLP 模块放在一个 server 上，然后将 LLM 放在另一个 server 上，这样就也可以提高整体的通信效率。框架示意图如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-internvl3.5/InternVL-3-5-DvD.png"
width="1297"
height="580"
loading="lazy"
alt="DvD of InternVL3.5"
class="gallery-image"
data-flex-grow="223"
data-flex-basis="536px"
>&lt;/p>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>模型整体表现如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-internvl3.5/InternVL-3-5-model-performance.png"
width="964"
height="1113"
loading="lazy"
alt="Performance of InternVL3.5"
class="gallery-image"
data-flex-grow="86"
data-flex-basis="207px"
>&lt;/p>
&lt;h3 id="ablation-study">&lt;a href="#ablation-study" class="header-anchor">&lt;/a>Ablation Study
&lt;/h3>&lt;p>作者首先探究了 Cascade RL 对模型表现的影响&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-internvl3.5/InternVL-3-5-ablation-cascade-RL.png"
width="1299"
height="331"
loading="lazy"
alt="Ablation study on Cascade RL"
class="gallery-image"
data-flex-grow="392"
data-flex-basis="941px"
>&lt;/p>
&lt;p>实验结果显示，SFT, MPO 和 offline RL 每个阶段都可以提高模型的多模态 reasoning 表现。&lt;/p>
&lt;p>作者进一步探究了不同阶段的投入产出比，也就是训练时间与最终模型表现的变化情况，如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-internvl3.5/InternVL-3-5-training-efficiency-effectiveness.png"
width="1237"
height="213"
loading="lazy"
alt="Training efficiency and effectiveness"
class="gallery-image"
data-flex-grow="580"
data-flex-basis="1393px"
>&lt;/p>
&lt;p>实验结果显示，尽管 GSPO 的效果提升比较明显，但是训练所需要的时间比较长。如果先 MPO 再进行 GSPO 的话，我们可以在较短的时间里取得较好的表现。&lt;/p>
&lt;p>接下来，作者探究了 ViR 的有效性和效率。作者首先对比了 InternVL3.5 以及 InternVL3.5-flash 的表现。结果显示，大部分情况下，这两个模型的表现没有较大差距。说明 ViR 不会损害模型的性能。&lt;/p>
&lt;p>作者进一步探究了 ViR 和 DvD 对提升模型效率的影响，实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-internvl3.5/InternVL-3-5-ablation-DvD.png"
width="877"
height="397"
loading="lazy"
alt="Ablation study on DvD and ViR"
class="gallery-image"
data-flex-grow="220"
data-flex-basis="530px"
>&lt;/p>
&lt;p>实验结果说明，对于高精度图片输入，DvD 和 ViR 均可以提高模型的推理效率。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>在本文中，作者提出了 InternVL3.5 系列多模态大模型，作者提出了 Cascade RL 框架，该框架结合了 offline RL 以及 online RL 来提高模型的 reasoning 表现以及训练效率。作者还提出了 ViR 以及 DvD 模块来提高模型处理高分辨率图片的效率。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2508.18265" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Ovis2.5 MLLM with stronger perception and reasoning capability</title><link>https://maosong.website/p/ovis2.5-mllm-with-stronger-perception-and-reasoning-capability/</link><pubDate>Sat, 30 Aug 2025 17:34:44 +0800</pubDate><guid>https://maosong.website/p/ovis2.5-mllm-with-stronger-perception-and-reasoning-capability/</guid><description>&lt;p>作者提出了 Ovis2.5, 一个基于 Ovis 改进的多模态大模型系列，包括 2B 和 9B 两个 size，Ovis2.5 主要强调了支持不同分辨率图片输入以及深度思考这两个 feature&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者首先回顾了 &lt;a class="link" href="https://maosong.website/p/ovis-discrete-visual-embedding/" target="_blank" rel="noopener"
>Ovis&lt;/a>, Ovis 主要是解决 text embedding 以及 visual embedding 对齐程度比较低的问题。&lt;/p>
&lt;p>接下来，作者介绍了以下 Ovis 的两个问题：&lt;/p>
&lt;ol>
&lt;li>只能支持固定大小的图片输入&lt;/li>
&lt;li>缺乏深度思考能力&lt;/li>
&lt;/ol>
&lt;p>为了解决这两个问题，作者提出了 Ovis 2.5, Ovis 主要做出了两点改进：&lt;/p>
&lt;ol>
&lt;li>使用了 NaViT 来处理不同分辨率图片的输入&lt;/li>
&lt;li>作者通过训练提高了模型的深度思考能力&lt;/li>
&lt;/ol>
&lt;p>最终 Ovis2.5 主要有以下 feature&lt;/p>
&lt;ol>
&lt;li>支持动态分辨率图片输入&lt;/li>
&lt;li>深度思考能力&lt;/li>
&lt;li>SOTA 的表现&lt;/li>
&lt;li>高效的训练方式&lt;/li>
&lt;/ol>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;p>Ovis2.5 的架构如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/ovis2.5-mllm-with-stronger-perception-and-reasoning-capability/Ovis-2-5-architecture.png"
width="855"
height="895"
loading="lazy"
alt="Architecture of Ovis2.5"
class="gallery-image"
data-flex-grow="95"
data-flex-basis="229px"
>&lt;/p>
&lt;p>Ovis 包括三个模块：&lt;/p>
&lt;ol>
&lt;li>visual tokenizer： ViT 架构，&lt;/li>
&lt;li>visual embedding table: 类似 LLM 中的 text embedding table, 见 &lt;a class="link" href="https://maosong.website/p/ovis-discrete-visual-embedding/" target="_blank" rel="noopener"
>Ovis&lt;/a>&lt;/li>
&lt;li>LLM: 基于 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a>&lt;/li>
&lt;/ol>
&lt;p>作者在架构上进行了如下改进：&lt;/p>
&lt;ol>
&lt;li>动态分辨率图片输入处理：作者使用了 NaViT 来支持动态分辨率图片输入&lt;/li>
&lt;li>LLM: 作者使用了 Qwen3 来进一步提高模型的表现&lt;/li>
&lt;/ol>
&lt;h3 id="training">&lt;a href="#training" class="header-anchor">&lt;/a>Training
&lt;/h3>&lt;p>模型训练包括 pre-training 和 post-training 两个大的 stage, 其中 pre-training 又包含 3 个小的 stage, post-training 包含 2 个 stage. 训练过程如下所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/ovis2.5-mllm-with-stronger-perception-and-reasoning-capability/Ovis-2-5-training-process.png"
width="930"
height="292"
loading="lazy"
alt="Training Process of Ovis2.5"
class="gallery-image"
data-flex-grow="318"
data-flex-basis="764px"
>&lt;/p>
&lt;p>pre-training 阶段的数据包括 COYO, Laion, Wukong, DataComp, SAM 等。作者介绍了几个部分的数据：&lt;/p>
&lt;ol>
&lt;li>OCR 数据，作者基于 MLLM 来标注数据和合成 QA&lt;/li>
&lt;li>Grounding 数据，作者使用了 RefCoCo 等数据集以及先进的 MLLM 来标注数据&lt;/li>
&lt;li>Reasoning 数据，作者收集了数据然后使用 MLLM 来合成 Reasoning path&lt;/li>
&lt;/ol>
&lt;p>训练时，&lt;/p>
&lt;ol>
&lt;li>VET pretraining: 训练 VET, 作者基于 SigLIP 来初始模型的参数，然后仅训练最后一层 ViT layer, visual head 以及 VET, 图片精度为 448-896. 作者采用了动态 position embedding&lt;/li>
&lt;li>Multimodal pretraining: 这阶段全量微调所有参数，主要目的是使用对话格式的数据。图片精度为 448-1792&lt;/li>
&lt;li>multimodal instruction tuning: 这阶段训练所有参数，主要提高模型跟随多模态指令的能力&lt;/li>
&lt;/ol>
&lt;p>post-training 包括 &lt;a class="link" href="https://maosong.website/p/notes-on-dpo/" target="_blank" rel="noopener"
>DPO&lt;/a> 和 GRPO 两个阶段。&lt;/p>
&lt;ol>
&lt;li>DPO: 训练所有参数，使用 pre-training checkpoint 来多次采样&lt;/li>
&lt;li>GRPO: 使用 RLVR 数据集进行训练&lt;/li>
&lt;/ol>
&lt;h2 id="infra">&lt;a href="#infra" class="header-anchor">&lt;/a>Infra
&lt;/h2>&lt;p>infra 方面，作者主要强调了 data packing 以及多种并行策略融合。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者在本文中提出了 Ovis2.5, 一个基于 Ovis 架构的多模态大模型，作者主要强调了模型的动态图片输入处理能力以及深度思考能力。&lt;/p>
&lt;p>作者提出了几个未来的方向：&lt;/p>
&lt;ol>
&lt;li>将输入图片精度提升到 4K&lt;/li>
&lt;li>处理长视频输入并进行 temporal reasoning&lt;/li>
&lt;li>在 Reasoning 过程中加入 tool-use.&lt;/li>
&lt;/ol>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2508.11737" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Ovis-discrete visual embedding</title><link>https://maosong.website/p/ovis-discrete-visual-embedding/</link><pubDate>Sat, 30 Aug 2025 17:32:22 +0800</pubDate><guid>https://maosong.website/p/ovis-discrete-visual-embedding/</guid><description>&lt;p>作者提出了 Ovis，一个离散化表示 visual encder 输出特征的方法，来更好对齐 LLM 的视觉输入和文本输入&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者分析了已有多模态大模型的架构，已有多模态大模型的输入对于文本来说是离散的 (text token), 对于图片来说是连续的 (visual embedding)。作者认为这种连续 - 离散的输入可能会影响模型最终的表现。&lt;/p>
&lt;p>为了解决这个问题，作者构建了一个 visual embedding table, 将 visual embedding 也转换成离散的 token 表示形式，进而统一 LLM 输出的粒度。&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;p>模型的架构如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/ovis-discrete-visual-embedding/Ovis-architecure.png"
width="1174"
height="964"
loading="lazy"
alt="Architecture of Ovis"
class="gallery-image"
data-flex-grow="121"
data-flex-basis="292px"
>&lt;/p>
&lt;p>我们首先会构建一个 visual vocabulary $\{e_k\}_{k=1}^K$, 其大小为 $K$, 然后对于 ViT 输出的 $n$ 个 visual feature $\{r_i\}_{i=1}^n$, 我们会加入一个 linear head 以及一个 softmax 来构建一个 vocabulary 上的分布，即&lt;/p>
$$
v_i = \mathrm{softmax}(Wr_i), W\in\mathbb{R}^{K\times d}
$$&lt;p>这里 $v_i\in\Delta^K$ 是 visual vocabulary 上的概率分布。最终，视觉模块的输入是 vocabulary 中 visual token 的一个加权求和&lt;/p>
$$
V_i = \sum_{k=1}^K v_{i,k}e_k\in\mathbb{R}^{d'}
$$&lt;p>训练分为三个阶段：&lt;/p>
&lt;ul>
&lt;li>Stage 1: 训练 $W$, visual encoder 最后一个 block 以及 visual vocabulary&lt;/li>
&lt;li>Stage 2: 训练 $W$, visual vocabulary 以及 visual encoder&lt;/li>
&lt;li>Stage 3: multimodal SFT, 提高模型的指令跟随能力，模型所有参数都参与训练&lt;/li>
&lt;/ul>
&lt;p>训练数据分布如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/ovis-discrete-visual-embedding/Ovis-training-dataset-distribution.png"
width="1023"
height="750"
loading="lazy"
alt="Statistics of Ovis training dataset"
class="gallery-image"
data-flex-grow="136"
data-flex-basis="327px"
>&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者提出了 Ovis，一个离散化表示 visual encder 输出特征的方法，来更好对齐 LLM 的视觉输入和文本输入&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2405.20797" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on ARC-Hunyuan-Video-7B</title><link>https://maosong.website/p/notes-on-arc-hunyuan-video-7b/</link><pubDate>Tue, 12 Aug 2025 10:57:57 +0800</pubDate><guid>https://maosong.website/p/notes-on-arc-hunyuan-video-7b/</guid><description>&lt;p>腾讯 ARC LAB 提出了 ARC-Hunyuan-Video-7B, 一个针对短视频理解和推理的视频多模态大模型。&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者首先提出了 Structured video comprehension 的概念&lt;/p>
&lt;blockquote>
&lt;p>the ability to decompose a video into its constituent events and narrative elements with temporal precision.&lt;/p>
&lt;/blockquote>
&lt;p>视频信息包含了 dense visual elements, 比如文字等信息，还有 rich audio streams, 比如音效，音乐，再就是 rapid narrative pacing, 用于强调情感。&lt;/p>
&lt;p>已有的多模态大模型对于视频的细粒度理解和推理能力存在不足。尽管 &lt;a class="link" href="https://maosong.website/p/notes-on-keye-vl/" target="_blank" rel="noopener"
>Keye-VL&lt;/a> 也是一个短视频理解多模态大模型，但是其并没有利用 audio 模态的信息。其他的 audio-visual LLM 虽然可以处理 audio 模态的信息，但是他们主要集中于通用场景的视频，这类视频的特点是：叙述节奏慢，信息密度低。因此，对于叙述节奏快，信息丰富的短视频，这些模型表现就比较差。&lt;/p>
&lt;p>基于以上这些问题，作者提出了 ARC-Hunyuan-Video-7B, 一个基于 Hunyuan-7B vision-language model 的短视频多模态大模型，其主要做了两点改进：&lt;/p>
&lt;ol>
&lt;li>加入了一个 audio encoder 用于提取 audio 信息，然后对 audio 信息与视觉信息进行同步&lt;/li>
&lt;li>使用了一个 timestamp overlap 机制，将时间戳印在视频的帧上，帮助模型理解事件的时序信息&lt;/li>
&lt;/ol>
&lt;p>作者还涉及了多阶段的训练策略，来提高模型的表现。最后在测试时，对于一个 1 分钟的视频，模型在 H20 GPU 上进需要 10 秒就可以处理完毕。&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;p>&lt;img src="https://maosong.website/p/notes-on-arc-hunyuan-video-7b/ARC-Hunyuan-Video-architecture.png"
width="682"
height="467"
loading="lazy"
alt="Architecture of ARC-Hunyuan-Video-7B"
class="gallery-image"
data-flex-grow="146"
data-flex-basis="350px"
>&lt;/p>
&lt;p>模型基于 Hunyuan-7B-VLM 开发得到，&lt;/p>
&lt;ul>
&lt;li>Visual Encoding: 作者将时间戳以 &lt;code>HH:MM:SS&lt;/code> 的形式印在视频的帧上。视频的采样率为 1FPS, 超过 150s 的视频会进行均匀采样降低至 150s. 每一帧会 resize 到 $640\times 640$, 最后每一帧输出 112 token&lt;/li>
&lt;li>Audio Encoding: 使用 Whisper 作为编码器。小于 300s 的视频，会按照 30s 为单位切分为不同的 chunk, 大于 300s 的视频，会分割为 150 个 chunk, 然后每个 chunk 只保留开头 2s 的内容。最后，使用 Whisper 进行特征提取，最后再通过 MLP 与 LLM 的特征空间进行对齐&lt;/li>
&lt;li>Visual-audio Synchronization: 先对 audio token 进行 zero padding 与 Visual token 的个数进行对齐，然后将两者相加&lt;/li>
&lt;/ul>
&lt;h3 id="training">&lt;a href="#training" class="header-anchor">&lt;/a>Training
&lt;/h3>&lt;p>&lt;img src="https://maosong.website/p/notes-on-arc-hunyuan-video-7b/ARC-Hunyuan-Video-training-overall.png"
width="682"
height="467"
loading="lazy"
alt="Training pipeline of ARC-Hunyuan-Video-7B"
class="gallery-image"
data-flex-grow="146"
data-flex-basis="350px"
>&lt;/p>
&lt;h4 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h4>&lt;p>&lt;img src="https://maosong.website/p/notes-on-arc-hunyuan-video-7b/ARC-Hunyuan-Video-annotation.png"
width="1362"
height="289"
loading="lazy"
alt="Boostrapped annotation pipeline of ARC-Hunyuan-Video-7B"
class="gallery-image"
data-flex-grow="471"
data-flex-basis="1131px"
>&lt;/p>
&lt;p>作者首先保住了一些数据。具体流程就是，使用 Whisper-v3 来转录带时间戳的音频，然后使用 InternVL-2.5-8B [[InternVL-2.5]] 来生成细粒度的 caption. 作者还使用 CoT prompt 策略，让模型一步步生成时间的描述，创作者的想法以及如何吸引用户的 tag 等。&lt;/p>
&lt;p>预训练数据如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Category&lt;/th>
&lt;th>data&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Video description and summary&lt;/td>
&lt;td>4.5M short-form video&lt;br>0.2M public video&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Image caption and OCR&lt;/td>
&lt;td>4.7M image-text pairs&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ASR&lt;/td>
&lt;td>3.2M audio-text pairs&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Video temporal grounding&lt;/td>
&lt;td>0.5M temporally grounding instances&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Video multi-granular caption&lt;/td>
&lt;td>50K high-quality samples&lt;br>80K in-house videos&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>训练包含两个 stage:&lt;/p>
&lt;ol>
&lt;li>Stage 1: 使用 ASR 数据训练模型接受音频输入的能力，为了避免视觉模态能力下降，作者还加入了 Image-text pair data 来训练，缺失的模态用 zero padding 来补齐&lt;/li>
&lt;li>Stage 2: 使用全部数据进行训练，仅训练 MLP 和 LLM&lt;/li>
&lt;/ol>
&lt;h4 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h4>&lt;p>作者首先进行了消融实验，探究 human-annotated 数据对模型表现的影响。作者收集了 140 条人类标注的数据，然后基于这些数据进行消融实验：&lt;/p>
&lt;ol>
&lt;li>直接使用人类标注数据进行 SFT, 模型表现变化不大&lt;/li>
&lt;li>直接使用人类标注数据作为正样本，合成数据作为负样本进行 DPO, 模型表现变化也不大&lt;/li>
&lt;/ol>
&lt;p>作者分析原因认为，&lt;strong>人类标注数据和合成数据之间存在 distribution shift&lt;/strong>.&lt;/p>
&lt;p>受 &lt;a class="link" href="https://maosong.website/p/notes-on-deepseek-r1/" target="_blank" rel="noopener"
>DeepSeek-R1&lt;/a> 启发，作者决定使用 GRPO 算法来提高模型的表现，训练任务包含两个：&lt;/p>
&lt;ol>
&lt;li>multi-dimensional Multi-choide QA: 提高模型的视频理解能力&lt;/li>
&lt;li>Temporal video grounding: 提高模型的时序感知能力&lt;/li>
&lt;/ol>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Stage&lt;/th>
&lt;th>Data&lt;/th>
&lt;th>Module&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>SFT&lt;/td>
&lt;td>MCQ:&lt;br> - 460K open-ended QA&lt;br> - 70K MCQA&lt;br> - 20K QA&lt;br>Grounding:&lt;br> - 10K academic&lt;br> - 5K real-world&lt;br>General:&lt;br> - 45K description&lt;br> - 12K caption&lt;/td>
&lt;td>MLP+LLM&lt;/td>
&lt;td>提高指令跟随能力&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Cold Start SFT&lt;/td>
&lt;td>- 90K MCQA&lt;br>- 18K temporal grounding&lt;br>- 20K open-ended QA&lt;br>- 15K summarization&lt;br>- 3K chapter-level captioning&lt;/td>
&lt;td>MLP+LLM&lt;/td>
&lt;td>初步激活模型的 reas 能力&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RL&lt;/td>
&lt;td>- 100K MCQ&lt;br>- 35K temporal grounding&lt;/td>
&lt;td>LLM&lt;/td>
&lt;td>提升模型的 reasoning 能力&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SFT&lt;/td>
&lt;td>- 25K human-annotated subjective question&lt;br>- 100K MCQ with CoT&lt;br>- 50K temporal grounding with reasoning traces&lt;/td>
&lt;td>-&lt;/td>
&lt;td>使用人类标注数据进一步提高模型的能力&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="experiments">&lt;a href="#experiments" class="header-anchor">&lt;/a>Experiments
&lt;/h2>&lt;p>作者构建了一个评估短视频理解能力的 benchmark: ShortVid-Bench, 评估以下方面：&lt;/p>
&lt;ol>
&lt;li>Temporal Reasoning and Localization&lt;/li>
&lt;li>Affective Intent Classification&lt;/li>
&lt;li>Creator Intent Taxonomy&lt;/li>
&lt;li>Narrative Comprehension&lt;/li>
&lt;li>Humor &amp;amp; Meme Deconstruction&lt;/li>
&lt;li>Creative Innovation Analysis&lt;/li>
&lt;/ol>
&lt;p>对比的模型包括 Qwen2.5-VL-7B, Qwen2.5-omni-7B 和 Keye-VL-8B&lt;/p>
&lt;p>评估结果如下：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-arc-hunyuan-video-7b/ARC-Hunyuan-Video-performance.png"
width="1367"
height="244"
loading="lazy"
alt="Performance of ARC-Hunyuan-Video-7B"
class="gallery-image"
data-flex-grow="560"
data-flex-basis="1344px"
>&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者提出了 ARC-Hunyuan-Video-7B, 一个针对短视频的视频理解多模态大模型。作者详细介绍了模型的架构，训练数据以及评估。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2507.20939" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/TencentARC/ARC-Hunyuan-Video-7B" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on AFM2025</title><link>https://maosong.website/p/notes-on-afm2025/</link><pubDate>Tue, 29 Jul 2025 12:36:28 +0800</pubDate><guid>https://maosong.website/p/notes-on-afm2025/</guid><description>&lt;p>Apple 在 7 月份发布了 AFM 技术报告，包括两个多语种多模态大模型，分别为 3B 和 xB, 一个面向 device, 另一个面向 server， 前者主要集中于效率，后者集中于表现。&lt;/p>
&lt;h2 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h2>&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;h4 id="on-device-model">&lt;a href="#on-device-model" class="header-anchor">&lt;/a>On-Device Model
&lt;/h4>&lt;p>对于 on-device model, 作者将模型分为两个 block, Block1 占 $62.5\%$ 的 transformer layers, Block2 占 $37.5\%$ 的 transformer layers. 但是，对于 Block2, 作者移除了 key, value projection, 对应的 KV cache 则直接从 Block1 中获取。通过这种方式，作者将 KV cache memory usage 减少了 $37.5\%$. 并且，由于 Block2 不产生任何 key values, prefill stage 可以跳过这些计算，这样 TTFT 也可以减少 $37.5\%$.&lt;/p>
&lt;h4 id="server-model">&lt;a href="#server-model" class="header-anchor">&lt;/a>Server Model
&lt;/h4>&lt;p>对于 server model, 作者对架构进行了改进，来提高效率。架构如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-afm2025/AFM-2025-PT-MoE-architecture.png"
width="1271"
height="525"
loading="lazy"
alt="Diagram of the PT-MoE architecture"
class="gallery-image"
data-flex-grow="242"
data-flex-basis="581px"
>&lt;/p>
&lt;p>&lt;strong>Parallel Track Transformer&lt;/strong>
作者提出了 Parallel Track (PT) Transformer 架构，PT-Transformer 将 transformer 模型分割多个小的 transformer, 作者将这些小的 transformer 称之为 &lt;em>track&lt;/em>. 每个 track 包含多个 transformer block. 不同的 track 只会在输入和输出的时候进行交互，这样就能够减少同步的开销。作者讲这种模式称为 &lt;strong>track parallelism&lt;/strong>.&lt;/p>
&lt;p>&lt;strong>PT-MoE&lt;/strong>
为了进一步提高 server model 的效率，作者将 MoE 和 PT-transformer 结合在一起。具体的做法就是，每两个 transformer block 为一组，每组里包含一个 dense layer 和一个 MoE layer.&lt;/p>
&lt;p>&lt;strong>Interleaving Global and Local Attention Layers&lt;/strong>
作者还设计额 interleaved attention 机制，也就是，将 transformer block 按照四个为 1 组，前面 3 个 block 使用 window attention, window size 为 4096 和 RoPE. 最后一个 block 使用 global attention layer 以及 &lt;a class="link" href="https://maosong.website/p/notes-on-nope/" target="_blank" rel="noopener"
>NoPE&lt;/a>. 作者认为，使用 NoPE 可以提高模型对长上下文的泛化性。&lt;/p>
&lt;blockquote>
&lt;p>Recall
Qwen2.5-VL 的 ViT 使用的类似的做法，即 8 个 block 为一组，前面 7 个 block 使用 window attention, 最后一个 block 使用 full self attention.&lt;/p>
&lt;/blockquote>
&lt;h4 id="vision-encoder">&lt;a href="#vision-encoder" class="header-anchor">&lt;/a>Vision Encoder
&lt;/h4>&lt;p>Vision encoder 包含 ViT 和 adapter 两个模块&lt;/p>
&lt;p>对于 ViT 来说，作者使用了 ViT 架构：&lt;/p>
&lt;ul>
&lt;li>server model 使用了 1B 参数的 ViT-g&lt;/li>
&lt;li>on-device model 使用了 300M 参数的 ViTDet-L backbone&lt;/li>
&lt;/ul>
&lt;p>作者在 ViTDet 的基础上加入了 Register-Window 机制，这个机制用于编码一个 global register token 来与不同的 loca windows 进行交互。&lt;/p>
&lt;p>对于 adapter 来说，其包含了一个 transformer layer, 一个 linear projection layer, 一个 $3\times 3$ 的 convolutional layer. 其中， linear projection 用于将 visual token 映射到 LLM 的特征空间，pooling layer 用于压缩 visual token 个数。&lt;/p>
&lt;h3 id="data">&lt;a href="#data" class="header-anchor">&lt;/a>Data
&lt;/h3>&lt;p>主要包括 web data 和 image data 两部分&lt;/p>
&lt;p>image data 部分：&lt;/p>
&lt;ol>
&lt;li>Image-Text Crawl Data: 包含 &lt;strong>175M&lt;/strong> 图文交错数据，包含 &lt;strong>550M&lt;/strong> images&lt;/li>
&lt;li>Synthetic Image Caption data: &lt;strong>5B&lt;/strong> image caption 数据&lt;/li>
&lt;li>Text-Rich Image Data&lt;/li>
&lt;li>High-quality Domain-Specific Image-text Data: 包括 caption 数据， grounding 数据，table, chart, plots 数据以及 knowledge-required domains 的数据&lt;/li>
&lt;/ol>
&lt;h3 id="training-recipe">&lt;a href="#training-recipe" class="header-anchor">&lt;/a>Training Recipe
&lt;/h3>&lt;p>text tokenizer 大小为 150K.&lt;/p>
&lt;p>Vision encoder 的训练包含两个 stage:&lt;/p>
&lt;ol>
&lt;li>基于 CLIP 的方法，使用 &lt;strong>6B&lt;/strong>的 image-text pair 数据进行训练，图片精度为 448, 作者还使用了 FLIP 来提高训练效率&lt;/li>
&lt;li>使用一个 compact LLM, 同时训练 vsion encoder, adapter 和 compact LLM. 加入了更高质量的数据，图片精度为 672.&lt;/li>
&lt;/ol>
&lt;p>LLM 的训练使用了 &lt;strong>13.4T&lt;/strong> token&lt;/p>
&lt;h2 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h2>&lt;h3 id="sft">&lt;a href="#sft" class="header-anchor">&lt;/a>SFT
&lt;/h3>&lt;p>SFT 数据包括：&lt;/p>
&lt;ol>
&lt;li>General knowledge&lt;/li>
&lt;li>Reasoning: 纯文本包括 math 和 reasoning, 多模态包括 STEM, math, CoT 数据&lt;/li>
&lt;li>Text-Rich Image understanding: chart, table 数据&lt;/li>
&lt;li>Multilingual OCR: OCR 相关数据&lt;/li>
&lt;li>Text and visual grounding: grounding 数据&lt;/li>
&lt;li>Multi-image reasoning: 多图推理数据&lt;/li>
&lt;/ol>
&lt;p>作者还基于 retrieval-based 方法来收集数据，具体做法就是给定一些 prompt, 然后通过一个 Image search pipeline 来进行检索。&lt;/p>
&lt;p>训练的时候，作者将图片精度从 672 提升到 1344, 处理方式就是将图片切分为四个子图，然后作者还加入了一个总蓝图。这样，vision encoder 的输入包括四个子图和一个 thumbnail 图.&lt;/p>
&lt;p>为了提高 on-device model 的效率，作者设置了三种模式：&lt;/p>
&lt;ul>
&lt;li>rapid mode: 图片精度为 224&lt;/li>
&lt;li>balanced mode: 只有 thumbnail 图&lt;/li>
&lt;li>high-resolution mode: 四个子图和一个 thumbnail 图&lt;/li>
&lt;/ul>
&lt;p>对于不同的 mode, 如果输入的是低精度图片，则 $50\%$ 概率为 rapid mode; 如果输入的是高精度图片，则 $1\%$ 的概率为 rapid mode. 对于其他数据，作者将 $20\%$ 的数据设置为 balanced mode.&lt;/p>
&lt;h3 id="rlhf">&lt;a href="#rlhf" class="header-anchor">&lt;/a>RLHF
&lt;/h3>&lt;p>作者使用 RLOO 作为 RLHF 的算法。&lt;/p>
&lt;p>RL 的 infra 如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-afm2025/AFM-2025-RL-infra.png"
width="1277"
height="350"
loading="lazy"
alt="AFM2025 RL Infra"
class="gallery-image"
data-flex-grow="364"
data-flex-basis="875px"
>&lt;/p>
&lt;p>infra 主要由两个部分组成：&lt;/p>
&lt;ol>
&lt;li>Trajectory Generators: 生成轨迹并提供反馈&lt;/li>
&lt;li>Policy updater: 更新 policy&lt;/li>
&lt;/ol>
&lt;p>训练时，作者首先训练了一个 reward model, 与 AFM-2024 相似，作者使用了一个 preference loss function 以及一个 single-sided grading 作为 regularization.&lt;/p>
&lt;p>数据包括以下类别:&lt;/p>
&lt;ul>
&lt;li>text-only prompts&lt;/li>
&lt;li>Image-text prompts&lt;/li>
&lt;li>Math prompts&lt;/li>
&lt;li>Image-text STEM reasoning prompts&lt;/li>
&lt;/ul>
&lt;p>其中，前面两个使用 reward function 进行打分，后面两个基于 ruled-based verifier 进行打分&lt;/p>
&lt;p>作者还发现，人类的打分和 reward model 的发奋可能会出现 $20\%\sim30\%$ 的偏差。为了解决这个问题，作者训练了一个单独的 reward model, 专门用于 prompt selection.&lt;/p>
&lt;h2 id="tool-use">&lt;a href="#tool-use" class="header-anchor">&lt;/a>Tool Use
&lt;/h2>&lt;p>工具调用数据由于 Multi-turn 和依赖软件工具，比较难以收集。为了解决这个问题，作者设计了一个交互式标注平台，包括一个 agent 和一个工具执行环境。环境包括了工具和数据库，能执行工具调用并反馈。&lt;/p>
&lt;p>标注时，用户发起一个请求，然后 agent 自动执行工具调用，最后平台返回反正的轨迹。&lt;/p>
&lt;h2 id="multilingual">&lt;a href="#multilingual" class="header-anchor">&lt;/a>Multilingual
&lt;/h2>&lt;p>作者逐步增加模型对于新语言的理解能力。默认情形下，输入和输出的语种一致，但是包含 $0.4\%$ 的跨语种数据。在 SFT 和 RLHF 阶段，英语和多语种数据的比例为 $80\%:20\%$.&lt;/p>
&lt;h2 id="optimization">&lt;a href="#optimization" class="header-anchor">&lt;/a>Optimization
&lt;/h2>&lt;p>作者使用了 QAT 来将 on-device model 压缩到 2 bits-per-weight, 使用 Adaptive Scalable Texture Compression (ASTC) 来 post-training 3.56 bits-per-weight 版本的 server model.&lt;/p>
&lt;h3 id="qat">&lt;a href="#qat" class="header-anchor">&lt;/a>QAT
&lt;/h3>&lt;p>QAT 是一个在模型训练过程中模拟量化误差，从而提升模型量化后表现的方法。它解决了传统后量化方法精度损失较大的问题，是平衡模型性能用户效率的关键手段。&lt;/p>
&lt;p>训练时，作者通过修改权重 $W$ 来模仿量化：&lt;/p>
$$
\tilde{W} = s\left(\mathrm{clamp}(\lfloor \frac{W}{s}+z\rceil, q_{\min}, q_{\max}) - z\right)
$$&lt;p>其中, $s$ 是 scaling factor, $z$ 是 zero point, $q_{\min}$, $q_{\max}$ 是 quantization 的 range. 为了解决 rounding operation 不可微的问题，作者使用了 straight-through estimator 的方法来近似梯度。&lt;/p>
&lt;p>作者还提出了一个可学习的 scaling factor $f$ 用于计算 quantization scale, 计算方法如下所示&lt;/p>
$$
s = \frac{f\cdot \max(|W|)}{q_{\max}}
$$&lt;p>作者通过精细设计 $f$ 的初始化来保证模型训练的 robust.&lt;/p>
&lt;h3 id="astc">&lt;a href="#astc" class="header-anchor">&lt;/a>ASTC
&lt;/h3>&lt;p>对于 server model, 作者使用了 ASTC, 一个针对 GPU 图形纹理压缩的技术，来压缩模型权重。具体做法就是，模型训练好之后，作者对模型权重应用 ASTC, 然后对每个块进行预处理。存储时，每个块用 ASTC-HAR-ch 模式压缩为 128 位。最小值单独存储为 float16.&lt;/p>
&lt;p>推理时，GPU 硬件自动解压缩 ASTC 块，然后解压的权重最小值相加参与矩阵计算&lt;/p>
&lt;h3 id="quality-recovery-adapters">&lt;a href="#quality-recovery-adapters" class="header-anchor">&lt;/a>Quality Recovery Adapters
&lt;/h3>&lt;p>作者还是用 LoRA 来恢复量化模型的精度，并通过选择性压缩策略优化 ASTC 过程，在极小的算力开销下实现了接近全量微调的性能。&lt;/p>
&lt;h2 id="evaluation">&lt;a href="#evaluation" class="header-anchor">&lt;/a>Evaluation
&lt;/h2>&lt;p>On-device model 表现如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>MMLU&lt;/th>
&lt;th>MMMLU&lt;/th>
&lt;th>MGSM&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>AFM On-Device&lt;/td>
&lt;td>67.85&lt;/td>
&lt;td>60.60&lt;/td>
&lt;td>74.91&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen-2.5-3B&lt;/td>
&lt;td>66.37&lt;/td>
&lt;td>56.53&lt;/td>
&lt;td>64.80&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen-3-4B&lt;/td>
&lt;td>&lt;strong>75.10&lt;/strong>&lt;/td>
&lt;td>&lt;strong>66.52&lt;/strong>&lt;/td>
&lt;td>&lt;strong>82.97&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Gemma-3-4B&lt;/td>
&lt;td>62.81&lt;/td>
&lt;td>56.71&lt;/td>
&lt;td>74.74&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Gemma-3n-E4B&lt;/td>
&lt;td>57.84&lt;/td>
&lt;td>50.93&lt;/td>
&lt;td>77.77&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>server model 表现如下&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>MMLU&lt;/th>
&lt;th>MMMLU&lt;/th>
&lt;th>MGSM&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>AFM Server&lt;/td>
&lt;td>80.20&lt;/td>
&lt;td>74.60&lt;/td>
&lt;td>87.09&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LLaMA 4 Scout&lt;/td>
&lt;td>84.88&lt;/td>
&lt;td>80.24&lt;/td>
&lt;td>90.34&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen-3-235B&lt;/td>
&lt;td>&lt;strong>87.52&lt;/strong>&lt;/td>
&lt;td>&lt;strong>82.95&lt;/strong>&lt;/td>
&lt;td>&lt;strong>92.00&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GPT-4o&lt;/td>
&lt;td>85.70&lt;/td>
&lt;td>84.00&lt;/td>
&lt;td>90.30&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者提出了 AFM-2025 多模态多语种大语言模型系列，包括 on-device 和 server 两个版本，作者介绍了模型的架构，训练数据和训练方式。&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://machinelearning.apple.com/papers/apple_intelligence_foundation_language_models_tech_report_2025.pdf" target="_blank" rel="noopener"
>Publication&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://machinelearning.apple.com/research/apple-foundation-models-tech-report-2025" target="_blank" rel="noopener"
>blog&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Keye-VL</title><link>https://maosong.website/p/notes-on-keye-vl/</link><pubDate>Wed, 23 Jul 2025 11:11:43 +0800</pubDate><guid>https://maosong.website/p/notes-on-keye-vl/</guid><description>&lt;p>Keye-VL 是快手在 25 年 7 月份提出的一个 8B 的多模态大模型，其亮点为短视频理解能力。预训练包括 4 个 stage，使用了 600B token，后训练包括 2 个 stage，用于提升模型的 reasoning 和 non-reasoning 能力。&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>作者首先回顾了已有的 MLLM 工作，然后强调理解短视频仍然是一个很难的任务，特别是要求模型基于 video 和 audio 来理解视频。因此，在本文中，作者提出了 Kwai Keye-VL，一个 8B 的多模态大模型，主要用于短视频理解任务。&lt;/p>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;p>Keye-VL 是一个标准的 ViT-MLP-LLM 的架构，其中，ViT 是 SigLIP-400M-384-14, MLP 是一个基于 SwiGLU 的 2 层 MLP，使用了和 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5-vl/" target="_blank" rel="noopener"
>Qwen2.5-VL&lt;/a> 一样的 patch merge 方法，LLM 使用的是 Qwen3-8B，模型架构示意图如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl/Keye-VL-architecture.png"
width="1371"
height="1000"
loading="lazy"
alt="Keye-VL model architecture"
class="gallery-image"
data-flex-grow="137"
data-flex-basis="329px"
>&lt;/p>
&lt;p>作者针对 ViT 和 visual encoding 分别做了如下改进&lt;/p>
&lt;h4 id="navit">&lt;a href="#navit" class="header-anchor">&lt;/a>NaViT
&lt;/h4>&lt;p>作者实现了 native resolution ViT，来处理不同分辨率的图片。&lt;/p>
&lt;p>具体做法为，作者基于 SigLIP-400M-384-14 来初始化 ViT。&lt;/p>
&lt;p>然后，作者首先采用了 interpolation 来将 ViT 的 position encoding 扩展到不同的图片精度下面去。&lt;/p>
&lt;p>接下来，作者提出了 2D RoPE 来进一步提升 position encoding 的表现。&lt;/p>
&lt;p>最后，作者加入了 NaViT 的 packing 技巧来继续预训练 ViT.&lt;/p>
&lt;p>在 ViT 预训练的过程中，作者使用了 &lt;strong>500B&lt;/strong> 的 token&lt;/p>
&lt;h4 id="visual-encoding">&lt;a href="#visual-encoding" class="header-anchor">&lt;/a>Visual Encoding
&lt;/h4>&lt;p>为了提升模型理解图片和视频的能力，作者针对图片和视频进行了进一步的处理。&lt;/p>
&lt;p>对于不同精度的图片，作者将最大 token 个数设置为 16384。&lt;/p>
&lt;p>对于视频，作者将每帧的 token 数限制在 $[128,768]$, 每个视频的最大 token 个数设置为 24576&lt;/p>
&lt;p>对于提取的 frames，作者重新计算了 FPS, 然后在 3D RoPE 中让时间维度与真实时间严格对齐。&lt;/p>
&lt;h3 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h3>&lt;h4 id="data">&lt;a href="#data" class="header-anchor">&lt;/a>Data
&lt;/h4>&lt;p>预训练数据一共包括 600B token，覆盖了 6 个类别：&lt;/p>
&lt;ul>
&lt;li>Image caption: 包括中英文数据，来源为 LAION, DataComp 以及 Coyo. 作者基于 CLIP 来计算相速度，然后过滤掉相似度比较低的数据，作者还对数据进行了 re-caption，实验发现 re-caption 可以提高模型的细粒度图片理解能力&lt;/li>
&lt;li>OCR &amp;amp; VQA: 数据包括开源数据和合成数据。合成数据包括 Synthesis 和 Rendering 两个方法，第一是基于 text-dense image 构建 OCR 数据以及基于 image-caption pair 数据使用 MLLM 构建 VQA 数据。第二是使用字体渲染工具合成高质量的 OCR 数据&lt;/li>
&lt;li>Grounding &amp;amp; Counting: Grounding 数据主要包括 RefCoCo, VisualGenome, TolokaVQA, Counting 数据包括 PixMo. 作者仍然使用 CLIP 对数据进行过滤&lt;/li>
&lt;li>Interleaved text-image data: 作者发现图文交错数据可以提供通用知识，并且还可以提高模型的视觉语言对齐能力，第三就是提升模型的泛化能力。作者主要从 academic PDF 以及结构化知识中提取对应的数据。作者基于 Garbled character recognition, low-resolution/broken image filtering 以及 text-image similarity validation 来保证数据的质量&lt;/li>
&lt;li>Video understanding: 作者使用 Qwen2.5-omni 从 interleaved video-asr 来将视频数据转化图文交错数据， 然后基于 ASR 的结果进行 recaption，最后对每一帧进行 OCR. 作者还构建了 Frame-level re-ordering 以及 multiple video matching 两个任务来提高模型的上下文理解能力&lt;/li>
&lt;li>Pure Text: 未提及&lt;/li>
&lt;/ul>
&lt;p>对于源数据，作者进行了数据清洗：&lt;/p>
&lt;ol>
&lt;li>使用 CLIP 对数据进行打分，然后过滤掉低质量的数据&lt;/li>
&lt;li>使用开源的 MLLM 作为 discriminator 来选择高质量的数据&lt;/li>
&lt;li>去重&lt;/li>
&lt;/ol>
&lt;h4 id="training-recipe">&lt;a href="#training-recipe" class="header-anchor">&lt;/a>Training Recipe
&lt;/h4>&lt;p>预训练包括 4 个 stage：&lt;/p>
&lt;ul>
&lt;li>Stage 0: 使用 SigLIP 损失函数来继续训练 ViT&lt;/li>
&lt;li>Stage 1: cross-modal Alignment，仅训练 MLP&lt;/li>
&lt;li>Stage 2: multi-task pre-training, 解冻所有参数，使用 Multi-task 数据来训练模型&lt;/li>
&lt;li>Stage 3: annealing, 在高质量数据集上进行 fine-tune，进一步提升模型的能力&lt;/li>
&lt;/ul>
&lt;p>作者发现，预训练后的模型在下游任务上的表现对训练数据配比非常敏感。为了解决这个问题，在最后一个训练阶段，作者使用了一个 merging 的技巧，来保持模型的能力。&lt;/p>
&lt;h3 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h3>&lt;p>post-training 阶段一共包含了 2 个 step, 5 个 stage, 第一个 step 包含 2 个 stage，用于提升模型的 non-reasoning 能力。第二个 step 包含 3 个 stage, 用于提升模型的 reasoning 能力&lt;/p>
&lt;h4 id="no-reasoning-training">&lt;a href="#no-reasoning-training" class="header-anchor">&lt;/a>No-reasoning Training
&lt;/h4>&lt;p>第一个 step 是 non-reasoning training, 包含了 SFT 和 MPO 两个 stage, 训练 pipeline 如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl/Keye-VL-non-reasoning-training.png"
width="1340"
height="472"
loading="lazy"
alt="Non reasoning training pipeline"
class="gallery-image"
data-flex-grow="283"
data-flex-basis="681px"
>&lt;/p>
&lt;p>&lt;strong>SFT&lt;/strong>
SFT 阶段一共使用了 &lt;strong>5M&lt;/strong> 的多模态 QA 样本，为了提升数据的多样性，作者使用 TaskGalaxy 来将数据分类为 70,000 种任务类型，然后作者对每条数据，使用 MLLM 评估问题的难度，来过滤掉过于简单的问题。最后，作者请人类来进行标注，保证数据的可靠性。&lt;/p>
&lt;p>&lt;strong>MPO&lt;/strong>
训练方面，作者使用了 MPO 进行训练。
数据方面，作者使用了：&lt;/p>
&lt;ol>
&lt;li>400,000 开源的数据，作者主要进行了去重以及过滤低质量的数据&lt;/li>
&lt;li>50,000 偏好数据，基于 MM-RLHF 和 MMPR 等数据构建，然后构建高质量的 negative examples&lt;/li>
&lt;li>10,000 条 self-improvement 样本：基于 benchmark 和人类反馈，使用 SFT model 的回答作为 chosen samples, 然后基于 reward model 或者 rule-based rewards 评估模型输出，选择分数最低的座位 rejected samples&lt;/li>
&lt;li>90,000 纯文本样本： in-house data&lt;/li>
&lt;li>30,000 人类标注样本：使用开源和闭源模型进行回答，然后请人类进行排序&lt;/li>
&lt;/ol>
&lt;h4 id="reasoning-training">&lt;a href="#reasoning-training" class="header-anchor">&lt;/a>Reasoning Training
&lt;/h4>&lt;p>第二个 step 是 reasoning training, 包含了 CoT Cold-Start, Mix-Mode RL 和 Iterative Alignment 三个 stage, 训练 pipeline 如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl/Keye-VL-reasoning-training.png"
width="1358"
height="596"
loading="lazy"
alt="Non reasoning training pipeline"
class="gallery-image"
data-flex-grow="227"
data-flex-basis="546px"
>&lt;/p>
&lt;p>&lt;strong>CoT cold-start&lt;/strong>
作者收集了如下数据：&lt;/p>
&lt;ul>
&lt;li>330, 000 条 non-reasoning 样本：和第一个 step 的数据分布类似，但是不重合&lt;/li>
&lt;li>230,000 reasoning 样本：作者构建了一个 data construction pipeline,用于保证 long-CoT 的正确性&lt;/li>
&lt;li>20,000 automatic reasoning 样本：作者基于 MMPR, MM-Eureka 以及 OpenR1-math 来收集数据，基于这些模型来训练模型自动化决定是否要进行 reasoning&lt;/li>
&lt;li>100,000 agentic reasoning 样本：训练模型的 &amp;ldquo;think with image&amp;rdquo; 能力。基于 3M QA pairs，作者使用 Qwen2.5-72B 来识别需要进行 manipulating 的问题，然后生成对应的代码。接下来作者构建了一部分 OCR 数据，让模型学会 constrast enhancement 或者 rotation 操作。最后对于数学问题，作者让 Gemini-2.5-Pro 生成对应的思考过程，再让 GPT-4o 将对应的计算转换为可执行的代码。&lt;/li>
&lt;li>32, 000 Video data: 包含了 24,000 条 thinking samples 和 80,000 条 non-thinking samples&lt;/li>
&lt;/ul>
&lt;p>训练时，所有样本混在一起进行训练。作者认为，将日常使用的 SFT 数据和 reasoning 数据放在一起训练，可以保持模型在通用场景下的能力。&lt;/p>
&lt;p>&lt;strong>Mix-Mode RL&lt;/strong>
训练数据主要包括 4 个任务：&lt;/p>
&lt;ol>
&lt;li>Multimodal perception: 复杂文本识别和 counting 任务&lt;/li>
&lt;li>Multimodal reasoning: MMPR 和 MM-Eureka&lt;/li>
&lt;li>Text-based mathematical reasoning: 数学推理问题&lt;/li>
&lt;li>Agentic reasoning: 从 DeepEyes 中获取的 47,000 条样本&lt;/li>
&lt;/ol>
&lt;p>作者使用了 GRPO 来训练，reward 基于 MLLM 进行，包括最终结果和思考过程。&lt;/p>
&lt;p>作者还使用 RL 来提高模型的短视频理解能力。作者发现 RL 训练之后，模型的短视频理解能力有了大幅度的提升。&lt;/p>
&lt;p>&lt;strong>Iterative Alignment&lt;/strong>
这一步主要解决模型的重复性输出，或者 reasoning logic 不对的问题。作者使用了 rejection-sampling 数据，包括 instruction following, OCR, mathematics, charts, counting 等。&lt;/p>
&lt;p>作者基于 Rule-based score 和 model-based score 来进行打分，最后使用 MPO 算法进行训练。通过这个过程，模型的输出格式和动态思考能力都有了提升。&lt;/p>
&lt;h2 id="evaluation">&lt;a href="#evaluation" class="header-anchor">&lt;/a>Evaluation
&lt;/h2>&lt;p>作者首先评估了 ViT 的表现，主要有两点：&lt;/p>
&lt;ol>
&lt;li>在 SigLIP 的基础上加入 1D interpolation 之后，模型的表现所有下降，作者认为这是由于 1D 的 position encoding 无法识别 2D 的 patch 排列导致的&lt;/li>
&lt;li>加入 2D RoPE 之后，ViT 与 SigLIP 的表现持平&lt;/li>
&lt;/ol>
&lt;p>接下来是 Keye-VL 在公开 benchmark 上的表现，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl/Keye-VL-performance.png"
width="1068"
height="1161"
loading="lazy"
alt="Performance of Keye-VL"
class="gallery-image"
data-flex-grow="91"
data-flex-basis="220px"
>&lt;/p>
&lt;p>作者还构建了一个内部的 benchmark, 用于进一步评估模型的能力。&lt;/p>
&lt;p>已有 benchmark 的问题：&lt;/p>
&lt;ol>
&lt;li>contamination&lt;/li>
&lt;li>多语种覆盖不足：大部分 benchmark 都是英文的&lt;/li>
&lt;li>任务和 domain 覆盖不足：大部分 benchmark 只考虑基本的 perception 和 reasoning 能力&lt;/li>
&lt;li>任务难度和评估格式单调&lt;/li>
&lt;/ol>
&lt;p>构建 benchmark 的原则：&lt;/p>
&lt;ol>
&lt;li>在中文场景下的真实用户需求，open ended QA, 包括短视频理解能力&lt;/li>
&lt;li>细粒度的评估&lt;/li>
&lt;li>多样性高&lt;/li>
&lt;li>没有 contamination&lt;/li>
&lt;li>多角度评估策略: 正确性，相关性，理解性，流畅性和创造性&lt;/li>
&lt;/ol>
&lt;p>结果如下：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-keye-vl/Keye-VL-performance-internal.png"
width="1339"
height="559"
loading="lazy"
alt="Performance of Keye-VL on the internal benchmark"
class="gallery-image"
data-flex-grow="239"
data-flex-basis="574px"
>&lt;/p>
&lt;p>分析：&lt;/p>
&lt;ol>
&lt;li>Keye-VL 在 OCR 等任务上的表现有所不足，其细粒度的识别能力也有所不足，会认错人。有时候还会忽略掉一些信息&lt;/li>
&lt;li>描述 temporal action 时会出现不稳定性，模型对镜头的感知能力不足。需要准确定位时间等&lt;/li>
&lt;li>在需要逻辑链条和数学计算时，模型能力不足，对于特定 domain 上的任务会出现事实性错误。写作时，模型倾向于输出通用的回答，而不是定制化的回答。&lt;/li>
&lt;/ol>
&lt;h2 id="discussion">&lt;a href="#discussion" class="header-anchor">&lt;/a>Discussion
&lt;/h2>&lt;p>作者讨论了两点关键发现：&lt;/p>
&lt;ol>
&lt;li>reasoning 和 non-reasoning 的数据可以互相促进彼此的表现，这与 ERNIE 4.5 的发现一致。&lt;/li>
&lt;li>作者认为通过 mix-mode 的训练，模型在简单和复杂任务上的表现都可以提升，因此作者使用了混合数据来进行训练，结果发现效果很好。&lt;/li>
&lt;/ol>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>本文中，作者提出了 Keye-VL 8B，一个短视频理解能力出色的多模态大模型，作者详细介绍了 pre-training 和 post-training. 其中，mix-mode training 可以有效提高模型的表现。&lt;/p>
&lt;p>作者认为 Keye-VL 有如下改进的地方：&lt;/p>
&lt;ol>
&lt;li>并没有优化 video encoder 或者是改进 video encoding 的策略&lt;/li>
&lt;li>Keye-VL 的视觉感知能力有进一步的提升空间，其 &amp;ldquo;reasoning with image&amp;rdquo; 能力依然落后于领先的 reasoning model&lt;/li>
&lt;li>使用一个额外的 MLLM 作为 reward model 会极大消耗算力，如何构建一个更可靠更高效的 reward model 需要进一步探索。&lt;/li>
&lt;/ol>
&lt;h2 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2507.01949" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/Kwai-Keye/Keye/tree/main" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Seed1.6</title><link>https://maosong.website/p/notes-on-seed1.6/</link><pubDate>Fri, 18 Jul 2025 14:59:35 +0800</pubDate><guid>https://maosong.website/p/notes-on-seed1.6/</guid><description>&lt;p>Seed 在 2025 年 6 月 25 号发布了 Seed 1.6，支持 adaptive deep thinking, multimodal understanding, GUI-based interaction, and deep reasoning with a 256K context window&lt;/p>
&lt;p>Seed 1.6 基于 Seed 1.5 开发，是一个 MoE 模型, 总参数为 230B, 激活参数为 23B.&lt;/p>
&lt;h2 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h2>&lt;p>pre-training 分为 3 个 stage:&lt;/p>
&lt;ol>
&lt;li>Text-only pre-training: 使用 web pages, books, papers, code 以及其他数据来进行训练，作者使用了 rule-based 以及 model-based 策略来提高数据的质量。&lt;/li>
&lt;li>Multimodal mixed continue training: 提高学科，代码以及 reasoning 相关的数据，来进一步提高知识密度以及 reasoning 的复杂度。同时，加入高质量的视觉数据。&lt;/li>
&lt;li>Long-context continual training: 将模型的上下文长度从 32K 扩展到 256K&lt;/li>
&lt;/ol>
&lt;p>Seed 1.6 Base 的表现&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Seed1.6 Base&lt;/th>
&lt;th>LLaMA-4 -Maverick Base&lt;/th>
&lt;th>DeepSeek-V3Base&lt;/th>
&lt;th>Qwen3-235B-A22B Base&lt;/th>
&lt;th>Seed1.5 Base&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>MMLU&lt;/td>
&lt;td>&lt;strong>88.83&lt;/strong>&lt;/td>
&lt;td>85.16&lt;/td>
&lt;td>87.19&lt;/td>
&lt;td>87.81&lt;/td>
&lt;td>88.35&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MMLU-Pro&lt;/td>
&lt;td>&lt;strong>69.98&lt;/strong>&lt;/td>
&lt;td>63.91&lt;/td>
&lt;td>59.84&lt;/td>
&lt;td>68.18&lt;/td>
&lt;td>66.47&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SuperGPQA&lt;/td>
&lt;td>&lt;strong>45.08&lt;/strong>&lt;/td>
&lt;td>40.85&lt;/td>
&lt;td>41.53&lt;/td>
&lt;td>44.06&lt;/td>
&lt;td>36.81&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>BBH&lt;/td>
&lt;td>&lt;strong>92.08&lt;/strong>&lt;/td>
&lt;td>83.62&lt;/td>
&lt;td>86.22&lt;/td>
&lt;td>88.87&lt;/td>
&lt;td>88.36&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GPQA-Diamond&lt;/td>
&lt;td>43.43&lt;/td>
&lt;td>43.94&lt;/td>
&lt;td>41.92&lt;/td>
&lt;td>&lt;strong>47.47&lt;/strong>&lt;/td>
&lt;td>45.25&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GSM8k&lt;/td>
&lt;td>93.10&lt;/td>
&lt;td>87.72&lt;/td>
&lt;td>87.57&lt;/td>
&lt;td>&lt;strong>94.39&lt;/strong>&lt;/td>
&lt;td>89.99&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MATH&lt;/td>
&lt;td>&lt;strong>72.86&lt;/strong>&lt;/td>
&lt;td>63.32&lt;/td>
&lt;td>62.62&lt;/td>
&lt;td>71.84&lt;/td>
&lt;td>66.18&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MBPP&lt;/td>
&lt;td>&lt;strong>83.60&lt;/strong>&lt;/td>
&lt;td>75.40&lt;/td>
&lt;td>74.20&lt;/td>
&lt;td>81.40&lt;/td>
&lt;td>81.40&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h2>&lt;p>作者在 post-training 构建了两个变体：&lt;/p>
&lt;ol>
&lt;li>Seed1.6-Thinking: 先进的多模态 reasoning 能力&lt;/li>
&lt;li>Seed1.5(Adaptive CoT): 使用 AdaCoT 来压缩 CoT 的长度，提高效率&lt;/li>
&lt;/ol>
&lt;h3 id="seed16-thinking">&lt;a href="#seed16-thinking" class="header-anchor">&lt;/a>Seed1.6-Thinking
&lt;/h3>&lt;p>Seed1.6-Thinking 与 Seed-Thinking-v1.5 的训练方式差不多，作者使用了 multi-stage [[rejection fine-tuning]] (RFT) 和 RL 来提高模型的表现。在每一轮中，先进行 RL 的训练，然后进行 RFT 的训练。相比于 Seed-Thinking-v1.5, Seed1.6-Thinking 使用了更多的算力和更高质量的数据，包括数学问题，代码，谜题以及 non-reasoning 数据。&lt;/p>
&lt;p>表现如下：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-seed1.6/Seed1_6_thinking_LLM_performance.png"
width="5436"
height="2498"
loading="lazy"
alt="Seed1.6-Thinking performance (LLM)"
class="gallery-image"
data-flex-grow="217"
data-flex-basis="522px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-seed1.6/Seed1_6_thinking_VLM_performance.png"
width="5255"
height="2514"
loading="lazy"
alt="Seed1.6-Thinking performance (VLM)"
class="gallery-image"
data-flex-grow="209"
data-flex-basis="501px"
>&lt;/p>
&lt;p>为了进一步提高模型的表现，作者在推理时使用了 parallel decoding, 来让模型在回答之前使用更多的 token, 作者发现 parallel decoding 可以显著提高模型在困难任务上的表现。&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-seed1.6/Seed1_6_thinking_parallel_decoding_performance.png"
width="4468"
height="1468"
loading="lazy"
alt="Performance of Seed1.6 Thinking parallel decoding"
class="gallery-image"
data-flex-grow="304"
data-flex-basis="730px"
>&lt;/p>
&lt;h3 id="seed15adaptive-cot">&lt;a href="#seed15adaptive-cot" class="header-anchor">&lt;/a>Seed1.5(Adaptive CoT)
&lt;/h3>&lt;p>reasoning model 的问题是会出现 overthinking,导致输出的过程中花费很多不必要的 token.&lt;/p>
&lt;p>为了解决这个问题，Seed1.6 提出了 AdaCoT, 来训练得到三个模型：FullCoT, NoCoT 和 AdaCoT&lt;/p>
&lt;p>通过 AdaCoT, 我们可以显著降低 CoT 的长度。为了完成这个目标，作者在 RL 训练阶段构建了一个 reward, 来惩罚模型的 overthinking. 用户可以基于 prompt 来选择不同的模型：&lt;/p>
&lt;ul>
&lt;li>FullCoT: 对每个 prompt，都进行 reasoning, 模型表现与 Seed1.6-Thinking 差不多，但是 CoT length 更小&lt;/li>
&lt;li>NoCoT: 直接回答，不进行 reasoning&lt;/li>
&lt;li>AdaCoT: 由模型自主决定是否需要进行 reasoning, 模型是 FullCoT 和 NoCoT 的一个 fusion 版本&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-seed1.6/Seed1_6_AdaCoT_performance.png"
width="6798"
height="3416"
loading="lazy"
alt="Seed1.6-AdaCoT performance"
class="gallery-image"
data-flex-grow="199"
data-flex-basis="477px"
>&lt;/p>
&lt;p>结果发现，对于不同难度的任务，AdaCoT 的出发概率是不同的，说明模型会基于任务难度调整思考方式。&lt;/p>
&lt;p>作者还评测了以下模型在高考中的表现，结果如下图：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-seed1.6/Seed1_6_gaokao_performance.png"
width="5195"
height="4645"
loading="lazy"
alt="Seed1.6 performance on gaokao"
class="gallery-image"
data-flex-grow="111"
data-flex-basis="268px"
>&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>作者提出了 Seed1.6 系列多模态大模型，模型在多个 benchmark 上表现优异。作者还提出了 AdaCoT, 让模型基于问题难度动态选择 CoT length&lt;/p>
&lt;p>作者认为下一步工作是：&lt;/p>
&lt;ol>
&lt;li>构建更加高效的架构，来进一步提高 reasoning 的有效性&lt;/li>
&lt;li>扩展模型的多模态能力&lt;/li>
&lt;li>提升模型的 agent 能力，特鄙视 end-to-end task execution&lt;/li>
&lt;/ol>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://seed.bytedance.com/en/blog/introduction-to-techniques-used-in-seed1-6" target="_blank" rel="noopener"
>blog&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on V-Triune</title><link>https://maosong.website/p/notes-on-v-triune/</link><pubDate>Thu, 17 Jul 2025 09:37:36 +0800</pubDate><guid>https://maosong.website/p/notes-on-v-triune/</guid><description>&lt;p>V-Triune 探究了如何使用一套统一的 RL 训练框架，来同时提高模型的 perception 和 reasoning 能力。&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>已有的 RL 训练方法主要提升模型在特定 domain 上的能力，比如 MM-Eureka, Vision-R1 等专注于提升 MLLM 的 reasoning 能力，而 R1-V, DeepPerception 等专注于提升模型的 perception 能力。&lt;/p>
&lt;p>因此，本文的 motivation 就是，如何用一套统一的 RL 训练框架，来同时提高模型的 perception 和 reasoning 能力。&lt;/p>
&lt;p>V-Triune 包含 3 个关键模块：&lt;/p>
&lt;ul>
&lt;li>Sample-Level data formatting: 即在 sample 的层面定义 reward 等信息&lt;/li>
&lt;li>Verifier-Level Computation: 即分离 verifier 和 generator, 提高整体效率&lt;/li>
&lt;li>Source-Level Metric Monitoring: 基于 data source 来监控模型的表现&lt;/li>
&lt;/ul>
&lt;p>作者还提出了 Dynamic IoU reward, 来提高训练的稳定性。&lt;/p>
&lt;p>基于 V-Triune 和 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5-vl/" target="_blank" rel="noopener"
>Qwen2.5-VL&lt;/a>, 作者提出了 Orsta,一个在 reasoning 和 perception 任务上均有提升的多模态大模型。&lt;/p>
&lt;p>论文的主要贡献为：&lt;/p>
&lt;ol>
&lt;li>提出了 V-Triune, 一个统一的 RL 训练框架，来同时提高模型的 reasoning 和 perception 能力&lt;/li>
&lt;li>在 infra 上进行了改进，提高整体的训练效率和稳定性&lt;/li>
&lt;li>基于 V-Triune, 构建了 Orsta, 相比于 Baseline, 模型的 perception 和 reasoning 能力均有了提升。&lt;/li>
&lt;/ol>
&lt;h2 id="preliminary">&lt;a href="#preliminary" class="header-anchor">&lt;/a>Preliminary
&lt;/h2>&lt;p>作者首先回顾了一下 GRPO 和 &lt;a class="link" href="https://maosong.website/p/notes-on-dapo/" target="_blank" rel="noopener"
>DAPO&lt;/a> , 我们这里就不再赘述。&lt;/p>
&lt;p>然后，作者介绍了一下 reward function, reward function 由两部分组成， 第一部分是 format reward, 第二部分是 accuracy reward,.&lt;/p>
&lt;p>对于 format reward, 作者采取了和 OpenR1 相同的做法，也就是要求输出仅包含 1 个给定的 token, 这里特定的 token 为 $s_1=\texttt{&lt;think>}$, $s_2=\texttt{&lt;/think>}$, $s_3=\texttt{&lt;answer>}$, $s_4=\texttt{&lt;/answer>}$, reward 的定义如下：&lt;/p>
$$
R_{\mathrm{format}}(o_q) = 0.25\sum_{i=1}^4 \mathbb{1}(\mathrm{count}(o_q,s_i)=1)
$$&lt;p>这里 $\mathbb{1}(\cdot)$ 是示性函数。&lt;/p>
&lt;p>对于 accuracy reward, 作者根据不同的任务分别进行处理。&lt;/p>
&lt;p>对于 reasoning 任务，reward function 的定义如下&lt;/p>
$$
R_{\mathrm{acc}}(\hat{a},a) = \begin{cases}
1, &amp;\text{ if }\texttt{verify(parse($\hat{a}$), parse($a$))}\text{ is True}\\
0, &amp;\text{ else}
\end{cases}
$$&lt;p>这里 $\hat{a}$, $a$ 分别是 prediction 和 ground truth.&lt;/p>
&lt;p>对于 perception 任务，reward function 与我们要处理的子任务相关。如果是 OCR 和 counting 任务，则我们采取和 reasoning 一样的 reward function. 如果我们要处理的是 grounding 和 detection 任务，则我们可以使用 IoU 以及 mAP 两种 reward 形式。&lt;/p>
&lt;p>IoU reward 的定义如下：&lt;/p>
$$
R_{\mathrm{acc}}(\hat{a},a) = \begin{cases}
\mathrm{IoU}(\hat{a}, a), &amp;\text{ if }\mathrm{IoU}(\hat{a},a)\geq \epsilon\\
0, &amp;\text{ else}
\end{cases}
$$&lt;p>这里 $\epsilon>0$ 为超参数， IoU 定义如下&lt;/p>
$$
\mathrm{IoU}(\hat{a},a) = \frac{\mathrm{Area}(\hat{a}\cap a)}{\mathrm{Area}(\hat{a}\cup a)}
$$&lt;p>mAP 的定义如下&lt;/p>
$$
\mathrm{AP}_c = \int_0^1 \max_{\tilde{r}\geq r}P_c(\tilde{r}) dr,\quad mAP=\frac1C\sum_{c=1}^C \mathrm{AP}_c
$$&lt;p>最终的 reward 是 format reward 和 accuracy reward 的加权求和：&lt;/p>
$$
R = \alpha_{\mathrm{acc}}R_{\mathrm{acc}} + \alpha_{\mathrm{format}}R_{\mathrm{format}}
$$&lt;h2 id="v-triune">&lt;a href="#v-triune" class="header-anchor">&lt;/a>V-Triune
&lt;/h2>&lt;p>V-Triune 框架如下图所示，其包含三个模块&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-v-triune/V-Triune_framework.png"
width="1351"
height="562"
loading="lazy"
alt="V-Triune System"
class="gallery-image"
data-flex-grow="240"
data-flex-basis="576px"
>&lt;/p>
&lt;h3 id="sample-level-data-formatting">&lt;a href="#sample-level-data-formatting" class="header-anchor">&lt;/a>Sample-Level Data Formatting
&lt;/h3>&lt;p>核心思想就是不同的任务的 reward function 可能是不一样的，如果在 task 层面设计 reward function 的话，RL 训练就没有了 flexibility, 因此，在本文中，作者在 sample 层面设计 reward function. 具体做法就是，每个样本除了 QA 相关信息，还有 reward model 的相关参数，这样就可以更加灵活地调整不同 sample 的损失了。&lt;/p>
&lt;h3 id="verifier-level-reward-computation">&lt;a href="#verifier-level-reward-computation" class="header-anchor">&lt;/a>Verifier-Level Reward Computation
&lt;/h3>&lt;p>核心思想就是构建多个不同的 Verifier, 来分别处理不同的 sample, 因为我们 reward model 也是在 sample 层面构建的，因此我们可以基于 sample 的 metadata 来动态分配对应的 verifier, 这样就提升了 veriier 的可扩展性。本文作者使用了 MathVerifier 和 DetectionVerifier 两种。&lt;/p>
&lt;h3 id="source-level-metric-monitoring">&lt;a href="#source-level-metric-monitoring" class="header-anchor">&lt;/a>Source-Level Metric Monitoring
&lt;/h3>&lt;p>核心思想就是根据 sample 的 metadata 信息来监控不同的 metric, 这样可以防止训练不稳定。&lt;/p>
&lt;h3 id="dynamic-iou-reward">&lt;a href="#dynamic-iou-reward" class="header-anchor">&lt;/a>Dynamic IoU Reward
&lt;/h3>&lt;p>核心思想就是根据训练进程，动态调整 IoU 的阈值。&lt;/p>
&lt;p>作者发现，如果将 IoU 阈值设置的比较低的话，模型很容易拿到比较高的 reward; 如果将 IoU 阈值设置的比较高的话，模型又很难拿到奖励。&lt;/p>
&lt;p>因此，作者的解决方案就是，使用课程学习，随着训练的进行，逐步提高 IoU 的阈值。&lt;/p>
&lt;h2 id="data">&lt;a href="#data" class="header-anchor">&lt;/a>Data
&lt;/h2>&lt;p>数据集列举如下：&lt;/p>
&lt;ul>
&lt;li>Math: mm_math, geometry3K, mmk12&lt;/li>
&lt;li>Puzzle: PuzzleVQA, AlgoPuzzleQA, VisualPuzzles&lt;/li>
&lt;li>Science: ScienceQA, SciVQA, ViRL39K (Broader STEM topics, (GradeSchool) Science)&lt;/li>
&lt;li>Chart: ChartQAPro, ChartX, Table-VQA, ViRL39K (Tables/Diagrams/Charts)&lt;/li>
&lt;li>Detection: V3Det, Object365&lt;/li>
&lt;li>Grounding: D3&lt;/li>
&lt;li>Counting: CLEVR&lt;/li>
&lt;li>OCR: LLaVA-OV (OCR questions), EST-VQA&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Rule-based filtering&lt;/strong>
对于 visual reasoning 数据，作者的过滤如下：&lt;/p>
&lt;ul>
&lt;li>多项选择题以及判断题，防止 reward hacking&lt;/li>
&lt;li>答案包含特殊符号如 &amp;ldquo;=&amp;rdquo;, &amp;ldquo;[&amp;rdquo; 等的数据&lt;/li>
&lt;li>答案字符数超过 20 个，防止答案太复杂&lt;/li>
&lt;/ul>
&lt;p>对于 visual perception 数据，作者过滤流程如下：&lt;/p>
&lt;ul>
&lt;li>Detection: 超过 10 个 bounding box 的，或者 box 占 50% 以上面积的数据，保持 single BB 和 multi BB 的比例为 1:2&lt;/li>
&lt;li>Grounding: box 占 50% 以上面积的数据, label 过于复杂的数据&lt;/li>
&lt;li>Counting: 保持类别平衡，仅使用英文数据&lt;/li>
&lt;li>OCR: 保留英文数据，对 labels 进行 verify&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Difficulty-based filtering&lt;/strong>
主要是移除比较简单的问题。&lt;/p>
&lt;p>经过这两个阶段的过滤之后，得到了 &lt;strong>20.6K&lt;/strong> perception samples 以及 &lt;strong>27.1K&lt;/strong> reasoning samples. 数据保存在 Parquet 格式&lt;/p>
&lt;h2 id="training-recipe">&lt;a href="#training-recipe" class="header-anchor">&lt;/a>Training Recipe
&lt;/h2>&lt;h3 id="disable-vit-training">&lt;a href="#disable-vit-training" class="header-anchor">&lt;/a>Disable ViT Training
&lt;/h3>&lt;p>作者首先发现，全量微调会导致训练崩溃。作者分析原因发现，主要是 ViT 在反向传播过程中，存在 gradient amplify 的问题，第一层与最后一层的梯度的 norm 相差 5-10 倍。作者认为有两点原因：&lt;/p>
&lt;ol>
&lt;li>RL 会强制要求模型的不同模态之间进行对齐&lt;/li>
&lt;li>对比学习训练得到的 VIT 可能使得其不适合 RL 的训练。&lt;/li>
&lt;/ol>
&lt;p>基于这两点考虑，作者再进行训练的时候，冻结了 ViT 模块，结果如下图&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-v-triune/V-Triune-frozen-ViT.png"
width="1383"
height="705"
loading="lazy"
alt="Analysis of ViT training instability"
class="gallery-image"
data-flex-grow="196"
data-flex-basis="470px"
>&lt;/p>
&lt;h3 id="spurious-image-special-tokens">&lt;a href="#spurious-image-special-tokens" class="header-anchor">&lt;/a>Spurious Image Special Tokens
&lt;/h3>&lt;p>作者发现，模型的输出可能会包含一些 &lt;code>&amp;lt;image_pad&amp;gt;&lt;/code> 等 token, 因此，作者对输出进行了过滤，然后再进行计算。&lt;/p>
&lt;h3 id="cot-prompt-pool">&lt;a href="#cot-prompt-pool" class="header-anchor">&lt;/a>CoT Prompt Pool
&lt;/h3>&lt;p>作者还发现，多样化的 CoT prompt 会损害模型的表现。因此，作者构建了 20 个 prompt 模版，每次随机挑选其中一个加入到 instruction 中。&lt;/p>
&lt;h3 id="training-configuration">&lt;a href="#training-configuration" class="header-anchor">&lt;/a>Training Configuration
&lt;/h3>&lt;p>模型基于 Qwen2.5-7B-instruct 和 Qwen2.5VL-32B 来进行开发&lt;/p>
&lt;p>作者探究了 on-policy 和 off-policy 两种配置。roll-out batch size 设置为 1024, 使用 GRPO 进行训练，GRPO 的 group size 设置为 8&lt;/p>
&lt;p>作者还设置 $\epsilon_{high}=0.28$ 以及 $\epsilon_{low}=0.2$ 来提高 token 的采样率。作者去除了 KL divergence loss, 并且在 token 层面上进行平均&lt;/p>
&lt;h2 id="evaluation">&lt;a href="#evaluation" class="header-anchor">&lt;/a>Evaluation
&lt;/h2>&lt;h3 id="performance">&lt;a href="#performance" class="header-anchor">&lt;/a>Performance
&lt;/h3>&lt;p>&lt;img src="https://maosong.website/p/notes-on-v-triune/V-Triune-performance.png"
width="1159"
height="1051"
loading="lazy"
alt="Performance of Orsta on MEGA-Bench core"
class="gallery-image"
data-flex-grow="110"
data-flex-basis="264px"
>&lt;/p>
&lt;h3 id="analysis">&lt;a href="#analysis" class="header-anchor">&lt;/a>Analysis
&lt;/h3>&lt;p>&lt;strong>on-policy v.s. off-policy&lt;/strong>
作者首先对比了以下 on-policy RL 和 off-policy RL 两种训练方式的表现， 结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-v-triune/V-triune-on-policy.png"
width="1137"
height="526"
loading="lazy"
alt="Ablation on on-policy v.s. off-policy"
class="gallery-image"
data-flex-grow="216"
data-flex-basis="518px"
>&lt;/p>
&lt;p>结果有两点发现：&lt;/p>
&lt;ol>
&lt;li>on-policy 的表现基本上都是比 off-policy 要好的。&lt;/li>
&lt;li>小模型通过 RL 带来的表现提升更明显，大模型的表现则更慢&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>generalization&lt;/strong>
作者还评估了以下模型的泛化性，结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-v-triune/V-Triune-generalization.png"
width="422"
height="673"
loading="lazy"
alt="Generalization of V-Triune"
class="gallery-image"
data-flex-grow="62"
data-flex-basis="150px"
>&lt;/p>
&lt;p>结果发现，RL 的训练更像是对齐，而不是泛化。也就是说，模型在 in-domain 的任务上表现较好，在 OOD 的任务上表现就一般。这与已有的结论是不一样的&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Recall
&lt;a class="link" href="https://maosong.website/p/notes-on-magistral/" target="_blank" rel="noopener"
>Magistral&lt;/a> 发现模型在 math domain 上进行训练，在 code domain 上的表现也会提升&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Training Dynamics&lt;/strong>
作者还分析了不同任务不同指标随训练进行的变化情况，如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-v-triune/V-Triune-training-dynamics.png"
width="1150"
height="900"
loading="lazy"
alt="V-Triune training dynamics"
class="gallery-image"
data-flex-grow="127"
data-flex-basis="306px"
>&lt;/p>
&lt;p>结果显示，reasoning 和 OCR 任务岁训练进行其输出长度和正确率都有所提升。但是 detection 相关任务则变化不是很明显。&lt;/p>
&lt;h3 id="ablation-study">&lt;a href="#ablation-study" class="header-anchor">&lt;/a>Ablation Study
&lt;/h3>&lt;p>作者进行了三个消融实验：&lt;/p>
&lt;ol>
&lt;li>训练策略：冻结 ViT, 冻结 LLM, 两者都不冻结&lt;/li>
&lt;li>任务分解策略：仅使用 reasoning 数据进行训练，仅使用 perception 数据进行训练，同时使用两种数据进行训练。&lt;/li>
&lt;li>learning rate: 不同的学习率&lt;/li>
&lt;/ol>
&lt;p>实验结果如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-v-triune/V-Triune-ablation-study.png"
width="1146"
height="585"
loading="lazy"
alt="Ablation study"
class="gallery-image"
data-flex-grow="195"
data-flex-basis="470px"
>&lt;/p>
&lt;p>结果发现，&lt;/p>
&lt;ol>
&lt;li>仅训练 ViT 对表现没有任何帮助，只有训练 LLM 才会带来性能的提升。&lt;/li>
&lt;li>同时使用两种数据进行训练的效果是最好的，其次是仅使用 reasoning 数据进行训练，最后是仅使用 perception 数据进行训练&lt;/li>
&lt;li>较大的学习率会损害模型的表现。&lt;/li>
&lt;/ol>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>本文中，作者提出了 V-Triune, 一个使用 RL 来同时提高 MLLM 的 reasoning 和 perception 能力的 RL 训练框架。&lt;/p>
&lt;p>作者发现 RL 对于 VLM 来说更像是一种 Alignment 策略，而不是一种 Generalization 策略，也就是模型并没有引入新的能力，而是提升模型本身的 utility 以及 robustness.&lt;/p>
&lt;p>作者认为本文有以下 Limitation:&lt;/p>
&lt;ol>
&lt;li>在 perception 任务上没有看到明显的 test-time scaling 现象。作者认为，探究使用 o3 类似的方法或许能提高模型的表现&lt;/li>
&lt;li>探究 RL-zero 在 VLM 中的应用，也就是直接在 base model 上进行 RL 的训练。&lt;/li>
&lt;/ol>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2505.18129" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on GLM-4.1V-Thinking</title><link>https://maosong.website/p/notes-on-glm-4.1v-thinking/</link><pubDate>Mon, 14 Jul 2025 10:32:04 +0800</pubDate><guid>https://maosong.website/p/notes-on-glm-4.1v-thinking/</guid><description>&lt;p>智谱 AI 在 25 年 7 月份发布了 GLM-4.1V-Thinking, 一个 9B 的多模态大语言模型，其在多个 benchmark 上达到了相同大小 MLLM 的 SOTA&lt;/p>
&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>已有工作如 &lt;a class="link" href="https://maosong.website/p/notes-on-mimo-vl/" target="_blank" rel="noopener"
>MiMo-VL&lt;/a> 和 V-Triune 主要集中在特定的领域，目前还没有一个在所有领域都能超过 non-reasoning model 表现的多模态 reasoning model.&lt;/p>
&lt;p>基于这个目标，作者提出了 GLM-4.1V-Thinking, 一个 9B 的，用于通用领域多模态 reasoning 的 MLLM. 在预训练阶段，作者通过构建多样化的数据来提升模型的基础能力。在 post-training 阶段，作者构建了 domain-specific 的数据来让模型学会 reasoning. 最后，作者提出了 Reinforcement Learning with Curriculum Sampling (RLCS) 来扩展模型的 reasoning 能力。&lt;/p>
&lt;p>作者主要发现如下：&lt;/p>
&lt;ol>
&lt;li>multi-domain RL 的泛化性非常强，在某一个 domain 上进行 RL 训练也可以提高模型在其他 domain 上的表现&lt;/li>
&lt;li>动态选择 RL 训练的数据可以有效提高模型的表现和训练效率&lt;/li>
&lt;li>一个 robust 以及 precise 的 reward model 对于 multi-domain RL 是至关重要的&lt;/li>
&lt;/ol>
&lt;h2 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h2>&lt;h3 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h3>&lt;p>GLM-4.1V-Thinking 是一个标准的 ViT-MLP-LLM 架构，其中：&lt;/p>
&lt;ul>
&lt;li>ViT: ViT 使用的是 AIMv2-Huge&lt;/li>
&lt;li>MLP: 是一个 3 层的 MLP，架构为 &lt;code>linear-LayerNorm-GELU-SwiGLU&lt;/code>, 并且在进入 MLP 之前，GLM-4.1V-Thinking 还会在 spatial 上进行降采样，降采样率为 2&lt;/li>
&lt;li>LLM: LLM 使用的是 GLM&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_Thinking_architecture.png"
width="1804"
height="988"
loading="lazy"
alt="Architecture of GLM4.1V-Thinking"
class="gallery-image"
data-flex-grow="182"
data-flex-basis="438px"
>&lt;/p>
&lt;p>与 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5-vl/" target="_blank" rel="noopener"
>Qwen2.5-VL&lt;/a> 一样，作者将 encoder 的 2D convolution 替换为了 3D convolution，用于处理视频输入，然后对于 single-image inputs，GLM-4.1V-Thinking 会将输入复制，变成一个含有两帧相同图片的 video 输入&lt;/p>
&lt;p>为了支持不同分辨率图片输入，作者进行了两点改进：&lt;/p>
&lt;p>第一是使用了 2D RoPE 来处理不同分辨率的图片输入&lt;/p>
&lt;p>第二是使用 bicubic interpolation 来将不同分辨率的图片适应到 encoder 的 position embedding 上。具体来讲，对一个拥有 $H_p\times W_p$ patches 的图片，每个 patch 的坐标 $(w,h)$ 首先会被 normalized 到 $[-1,1]$ 中：&lt;/p>
$$
g_{norm} = (w_{norm}, h_{norm}) = 2* \left(\frac{w+0.5}{W_p}, \frac{h+0.5}{H_p}\right) - 1
$$&lt;p>然后，我们再使用 bicubic interpolation $\mathcal{I}_{bicubic}$ 将坐标映射到原始的 position embedding 上：&lt;/p>
$$
P_{adapted} = \mathcal{I}_{bicubic}(P_{orig}, g_{norm})
$$&lt;p>对于视频输入，作者在每一帧之后插入了一个 time index token, 用每一帧的 timestamp string 来表示。这个做法与 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen2.5-vl/" target="_blank" rel="noopener"
>Qwen2.5-VL&lt;/a> 一样。&lt;/p>
&lt;h3 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h3>&lt;h4 id="pre-training-data">&lt;a href="#pre-training-data" class="header-anchor">&lt;/a>Pre-training Data
&lt;/h4>&lt;p>Pre-training 数据包括 image-caption data, interleaved image-text data, OCR data, grounding data, video data 和 instruction tuning data&lt;/p>
&lt;p>&lt;strong>Image caption data&lt;/strong>
Image caption data 用于提升模型的世界知识。作者从 LAION, DataComp, DNF 以及 Wukong 等数据集收集了 10B 的 image-text pairs, 然后进行数据清洗：&lt;/p>
&lt;ol>
&lt;li>Heuristic-based filtering: 基于规则，如 resolution, caption length 等过滤掉低质量的图片&lt;/li>
&lt;li>Relevance filtering: 计算 CLIP score, 过滤掉低质量的数据&lt;/li>
&lt;li>Concept-balanced resampling: 基于 MetaCLIP , 来提升数据的覆盖程度，解决长尾分布的问题。这里 &lt;a class="link" href="https://maosong.website/p/notes-on-seed1.5-vl/" target="_blank" rel="noopener"
>Seed1.5-VL&lt;/a> 有一个类似的消融实验。&lt;/li>
&lt;li>Factual-centered re-captioning: 通过 rewrite caption 来提升 caption 的质量和信息密度。Idefics2 通过实验发现，rewriting caption 可以提高模型的表现。作者在最终的数据集里，混合了一部分 re-caption 的数据。&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Interleaved image-text data&lt;/strong>
相比于 image-caption data, interleaved image-text data 体量更大，且逻辑关系更强。但是噪声也更多，因此作者针对不同来源的数据构建了不同的数据清洗策略：&lt;/p>
&lt;ol>
&lt;li>Web data processing pipeline: 数据来源为 MINT, MMC4, OmniCorpus 等。作者首先基于 CLIP-score，去掉与上下文不相关的图片；然后，作者去除掉广告，二维码等低质量的图片；最后，作者将图多文少的样本也给去除掉，比如相册图片。作者还训练了一个 high-knowledge-density image classifier, 来识别信息密度高的样本。&lt;/li>
&lt;li>Academic book processing pipeline: 作者从电子书中收集了 100M 的样本，这些电子书主要是 STEM domain，然后作者构建了一个 PDF parsing tool 来提取文档内容。&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>OCR data&lt;/strong>
OCR 数据包含&lt;strong>220M images&lt;/strong>, 数据集由三部分组成：&lt;/p>
&lt;ol>
&lt;li>Synthetic document images: 基于预训练语料，使用不同的格式来渲染文字生成不同的图片，然后基于 LAION 的图片作为背景&lt;/li>
&lt;li>Natural scene text images: 使用 Paddle-OCR 来从图片中提取文字以及对应的 bounding box&lt;/li>
&lt;li>Academic documents: 使用类似 Nougat 的方法来构建 PDF page-Markdown 的数据。&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Grounding data&lt;/strong>
主要包含自然场景和 GUI 两个 domain 的 grounding 数据：&lt;/p>
&lt;ol>
&lt;li>Natural image grounding: 基于 LAION-115M 得到。作者使用 GLIPv2, 来自动化解析 caption 中的实体，然后预测对应的 bounding box, 作者将 bounding box 较少的样本去除掉。最终得到&lt;strong>40M&lt;/strong>高质量的自然场景数据&lt;/li>
&lt;li>GUI grounding: 基于 CC 提取对应的 url, 然后使用 Playwright 来与网页进行交互，进而获取到所有的 DOM elements 与其对应的 bounding box. 最终收集到 &lt;strong>140M&lt;/strong>高质量的 QA 样本&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Video data&lt;/strong>
Video data 从多个来源得到，作者使用了 fine-grained human annotation 来保证数据质量。作者还标注了 camera motion 等信息&lt;/p>
&lt;p>为了保证数据的质量，作者进行了去重，来减少数据语义的相似性。&lt;/p>
&lt;p>&lt;strong>Instruction tuning data&lt;/strong>
instruction tuning data 的策略如下：&lt;/p>
&lt;ol>
&lt;li>Task coverage and taxonomy: 优化数据分布&lt;/li>
&lt;li>Complex scenario augmentation: 构建复杂的指令跟随数据&lt;/li>
&lt;li>Data contamination check: 数据污染检查&lt;/li>
&lt;/ol>
&lt;p>最终一共收集到 &lt;strong>50M&lt;/strong>样本，覆盖 visual perception, multimodal reasoning, GUI agent 等任务。&lt;/p>
&lt;h4 id="pre-training-training">&lt;a href="#pre-training-training" class="header-anchor">&lt;/a>Pre-training Training
&lt;/h4>&lt;p>Pre-training 由两个 stage 组成：&lt;/p>
&lt;ol>
&lt;li>Multimodal pre-training: 提高模型的通用多模态能力，上下文长度为 8192, global batch size 为 1536. 使用了 data packing 策略，使用了 2-way tensor parallelism&lt;/li>
&lt;li>Long-context continue training: 提升模型在高精度图片，长视频，长上下文场景下的表现，上下文长度为 32768, 使用了 2-way tensor parallelism 和 4-way context parallelism.&lt;/li>
&lt;/ol>
&lt;h3 id="sft">&lt;a href="#sft" class="header-anchor">&lt;/a>SFT
&lt;/h3>&lt;h4 id="sft-data">&lt;a href="#sft-data" class="header-anchor">&lt;/a>SFT Data
&lt;/h4>&lt;p>数据包括 long CoT reasoning 数据，来让模型以标准格式输出 multi-step solution.&lt;/p>
&lt;p>数据包括 verifiable tasks 以及 non-verifiable tasks, 覆盖中英两种语言。作者过滤了非常简单和非常困难的样本。&lt;/p>
&lt;p>数据格式如下&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;lt;think&amp;gt; {think_content} &amp;lt;/think&amp;gt; &amp;lt;answer&amp;gt; {answer_content} &amp;lt;/answer&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>对于 verifiable tasks, 输出格式为&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">&amp;lt;answer&amp;gt; &amp;lt;|begin_of_box|&amp;gt; {answer_content} &amp;lt;|end_of_box|&amp;gt; &amp;lt;/answer&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>数据清洗策略包括：&lt;/p>
&lt;ol>
&lt;li>严格遵守格式要求&lt;/li>
&lt;li>reasoning style 不一致或者有噪声&lt;/li>
&lt;li>混合语言输出。&lt;/li>
&lt;/ol>
&lt;p>作者还使用 RL checkpoints 来生成一部分高质量数据，加入到 SFT 阶段的数据集里面。&lt;/p>
&lt;h4 id="sft-training">&lt;a href="#sft-training" class="header-anchor">&lt;/a>SFT Training
&lt;/h4>&lt;p>全参数微调。上下文长度为 32768， batch size 为 32.数据包括 long-form reasoning data 以及纯文本数据，包括 math, multi-turn conversation, agent planning 以及 instruction following 等.&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Observation
作者发现，在 SFT 阶段即使是使用带噪声的 reasoning data，模型在 RL 阶段也能继续训练。也就是说，不完美的 reasoning trace 也能提供 guidance. 但是，使用更高质量的数据可以让模型 RL 阶段的训练更稳定且表现更好。&lt;/p>
&lt;/blockquote>
&lt;h3 id="rl">&lt;a href="#rl" class="header-anchor">&lt;/a>RL
&lt;/h3>&lt;p>与 &lt;a class="link" href="https://maosong.website/p/notes-on-seed1.5-vl/" target="_blank" rel="noopener"
>Seed1.5-VL&lt;/a> 一样，作者在 RL 阶段结合了 RLVR 和 RLHF 两个训练目标。任务包括 STEM problem solving (such as mathematics, physics, chemistry), grounding, optical character recognition (OCR), video understanding, GUI agents, chart and document understanding, logical reasoning, and instruction following.&lt;/p>
&lt;p>&lt;strong>Data&lt;/strong>
作者首先构建了一个任务集合，然后基于这些任务，来过滤或者生成对应的 QA pair, 接下来作者基于难度和质量来过滤，最后作者在每个 domain 上进行 RL 的训练来保证数据的有效性。&lt;/p>
&lt;p>&lt;strong>Reward modelling&lt;/strong>
作者构建了一个 reward system, 用于给不同 domain 的任务进行奖励。为了探究不同 domain 的 reward 对模型整体表现的影响，作者设计了一个 low-quality reward system，实验结果如下，可以看到模型在训练后期出现了 collapse 或者 reward hacking 的情况。&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_reward_ablation.png"
width="1778"
height="876"
loading="lazy"
alt="GLM-4.1V-Thinking reward ablation"
class="gallery-image"
data-flex-grow="202"
data-flex-basis="487px"
>&lt;/p>
&lt;blockquote>
&lt;p>[!tip] Observation
作者发现，不同任务的不同信号奖励会对模型最终表现产生巨大影响。&lt;/p>
&lt;/blockquote>
&lt;p>在使用 LLM 提取模型回答的 answer 的时候，作者要求模型的最终答案为 &lt;code>&amp;lt;|begin_of_box|&amp;gt; {answer_content} &amp;lt;|end_of_box|&amp;gt;&lt;/code> 这种形式，避免大语言模型提取出错。&lt;/p>
&lt;p>作者构建的 reward system 包含如下模块：&lt;/p>
&lt;ol>
&lt;li>shared verification functions: 包括格式检查等&lt;/li>
&lt;li>domain specific modules: 与 domain 相关的模块&lt;/li>
&lt;li>unit testing: 观测 domain 输出的分布，然后基于输出的分布来调整 reward logic&lt;/li>
&lt;/ol>
&lt;p>最终 reward 的 design 如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_reward_design.png"
width="1812"
height="712"
loading="lazy"
alt="GLM-4.1V-Thinking reward design"
class="gallery-image"
data-flex-grow="254"
data-flex-basis="610px"
>&lt;/p>
&lt;p>&lt;strong>RLCS&lt;/strong>
作者使用了 GRPO 来进行优化训练。作者发现，仅需 200 training steps 模型在一半以上问题的准确率就达到了 $90\%$.&lt;/p>
&lt;p>为了提升模型的训练效率，作者提出了 Reinforcement Learning with Curriculum Sampling (RLCS), 也就是将课程学习与 RL 结合起来。作者基于模型训练情况动态调整训练样本的难度，来保证每个样本对模型训练的提升都是最大的。&lt;/p>
&lt;p>在 RLCS 训练之前，作者先使用模型评测得到每个样本的 pass@k, 然后根据结果将样本进行分类。在训练过程中，作者记录每个样本的 pass@k， 然后动态更新其分类结果。在采样的时候，作者基于难度来对样本进行加权，来降低太简单或者太难样本的采样概率。&lt;/p>
&lt;p>作者还提出了一些提高 RL 表现的方法：&lt;/p>
&lt;ol>
&lt;li>Larger batch size: 使用更大的 batch size 效果更好&lt;/li>
&lt;li>Dynamic sampling expansion via ratio EMA: 作者定义了 naive 样本和高质量样本的比例，然后根据这个比例进行采样，来提升整体的采样效率。&lt;/li>
&lt;li>Force answering: 与 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen3/" target="_blank" rel="noopener"
>Qwen3&lt;/a> 一样，对于比较难的问题，当输出的 token 数过长时，作者对模型输出进行截断，然后让模型基于已有思考过程直接输出结果。&lt;/li>
&lt;li>Discard KL loss: 与 DAPO 一样，作者也移除了 KL divergence loss&lt;/li>
&lt;li>Clip-higher: 与 DAPO 一样，作者通过修改超参数来提高模型的探索能力&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>[!tip] Observation&lt;/p>
&lt;ol>
&lt;li>RL 阶段的表现与 cold-start SFT 的表现并不完全相关。作者发现，cold-start SFT 取得更好的表现并不一定保证 RL 的表现就更好。&lt;/li>
&lt;li>RL 阶段各个 domain 数据的影响是正交的，这与 cold-start SFT 阶段的结果不同。&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>作者还提出了一些提高训练稳定性的方法：&lt;/p>
&lt;ol>
&lt;li>提高 cold-start SFT 阶段数据的质量&lt;/li>
&lt;li>移除 KL divergence loss&lt;/li>
&lt;li>使用 top-p 为 1，而不是 0.9 (如 &lt;a class="link" href="https://maosong.website/p/notes-on-qwen-llm/" target="_blank" rel="noopener"
>Qwen-LLM&lt;/a> 中的设置)，top-p 为 1 可以 cover 整个 vocabulary，避免某些 token 没有参与训练&lt;/li>
&lt;li>per-sample loss 和 per-token loss 没有显著区别，但是 per-sample loss 稳定性更高&lt;/li>
&lt;li>在 cold-start SFT 阶段让模型学会格式要求，而不是在 RL 阶段加入 format reward&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Infra&lt;/strong>
在 infra 方面，作者做了如下改进：&lt;/p>
&lt;ol>
&lt;li>Load balancing of sequence lengths across DP ranks. 平衡每个 rank 的 sequensequence length 以及 compute load&lt;/li>
&lt;li>Intra-rank training with sequence packing and gradient accumulation. 将 sequence packing 和 gradient accumulation 结合起来&lt;/li>
&lt;li>Sample packing and reorganization within DP ranks. data packing&lt;/li>
&lt;li>Dynamic sampling expansion via ratio EMA. 平衡 naive 样本和高质量样本之间的比例&lt;/li>
&lt;/ol>
&lt;h2 id="evaluation">&lt;a href="#evaluation" class="header-anchor">&lt;/a>Evaluation
&lt;/h2>&lt;h3 id="performance">&lt;a href="#performance" class="header-anchor">&lt;/a>Performance
&lt;/h3>&lt;p>&lt;img src="https://maosong.website/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_performance.png"
width="1066"
height="1446"
loading="lazy"
alt="Performance of GLM4.1V Thinking"
class="gallery-image"
data-flex-grow="73"
data-flex-basis="176px"
>&lt;/p>
&lt;p>可以看到，GLM-4.1V-Thinking 在多个 benchmark 上都达到了 SOTA.&lt;/p>
&lt;h3 id="ablation-study">&lt;a href="#ablation-study" class="header-anchor">&lt;/a>Ablation Study
&lt;/h3>&lt;p>作者评估了一下不同 domain 的 RL 训练对模型整体表现的影响。作者使用不同的数据来进行训练，然后分析结果的相关性，结果如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-glm-4.1v-thinking/GLM_4_1_V_cross_domain_generalization.png"
width="1226"
height="962"
loading="lazy"
alt="Cross-domain generalization in reinforcement learning"
class="gallery-image"
data-flex-grow="127"
data-flex-basis="305px"
>&lt;/p>
&lt;p>实验结果发现：&lt;/p>
&lt;ol>
&lt;li>在某个 domain 上的训练会提高模型在其他 domain 上的表现&lt;/li>
&lt;li>联合多个 domain 进行训练最终的表现会更好&lt;/li>
&lt;/ol>
&lt;p>作者还发现，不同的任务之间存在关联，如 GUI-agent 和 grounding 这两个任务是高度相关的。OCR &amp;amp; Chart 以及 GUI-agent 也是高度相关的。&lt;/p>
&lt;h2 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h2>&lt;p>在本文中，作者提出了 GLM-4.1V-Thinking, 一个 9B 的通用多模态 reasoning model, 作者通过构建高质量的数据，多阶段的训练，让模型在 reasoning 和 non-reasoning 任务上均超过了已有的 SOTA 模型的表现&lt;/p>
&lt;p>作者认为，模型有以下局限：&lt;/p>
&lt;ol>
&lt;li>模型并没有提升 reasoning 的质量，在某些情况下，模型结果正确，但是中间过程错误。作者分析这是因为目前只有针对结果的奖励机制。因此，未来需要能够评估中间过程的 reward model&lt;/li>
&lt;li>RL 的训练具有不稳定性，作者发现模型对设置比较敏感，这对 RL 训练的 scaling 提出了挑战。&lt;/li>
&lt;li>模型在复杂场景下表现依旧不太好，比如图片中有物体遮挡的情况，这些感知误差也会影响最终的 reasoning 表现。作者认为如何同时提高 perception 和 reasoning 的表现是一个需要研究的课题。&lt;/li>
&lt;/ol>
&lt;p>未来的工作有：&lt;/p>
&lt;ol>
&lt;li>如何构建更好的 reward 机制，来同时评估结果和中间过程。从而防止模型 reward hacking&lt;/li>
&lt;li>如何基于 multimodal training 来提升模型在纯文本任务上的表现。探究 visual reasoning 和 text reasoning 如何互相促进也是一个课题&lt;/li>
&lt;li>如何更好评估模型的表现，即如何更好评估模型的 failure modes, 比如幻觉，short reasoning 等&lt;/li>
&lt;/ol>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2507.01006" target="_blank" rel="noopener"
>Arxiv&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://huggingface.co/THUDM/GLM-4.1V-9B-Thinking" target="_blank" rel="noopener"
>Huggingface&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="appendix">&lt;a href="#appendix" class="header-anchor">&lt;/a>Appendix
&lt;/h2>&lt;p>GLM-4.1V-Thinking 的 patch merger 代码&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Glm4vVisionPatchMerger&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">context_dim&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_act&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">bool&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="fm">__init__&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">bias&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">post_projection_norm&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">LayerNorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">gate_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">context_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">bias&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">up_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">context_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">bias&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">down_proj&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">context_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">bias&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">act1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">GELU&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">act_fn&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ACT2FN&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">hidden_act&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_state&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Tensor&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_state&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_state&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_state&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">act1&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">post_projection_norm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_state&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">down_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">act_fn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">gate_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_state&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">up_proj&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_state&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Glm4vVisionModel&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">downsample&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Conv2d&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">in_channels&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">out_channels&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">out_hidden_size&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">kernel_size&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">spatial_merge_size&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">stride&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">spatial_merge_size&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">merger&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Glm4vVisionPatchMerger&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">out_hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">context_dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">intermediate_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_act&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_act&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">...&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">spatial_merge_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">spatial_merge_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">hidden_states&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">permute&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">downsample&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">out_hidden_size&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">hidden_states&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">merger&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_states&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div></description></item><item><title>Notes on MiMo-VL</title><link>https://maosong.website/p/notes-on-mimo-vl/</link><pubDate>Thu, 05 Jun 2025 10:51:43 +0800</pubDate><guid>https://maosong.website/p/notes-on-mimo-vl/</guid><description>&lt;h1 id="介绍">&lt;a href="#%e4%bb%8b%e7%bb%8d" class="header-anchor">&lt;/a>介绍
&lt;/h1>&lt;p>小米在5月30号发布了MiMo-VL 7B多模态推理大模型，模型包括SFT和RL两个版本，作者在技术报告里讲解了MiMo-VL的架构，预训练和后训练过程，最后对MiMo-VL的性能进行了评测&lt;/p>
&lt;h1 id="架构">&lt;a href="#%e6%9e%b6%e6%9e%84" class="header-anchor">&lt;/a>架构
&lt;/h1>&lt;p>MiMo-VL 7B的架构是一个标准的 &amp;ldquo;ViT-MLP-LLM&amp;quot;的架构，其中：&lt;/p>
&lt;ul>
&lt;li>ViT使用了Qwen2.5-VL的Qwen2.5-ViT&lt;/li>
&lt;li>MLP是一个两层的SwiGLU MLP Layer&lt;/li>
&lt;li>LLM是之前发布的MiMo-7B&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-mimo-vl/architecture.png"
width="1237"
height="798"
loading="lazy"
alt="Architecture of MiMo-VL 7B"
class="gallery-image"
data-flex-grow="155"
data-flex-basis="372px"
>&lt;/p>
&lt;h1 id="预训练">&lt;a href="#%e9%a2%84%e8%ae%ad%e7%bb%83" class="header-anchor">&lt;/a>预训练
&lt;/h1>&lt;h2 id="数据">&lt;a href="#%e6%95%b0%e6%8d%ae" class="header-anchor">&lt;/a>数据
&lt;/h2>&lt;p>预训练阶段一共使用了2.4T的token&lt;/p>
&lt;ul>
&lt;li>Image caption data: 使用了rewrite caption等方法提升和过滤数据，作者还基于MetaCLIP来降低数据的不平衡性&lt;/li>
&lt;li>Interleaved data: 主要从webpage, books, papers中提取，最后基于relevance, complementarity, balance of information density来保证数据的质量&lt;/li>
&lt;li>OCR and grounding data: OCR数据包括文档，表格，数学公式等。grounding data包括单物体和多物体&lt;/li>
&lt;li>video data: rewrite caption, caption有对应的时间戳，加入了video analysis数据&lt;/li>
&lt;li>GUI data: 覆盖mobile, web, desktop三种场景, 加入了GUI grounding和GUI action数据&lt;/li>
&lt;li>Synthetic reasoning data:使用一个reasoning model来生成带有思考过程的reasoning数据&lt;/li>
&lt;/ul>
&lt;h2 id="训练">&lt;a href="#%e8%ae%ad%e7%bb%83" class="header-anchor">&lt;/a>训练
&lt;/h2>&lt;p>训练有四个阶段，基本上和之前的模型一致，先训练MLP，然后解冻ViT，再训练全部参数，最后扩展模型的上下文。训练过程如下表所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-mimo-vl/pretraining.png"
width="1339"
height="570"
loading="lazy"
alt="Pretraining of MiMo-VL 7B"
class="gallery-image"
data-flex-grow="234"
data-flex-basis="563px"
>&lt;/p>
&lt;h1 id="后训练">&lt;a href="#%e5%90%8e%e8%ae%ad%e7%bb%83" class="header-anchor">&lt;/a>后训练
&lt;/h1>&lt;p>作者提出了Mixed On-policy Reinforcement Learning (MORL)框架来把RLVR和RLHF结合起来&lt;/p>
&lt;h2 id="数据-1">&lt;a href="#%e6%95%b0%e6%8d%ae-1" class="header-anchor">&lt;/a>数据
&lt;/h2>&lt;p>RLVR的数据包括:&lt;/p>
&lt;ol>
&lt;li>visual reasoning, 80K数据，使用rule-based math-verify library来评估&lt;/li>
&lt;li>text reasoning, 使用了MiMo的数学推理数据，评估方式与visual reasoning一致&lt;/li>
&lt;li>Image grounding, 包括general and GUI grounding任务，使用GIoU来计算奖励&lt;/li>
&lt;li>Visual Counting, 使用准确率来打分&lt;/li>
&lt;li>Temporal Video Grounding, temporal video grounding任务，使用IoU来计算奖励&lt;/li>
&lt;/ol>
&lt;p>RLHF的数据包括中英文的query，作者使用MiMo-VL-7B和其他模型来采样，然后使用一个advanced VLM来排序.
RLHF的reward model使用Breadley-Terry model作为训练目标，text-only reward由MiMo-7B初始化得到，多模态reward model由MiMo-VL-7B初始化得到&lt;/p>
&lt;h2 id="训练-1">&lt;a href="#%e8%ae%ad%e7%bb%83-1" class="header-anchor">&lt;/a>训练
&lt;/h2>&lt;p>训练过程就是将RLVR和RLHF放在一起进行训练，作者使用了MiMo的seamless rollout engine.&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-mimo-vl/RL_training.png"
width="1328"
height="466"
loading="lazy"
alt="RL Training"
class="gallery-image"
data-flex-grow="284"
data-flex-basis="683px"
>&lt;/p>
&lt;p>训练目标与GRPO一致，但是本文中作者使用了on-policy的训练方式。&lt;/p>
&lt;h1 id="评估">&lt;a href="#%e8%af%84%e4%bc%b0" class="header-anchor">&lt;/a>评估
&lt;/h1>&lt;p>作者评估了MiMo-VL-7B的通用能力，reasoning能力，GUI交互理解能力，最后还给出了MiMo-VL-7B在ChatbotArena上的排名&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-mimo-vl/Performance.png"
width="1367"
height="688"
loading="lazy"
alt="Performance of MiMo-VL-7B"
class="gallery-image"
data-flex-grow="198"
data-flex-basis="476px"
>&lt;/p>
&lt;h2 id="ablation-study">&lt;a href="#ablation-study" class="header-anchor">&lt;/a>Ablation study
&lt;/h2>&lt;p>作者首先探究了在预训练阶段加入Long CoT数据对模型表现的影响，结果发现模型正在MMMU, OlympiadBench等benchmark上都有了提升&lt;/p>
&lt;p>作者还比较了原始的GRPO和本文中使用的on-policy版本的GRPO，结果发现on-policy版本的GRPO效果更好&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-mimo-vl/on-policy-GRPO-ablation.png"
width="1368"
height="843"
loading="lazy"
alt="Ablation of on-policy GRPO"
class="gallery-image"
data-flex-grow="162"
data-flex-basis="389px"
>&lt;/p>
&lt;p>最后，作者探讨了一下将不同任务放在一起训练导致的互相干扰问题，这个在MiMo里也提到过，核心问题就是有的任务是short CoT的，有的任务是Long CoT的，如何让模型根据任务来选择不同的reasoning length是一个需要探讨的问题，这个问题在&lt;a class="link" href="https://maosong.website/p/notes-on-vc-ppo/" target="_blank" rel="noopener"
>VC-PPO&lt;/a>里进行了初步的探讨。&lt;/p>
&lt;h1 id="结论">&lt;a href="#%e7%bb%93%e8%ae%ba" class="header-anchor">&lt;/a>结论
&lt;/h1>&lt;p>与MiMo一样，MiMo-VL-7B通过在预训练阶段加入reasoning data，来提高模型的reasoning能力，并指出模型可以通过这个方式来媲美其他模型的表现&lt;/p>
&lt;h1 id="参考文献">&lt;a href="#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae" class="header-anchor">&lt;/a>参考文献
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://github.com/XiaomiMiMo/MiMo-VL" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Seed1.5-VL</title><link>https://maosong.website/p/notes-on-seed1.5-vl/</link><pubDate>Wed, 14 May 2025 09:28:07 +0800</pubDate><guid>https://maosong.website/p/notes-on-seed1.5-vl/</guid><description>&lt;p>字节Seed在5月11号发布了Seed1.5-VL技术报告。技术报告详细介绍了Seed1.5-VL的架构，训练和评估细节&lt;/p>
&lt;h1 id="简介">&lt;a href="#%e7%ae%80%e4%bb%8b" class="header-anchor">&lt;/a>简介
&lt;/h1>&lt;p>Seed1.5-VL是多模态大模型，它是一个标准的 Vision Encoder - MLP - LLM 的架构，Vision Encoder是自研的Seed-ViT，参数量为532M，大语言模型的激活参数为20B（虽然论文未提及总参数量，但是从seed-Thinking v1.5来看，总参数量应该是200B）。Seed1.5-VL强调了agent能力，比如GUI control和gameplay。&lt;/p>
&lt;h1 id="介绍">&lt;a href="#%e4%bb%8b%e7%bb%8d" class="header-anchor">&lt;/a>介绍
&lt;/h1>&lt;p>现有的LVLM还不能与人类的通用能力相比，特别是在3D空间理解，物体技术，交互游戏等方面。并且，LVLM训练预料的多样性也不如LLM。最后，多模态数据的异质性也对模型的训练和推理提出了挑战。&lt;/p>
&lt;p>基于这些问题，作者提出了Seed1.5-VL，一个视觉多模态大模型。通过构建高质量的训练数据，作者大幅度提高了模型的表现。作者还进一步探究了模型的scaling law以及pre-training和post training算法的设计。为了提高训练和推理效率，作者在infra上也进行了改进&lt;/p>
&lt;h1 id="架构">&lt;a href="#%e6%9e%b6%e6%9e%84" class="header-anchor">&lt;/a>架构
&lt;/h1>&lt;p>Seed1.5-VL的架构如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-seed1.5-vl/architecture.png"
width="1387"
height="1062"
loading="lazy"
alt="Architecture of See1.5-VL"
class="gallery-image"
data-flex-grow="130"
data-flex-basis="313px"
>&lt;/p>
&lt;p>Seed1.5-VL的架构包含三个部分：&lt;/p>
&lt;ol>
&lt;li>Vision Encoder: vision encoder是seed自研的Seed-ViT，拥有532M参数&lt;/li>
&lt;li>Adapter：一个两层的MLP&lt;/li>
&lt;li>LLM：Seed1.5-LLM，一个MoE架构的LLM&lt;/li>
&lt;/ol>
&lt;p>Seed1.5-VL支持文本，图片和视频输入，输入为文本。其可以进行理解和推理(reasoning)。&lt;/p>
&lt;h2 id="seed-vit">&lt;a href="#seed-vit" class="header-anchor">&lt;/a>Seed-ViT
&lt;/h2>&lt;p>Seed-ViT主要是在两个方面进行了改进：&lt;/p>
&lt;ol>
&lt;li>在pre-training阶段使用了视频数据，来提高模型的时空间感知能力&lt;/li>
&lt;li>使用了2D-RoPE来处理动态分辨率的图片&lt;/li>
&lt;/ol>
&lt;p>Seed-ViT的超参数如下表所示&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Patch size&lt;/th>
&lt;th>Pos Embedding&lt;/th>
&lt;th>Head dim&lt;/th>
&lt;th>#heads&lt;/th>
&lt;th>Embedding dim&lt;/th>
&lt;th>MLP ratio&lt;/th>
&lt;th>#layers&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>14&lt;/td>
&lt;td>2D RoPE&lt;/td>
&lt;td>64&lt;/td>
&lt;td>20&lt;/td>
&lt;td>1280&lt;/td>
&lt;td>4.0&lt;/td>
&lt;td>27&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>对于输入的图片，Seed-ViT首先将图片resize到28*28的倍数，接下来图片会被分割为14*14的patch. 与NaViT一样，Seed-ViT也是用了token-packing的操作，来将多个图片packing到一个sequence里进行训练。最后，对于Seed-ViT输出的图片特征，作者还是用了 $2\times 2$的 average pooling来降低token个数。&lt;/p>
&lt;p>Seed-ViT的预训练包括三个阶段，其设置和超参数如下表所示：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Categories&lt;/th>
&lt;th>Unlabeled Image&lt;/th>
&lt;th>Image-text pairs&lt;/th>
&lt;th>Video-audio-text tuples&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Training samples&lt;/td>
&lt;td>2.2B&lt;/td>
&lt;td>4.8B&lt;/td>
&lt;td>65M&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Token percentages&lt;/td>
&lt;td>4.0%&lt;/td>
&lt;td>91.2%&lt;/td>
&lt;td>4.8%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Batch sizes&lt;/td>
&lt;td>55,296&lt;/td>
&lt;td>32,768&lt;/td>
&lt;td>1,024&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LR warm up steps&lt;/td>
&lt;td>1,692&lt;/td>
&lt;td>2,000&lt;/td>
&lt;td>12,800&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Maximum LR&lt;/td>
&lt;td>$7.06\times 10^{-3}$&lt;/td>
&lt;td>$1.0\times 10^{-4}$&lt;/td>
&lt;td>$5.0\times 10^{-5}$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Minimum LR&lt;/td>
&lt;td>$1.05\times 10^{-5}$&lt;/td>
&lt;td>$1.2\times 10^{-6}$&lt;/td>
&lt;td>$2.02\times 10^{-7}$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>在预训练时，作者考虑了三点：&lt;/p>
&lt;ol>
&lt;li>训练效率：尽可能提高模型的训练效率&lt;/li>
&lt;li>原生图片精度输入处理：让模型可以尽早处理不同分辨率图片输入&lt;/li>
&lt;li>数据利用率：可以使用不同类型的数据&lt;/li>
&lt;/ol>
&lt;p>基于这三点考虑，Seed-ViT的训练包括三个阶段：&lt;/p>
&lt;ol>
&lt;li>Masked Image modeling with 2D RoPE：这一阶段的目的是提高模型对与几何和结构的感知能力。作者使用 &lt;code>EVA02-CLIP-E&lt;/code>作为教师模型，然后随机将75%的patches进行mask，最后让Seed-ViT输出的特征尽可能匹配教师模型输出的特征。这里的损失函数是cosine similarity. 作者发现，scale up MIM之后，VLM在OCR和文档理解任务上的表现得到了大幅度的提升。&lt;/li>
&lt;li>Native Resolution Contrastive Learning：这一阶段和CLIP,SigLIP的训练差不多，text encoder与&lt;code>EVA02-CLIP-E&lt;/code>相同。损失函数为SigLIP loss和SuperClass loss&lt;/li>
&lt;li>Omni-modal pret-raining：这一阶段使用了MiCo框架，来将video frames, audio, visual captions以及audio captions进行对齐，text encoder编码caption，ViT编码其他部分。通过这一阶段的训练，模型可以学习omni-model representations,并且还可以提高模型的图片和视频理解人物上的表现。&lt;/li>
&lt;/ol>
&lt;h2 id="视频编码">&lt;a href="#%e8%a7%86%e9%a2%91%e7%bc%96%e7%a0%81" class="header-anchor">&lt;/a>视频编码
&lt;/h2>&lt;p>对于视频输入，Seed1.5-VL采用了 &lt;strong>Dynamic Frame-Resolution Sampling&lt;/strong>技巧，核心思想在于根据budget和处理的任务来动态调整采样率(frame)和每一帧的图片精度(resolution)。&lt;/p>
&lt;p>具体来讲，在时序上，默认的FPS为1，如果要处理有关时序信息任务的话，FPS调整为2，如果要处理motion tracking等任务的话，FPS提高到5.为了进一步提高模型的时间感知能力，Seed 1.5-VL加入了timestamp tokens (e.g., &lt;code>[1.5 second]&lt;/code>)。&lt;/p>
&lt;p>在空间上，每个frame的可选精度为 $\{640, 512, 384, 256, 160, 128\}$，选择的精度根据我们的budget进行决定。如果说我们使用最低配置还是不能处理视频输入的话，我们就会先进行均匀采样，然后再进行处理，这样可以让模型处理所有的视频信息。&lt;/p>
&lt;h1 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre-training
&lt;/h1>&lt;p>pre-training阶段一共包括了3T token. 作者详细介绍了每一个类别。这里我们就简单总结一下。数据一共包括以下7个类别&lt;/p>
&lt;ol>
&lt;li>Generic Image-text pairs &amp;amp; knowledge data. 这部分数据主要是图文交错以及image text-pairs数据。作者介绍了一下针对长尾数据（稀有物种识别）的数据构建方式&lt;/li>
&lt;li>OCR. 这部分数据有1B左右的sample，包括documents, scene texts, tables, charts以及flowcharts等。其中，作者通过SynthDog和Latex合成了200M左右的文本密集图片。作者还对合成的数据进行了增广;Chart数据包括公开数据集和合成的数据集，最终一共有100M的样本; Table数据集也是通过提取得到的，一共包含50M左右的图片。作者还基于OCR结果构建了VQA数据集，来提高模型理解图片文字的能力&lt;/li>
&lt;li>Visual Grounding &amp;amp; Counting: 提高模型识别和定位物体的能力。数据格式包括bounding box和center points. 其中bounding box主要是公开数据集；作者还是用Grounding DINO来标记了一部分数据，最后一共收集了200M的样本; point数据主要是PixMo, 作者还使用了Molmo以及CountGD标注了一部分数据，最后一共包括170M的指令以及110B的token; Counting数据从bounding box 和point数据集中筛选得到，包括了8M的样本和13B的token。最后在训练时，与InternVL一样，坐标会被normalize到 &lt;code>[1,1000]&lt;/code>&lt;/li>
&lt;li>3D spatial understanding. 主要包括相对深度，绝对深度和3D Grounding数据；对于相对深度，作者使用DepthAnything V2来标注，得到了3.2B的token; 对于绝对深度，作者使用了公开数据集，最终包含18M的指令以及28B的token;对于3D Grounding，作者使用了公开数据集，最终包含770的指令以及1.3B的token&lt;/li>
&lt;li>Video. 主要包含general video understanding数据, video temporal grounding and moment retrieval以及video streaming data.&lt;/li>
&lt;li>STEM. 主要包含image comprehension data和problem-solving data. 前者包括收集的3.2M教育相关的数据，以及10M结构化表格，4.5M化学结构表达式，1.5M坐标系数据。后者包括100M K12的习题&lt;/li>
&lt;li>GUI. 作者使用UI-TARS来合成数据。数据主要包括perception, grounding 以及 reasoning&lt;/li>
&lt;/ol>
&lt;p>预训练包括三个stage，其实验设置如下表所示：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Stages&lt;/th>
&lt;th>Stage 0&lt;/th>
&lt;th>Stage 1&lt;/th>
&lt;th>Stage 2&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Training budget (tokens)&lt;/td>
&lt;td>16B&lt;/td>
&lt;td>3T&lt;/td>
&lt;td>240B&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Sequence length&lt;/td>
&lt;td>32,768&lt;/td>
&lt;td>32,768&lt;/td>
&lt;td>131,072&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Trainable components&lt;/td>
&lt;td>MLP adaptor&lt;/td>
&lt;td>all&lt;/td>
&lt;td>all&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Batch sizes (tokens)&lt;/td>
&lt;td>8.4M&lt;/td>
&lt;td>71M&lt;/td>
&lt;td>71M&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LR warmup steps&lt;/td>
&lt;td>100&lt;/td>
&lt;td>500&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Maximum LR&lt;/td>
&lt;td>$2.52 \times 10^{-4}$&lt;/td>
&lt;td>$5.22 \times 10^{-5}$&lt;/td>
&lt;td>$5.22 \times 10^{-6}$&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Minimum LR&lt;/td>
&lt;td>$4.50 \times 10^{-5}$&lt;/td>
&lt;td>$5.22 \times 10^{-6}$&lt;/td>
&lt;td>$5.22 \times 10^{-6}$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;ul>
&lt;li>Stage 0: 仅训练MLP，用于对齐vision encoder和LLM,使用了16B token&lt;/li>
&lt;li>Stage 1：训练全部参数，用于获取知识以及掌握grounding和OCR能力，使用了3T token&lt;/li>
&lt;li>Stage 2：训练全部参数，提升模型的长上下文能力以及对下游任务的适应性，包括视频理解，coding和3D空间理解，上下文长度从32,768提升到131,072&lt;/li>
&lt;/ul>
&lt;p>作者还进行了消融实验，采取了和Qwen2-VL一样的训练方式，结果发现，本文的训练方式效果更好。作者认为，解冻vision encoder，可能让vision encoder弥补LLM的不足，进而损害其自身的感知能力。&lt;/p>
&lt;p>作者还探究了模型的scaling law，使用的建模函数为：&lt;/p>
$$
\log(\hat{L}) \sim \log(B) - \beta \log(D) = -a \log(D) + b
$$&lt;p>
这里 $L$ 是NLL loss, D是训练的token数，$a$和$b$是待拟合的参数。实验结果如下图所示。
&lt;img src="https://maosong.website/p/notes-on-seed1.5-vl/scaling_law.png"
width="1392"
height="1065"
loading="lazy"
alt="scaling law"
class="gallery-image"
data-flex-grow="130"
data-flex-basis="313px"
>&lt;/p>
&lt;p>结论是在特定领域数据训练的loss可以作为模型在下游任务上表现的一个估计&lt;/p>
&lt;h1 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post-training
&lt;/h1>&lt;p>post-training分为两个阶段：SFT和RL，其中RL包括RLHF, RLVR. 总流程如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-seed1.5-vl/post_training.png"
width="1367"
height="598"
loading="lazy"
alt="post training"
class="gallery-image"
data-flex-grow="228"
data-flex-basis="548px"
>&lt;/p>
&lt;h2 id="sft">&lt;a href="#sft" class="header-anchor">&lt;/a>SFT
&lt;/h2>&lt;p>SFT数据主要包括General Instruction data以及Long Chain-of-Thought (LongCoT) data，劝着途胜模拟徐保国的指令跟随能力，后者提升模型的reasoning能力。&lt;/p>
&lt;p>通用指令微调数据包括13,000条收集的数据和40,0000高质量的公开数据集。&lt;/p>
&lt;h2 id="rlhf">&lt;a href="#rlhf" class="header-anchor">&lt;/a>RLHF
&lt;/h2>&lt;p>RLHF的数据包括人类标注数据和合成数据。对于reward model，作者使用了LVLM作为reward model，来给不同的response进行打分。最后，使用变体的PPO算法进行训练。&lt;/p>
&lt;h2 id="rlvr">&lt;a href="#rlvr" class="header-anchor">&lt;/a>RLVR
&lt;/h2>&lt;p>RLVR的训练数据主要包含Visual STEM和Visual Perception and Reasoning. 前者包含1M左右的问题，主要是STEM和K12的相关问题。后者包括grounding, visual instruction following以及visual puzzles &amp;amp; games等&lt;/p>
&lt;h2 id="hybrid-rl">&lt;a href="#hybrid-rl" class="header-anchor">&lt;/a>Hybrid RL
&lt;/h2>&lt;p>RL的训练过程包含了RLHF和RLVR两个RL算法。作者介绍了以下细节：&lt;/p>
&lt;ol>
&lt;li>format reward. 要求模型输出格式为 &lt;code>&amp;lt;think&amp;gt;{thought}&amp;lt;/think&amp;gt;{solution}&lt;/code>&lt;/li>
&lt;li>hybrid reward. reward包括RM和verifier两部分&lt;/li>
&lt;li>Shared critic. 使用一个critic model来估计value function&lt;/li>
&lt;li>KL coefficients. 对于RLHF和RLVE，使用不同的KL divergence稀疏&lt;/li>
&lt;/ol>
&lt;h2 id="iterative-update-by-rejection-sampling-fine-tuning">&lt;a href="#iterative-update-by-rejection-sampling-fine-tuning" class="header-anchor">&lt;/a>Iterative Update by Rejection Sampling Fine-tuning
&lt;/h2>&lt;p>在训练的过程中，作者使用了一个迭代训练策略。一个开始模型在低质量的Long-CoT数据上进行SFT，在RL过后，作者会重新评估问题的难度，然后过程掉一部分数据，再进行SFT，这样反复进行迭代训练以提高模型的表现。&lt;/p>
&lt;blockquote>
&lt;p>Infra部分好像都是一些细节，这里就跳过了。&lt;/p>
&lt;/blockquote>
&lt;h1 id="evaluation">&lt;a href="#evaluation" class="header-anchor">&lt;/a>Evaluation
&lt;/h1>&lt;p>作者评估了&lt;/p>
&lt;ol>
&lt;li>Seed-ViT的表现&lt;/li>
&lt;li>Seed1.5-VL的通用视觉理解和推理能力&lt;/li>
&lt;li>Seed1.5-VL的视频相关能力&lt;/li>
&lt;li>Seed1.5-VL的agent能力&lt;/li>
&lt;li>Seed1.5-VL在内部benchmark上的表现&lt;/li>
&lt;/ol>
&lt;p>由于Seed1.5-VL是一个API模型，并且其内部benchmark不公开，因此Seed-ViT以及其在内部benchmark上的表现我们就不列出来了。我们主要看一下其和领先多模态大模型的对比&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-seed1.5-vl/vision_benchmarks.png"
width="1240"
height="1184"
loading="lazy"
alt="vision benchmarks, 论文表6"
class="gallery-image"
data-flex-grow="104"
data-flex-basis="251px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-seed1.5-vl/video_benchmarks.png"
width="1254"
height="1025"
loading="lazy"
alt="Video benchmarks, 论文表7"
class="gallery-image"
data-flex-grow="122"
data-flex-basis="293px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-seed1.5-vl/gui_benchmarks.png"
width="1190"
height="393"
loading="lazy"
alt="GUI benchmarks"
class="gallery-image"
data-flex-grow="302"
data-flex-basis="726px"
>&lt;/p>
&lt;h1 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h1>&lt;p>在本文中，作者介绍了Seed1.5-VL，一个领先的多模态大模型，可以处理图片和视频输入。作者介绍了Seed-ViT以及Seed1.5-VL的数据，训练和评估。&lt;/p>
&lt;p>作者还分析了以下Seed1.5-VL的不足以及未来的改进方向&lt;/p>
&lt;ol>
&lt;li>Seed1.5-VL在复杂视觉感知任务中，其计数能力会下降。&lt;/li>
&lt;li>Seed1.5-VL的高阶推理能力还尚有不足，作者拿Klotski举了一个例子&lt;/li>
&lt;li>Seed1.5-VL的3D spatial reasoning能力不足&lt;/li>
&lt;li>Seed1.5-VL的temporal reasoning能力不足&lt;/li>
&lt;li>Seed1.5-VL与其他模型一样，存在幻觉问题&lt;/li>
&lt;/ol>
&lt;p>未来，有以下值得探索的方向：&lt;/p>
&lt;ol>
&lt;li>涌现现象(emergent abilities)&lt;/li>
&lt;li>scaling law，进一步探索模型参数量，算力以及数据对模型表现的影响&lt;/li>
&lt;li>提高模型的3D spatial reasoning能力，降低模型的幻觉，提高模型分组合搜索能力&lt;/li>
&lt;/ol>
&lt;h1 id="参考文献">&lt;a href="#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae" class="header-anchor">&lt;/a>参考文献
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2505.07062" target="_blank" rel="noopener"
>arxiv&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Data mixture in MLLM</title><link>https://maosong.website/p/data-mixture-in-mllm/</link><pubDate>Fri, 25 Apr 2025 10:25:48 +0800</pubDate><guid>https://maosong.website/p/data-mixture-in-mllm/</guid><description>&lt;h1 id="介绍">&lt;a href="#%e4%bb%8b%e7%bb%8d" class="header-anchor">&lt;/a>介绍
&lt;/h1>&lt;p>简单总结一下已有的介绍了训练数据配比的多模态大模型，方便后续使用。
根据之前看的一些论文进行总结，如有缺漏，欢迎批评指正。&lt;/p>
&lt;h1 id="相关工作">&lt;a href="#%e7%9b%b8%e5%85%b3%e5%b7%a5%e4%bd%9c" class="header-anchor">&lt;/a>相关工作
&lt;/h1>&lt;h2 id="llava">&lt;a href="#llava" class="header-anchor">&lt;/a>LLaVA
&lt;/h2>&lt;p>&lt;a class="link" href="https://arxiv.org/abs/2310.03744" target="_blank" rel="noopener"
>LLaVA 1.5&lt;/a> SFT数据配比如下：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/data-mixture-in-mllm/llava_v1_5_sft_mixture.png"
width="693"
height="569"
loading="lazy"
alt="LLaVA 1.5 SFT data mixture"
class="gallery-image"
data-flex-grow="121"
data-flex-basis="292px"
>&lt;/p>
&lt;h2 id="llava-onevision">&lt;a href="#llava-onevision" class="header-anchor">&lt;/a>LLaVA-OneVision
&lt;/h2>&lt;p>&lt;a class="link" href="http://arxiv.org/abs/2408.03326" target="_blank" rel="noopener"
>LLaVA OneVision&lt;/a> 的数据集统计在表16里，这里总结一下数据配比(总量为3.15M)&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">Category&lt;/th>
&lt;th style="text-align: right">Ratio&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">general vqa&lt;/td>
&lt;td style="text-align: right">36.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Doc/Chart/Screen&lt;/td>
&lt;td style="text-align: right">20.6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Math/Reasoning&lt;/td>
&lt;td style="text-align: right">20.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">general OCR&lt;/td>
&lt;td style="text-align: right">8.9&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">text only&lt;/td>
&lt;td style="text-align: right">14.3&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="apollo">&lt;a href="#apollo" class="header-anchor">&lt;/a>Apollo
&lt;/h2>&lt;p>&lt;a class="link" href="http://arxiv.org/abs/2412.10360" target="_blank" rel="noopener"
>Apollo&lt;/a> 是Meta发布的一个视频理解多模态大模型，其数据集没有开源，论文中给出了其训练数据配比
&lt;img src="https://maosong.website/p/data-mixture-in-mllm/apollo.png"
width="1389"
height="589"
loading="lazy"
alt="apollo data mixture"
class="gallery-image"
data-flex-grow="235"
data-flex-basis="565px"
>&lt;/p>
&lt;h2 id="cambiran-1">&lt;a href="#cambiran-1" class="header-anchor">&lt;/a>Cambiran-1
&lt;/h2>&lt;p>&lt;a class="link" href="http://arxiv.org/abs/2406.16860" target="_blank" rel="noopener"
>Cambiran-1&lt;/a> 是纽约大学提出了一个多模态大模型系列，论文发表在了 NeurIPS 2024(Oral) 上，作者给出了 Cambrain-10M 和 Cambrain-7M 两个数据集，Cambrain-7M 的数据分布如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/data-mixture-in-mllm/cambrain-7M.png"
width="1294"
height="513"
loading="lazy"
alt="Cambrain-7M"
class="gallery-image"
data-flex-grow="252"
data-flex-basis="605px"
>&lt;/p>
&lt;h2 id="idefics">&lt;a href="#idefics" class="header-anchor">&lt;/a>Idefics
&lt;/h2>&lt;p>Idefics系列(1/2/3)是huggingface提出的视觉多模态大模型系列，在 &lt;a class="link" href="https://arxiv.org/abs/2405.02246" target="_blank" rel="noopener"
>Idefics2&lt;/a> 中，作者构建了The Cauldron数据集，其数据配比在表14里面。总结如下：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">Category&lt;/th>
&lt;th style="text-align: right">Ratio&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">general vqa&lt;/td>
&lt;td style="text-align: right">11.02&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">captioning&lt;/td>
&lt;td style="text-align: right">5.14&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">OCR&lt;/td>
&lt;td style="text-align: right">17.47&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">chart/figures&lt;/td>
&lt;td style="text-align: right">14.05&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">table&lt;/td>
&lt;td style="text-align: right">11.3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">reasoning&lt;/td>
&lt;td style="text-align: right">10.32&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">textbook&lt;/td>
&lt;td style="text-align: right">1.58&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">difference&lt;/td>
&lt;td style="text-align: right">2.38&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">screenshot2code&lt;/td>
&lt;td style="text-align: right">0.31&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">text only&lt;/td>
&lt;td style="text-align: right">26.41&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>在 &lt;a class="link" href="http://arxiv.org/abs/2408.12637" target="_blank" rel="noopener"
>Idefics3&lt;/a> 中，作者基于The Cauldron进行了扩展，最终数据集的比例如下：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/data-mixture-in-mllm/mixture_the_cauldron.png"
width="3430"
height="886"
loading="lazy"
alt="data mixture of SmolVLM blog"
class="gallery-image"
data-flex-grow="387"
data-flex-basis="929px"
>&lt;/p>
&lt;h2 id="molmo">&lt;a href="#molmo" class="header-anchor">&lt;/a>Molmo
&lt;/h2>&lt;p>&lt;a class="link" href="http://arxiv.org/abs/2409.17146" target="_blank" rel="noopener"
>Molmo&lt;/a> 是Allen AI发布的一个多模态大模型，其SFT数据配比如下&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/data-mixture-in-mllm/molmo_sft_data_mixture.png"
width="693"
height="785"
loading="lazy"
alt="SFT data mixture of Molmo"
class="gallery-image"
data-flex-grow="88"
data-flex-basis="211px"
>&lt;/p>
&lt;h2 id="eagle-225">&lt;a href="#eagle-225" class="header-anchor">&lt;/a>Eagle 2/2.5
&lt;/h2>&lt;p>Eagle (1/2/2.5) 是NVLab提出了系列多模态大模型，&lt;a class="link" href="http://arxiv.org/abs/2501.14818" target="_blank" rel="noopener"
>Eagle 2&lt;/a> 给出了 stage 1.5 和 stage 2的数据配比&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/data-mixture-in-mllm/eagle2_data_mixture.png"
width="693"
height="400"
loading="lazy"
alt="Eagle 2 data mixture"
class="gallery-image"
data-flex-grow="173"
data-flex-basis="415px"
>&lt;/p>
&lt;p>&lt;a class="link" href="http://arxiv.org/abs/2504.15271" target="_blank" rel="noopener"
>Eagle 2.5&lt;/a> 在 Eagle 2的基础上加入了一些 long context 数据，其数据列表在表11里&lt;/p>
&lt;h2 id="smolvlm">&lt;a href="#smolvlm" class="header-anchor">&lt;/a>SmolVLM
&lt;/h2>&lt;p>&lt;a class="link" href="http://arxiv.org/abs/2504.05299" target="_blank" rel="noopener"
>SmolVLM&lt;/a> 是huggingface开发的一款轻量化视觉多模态大模型，论文中的数据配比如下：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/data-mixture-in-mllm/smol_vlm_data_mixture.png"
width="1382"
height="560"
loading="lazy"
alt="data mixture of SmolVLM paper"
class="gallery-image"
data-flex-grow="246"
data-flex-basis="592px"
>&lt;/p>
&lt;h2 id="mm115">&lt;a href="#mm115" class="header-anchor">&lt;/a>MM1/1.5
&lt;/h2>&lt;p>&lt;a class="link" href="http://arxiv.org/abs/2403.09611" target="_blank" rel="noopener"
>MM1&lt;/a> 通过实验确定了训练数据的配比：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">Category&lt;/th>
&lt;th style="text-align: right">Ratio&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">interleaved image-text&lt;/td>
&lt;td style="text-align: right">45&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">imagetext&lt;/td>
&lt;td style="text-align: right">45&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">text only&lt;/td>
&lt;td style="text-align: right">10&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;a class="link" href="http://arxiv.org/abs/2409.20566" target="_blank" rel="noopener"
>MM1.5&lt;/a> 的SFT数据配比如下：
&lt;img src="https://maosong.website/p/data-mixture-in-mllm/MM1_5_data_mixture.png"
width="1183"
height="845"
loading="lazy"
alt="MM1.5 SFT data mixture"
class="gallery-image"
data-flex-grow="140"
data-flex-basis="336px"
>&lt;/p>
&lt;h2 id="internvl">&lt;a href="#internvl" class="header-anchor">&lt;/a>InternVL
&lt;/h2>&lt;p>&lt;a class="link" href="http://arxiv.org/abs/2412.05271" target="_blank" rel="noopener"
>InternVL2.5&lt;/a> 在论文里总结了其使用的pretraining数据和SFT数据，但是没有具体的数据配比，请参考论文的表4和表5&lt;/p>
&lt;p>SFT数据配比&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">Category&lt;/th>
&lt;th style="text-align: left">Ratio&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">single-image&lt;/td>
&lt;td style="text-align: left">45.92%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">multi-image&lt;/td>
&lt;td style="text-align: left">9.37%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">video&lt;/td>
&lt;td style="text-align: left">39.79%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">pure-text&lt;/td>
&lt;td style="text-align: left">4.92%&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="minicpm-v">&lt;a href="#minicpm-v" class="header-anchor">&lt;/a>MiniCPM V
&lt;/h2>&lt;p>&lt;a class="link" href="http://arxiv.org/abs/2408.01800" target="_blank" rel="noopener"
>MiniCPM V&lt;/a> 是 OpenBMB发布的轻量化多模态大模型，其在表1和表2列出了ptraining和SFT数据的具体量级和类别。&lt;/p>
&lt;h2 id="flash-vl">&lt;a href="#flash-vl" class="header-anchor">&lt;/a>Flash-VL
&lt;/h2>&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align: left">Category&lt;/th>
&lt;th style="text-align: right">Ratio&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align: left">Special Enhancement&lt;/td>
&lt;td style="text-align: right">4%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Text&lt;/td>
&lt;td style="text-align: right">21%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Caption&lt;/td>
&lt;td style="text-align: right">4%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Chart&lt;/td>
&lt;td style="text-align: right">16%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Math&lt;/td>
&lt;td style="text-align: right">11%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">OCR&lt;/td>
&lt;td style="text-align: right">3%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">Code&lt;/td>
&lt;td style="text-align: right">8%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align: left">General&lt;/td>
&lt;td style="text-align: right">33%&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>具体数据参见&lt;a class="link" href="http://arxiv.org/abs/2505.09498" target="_blank" rel="noopener"
>原论文&lt;/a>&lt;/p>
&lt;h1 id="参考文献">&lt;a href="#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae" class="header-anchor">&lt;/a>参考文献
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2310.03744" target="_blank" rel="noopener"
>LLaVA 1.5&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2408.03326" target="_blank" rel="noopener"
>LLaVA OneVision&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2412.10360" target="_blank" rel="noopener"
>Apollo&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2406.16860" target="_blank" rel="noopener"
>Cambiran-1&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2405.02246" target="_blank" rel="noopener"
>Idefics2&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2408.12637" target="_blank" rel="noopener"
>Idefics3&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2409.17146" target="_blank" rel="noopener"
>Molmo&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2501.14818" target="_blank" rel="noopener"
>Eagle 2&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2504.15271" target="_blank" rel="noopener"
>Eagle 2.5&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2504.05299" target="_blank" rel="noopener"
>SmolVLM&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2403.09611" target="_blank" rel="noopener"
>MM1&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2409.20566" target="_blank" rel="noopener"
>MM1.5&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2412.05271" target="_blank" rel="noopener"
>InternVL2.5&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2408.01800" target="_blank" rel="noopener"
>MiniCPM V&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Qwen2.5 omni</title><link>https://maosong.website/p/notes-on-qwen2.5-omni/</link><pubDate>Tue, 01 Apr 2025 10:29:00 +0800</pubDate><guid>https://maosong.website/p/notes-on-qwen2.5-omni/</guid><description>&lt;h1 id="介绍">&lt;a href="#%e4%bb%8b%e7%bb%8d" class="header-anchor">&lt;/a>介绍
&lt;/h1>&lt;p>2025年3月26号，Qwen团队发布了Qwen2.5 omni，Qwen2.5 omni是一个多模态模型，支持文本、音频、视频、图像等多个模态的输入，支持文本，音频的输出。
作者提出了TMRoPE来对齐不同的模态，然后还是用了Thinker-Talker的架构来同时生成文本和音频。
Thinker是一个大语言模型，Talker是一个dual-track自回归模型，Talker基于Thinker的hideen states来生成audio token，最后通过一个sliding-window DiT来解码audio token&lt;/p>
&lt;p>现有omni-model存在的问题：&lt;/p>
&lt;ol>
&lt;li>缺乏一个系统的，联合训练多个不同模态的方法&lt;/li>
&lt;li>需要处理不同模态输出时互相干扰的问题&lt;/li>
&lt;li>不能实时理解或者流式输出多模态信息&lt;/li>
&lt;/ol>
&lt;p>Qwen2.5 omni的解决方法：&lt;/p>
&lt;ol>
&lt;li>使用TMRoPE来对齐不同的模态。作者将audio以及video frame按照时间顺序组织成一个交替的架构，然后使用TMRoPE来对齐&lt;/li>
&lt;li>使用Thinker-Talker的架构来同时生成文本和音频。Thinker负责文本输出，Talker负责音频的流式输出。&lt;/li>
&lt;li>在multimodal encoder中使用Block-wise streaming处理技巧来实现实时理解；在输出时，实现了一个dual-track自回归架构来生成audio token，以及一个DiT模型来解码audio token&lt;/li>
&lt;/ol>
&lt;p>Qwen2.5 omni的贡献：&lt;/p>
&lt;ol>
&lt;li>提出了Qwen2.5-omni,可以实时理解多模态信息，并流式输出文本和音频&lt;/li>
&lt;li>提出了TMRoPE，一个可以对齐不同模态的RoPE算法&lt;/li>
&lt;li>提出了Thinker-Talker的架构，来完成实时理解和音频输出&lt;/li>
&lt;li>在多个benchmark上取得了SOTA&lt;/li>
&lt;/ol>
&lt;h1 id="架构">&lt;a href="#%e6%9e%b6%e6%9e%84" class="header-anchor">&lt;/a>架构
&lt;/h1>&lt;h2 id="总览">&lt;a href="#%e6%80%bb%e8%a7%88" class="header-anchor">&lt;/a>总览
&lt;/h2>&lt;p>Qwen2.5-omni的架构如下图所示
&lt;img src="https://maosong.website/p/notes-on-qwen2.5-omni/architecture.png"
width="1346"
height="1110"
loading="lazy"
alt="architecture of Qwen2.5-omni"
class="gallery-image"
data-flex-grow="121"
data-flex-basis="291px"
>&lt;/p>
&lt;p>其中Talker类似于人的嘴，负责基于脑中的信息来生成对话；Thinker类似于人的大脑，负责理解输入的信息，并生成对话的上下文。&lt;/p>
&lt;h2 id="输入">&lt;a href="#%e8%be%93%e5%85%a5" class="header-anchor">&lt;/a>输入
&lt;/h2>&lt;ul>
&lt;li>text: Qwen的tokenizer&lt;/li>
&lt;li>audio： Qwen2-Audio的tokenizer&lt;/li>
&lt;li>vision： Qwen2.5-VL的vision tokenizer&lt;/li>
&lt;/ul>
&lt;p>视频以及TMRoPE。 对于视频来说，Qwen2.5-omni采取了和Qwen2.5-VL一样的MRoPE算法。只是这里的temporal ID对应40ms&lt;/p>
&lt;p>音频：对于音频来说，Qwen2.5-omni也是以40ms进行采样然后encoding。&lt;/p>
&lt;p>视频+音频：对于视频+音频来说，Qwen2.5-omni将视频和音频的token进行交替的排列，来保证时间上信息一致&lt;/p>
&lt;h2 id="输出">&lt;a href="#%e8%be%93%e5%87%ba" class="header-anchor">&lt;/a>输出
&lt;/h2>&lt;ul>
&lt;li>Text：text由Thinker直接输出，这和LLM的输出方式是一样的。&lt;/li>
&lt;li>Speech：Qwen2.5-omni首先构建了&lt;code>qwen-tts-tokenizer&lt;/code>，一个speech codec，用于表示speech的关键信息。
然后基于这些关键信息，Talker通过自回归的方式生成audio tokens以及text tokens。&lt;/li>
&lt;/ul>
&lt;h2 id="流式输出">&lt;a href="#%e6%b5%81%e5%bc%8f%e8%be%93%e5%87%ba" class="header-anchor">&lt;/a>流式输出
&lt;/h2>&lt;p>对于流式输出来说，现在的模型存在latency，具体影响因素如下：&lt;/p>
&lt;ol>
&lt;li>处理多模态输入的延迟&lt;/li>
&lt;li>TTFT （time to first token）&lt;/li>
&lt;li>将第一段speech token解码为audio的时间&lt;/li>
&lt;li>模型架构导致的latency，如模型参数等&lt;/li>
&lt;/ol>
&lt;p>为了解决这个问题，Qwen2.5-omni采取了以下措施：&lt;/p>
&lt;ol>
&lt;li>Support prefilling. 作者修改了audio以及vision encoder来在temporal dimension上支持block-wise attention.
具体来讲，audio encoder只关注2秒内的信息，而vision encoder和Qwen2.5-VL一样，使用了一个MLP patch merger来压缩视觉token&lt;/li>
&lt;li>Streaming Codec generation. 为了提高流式输出的效率，作者还是用了基于Flow-Matching的DiT，输出的code收线使用Flow-Matching转化为一个mel-spectrogram.
然后再通过一个BigVGAN来重建得到对应的waveform. 作者在这里修改了DiT的attention，将其receptive field 限制为4个block，包括lookback of 2 blocks以及lookahead of 1 block.&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen2.5-omni/DiT-sliding-window.png"
width="410"
height="433"
loading="lazy"
alt="Sliding Window DiT"
class="gallery-image"
data-flex-grow="94"
data-flex-basis="227px"
>&lt;/p>
&lt;h1 id="pre-training">&lt;a href="#pre-training" class="header-anchor">&lt;/a>Pre Training
&lt;/h1>&lt;p>模型参数初始化：&lt;/p>
&lt;ul>
&lt;li>LLM： 从Qwen2.5进行初始化&lt;/li>
&lt;li>vision encoder：从Qwen2.5-VL的vision encoder进行初始化&lt;/li>
&lt;li>audio encoder：从Whisper-large-v3进行初始化&lt;/li>
&lt;/ul>
&lt;p>Pretraining和Qwen2.5-VL的训练过程类似，分成了3个stage：&lt;/p>
&lt;ol>
&lt;li>冻结LLM的参数，仅训练vision encoder和audio encoder， 该阶段使用了audio-text以及image-text pairs来进行训练。 数据：image-text, video-text, video-audio, audio-text以及text corpus，
跟Qwen2-Audio类似，作者将hierarchical tags用自然语言prompt进行替换&lt;/li>
&lt;li>训练所有参数，使用更多的多模态数据进行训练。数据包括800B token的image和Video相关数据，300B token的audio数据，100B token的video-audio相关数据。&lt;/li>
&lt;li>将模型的上下文扩展到32K。前两个阶段的上下文长度为8192，本阶段使用了long video data等数据来将模型的上下文扩展到32K。&lt;/li>
&lt;/ol>
&lt;h1 id="post-training">&lt;a href="#post-training" class="header-anchor">&lt;/a>Post Training
&lt;/h1>&lt;h2 id="thinker">&lt;a href="#thinker" class="header-anchor">&lt;/a>Thinker
&lt;/h2>&lt;p>数据格式为ChatML，数据类型包括pure text-based dialogue data, visual-modality conversation data, audio-modality conversation data and mix-modality conversation data.&lt;/p>
&lt;h2 id="talker">&lt;a href="#talker" class="header-anchor">&lt;/a>Talker
&lt;/h2>&lt;p>包括三个阶段&lt;/p>
&lt;ol>
&lt;li>ICL training: 训练Talker学习context continuation&lt;/li>
&lt;li>&lt;a class="link" href="https://maosong.website/p/notes-on-dpo/" target="_blank" rel="noopener"
>DPO&lt;/a> training: 提升speech generation的stability&lt;/li>
&lt;li>multi-speaker instruction fine-tuning: 提升模型的naturalness和controllability&lt;/li>
&lt;/ol>
&lt;h1 id="evaluation">&lt;a href="#evaluation" class="header-anchor">&lt;/a>Evaluation
&lt;/h1>&lt;p>Qwen2.5-omni在两类benchmark上进行来的评测，分别时多模态理解（X-&amp;gt;text）以及音频生成(X-&amp;gt; Speech)&lt;/p>
&lt;p>我们这里主要关注一下speech understanding以及speech generation的benchmark.&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen2.5-omni/audio-to-text-performance.png"
width="1093"
height="1125"
loading="lazy"
alt="Audio-to-Text Performance"
class="gallery-image"
data-flex-grow="97"
data-flex-basis="233px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen2.5-omni/zero-shot-speech-generation.png"
width="904"
height="685"
loading="lazy"
alt="Speech Generation Performance"
class="gallery-image"
data-flex-grow="131"
data-flex-basis="316px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen2.5-omni/single-speaker-speech-generation.png"
width="864"
height="513"
loading="lazy"
alt="single speaker speech generation"
class="gallery-image"
data-flex-grow="168"
data-flex-basis="404px"
>&lt;/p>
&lt;h1 id="总结">&lt;a href="#%e6%80%bb%e7%bb%93" class="header-anchor">&lt;/a>总结
&lt;/h1>&lt;p>Qwen2.5-omni相当于是结合了Qwen2.5-VL以及Mini-omni，跟mini-omni2的输入输出是类似的。不同的点在于，Qwen2.5-omni使用了Thinker-Talker的架构，然后还使用了TMRoPE来对齐不同的模态。总的来说，感觉模型还是更偏重于audio的理解与生成。&lt;/p>
&lt;h1 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2503.20215" target="_blank" rel="noopener"
>Qwen2.5-omni&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2408.16725" target="_blank" rel="noopener"
>Mini-omni&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2410.11190" target="_blank" rel="noopener"
>Mini-omni2&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Understanding Sigmoid Loss in SigLip</title><link>https://maosong.website/p/understanding-sigmoid-loss-in-siglip/</link><pubDate>Fri, 28 Mar 2025 14:55:50 +0800</pubDate><guid>https://maosong.website/p/understanding-sigmoid-loss-in-siglip/</guid><description>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>A simple note to understand Sigmoid Loss in SigLip &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Supported by DeepSeek&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;h2 id="binary-cross-entropy-loss">&lt;a href="#binary-cross-entropy-loss" class="header-anchor">&lt;/a>Binary cross entropy loss
&lt;/h2>&lt;p>Suppose we want to solve the binary classification problem, with label $y\in\{0, 1\}$, a common option is to use binary cross entropy loss:&lt;/p>
$$\mathcal{L}(x, y) = -[y\log (\sigma(z)) + (1-y)\log (1-\sigma(z))]$$&lt;p>where $z=f_\theta(x)$ is the logits predicted by our model $f_\theta$, and $\sigma$ is the sigmoid function:&lt;/p>
$$\sigma(z) := \frac{1}{1 + e^{-z}}$$&lt;p>Let $\sigma(\cdot)$ be the sigmoid function, then we have:&lt;/p>
$$
\sigma(-z) = \frac{1}{1 + e^{z}} = \frac{e^{-z}}{1 + e^{-z}} = 1 - \frac{1}{1 + e^{-z}} = 1- \sigma(z)
$$&lt;p>Now we substitute $\sigma(-z)=1-\sigma(z)$ into the loss function, we obtain:&lt;/p>
$$\mathcal{L}(x, y) = -[y\log (\sigma(z)) + (1-y)\log (\sigma(-z))]$$&lt;p>Note that $y\in\{0, 1\}$ thus for each instance, there are two cases:&lt;/p>
&lt;ul>
&lt;li>If $y=0$, then $\mathcal{L}(x, y) =-\log (\sigma(-z))$&lt;/li>
&lt;li>If $y=1$, then $\mathcal{L}(x, y) =-\log (\sigma(z))$&lt;/li>
&lt;/ul>
&lt;p>Now we want to use a unified expression to express these two cases. Note that this requires fitting a curve that passes two points $(0, -1)$ and $(1, 1)$. The simplest curve is a straight line $y=2x-1$. So, we can further simplify the loss expression into:&lt;/p>
$$\mathcal{L}(x, y) = -\log\left[\sigma((2y-1)z)\right]$$&lt;h2 id="sigmoid-loss-in-siglip">&lt;a href="#sigmoid-loss-in-siglip" class="header-anchor">&lt;/a>Sigmoid Loss in SigLip
&lt;/h2>&lt;p>Now we recall the sigmoid loss in SigLip:&lt;/p>
$$\mathcal{L}(\{\bm{x}, \bm{y}\}_{i=1}^N)=-\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^N\log \frac{1}{1+\exp\left[z_{ij}(-t\bm{x}_i\cdot \bm{y_j}+b)\right]}$$&lt;p>where $t, b$ are learnable parameters, and $z_{ij}=1$ if $i=j$ and $z_{ij}=-1$ otherwise.&lt;/p>
&lt;p>To understand Sigmoid loss, notice that $z_{ij}=2\mathbb{I}_{i=j}-1$, which exactly matches the form we derived earlier.&lt;/p>
&lt;h2 id="why-use-sigmoid-loss">&lt;a href="#why-use-sigmoid-loss" class="header-anchor">&lt;/a>Why Use Sigmoid Loss?
&lt;/h2>&lt;ol>
&lt;li>More stable: avoids $\log 0$.&lt;/li>
&lt;li>More efficient: Compute Sigmoid once.&lt;/li>
&lt;li>More Precise: one line of code without condition checking.&lt;/li>
&lt;/ol>
&lt;h1 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h1>&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>&lt;a class="link" href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhai_Sigmoid_Loss_for_Language_Image_Pre-Training_ICCV_2023_paper.pdf" target="_blank" rel="noopener"
>SigLip&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>&lt;a class="link" href="https://chat.deepseek.com/" target="_blank" rel="noopener"
>DeepSeek&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Notes on Aya Vision</title><link>https://maosong.website/p/notes-on-aya-vision/</link><pubDate>Mon, 17 Mar 2025 17:58:24 +0800</pubDate><guid>https://maosong.website/p/notes-on-aya-vision/</guid><description>&lt;p>Aya Vision是一个多模态大语言模型，包含8B, 32B两个size，支持23种语言。Aya Vision基于 Aya Expanse大语言模型。&lt;/p>
&lt;h2 id="模型架构">&lt;a href="#%e6%a8%a1%e5%9e%8b%e6%9e%b6%e6%9e%84" class="header-anchor">&lt;/a>模型架构
&lt;/h2>&lt;p>Aya Vision的模型架构如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-aya-vision/architecture.png"
width="3624"
height="1316"
loading="lazy"
alt="Aya Vision模型架构"
class="gallery-image"
data-flex-grow="275"
data-flex-basis="660px"
>&lt;/p>
&lt;ul>
&lt;li>Vision Encoder: SigLip2-patch14-384&lt;/li>
&lt;li>Vision-text connector: 2 layer MLP&lt;/li>
&lt;li>LLM: Aya Expanse 8B/ 32B&lt;/li>
&lt;/ul>
&lt;h2 id="训练">&lt;a href="#%e8%ae%ad%e7%bb%83" class="header-anchor">&lt;/a>训练
&lt;/h2>&lt;p>训练包含两个stage：&lt;/p>
&lt;ol>
&lt;li>Vision-language alignment: 仅训练vision-text connector，基于image-text pairs进行训练&lt;/li>
&lt;li>SFT：训练connector和LLM，基于合成的多语种数据进行训练&lt;/li>
&lt;/ol>
&lt;h2 id="多语种数据">&lt;a href="#%e5%a4%9a%e8%af%ad%e7%a7%8d%e6%95%b0%e6%8d%ae" class="header-anchor">&lt;/a>多语种数据
&lt;/h2>&lt;p>为了提高模型的多语种能力，作者先基于English的高质量数据集合成了annotation，然后作者讲这些数据转化为22中语言对应的文本&lt;/p>
&lt;h2 id="model-merging">&lt;a href="#model-merging" class="header-anchor">&lt;/a>Model merging
&lt;/h2>&lt;p>最后为了提高模型在纯文本任务上的表现，作者还使用了model merging的技巧。具体做法就是merge使用的base language model和SFT之后的vision-language model&lt;/p>
&lt;h2 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h2>&lt;ol>
&lt;li>&lt;a class="link" href="https://huggingface.co/blog/aya-vision" target="_blank" rel="noopener"
>Aya Vision Blog&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>Notes on Gemma3</title><link>https://maosong.website/p/notes-on-gemma3/</link><pubDate>Sat, 15 Mar 2025 11:15:29 +0800</pubDate><guid>https://maosong.website/p/notes-on-gemma3/</guid><description>&lt;img src="https://maosong.website/p/notes-on-gemma3/cover.png" alt="Featured image of post Notes on Gemma3" />&lt;p>作者提出了Gemma3系列大模型，包括1B, 4B, 12B, 27B四个size。4B, 12B, 27B三个size均支持多模态，128K的上下文长度以及140种语言。&lt;/p>
&lt;h1 id="方法">&lt;a href="#%e6%96%b9%e6%b3%95" class="header-anchor">&lt;/a>方法
&lt;/h1>&lt;h2 id="数据处理">&lt;a href="#%e6%95%b0%e6%8d%ae%e5%a4%84%e7%90%86" class="header-anchor">&lt;/a>数据处理
&lt;/h2>&lt;h3 id="数据格式">&lt;a href="#%e6%95%b0%e6%8d%ae%e6%a0%bc%e5%bc%8f" class="header-anchor">&lt;/a>数据格式
&lt;/h3>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">[BOS]&amp;lt;start_of_turn&amp;gt;user
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Who are you?&amp;lt;end_of_turn&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&amp;lt;start_of_turn&amp;gt;model
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">My name is Gemma!&amp;lt;end_of_turn&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&amp;lt;start_of_turn&amp;gt;user
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">What is 2+2?&amp;lt;end_of_turn&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&amp;lt;start_of_turn&amp;gt;model
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">2+2=4.&amp;lt;end_of_turn&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>pretrain和SFT的区别在于,pretrain时模型输出以&lt;code>&amp;lt;eos&amp;gt;&lt;/code>结束,SFT时模型输出以&lt;code>&amp;lt;end_of_turn&amp;gt;&lt;/code>结束.&lt;/p>
&lt;h3 id="图片处理">&lt;a href="#%e5%9b%be%e7%89%87%e5%a4%84%e7%90%86" class="header-anchor">&lt;/a>图片处理
&lt;/h3>&lt;p>输入的图片都会被resize到896x896，如果图片精度过大或者不是正方形，则会通过Pan &amp;amp; Scan技巧裁剪为多个子图，然后每个子图分别进行resize。核心处理代码为：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">crop_size_w&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">math&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ceil&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">width&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">num_crops_w&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">crop_size_h&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">math&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ceil&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">height&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">num_crops_h&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Don&amp;#39;t apply PaS if crop size is too small.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">if&lt;/span> &lt;span class="nb">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">crop_size_w&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">crop_size_h&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">pan_and_scan_min_crop_size&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">crop_positions_w&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">crop_size_w&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num_crops_w&lt;/span>&lt;span class="p">)]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">crop_positions_h&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">crop_size_h&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num_crops_h&lt;/span>&lt;span class="p">)]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">image_crops&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">image&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">pos_h&lt;/span> &lt;span class="p">:&lt;/span> &lt;span class="n">pos_h&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">crop_size_h&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">pos_w&lt;/span> &lt;span class="p">:&lt;/span> &lt;span class="n">pos_w&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">crop_size_w&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">pos_h&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">pos_w&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">itertools&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">product&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">crop_positions_h&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">crop_positions_w&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="模型架构">&lt;a href="#%e6%a8%a1%e5%9e%8b%e6%9e%b6%e6%9e%84" class="header-anchor">&lt;/a>模型架构
&lt;/h2>&lt;p>模型包括4个size，分别是1B, 4B, 12B, 27B。1B的模型是单模态的，4B, 12B, 27B的模型是多模态的。对于多模态模型来说：&lt;/p>
&lt;ul>
&lt;li>Vision encoder: Siglip-400M&lt;/li>
&lt;li>Projection layer: linear layer&lt;/li>
&lt;li>LLM: Gemma 3&lt;/li>
&lt;/ul>
&lt;h2 id="模型参数">&lt;a href="#%e6%a8%a1%e5%9e%8b%e5%8f%82%e6%95%b0" class="header-anchor">&lt;/a>模型参数
&lt;/h2>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Vision Encoder&lt;/th>
&lt;th>Embedding Parameters&lt;/th>
&lt;th>Non-embedding Parameters&lt;/th>
&lt;th>context length&lt;/th>
&lt;th>multilingual&lt;/th>
&lt;th>training data&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>1B&lt;/td>
&lt;td>0&lt;/td>
&lt;td>302M&lt;/td>
&lt;td>698M&lt;/td>
&lt;td>32K&lt;/td>
&lt;td>English&lt;/td>
&lt;td>2T tokens&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4B&lt;/td>
&lt;td>417M&lt;/td>
&lt;td>675M&lt;/td>
&lt;td>3,209M&lt;/td>
&lt;td>128K&lt;/td>
&lt;td>140+ languages&lt;/td>
&lt;td>4T tokens&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>12B&lt;/td>
&lt;td>417M&lt;/td>
&lt;td>1,012M&lt;/td>
&lt;td>10,759M&lt;/td>
&lt;td>128K&lt;/td>
&lt;td>140+ languages&lt;/td>
&lt;td>12T tokens&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>27B&lt;/td>
&lt;td>417M&lt;/td>
&lt;td>1,416M&lt;/td>
&lt;td>25,600M&lt;/td>
&lt;td>128K&lt;/td>
&lt;td>140+ languages&lt;/td>
&lt;td>14T tokens&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="attention-layers">&lt;a href="#attention-layers" class="header-anchor">&lt;/a>Attention layers
&lt;/h2>&lt;p>为了提高效率，作者将部分layer的self-attention替换为sliding window attention。论文里是将6 layers为一组，替换一组中最后一层为sliding window attention，其余层为self attention。判断某layer是否为sliding window attention的代码为：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># config.sliding_window_pattern = 6&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">is_sliding&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">bool&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">layer_idx&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sliding_window_pattern&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="long-context">&lt;a href="#long-context" class="header-anchor">&lt;/a>Long context
&lt;/h2>&lt;p>作者将global self-attention的RoPE的base frequency从10K提升到了1M，对于sliding window attention，RoPE的base frequency保持10k不变。&lt;/p>
&lt;h2 id="预训练">&lt;a href="#%e9%a2%84%e8%ae%ad%e7%bb%83" class="header-anchor">&lt;/a>预训练
&lt;/h2>&lt;ol>
&lt;li>数据在模型参数里进行了汇总&lt;/li>
&lt;li>tokenizer使用的是Gemini2.0的tokenizer，基于SentencePiece，vocab大小为262K&lt;/li>
&lt;li>使用知识蒸馏的方法进行训练，每个token按照教师模型的概率采样256个logits，然后使用cross-entropy loss进行训练&lt;/li>
&lt;li>4B, 12B, 27B的模型先在32K的context length上进行训练，然后在128K的context length上进行训练&lt;/li>
&lt;/ol>
&lt;h2 id="quantization-aware-training">&lt;a href="#quantization-aware-training" class="header-anchor">&lt;/a>Quantization Aware training
&lt;/h2>&lt;p>作者提供了quantized版本的模型，模型基于预训练好的模型使用QAT方法进行SFT 5000 steps左右。最后是各个模型的内存占用情况&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Raw (GB)&lt;/th>
&lt;th>&lt;/th>
&lt;th>Quantized (GB)&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Model&lt;/td>
&lt;td>bf16&lt;/td>
&lt;td>Int4&lt;/td>
&lt;td>Int4(blocks=32)&lt;/td>
&lt;td>SFP8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1B&lt;/td>
&lt;td>2.0&lt;/td>
&lt;td>0.5&lt;/td>
&lt;td>0.7&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>+KV&lt;/td>
&lt;td>2.9&lt;/td>
&lt;td>1.4&lt;/td>
&lt;td>1.6&lt;/td>
&lt;td>1.9&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4B&lt;/td>
&lt;td>8.0&lt;/td>
&lt;td>2.6&lt;/td>
&lt;td>2.9&lt;/td>
&lt;td>4.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>+KV&lt;/td>
&lt;td>12.7&lt;/td>
&lt;td>7.3&lt;/td>
&lt;td>7.6&lt;/td>
&lt;td>9.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>12B&lt;/td>
&lt;td>24.0&lt;/td>
&lt;td>6.6&lt;/td>
&lt;td>7.1&lt;/td>
&lt;td>12.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>+KV&lt;/td>
&lt;td>38.9&lt;/td>
&lt;td>21.5&lt;/td>
&lt;td>22.0&lt;/td>
&lt;td>27.3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>27B&lt;/td>
&lt;td>54.0&lt;/td>
&lt;td>14.1&lt;/td>
&lt;td>15.3&lt;/td>
&lt;td>27.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>+KV&lt;/td>
&lt;td>72.7&lt;/td>
&lt;td>32.8&lt;/td>
&lt;td>34.0&lt;/td>
&lt;td>46.1&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="后训练">&lt;a href="#%e5%90%8e%e8%ae%ad%e7%bb%83" class="header-anchor">&lt;/a>后训练
&lt;/h2>&lt;p>数据过滤：筛选掉包含PII，不安全的或者有毒的输出等。留下上下文依赖高，幻觉小的数据&lt;/p>
&lt;p>训练包括升级版的知识蒸馏和基于RL的finetuning，其中RL的reward来自于weight averaged reward models, code execution feedback, ground-truth rewards&lt;/p>
&lt;h1 id="实验">&lt;a href="#%e5%ae%9e%e9%aa%8c" class="header-anchor">&lt;/a>实验
&lt;/h1>&lt;h2 id="表现">&lt;a href="#%e8%a1%a8%e7%8e%b0" class="header-anchor">&lt;/a>表现
&lt;/h2>&lt;p>Gemma3的表现如下图所示&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gemma3/performance.png"
width="1762"
height="767"
loading="lazy"
alt="performance of Gemma3"
class="gallery-image"
data-flex-grow="229"
data-flex-basis="551px"
>&lt;/p>
&lt;p>与PaliGemma 2的表现对比&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gemma3/comparison_with_pali_gemma_2.png"
width="843"
height="933"
loading="lazy"
alt="Comparison with PaliGemma 2"
class="gallery-image"
data-flex-grow="90"
data-flex-basis="216px"
>&lt;/p>
&lt;h2 id="消融实验">&lt;a href="#%e6%b6%88%e8%9e%8d%e5%ae%9e%e9%aa%8c" class="header-anchor">&lt;/a>消融实验
&lt;/h2>&lt;h3 id="local-attention-layers">&lt;a href="#local-attention-layers" class="header-anchor">&lt;/a>Local attention layers
&lt;/h3>&lt;ol>
&lt;li>&lt;code>sliding_window_pattern&lt;/code>对模型的表现影响不大&lt;/li>
&lt;li>sliding window size对模型的表现也不是很大，如下图&lt;/li>
&lt;li>使用sliding window attention可以降低KV cache的内存占用&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gemma3/sliding_window_size.png"
width="865"
height="618"
loading="lazy"
alt="sliding window size"
class="gallery-image"
data-flex-grow="139"
data-flex-basis="335px"
>&lt;/p>
&lt;h3 id="long-context-ablation">&lt;a href="#long-context-ablation" class="header-anchor">&lt;/a>Long context ablation
&lt;/h3>&lt;p>结论为long context会降低模型的性能&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gemma3/long_context.png"
width="864"
height="887"
loading="lazy"
alt="long context"
class="gallery-image"
data-flex-grow="97"
data-flex-basis="233px"
>&lt;/p>
&lt;h3 id="知识蒸馏">&lt;a href="#%e7%9f%a5%e8%af%86%e8%92%b8%e9%a6%8f" class="header-anchor">&lt;/a>知识蒸馏
&lt;/h3>&lt;p>训练token个数比较少的时候，使用小的教师模型效果更好；训练token个数比较多的时候，使用大的教师模型效果更好。&lt;/p>
&lt;h3 id="pan--scan">&lt;a href="#pan--scan" class="header-anchor">&lt;/a>Pan &amp;amp; Scan
&lt;/h3>&lt;p>使用图片原始的aspect ratio训练的模型效果更好。&lt;/p>
&lt;h3 id="memorization">&lt;a href="#memorization" class="header-anchor">&lt;/a>memorization
&lt;/h3>&lt;p>memorization指模型输出的文本与训练数据中文本的重复率。结果发现Gemma3的memorization要更低一些。&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-gemma3/memorization.png"
width="865"
height="824"
loading="lazy"
alt="memorization"
class="gallery-image"
data-flex-grow="104"
data-flex-basis="251px"
>&lt;/p>
&lt;h1 id="结论">&lt;a href="#%e7%bb%93%e8%ae%ba" class="header-anchor">&lt;/a>结论
&lt;/h1>&lt;ol>
&lt;li>sliding window attention在Qwen2.5-VL里已经验证过有效,这里在Gemma3上同样验证了有效性.&lt;/li>
&lt;li>模型架构与PaliGemma系列基本一致，只是attention改变了，然后LLM从Gemma 2升级到了Gemma 3。&lt;/li>
&lt;/ol>
&lt;h1 id="参考文献">&lt;a href="#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae" class="header-anchor">&lt;/a>参考文献
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf" target="_blank" rel="noopener"
>Gemma3 Technical Report&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/huggingface/transformers/tree/main/src/transformers/models/gemma3" target="_blank" rel="noopener"
>transformers-Gemma3 code&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Overview of Qwen-VL series</title><link>https://maosong.website/p/overview-of-qwen-vl-series/</link><pubDate>Sun, 09 Mar 2025 15:11:29 +0800</pubDate><guid>https://maosong.website/p/overview-of-qwen-vl-series/</guid><description>&lt;h1 id="timeline">&lt;a href="#timeline" class="header-anchor">&lt;/a>Timeline
&lt;/h1>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>时间&lt;/th>
&lt;th>模型系列&lt;/th>
&lt;th>Feature&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>2023.08&lt;/td>
&lt;td>Qwen-VL&lt;/td>
&lt;td>Vision-centric understanding&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>Multi-lingual&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>Multi-image&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>Fine-grained visual understanding&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2024.09&lt;/td>
&lt;td>Qwen2-VL&lt;/td>
&lt;td>Image understanding&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>Video understanding (20min)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>Agent capability&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>Multilingual support&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2025.02&lt;/td>
&lt;td>Qwen2.5-VL&lt;/td>
&lt;td>Document parsing&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>Object grounding&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>Long video understadning and grouding (1 hour)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>Agent functionality&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h1 id="models">&lt;a href="#models" class="header-anchor">&lt;/a>Models
&lt;/h1>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>name&lt;/th>
&lt;th>size&lt;/th>
&lt;th>Vision&lt;/th>
&lt;th>Adapter&lt;/th>
&lt;th>LLM&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>1.0&lt;/td>
&lt;td>Qwen-VL&lt;/td>
&lt;td>9.6B&lt;/td>
&lt;td>ViT(1.9B)&lt;/td>
&lt;td>Cross-attention(0.08B)&lt;/td>
&lt;td>Qwen-LLM(7B)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2.0&lt;/td>
&lt;td>Qwen2-VL-2B&lt;/td>
&lt;td>2B&lt;/td>
&lt;td>ViT(675M)&lt;/td>
&lt;td>MLP(0.0341B)&lt;/td>
&lt;td>Qwen2-LLM(1.5B)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Qwen2-VL-7B&lt;/td>
&lt;td>7B&lt;/td>
&lt;td>&lt;/td>
&lt;td>MLP(0.0446B)&lt;/td>
&lt;td>Qwen2-LLM(7.6B)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Qwen2-VL-72B&lt;/td>
&lt;td>72B&lt;/td>
&lt;td>&lt;/td>
&lt;td>MLP(0.0682B)&lt;/td>
&lt;td>Qwen2-LLM(72B)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2.5&lt;/td>
&lt;td>Qwen2.5-VL-3B&lt;/td>
&lt;td>3B&lt;/td>
&lt;td>ViT(675M)&lt;/td>
&lt;td>MLP(0.1817B)&lt;/td>
&lt;td>Qwen2.5-LLM(3B)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Qwen2.5-VL-7B&lt;/td>
&lt;td>7B&lt;/td>
&lt;td>&lt;/td>
&lt;td>MLP(0.3179B)&lt;/td>
&lt;td>Qwen2.5-LLM(7.6B)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Qwe2.5n-VL-72B&lt;/td>
&lt;td>72B&lt;/td>
&lt;td>&lt;/td>
&lt;td>MLP(0.7267B)&lt;/td>
&lt;td>Qwen2.5-LLM(72B)&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Remark&lt;/p>
&lt;ul>
&lt;li>Qwen2-VL的MLP使用的是LayerNorm&lt;/li>
&lt;li>Qwen2.5-VL的MLP使用的是RMSNorm&lt;/li>
&lt;li>出了Qwen-VL之外，所有模型均有对应的Instruct版本，这里为了方便没有列出。Qwen-VL对应的是Qwen-VL-chat。&lt;/li>
&lt;/ul>
&lt;h1 id="data--training">&lt;a href="#data--training" class="header-anchor">&lt;/a>Data &amp;amp; training
&lt;/h1>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model series&lt;/th>
&lt;th>stage&lt;/th>
&lt;th>data&lt;/th>
&lt;th>Vision&lt;/th>
&lt;th>Adapter&lt;/th>
&lt;th>LLM&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Qwen-VL&lt;/td>
&lt;td>pretraining&lt;/td>
&lt;td>1.4B&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Multi-task pretraining&lt;/td>
&lt;td>96.8M&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>SFT&lt;/td>
&lt;td>350K&lt;/td>
&lt;td>&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen2-VL&lt;/td>
&lt;td>pretraining&lt;/td>
&lt;td>600b tokens&lt;/td>
&lt;td>✅？&lt;/td>
&lt;td>✅？&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Multi-task pretraining&lt;/td>
&lt;td>800b tokens&lt;/td>
&lt;td>✅？&lt;/td>
&lt;td>✅？&lt;/td>
&lt;td>✅？&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>SFT&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>✅？&lt;/td>
&lt;td>✅？&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Qwen2.5-VL&lt;/td>
&lt;td>Visual Pre-Training&lt;/td>
&lt;td>1.5T token&lt;/td>
&lt;td>✅？&lt;/td>
&lt;td>✅？&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Multimodal Pre-Training&lt;/td>
&lt;td>2T token&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>Long-Context Pre-Training&lt;/td>
&lt;td>0.6T token&lt;/td>
&lt;td>✅？&lt;/td>
&lt;td>✅？&lt;/td>
&lt;td>✅？&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>SFT&lt;/td>
&lt;td>2M samples&lt;/td>
&lt;td>&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>&lt;a class="link" href="https://maosong.website/p/notes-on-dpo/" target="_blank" rel="noopener"
>DPO&lt;/a>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Remark&lt;/p>
&lt;ol>
&lt;li>Qwen2-VL按照技术报告的说法是follow了Qwen-VL的训练方式，因此这里也使用了同样的表示&lt;/li>
&lt;li>带问号的地方代表不确定，为个人猜测。请谨慎参考。&lt;/li>
&lt;li>难以确定的原因是Qwen-VL系列将projection layer和ViT作为一个模块，因此比较难区分是否训练了projection layer&lt;/li>
&lt;/ol>
&lt;p>技术报告的训练框架图&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/overview-of-qwen-vl-series/qwen-vl-training.png"
width="1908"
height="868"
loading="lazy"
alt="qwen-vl-training"
class="gallery-image"
data-flex-grow="219"
data-flex-basis="527px"
>&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/overview-of-qwen-vl-series/qwen2-5-training.png"
width="1354"
height="468"
loading="lazy"
alt="qwen2-5-training"
class="gallery-image"
data-flex-grow="289"
data-flex-basis="694px"
>&lt;/p>
&lt;p>References&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2308.12966" target="_blank" rel="noopener"
>Qwen-VL&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2409.12191" target="_blank" rel="noopener"
>Qwen2-VL&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2502.13923" target="_blank" rel="noopener"
>Qwen2.5-VL&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Qwen2.5 VL</title><link>https://maosong.website/p/notes-on-qwen2.5-vl/</link><pubDate>Tue, 04 Mar 2025 10:46:42 +0800</pubDate><guid>https://maosong.website/p/notes-on-qwen2.5-vl/</guid><description>&lt;h1 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h1>&lt;p>2025年2月20号Qwen团队发布了Qwen2.5 VL技术报告，Qwen2.5 VL包括3B，7B， 72B三个size。Qwen2.5-VL主要在架构，数据上进行了改进。通过评测，Qwen2.5-VL在多个benchmark上取得了SOTA。&lt;/p>
&lt;p>Qwen2.5 VL认为已有模型的缺点为：&lt;/p>
&lt;ul>
&lt;li>计算复杂度高&lt;/li>
&lt;li>上下文理解能力有限&lt;/li>
&lt;li>细粒度visual perception能力不足&lt;/li>
&lt;li>在不同上下文长度下表现不一致&lt;/li>
&lt;/ul>
&lt;p>Qwen2.5 VL的贡献为：&lt;/p>
&lt;ol>
&lt;li>使用了一个从零开始训练的ViT作为vision encoder，并且在ViT中使用了window attention，来提高计算效率&lt;/li>
&lt;li>使用了dynamic FPS sampling，用于处理不同采样率的视频输入&lt;/li>
&lt;li>将MRoPE扩展到了temporal domain上，进一步提高了模型在与时间相关任务上的表现&lt;/li>
&lt;li>使用了更高质量的数据集，其中预训练阶段使用了4.1T的token&lt;/li>
&lt;/ol>
&lt;p>Qwen2.5 VL的主要亮点为：&lt;/p>
&lt;ul>
&lt;li>优秀的document parsing能力&lt;/li>
&lt;li>精确的object grounding能力&lt;/li>
&lt;li>针对长视频的理解和细粒度grounding能力&lt;/li>
&lt;li>针对UI的agent functionality&lt;/li>
&lt;/ul>
&lt;h1 id="模型架构">&lt;a href="#%e6%a8%a1%e5%9e%8b%e6%9e%b6%e6%9e%84" class="header-anchor">&lt;/a>模型架构
&lt;/h1>&lt;h2 id="总览">&lt;a href="#%e6%80%bb%e8%a7%88" class="header-anchor">&lt;/a>总览
&lt;/h2>&lt;p>Qwen2.5 VL和Qwen2 VL的架构基本一致，包括Vision Encoder，Language Encoder，以及projector layer三个部分，其架构图如下：&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen2.5-vl/architecture.png"
width="1345"
height="889"
loading="lazy"
alt="Qwen2.5 VL架构图"
class="gallery-image"
data-flex-grow="151"
data-flex-basis="363px"
>&lt;/p>
&lt;ul>
&lt;li>LLM： 使用Qwen2.5 LLM作为LLM，并且将1D-RoPE升级为了MRoPE&lt;/li>
&lt;li>Vision Encoder： 使用一个从零开始训练的ViT架构，patch_size为14，position embedding为2D-RoPE， attention为window attention和self-attention的混合，其中，只有四层使用的是self-attention.对于输入的图片，ViT会将图片resize到28的整数倍。&lt;/li>
&lt;li>Projector Layer： 使用的是一个两层的MLP&lt;/li>
&lt;/ul>
&lt;p>模型的参数配置如下图&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen2.5-vl/model_config.png"
width="1180"
height="861"
loading="lazy"
alt="Qwen2.5 VL模型参数配置"
class="gallery-image"
data-flex-grow="137"
data-flex-basis="328px"
>&lt;/p>
&lt;h2 id="vision-encoder">&lt;a href="#vision-encoder" class="header-anchor">&lt;/a>Vision Encoder
&lt;/h2>&lt;p>Vision encoder的主要改进点为：&lt;/p>
&lt;ol>
&lt;li>使用了window attention来提升计算效率，window attention的size为 $112\times 112$， 对应为 $8\times 8$ 个patch. 这样做的好处是可以不用对图片做scaling&lt;/li>
&lt;li>使用了2D RoPE来捕捉空间信息，使用3D RoPE来捕捉视频输入的时间信息&lt;/li>
&lt;li>与LLM的结构进行对齐，包括使用RMSNorm替换LayerNorm，使用SwiGLU替换ReLU&lt;/li>
&lt;/ol>
&lt;h2 id="输入处理">&lt;a href="#%e8%be%93%e5%85%a5%e5%a4%84%e7%90%86" class="header-anchor">&lt;/a>输入处理
&lt;/h2>&lt;p>对于图片输入，Qwen2.5 VL使用原始图片的空间信息来构建坐标，而不是将坐标normalize到[0, 1000]之间，这样让模型可以处理不同精度的图片输入。&lt;/p>
&lt;p>对于视频输入，因为使用了3D RoPE，因此Qwen2.5 VL可以处理不同帧率的视频输入，这样就避免了不同帧率视频对模型视频理解带来的影响。这一点和Apollo里的想法是一样的。具体来说，Qwen2.5 VL首先将连续的两帧group到了一起，然后使用了temporal ID来将position和视频所对应的时间进行对齐。这里可以看Qwen2.5 VL的代码：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Temporal (Time): 3 patches, representing different segments of the video in time.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Height: 2 patches, dividing each frame vertically.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Width: 2 patches, dividing each frame horizontally.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">We also have some important parameters:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">fps (Frames Per Second): The video&amp;#39;s frame rate, set to 1. This means one frame is processed each second.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">tokens_per_second: This is a crucial parameter. It dictates how many &amp;#34;time-steps&amp;#34; or &amp;#34;temporal tokens&amp;#34; are conceptually packed into a one-second interval of the video. In this case, we have 25 tokens per second. So each second of the video will be represented with 25 separate time points. It essentially defines the temporal granularity.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">temporal_patch_size: The number of frames that compose one temporal patch. Here, it&amp;#39;s 2 frames.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">interval: The step size for the temporal position IDs, calculated as tokens_per_second * temporal_patch_size / fps. In this case, 25 * 2 / 1 = 50. This means that each temporal patch will be have a difference of 50 in the temporal position IDs.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">input_ids: [V V V V V V V V V V V V T T T T T], here V is for vision.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">vision temporal position_ids: [0, 0, 0, 0, 50, 50, 50, 50, 100, 100, 100, 100]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">vision height position_ids: [0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">vision width position_ids: [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">text temporal position_ids: [101, 102, 103, 104, 105]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">text height position_ids: [101, 102, 103, 104, 105]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">text width position_ids: [101, 102, 103, 104, 105]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Here we calculate the text start position_ids as the max vision position_ids plus 1.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>这里&lt;code>fps&lt;/code>为1，表示每一帧对应一秒，&lt;code>tokens_per_second&lt;/code>为25，表示每秒包含25个token，&lt;code>temporal_patch_size&lt;/code>为2，表示每个temporal patch包含2个frame。因此一个patch里面，就包含了2frame, 对应50tokens. 然后前面提到连续两帧会被group到一起，因此每个temporal patch对应4个spatial patches. 其position_ids为：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="p">[(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h1 id="训练">&lt;a href="#%e8%ae%ad%e7%bb%83" class="header-anchor">&lt;/a>训练
&lt;/h1>&lt;h2 id="预训练">&lt;a href="#%e9%a2%84%e8%ae%ad%e7%bb%83" class="header-anchor">&lt;/a>预训练
&lt;/h2>&lt;h3 id="数据">&lt;a href="#%e6%95%b0%e6%8d%ae" class="header-anchor">&lt;/a>数据
&lt;/h3>&lt;p>预训练阶段使用了4.1T的token，包括image captions, interleaved image-text data, optical character recognition (OCR) data, visual knowledge (e.g., celebrity, landmark, flora, and fauna identification), multi-modal academic questions, localization data, document parsing data, video descriptions, video localization, and agent-based interaction data. 作者详细介绍了以下几种数据：&lt;/p>
&lt;ul>
&lt;li>Interleaved image-text data： 主要1. 提高模型的上下文学习能力；2. 保持模型的text-only能力；3.包含一些通用信息。数据清洗包括：1. 基于text-only quality过滤； 2. 基于image-text相关性过滤；3. 基于image-text互补程度过滤；4.基于information density balance过滤.&lt;/li>
&lt;li>Grounding data：作者使用了Grounding DINO, SAM等模型来生成一些grounding data.为了提升模型在open-vocabulary detection上的能力，作者将训练数据集扩展到了1万个object category上，作者还使用了一些point-based object grounding data来提升模型的能力&lt;/li>
&lt;li>Document Parsing data：作者基于text,image, music sheets和chemical formulas合成了一些HTML格式的数据，然后根据tag和bounding box来提升模型document parsing的能力&lt;/li>
&lt;li>OCR data：作者使用了开源，合成和in-house的数据集，数据集主要提到multi-lingual以及1M的chart-type数据&lt;/li>
&lt;li>Video data：作者通过pipeline构建了long video caption来提升模型长视频的理解能力&lt;/li>
&lt;li>Agent data：作者收集了一些mobile device和网页的screenshots,然后基于agent框架来合成控制轨迹&lt;/li>
&lt;/ul>
&lt;h3 id="训练-1">&lt;a href="#%e8%ae%ad%e7%bb%83-1" class="header-anchor">&lt;/a>训练
&lt;/h3>&lt;p>&lt;img src="https://maosong.website/p/notes-on-qwen2.5-vl/pretraining.png"
width="1217"
height="428"
loading="lazy"
alt="Qwen2.5 VL预训练阶段"
class="gallery-image"
data-flex-grow="284"
data-flex-basis="682px"
>&lt;/p>
&lt;p>如上图所示，Qwen2.5 VL的预训练阶段包括了三个阶段：&lt;/p>
&lt;ol>
&lt;li>Visual Pretraining:这一阶段使用了image-caption, knowledge, OCR数据集，旨在提升ViT提取视觉特征的能力&lt;/li>
&lt;li>multimodal pretraining:这一阶段在第一阶段的基础上增加了pure-text, interleaved data, VQA, Video grounding, agent data, 旨在提升模型处理复杂视觉信息的能力&lt;/li>
&lt;li>long-context pretraining: 这一阶段，在第二阶段的基础上，增加了long video, long agent, long document data，旨在提升模型处理长上下文的能力&lt;/li>
&lt;/ol>
&lt;h2 id="后训练">&lt;a href="#%e5%90%8e%e8%ae%ad%e7%bb%83" class="header-anchor">&lt;/a>后训练
&lt;/h2>&lt;h3 id="数据-1">&lt;a href="#%e6%95%b0%e6%8d%ae-1" class="header-anchor">&lt;/a>数据
&lt;/h3>&lt;p>SFT阶段使用大概2M的样本进行训练，其中纯文本和多模态数据占比为1:1，语言主要是中文和英文。&lt;/p>
&lt;p>为了保证数据质量，作者提供了一个数据清洗的pipeline，包括：&lt;/p>
&lt;ol>
&lt;li>domain-specific categorization: 作者基于Qwen2-VL-72B构建了Qwen2-VL-Instag，用于将QA pair分为8个大类别，30个小类别&lt;/li>
&lt;li>Domain-tailed filtering:作者使用了rule-based和model-based方法来提升数据的质量
&lt;ul>
&lt;li>rule-based filtering: 重复性检测，格式检测等&lt;/li>
&lt;li>model-based filtering: 使用基于Qwen2.5-VL训练的reward model来从多个维度评估QA pair的质量&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>为了进一步提升模型的推理能力，作者还是用了rejection sampling来refine 数据集。&lt;/p>
&lt;h3 id="训练-2">&lt;a href="#%e8%ae%ad%e7%bb%83-2" class="header-anchor">&lt;/a>训练
&lt;/h3>&lt;p>post-training阶段分为SFT和&lt;a class="link" href="https://maosong.website/p/notes-on-dpo/" target="_blank" rel="noopener"
>DPO&lt;/a>两个小阶段，这个阶段都会冻结VIT. SFT阶段使用大多数的训练数据，而DPO阶段专注于image-text data和pure text data，以更好地进行对齐。具体做法就是基于Grounding truth，使用checkpoint来评估数据的质量，然后只保留答案正确的数据来训练。&lt;/p>
&lt;h1 id="评测">&lt;a href="#%e8%af%84%e6%b5%8b" class="header-anchor">&lt;/a>评测
&lt;/h1>&lt;ol>
&lt;li>通用VQA
&lt;img src="https://maosong.website/p/notes-on-qwen2.5-vl/general_vqa.png"
width="1356"
height="656"
loading="lazy"
alt="通用VQA"
class="gallery-image"
data-flex-grow="206"
data-flex-basis="496px"
>&lt;/li>
&lt;li>文档理解和OCR
&lt;img src="https://maosong.website/p/notes-on-qwen2.5-vl/document_understanding.png"
width="1358"
height="655"
loading="lazy"
alt="文档理解和OCR"
class="gallery-image"
data-flex-grow="207"
data-flex-basis="497px"
>&lt;/li>
&lt;li>空间理解
&lt;img src="https://maosong.website/p/notes-on-qwen2.5-vl/grounding.png"
width="1324"
height="714"
loading="lazy"
alt="空间理解"
class="gallery-image"
data-flex-grow="185"
data-flex-basis="445px"
>&lt;/li>
&lt;li>视频理解和Grounding
&lt;img src="https://maosong.website/p/notes-on-qwen2.5-vl/video.png"
width="1320"
height="672"
loading="lazy"
alt="视频理解和Grounding"
class="gallery-image"
data-flex-grow="196"
data-flex-basis="471px"
>&lt;/li>
&lt;li>Agent
&lt;img src="https://maosong.website/p/notes-on-qwen2.5-vl/agent.png"
width="1374"
height="337"
loading="lazy"
alt="Agent"
class="gallery-image"
data-flex-grow="407"
data-flex-basis="978px"
>&lt;/li>
&lt;/ol>
&lt;h1 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://github.com/QwenLM/Qwen2.5-VL" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2502.13923" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Notes on Kimi k1.5</title><link>https://maosong.website/p/notes-on-kimi-k1.5/</link><pubDate>Sat, 08 Feb 2025 10:09:52 +0800</pubDate><guid>https://maosong.website/p/notes-on-kimi-k1.5/</guid><description>&lt;p>论文提出了Kimi k1.5， 一个基于强化学习训练的多模态推理模型。作者介绍了Kimi k1.5的训练方法，以及infra上的优化。作者的主要贡献如下：&lt;/p>
&lt;ol>
&lt;li>作者发现，模型的上下文长度可以有效提升LLM的推理能力。&lt;/li>
&lt;li>作者提出了基于online mirror descent的强化学习训练方法。&lt;/li>
&lt;li>作者发现可以通过long context scaling和policy optimization来提升模型的推理能力。&lt;/li>
&lt;li>作者提出了long2short的推理方法，可以有效提升short CoT模型的推理能力。&lt;/li>
&lt;/ol>
&lt;h2 id="architecture">&lt;a href="#architecture" class="header-anchor">&lt;/a>Architecture
&lt;/h2>&lt;p>论文中包括两个模型，一个是k1.5 base model， 其是一个多模态大模型。另一个是Kimi-1.5 reasoning model（简称为k1.5），k1.5是基于kimi-1.5 base model， 通过强化学习训练得到的推理模型。&lt;/p>
&lt;h3 id="kimi-15-base-model">&lt;a href="#kimi-15-base-model" class="header-anchor">&lt;/a>Kimi-1.5 base model
&lt;/h3>&lt;p>k1.5 base model是一个基于transformer的多模态大模型，论文没有给出详细的模型结构，只有如下的示意图&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k1.5/k1-5-model-arch.png"
width="1210"
height="287"
loading="lazy"
alt="Architecture of k1.5 base model"
class="gallery-image"
data-flex-grow="421"
data-flex-basis="1011px"
>&lt;/p>
&lt;p>k1.5 base model的训练包括三个阶段：&lt;/p>
&lt;ol>
&lt;li>Vision-language pretraining stage: 这一阶段的目的是让模型拥有语言和视觉的表征能力。训练时，模型先在纯文本的数据上进行训练，然后提高加入图文交错的数据进行训练，直到训练数据中图文交错的数据占比达到30%。模型的更新方式为先冻结LLM更新visual tower, 然后解冻LLM更新所有参数。&lt;/li>
&lt;li>Vision-language cooldown stage: 这一阶段的目的是让模型保持多模态理解能力。作者发现，使用合成数据可以提高模型在这一阶段的表现。因此，作者使用闭源大语言模型，基于math, knowledge和code domain的pretraining data来合成了一些QA pair数据对，然后讲这些QA pair数据对加入到训练数据中。&lt;/li>
&lt;li>Long-context activation stage: 这一阶段的目的是让模型在长上下文的数据上进行训练，以提升模型在长上下文上的理解能力。这一阶段包含了40%的full attention data以及60%的partial attention data。其中，full attention data是基于真实数据和合成的QA以及summary数据得到的，partial attention data是在cooldown data找那个通过均匀采样得到的。这一阶段，模型的上下文长度从4096逐步增加到131072。&lt;/li>
&lt;/ol>
&lt;h3 id="kimi-k15">&lt;a href="#kimi-k15" class="header-anchor">&lt;/a>Kimi k1.5
&lt;/h3>&lt;p>Kimi k1.5是基于k1.5 base model， 通过进一步训练得到的推理模型。k1.5的训练包括四个阶段：&lt;/p>
&lt;ol>
&lt;li>pretraining：这一步就是k1.5 base model的训练， 包括三个小阶段。前面已经介绍过了。&lt;/li>
&lt;li>vanilla SFT：这一步的目的是使得k1.5 base model具有指令跟随能力。对于non-reasoning任务，作者先使用人类标注产生一个seed dataset，然后基于seed dataset训练一个seed model，最后根据收集的prompt来使用seed model来生成回答。对于reasoning任务，作者使用了rejection sampling来扩展SFT数据集&lt;/li>
&lt;li>long-CoT SFT： 这一步的目的是让模型能够像人类一样进行推理，能够掌握最基本的推理策略，即：planning，evaluation，reflection和exploration。从而为RL训练提供一个良好的初始化。&lt;/li>
&lt;li>RL：这一步就是通过RL来提高模型的推理能力。我们在下一节中进行详细的介绍。&lt;/li>
&lt;/ol>
&lt;h2 id="rl">&lt;a href="#rl" class="header-anchor">&lt;/a>RL
&lt;/h2>&lt;h3 id="problem-definition">&lt;a href="#problem-definition" class="header-anchor">&lt;/a>Problem Definition
&lt;/h3>&lt;p>给定一个训练数据集 $\mathcal{D} = \{(x_i, y_i^\star)\}_ {i=1}^n$， 其中 $x_ i$ 是问题， $y_{i}^\star$ 是ground truth。我们希望找到一个模型 $\pi_{\theta}$ ，来解决这个问题。通常问题比较难，因此我们使用chain of thought（CoT）的方法来解决这个问题。具体做法就是让模型输出中间步骤 $z=(z_1, z_2, ..., z_m)$， 来连接问题 $x_i$ 和答案 $y$。其中， $z_j$ 是模型在第 $j$ 步的推理结果，即 $z_t\sim\pi_{\theta}(x_i, z_{&lt;t})$, $y\sim \pi_{\theta}(x_i, z)$。&lt;/p>
&lt;p>通常，我们还会使用一个reward model $r_{\phi}$，来评估模型输出的答案的质量。一个常用的reward model是基于答案的正确性来评估的，即 $r_{\phi}(x_i, y, y^\star) = \mathbb{I}[y=y_i^\star]$ 。这样，我们要求解的问题就变成了最大化中间步骤和最终答案的reward，即&lt;/p>
$$
\max_{\theta} \mathbb{E}_ {(x, y^\star)\sim\mathcal{D},(y,z,)\sim\pi_{\theta}} r_{\phi}(x, y, y^\star)
$$&lt;h3 id="policy-optimization">&lt;a href="#policy-optimization" class="header-anchor">&lt;/a>Policy Optimization
&lt;/h3>&lt;p>作者使用了online policy mirror descent来解决上面提到的优化问题。在每个iteration中，存在一个reference model $\pi_{\theta_r}$ ， 以及一个当前要更新的模型 $\pi_{\theta}$ 。online policy mirror descent要解决的优化问题为：&lt;/p>
$$
\min_{\theta} \mathbb{E}_ {(x, y^\star)\sim\mathcal{D}} \left[\mathbb{E}_ {(y,z,)\sim\pi_{\theta}}\left[r_{\phi}(x, y, y^\star) - \beta \mathcal{D}_ {KL}(\pi_{\theta}(y,z\mid x)\Vert \pi_{\theta_r}(y,z\mid x))\right]\right]
$$&lt;p>其中， $\beta$ 是超参数，用于平衡reward和KL散度 $\mathcal{D}_{KL}(\cdot)$.&lt;/p>
&lt;p>上述问题有一个闭式解，即：&lt;/p>
$$
\pi^\star(y,z\mid x)=\pi_{\theta_r}(y,z\mid x)\exp\left(\frac{r_{\phi}(x, y, y^\star)}{\beta}\right)/Z(x;\beta)
$$&lt;p>其中，&lt;/p>
$$
Z(x;\beta):=\sum_{y,z}\pi_{\theta_r}(y,z\mid x)\exp\left(\frac{r_{\phi}(x, y, y^\star)}{\beta}\right)
$$&lt;p>是归一化因子。在闭式解的表达式中，我们对两边取对数，得到：&lt;/p>
$$
r_{\phi}(x, y, y^\star) - \beta \log Z - \beta \log\frac{\pi^\star(y,z\mid x)}{\pi_{\theta_r}(y,z\mid x)}=0
$$&lt;p>这是最优策略满足的等式。作者这里使用了L2 loss来优化当前策略 $\pi_{\theta}$ ，即：&lt;/p>
$$
\min_{\theta} \mathbb{E}_ {(x, y^\star)\sim\mathcal{D}} \left[\mathbb{E}_ {(y,z,)\sim\pi_ {\theta_r}}\left[r_{\phi}(x, y, y^\star) - \beta \log Z - \beta \log\frac{\pi_{\theta}(y,z\mid x)}{\pi_{\theta_r}(y,z\mid x)}\right]^2\right]
$$&lt;p>这里需要注意的一点是response $(y,z)$ 是基于reference model $\pi_{\theta_r}$ 生成的，而不再是基于当前策略 $\pi_{\theta}$ 生成的。这是Kimi-1.5的RL训练和传统RL训练的一个不同点。这也是为什么k1.5说他是off-policy的原因。&lt;/p>
&lt;p>接下来，我们就可以对上面的目标函数求梯度，我们将里层的函数展开为 $(a-b-c)^2=(a-b)^2-2(a-b)c+c^2$ 的形式，然后对 $\theta$ 求梯度，得到：&lt;/p>
$$
\mathbb{E}_ {(x, y^\star)\sim\mathcal{D}} \left[\mathbb{E}_ {(y,z,)\sim\pi_{\theta_r}}\left[\nabla_{\theta}\log \pi_{\theta}(y,z\mid x)(r_{\phi}(x, y, y^\star) - \beta \log Z) - \frac{\beta}{2} \nabla_{\theta}\left(\log\frac{\pi_{\theta}(y,z\mid x)}{\pi_{\theta_r}(y,z\mid x)}\right)^2\right]\right]
$$&lt;p>如果我们对每个问题$x$,采样 $k$ 个response $(y,z)$ ，那么我们可以近似上式中内部的期望，得到：&lt;/p>
$$
\frac{1}{k}\sum_{i=1}^k\left[\nabla_{\theta}\log \pi_{\theta}(y_i,z_i\mid x)(r_{\phi}(x, y_i, y^\star) - \bar{r}) - \frac{\beta}{2} \nabla_{\theta}\left(\log\frac{\pi_{\theta}(y_i,z_i\mid x)}{\pi_{\theta_r}(y_i,z_i\mid x)}\right)^2\right]
$$&lt;p>这里， $\bar{r}$ 是 $\beta \log Z$ 的估计值。作者提供了两种估计方法：&lt;/p>
&lt;ol>
&lt;li>基于采样的 $(y_i,z_i)\sim \pi_{\theta_r}$ 对 $\beta \log Z$ 进行估计：&lt;/li>
&lt;/ol>
$$
\bar{r}=\beta\log\frac{1}{k}\sum_{j=1}^k\exp\left(\frac{r_{\phi}(x, y_j, y^\star)}{\beta}\right)
$$&lt;ol start="2">
&lt;li>直接使用samples rewards的平均值来近似：&lt;/li>
&lt;/ol>
$$
\bar{r}=\frac{1}{k}\sum_{i=1}^k r_{\phi}(x, y_i, y^\star)
$$&lt;p>作者提到，第二种方法效果很好并且计算效率更高，因此作者在实验中使用了第二种方法。&lt;/p>
&lt;p>训练时，作者每次从数据集中采样一个batch的样本，然后使用上述的梯度更新模型。模型参数更新完之后，作者将新的模型作为reference model，然后重复上述过程。&lt;/p>
&lt;blockquote>
&lt;p>Remark
作者还探究了为什么不使用value network。作者认为，在强化学习中，value network通常用于评估当前状态的价值，但是在reasoning model中，训练的目的是让模型能够充分探索，以提高模型在不同任务中的泛化性。因此，作者认为使用value network可能会限制模型的探索能力。&lt;/p>
&lt;/blockquote>
&lt;h3 id="training-strategy">&lt;a href="#training-strategy" class="header-anchor">&lt;/a>Training Strategy
&lt;/h3>&lt;p>作者提出了两个提高采样效率的方法：&lt;/p>
&lt;ol>
&lt;li>Curriculum Sampling. 这种采样方式会将问题按照难度进行排序，然后按照难度从易到难进行采样。以逐步提高模型的推理能力。&lt;/li>
&lt;li>Prioritized Sampling.作者还采用了prioritized sampling来提高采样效率。也就是说，对于回答错误的问题，模型会进行更多的采样。&lt;/li>
&lt;/ol>
&lt;h2 id="infra">&lt;a href="#infra" class="header-anchor">&lt;/a>Infra
&lt;/h2>&lt;h3 id="training-system">&lt;a href="#training-system" class="header-anchor">&lt;/a>Training System
&lt;/h3>&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k1.5/k1-5-training-system.png"
width="1112"
height="530"
loading="lazy"
alt="Large Scale Reinforcement Learning Training System for LLM"
class="gallery-image"
data-flex-grow="209"
data-flex-basis="503px"
>&lt;/p>
&lt;p>为了高效地进行RL训练，作者构建了一个iterative synchronous RL训练框架。框架如上图所示。框架的创新之处在于&lt;strong>Partial Rollout&lt;/strong>，该方法可以更高效的处理复杂的推理轨迹。框架包含了以下几部分：&lt;/p>
&lt;ol>
&lt;li>rollout worker: 该部分负责生成推理轨迹。然后将生成的推理轨迹加入到replay buffer中。&lt;/li>
&lt;li>master: 管理data flow和replay buffer和train worker之间的通信。&lt;/li>
&lt;li>train worker: 根据获取的rollout轨迹更新模型。&lt;/li>
&lt;li>code execution service: 在代码相关任务中，该部分负责执行代码，并返回执行结果。&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Partial Rollout&lt;/strong>：Partial Rollout的目的是为了整体的训练效率。其做法就是给定一个固定的outpu token budget，然后限制推理轨迹的长度。当长度超过token限制时，没有完成的部分就会被存入到replay buffer中，以供后续的训练使用。&lt;/p>
&lt;h3 id="deployment-framework">&lt;a href="#deployment-framework" class="header-anchor">&lt;/a>Deployment Framework
&lt;/h3>&lt;p>根据前面的介绍，我们知道k1.5每个iteration的训练包括两步：&lt;/p>
&lt;ol>
&lt;li>inference phase: 基于Reference model进行采样，生成推理轨迹。&lt;/li>
&lt;li>training phase: 基于采样得到的推理轨迹，更新模型。&lt;/li>
&lt;/ol>
&lt;p>如果我们直接使用Megatron和vLLM来实现上述的训练和推理，那么会存在以下问题：&lt;/p>
&lt;ol>
&lt;li>Megatron和vLLM的并行策略可能并不一致，因此很难进行参数共享&lt;/li>
&lt;li>在训练过程中，vLLM可能会保留一些GPU，这就导致了训练和推理的资源分配不均。&lt;/li>
&lt;li>推理和训练的资源需求可能并不一致，无法动态调控显卡资源&lt;/li>
&lt;/ol>
&lt;p>因此，作者构建了一个hybrid deployment framework来解决上述问题，其框架如下图所示。&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k1.5/k1-5-hybrid-deployment-framework.png"
width="867"
height="560"
loading="lazy"
alt="Hybrid Deployment Framework"
class="gallery-image"
data-flex-grow="154"
data-flex-basis="371px"
>&lt;/p>
&lt;p>该框架的主要优势在于使用Kubernetes Sidecar containers来在一个pod中共享所有的GPU资源。这样就避免了资源分配不均的问题。&lt;/p>
&lt;h2 id="数据">&lt;a href="#%e6%95%b0%e6%8d%ae" class="header-anchor">&lt;/a>数据
&lt;/h2>&lt;h3 id="数据处理">&lt;a href="#%e6%95%b0%e6%8d%ae%e5%a4%84%e7%90%86" class="header-anchor">&lt;/a>数据处理
&lt;/h3>&lt;ol>
&lt;li>RL prompt set. 作者认为prompt的quality和diversity对RL训练至关重要。因此作者基于diverse coverage, balanced difficuty以及accurate evaluation来构建了一个RL prompt set。对于多样性，作者确保数据集覆盖多个domain。对于平衡难度，作者通过多次采样，根据通过率来给每个样本的难易程度打分。对于准确性，作者筛掉了多选题，正确题和证明题，确保模型能够生成正确的推理步骤。最后，作者还让模型进行猜测，如果模型猜测的正确率比较高，那么就认为该样本是easy-to-hack的，就会筛掉。&lt;/li>
&lt;li>Test Case Generation for Coding. 作者使用&lt;a class="link" href="https://github.com/luogu-dev/cyaron" target="_blank" rel="noopener"
>CYaRon&lt;/a>来生成coding任务的测试用例。通过与Kimi k1.5结合，作者构建了一个包含323个测试用例的训练集。&lt;/li>
&lt;li>Reward modeling for Math. 作者使用了两种方法来提高reward model的准确性。第一个是classic RM，作者基于InstructGPT，构造了大约800K的数据训练了一个value-head based RM。模型会基于问题，参考答案和模型生成的答案来判断模型生成的答案是否正确。第二个是Chain of Thought RM，作者基于收集的800k CoT数据对kimi进行了finetune，然后对模型生成的推理步骤进行打分。最终发现，Chain of Thought RM的效果更好。因此，作者在RL训练中使用了Chain of Thought RM。&lt;/li>
&lt;li>Vision RL Data。 作者从三个方面构建了vision RL数据集：real-world data, synthetic visual reasoning data以及text-rendered data.&lt;/li>
&lt;/ol>
&lt;h3 id="数据集">&lt;a href="#%e6%95%b0%e6%8d%ae%e9%9b%86" class="header-anchor">&lt;/a>数据集
&lt;/h3>&lt;ol>
&lt;li>pretraining：其中，纯文本数据包括Engligh, Chinese, Code, math reasoning以及Knowledge这5个领域。多模态数据包括captioning, image-text interleaving, OCR, Knowledge以及QA数据集等。附录B介绍了数据集的来源和处理过程。&lt;/li>
&lt;li>SFT： vanilla SFT数据集包含1M的纯文本数据，其中包含500K的general QA数据，200K的coding数据，200K的数学和科学数据，5K的创作写作数据以及20K的长上下文任务（总结，QA，翻译和写作等）。作者还构建了1M的图文数据，包括chart理解，OCR，grounding，visual coding, visual reasoning以及visual aided math/science problems等&lt;/li>
&lt;li>long-CoT SFT：该阶段的数据基于refined RL prompt set，使用了prompt engineering来构建了一个少量但高质量的long-CoT数据集。用于将reasoning能力内化到模型中。&lt;/li>
&lt;/ol>
&lt;h2 id="long2short">&lt;a href="#long2short" class="header-anchor">&lt;/a>Long2short
&lt;/h2>&lt;p>为了降低推理成本，作者提出了long2short的推理方法。该方法通过将长上下文推理转化为短上下文推理，让模型能够更加高效的完成推理。作者尝试了几种方法来实现long2short：&lt;/p>
&lt;ol>
&lt;li>Model merging:将long-CoT模型和short-CoT模型进行合并，然后进行推理。这里采用的办法是对两个模型的权重取平均。&lt;/li>
&lt;li>shortest rejection sampling：通过对同一个问题进行多次采样（论文中每个问题采样8次），然后选择最短的，正确的推理步骤作为最终答案。&lt;/li>
&lt;li>&lt;a class="link" href="https://maosong.website/p/notes-on-dpo/" target="_blank" rel="noopener"
>DPO&lt;/a>：通过对同一个问题进行多次采样，选择最短的正确的回答作为正样本，最长的回答作为负样本，然后使用DPO进行训练。&lt;/li>
&lt;li>Long2short RL：作者在RL阶段，选择一个在performance和token efficiency之间达到平衡的模型作为 base model，然后加入了一个long2short RL训练阶段。该阶段，作者加入了length penalty以及降低了maximum rollout length来鼓励模型生成更短的推理步骤。&lt;/li>
&lt;/ol>
&lt;h3 id="length-penalty">&lt;a href="#length-penalty" class="header-anchor">&lt;/a>Length penalty
&lt;/h3>&lt;p>Length penalty的目的是降低模型的overthinking和训练成本。作者加入了一个length reward，对于问题 $x$ 和采样的回答 $(y_i,z_i)$ ，回答 $(y_i,z_i)$ 的length定义为 $len(x_i)=\text{length}([z_i, y_i])$ length reward定义为：&lt;/p>
$$
r_{\text{length}}(x, (y_i,z_i))=\begin{cases}
\lambda, &amp; \text{if } r(x, y_i, y^\star) = 1 \\
\min(0,\lambda), &amp; \text{if } r(x, y_i, y^\star) = 0 \\
\end{cases}
$$&lt;p>其中&lt;/p>
$$
\lambda = 0.5 - \frac{len(x_i)-\min_{j}len(x_j)}{\max_{j}len(x_j)-\min_{j}len(x_j)}
$$&lt;p>也就是说，当回答正确时，我们鼓励模型生成更长的推理步骤。反之，我们鼓励模型生成更短的推理步骤。&lt;/p>
&lt;h2 id="实验">&lt;a href="#%e5%ae%9e%e9%aa%8c" class="header-anchor">&lt;/a>实验
&lt;/h2>&lt;h3 id="实验结果">&lt;a href="#%e5%ae%9e%e9%aa%8c%e7%bb%93%e6%9e%9c" class="header-anchor">&lt;/a>实验结果
&lt;/h3>&lt;ol>
&lt;li>K1.5在long-CoT任务上的表现&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k1.5/k1-5-long-cot-benchmark.png"
width="1118"
height="425"
loading="lazy"
alt="Long-CoT-benchmark"
class="gallery-image"
data-flex-grow="263"
data-flex-basis="631px"
>&lt;/p>
&lt;ol start="2">
&lt;li>K1.5在Short-CoT任务上的表现&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k1.5/k1-5-short-cot-benchmark.png"
width="1357"
height="566"
loading="lazy"
alt="Short-CoT-benchmark"
class="gallery-image"
data-flex-grow="239"
data-flex-basis="575px"
>&lt;/p>
&lt;ol start="3">
&lt;li>
&lt;p>Long Context Scaling
作者在实验中还探究了上下文长度对模型推理能力的影响。作者针对几个模型进行了实验，结果在图5和图6里面。结果发现，随着上下文长度的增加，模型的推理能力会逐渐提升。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Long2short&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-kimi-k1.5/k1-5-long2short.png"
width="1354"
height="590"
loading="lazy"
alt="Long2short"
class="gallery-image"
data-flex-grow="229"
data-flex-basis="550px"
>&lt;/p>
&lt;h3 id="消融实验">&lt;a href="#%e6%b6%88%e8%9e%8d%e5%ae%9e%e9%aa%8c" class="header-anchor">&lt;/a>消融实验
&lt;/h3>&lt;ol>
&lt;li>Scaling of model size and context length. 作者探究了模型大小和上下文长度对模型推理能力的影响，结果在图8里面。结果发现
&lt;ol>
&lt;li>小模型可以通过Long CoT来达到大模型的效果&lt;/li>
&lt;li>大模型具有更高的token效率&lt;/li>
&lt;li>理想情况下，使用具有更长上下文的大模型，可以同时达到更高的推理能力和更高的token效率，否则，使用上下文长度更长的小模型&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Effects of using negative gradients. 作者探究了使用负梯度对模型推理能力的影响。作者与ReST方法进行了对比，结果在图10里面。结果发现，ReST回到只模型产生更长的推理步骤。作者认为，选取合适的优化方法，可以有效提升模型的推理能力。&lt;/li>
&lt;li>Sampling strategies. 作者还探究了采样策略对模型推理能力的影响。作者与均匀采样进行了对比，结果在图9里面。结果发现，论文提出的采样策略可以有效提升模型的推理能力。&lt;/li>
&lt;/ol>
&lt;h2 id="结论">&lt;a href="#%e7%bb%93%e8%ae%ba" class="header-anchor">&lt;/a>结论
&lt;/h2>&lt;p>作者提出了Kimi k1.5，一个基于强化学习训练的多模态推理模型。作者介绍了Kimi k1.5的训练方法，数据集，infra上的优化以及long2short的推理方法。作者的主要贡献在于：&lt;/p>
&lt;ol>
&lt;li>作者发现，模型的上下文长度可以有效提升LLM的推理能力。&lt;/li>
&lt;li>作者提出了基于online mirror descent的强化学习训练方法。&lt;/li>
&lt;li>作者提出了long2short的推理方法，可以有效提升short CoT模型的推理能力。&lt;/li>
&lt;/ol>
&lt;p>论文的问题在于对于多模态的内容介绍较少，感觉还是更倾向于文本推理任务。&lt;/p>
&lt;h2 id="参考文献">&lt;a href="#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae" class="header-anchor">&lt;/a>参考文献
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2501.12599" target="_blank" rel="noopener"
>Kimi k1.5: Scaling Reinforcement Learning with LLMs&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>An overview of adaption layer in multimodal large language models.</title><link>https://maosong.website/p/an-overview-of-adaption-layer-in-multimodal-large-language-models./</link><pubDate>Sat, 09 Nov 2024 09:53:43 +0800</pubDate><guid>https://maosong.website/p/an-overview-of-adaption-layer-in-multimodal-large-language-models./</guid><description>&lt;h1 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h1>&lt;p>A multimodal large language model (MLLM) usually consists of three parts: an encoder $E$ that ingests the information from different modality, a large language model (LLM) that is corresponds to complete various of downstream tasks given multimodal input such as image and text, and an adaption layer $C$ that aligns features of different modality to word embedding space of the LLM.
Below is an example MLLM adopting aforementioned architecture: LLaVA [1]&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/an-overview-of-adaption-layer-in-multimodal-large-language-models./llava.png"
width="1032"
height="360"
loading="lazy"
alt="Architecture of LlaVA"
class="gallery-image"
data-flex-grow="286"
data-flex-basis="688px"
>&lt;/p>
&lt;p>Efforts have been made to improve the performance of MLLMs. In this post, we aim to review the design of adaption layer and its potential effect on the downstream tasks.&lt;/p>
&lt;h1 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h1>&lt;p>Suppose the hidden size of the LLM is $d$, the feature produced by encoder $E$ is $V\in\mathbb{R}^{P\times d_v}$, where $P$ is the number of features (number of visual patches if $E$ is an visual encoder) and $d_v$ is the channel dimension.
The adaption layer $C$ then aligns the feature $V$ with the word embedding space with $x=C(V)\in\mathbb{R}^{Q\times d}$, where $Q$ is the number of tokens. As we can see, $C$ is actually a mapping from $\mathbb{R}^{P\times d_v}$ to $\mathbb{R}^{Q\times d}$.&lt;/p>
&lt;p>Based on relationship between $d_v$ and $d$, we can divide projection layers into two types:&lt;/p>
&lt;ol>
&lt;li>Feature-preserving adaption layer, where $P=Q$&lt;/li>
&lt;li>Feature-compressing adaption layer, where $P>Q$.&lt;/li>
&lt;/ol>
&lt;h2 id="feature-preserving-adaption-layer">&lt;a href="#feature-preserving-adaption-layer" class="header-anchor">&lt;/a>Feature-preserving adaption layer
&lt;/h2>$$ x = VW^T, \text{ where } W\in\mathbb{R}^{d\times d_v}$$&lt;p>
the code reads as:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># linear layer&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">adaption_layer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_features&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>$$ x = \phi(VW_1^T)W_2^T$$&lt;p>
where $W_1\in\mathbb{R}^{d\times d_v}$, $W_2\in\mathbb{R}^{d\times d}$, $\phi$ is a activation function, specified as &lt;code>nn.GELU()&lt;/code>. The code reads as:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># two-layer MLP&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">adaption_layer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Sequential&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_features&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">GELU&lt;/span>&lt;span class="p">(),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">config&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="feature-compressing-adaption-layer">&lt;a href="#feature-compressing-adaption-layer" class="header-anchor">&lt;/a>Feature-compressing adaption layer
&lt;/h2>&lt;p>The feature compression adaption layers can be categorized into three types:&lt;/p>
&lt;ol>
&lt;li>average pooling&lt;/li>
&lt;li>attention pooling&lt;/li>
&lt;li>convolution mapping&lt;/li>
&lt;/ol>
&lt;p>They usually comprise two steps:&lt;/p>
&lt;ol>
&lt;li>reduce the number of features from $P$ to $Q$ with a pooling operation:
$$ f' = \mathcal{P}(f)\in\mathbb{R}^{Q\times d_v} $$&lt;/li>
&lt;li>project compressed features $f'$ to word embedding space with a transformation $\mathcal{T}$:
$$ x = \mathcal{T}(f')\in\mathbb{R}^{Q\times d} $$&lt;/li>
&lt;/ol>
$$ f'_i = \frac{1}{n}\sum_{j=1}^{n}f_{(i-1)n+j}, i=1,\dots,Q $$$$ K = W_kf\in\mathbb{R}^{d_c}, V=W_vf\in\mathbb{R}^{d_c}, f'=\mathrm{softmax}\left(\frac{QK^T}{\sqrt{d_c}}\right)V\in\mathbb{R}^{Q\times d_v} $$&lt;p>
where $W_k, W_v\in\mathbb{R}^{d_c\times d_v}$ and $Q\in\mathbb{R}^{Q\times d_c}$ is a learnable query.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">PerceiverResampler&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_queries&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_features&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_heads&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_queries&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">num_queries&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">hidden_size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">hidden_size&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_features&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">num_features&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">query_tokens&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Parameter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zeros&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_queries&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">num_features&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">requires_grad&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">query_tokens&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">data&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">normal_&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">mean&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">std&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.02&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">attention&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">MultiheadAttention&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_heads&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">layer_norm_kv&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">LayerNorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">layer_norm_q&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">LayerNorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">attention_mask&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">layer_norm_kv&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">permute&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">N&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">layer_norm_q&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">query_tokens&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">q&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">q&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">unsqueeze&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">repeat&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">N&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">out&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">attention&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">q&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">k&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">v&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">attention_mask&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">attention_mask&lt;/span>&lt;span class="p">)[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">out&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">out&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">permute&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">adaption_layer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Sequential&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">PerceiverResampler&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num_queries&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_features&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_heads&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">MLP&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">intermediate_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_size&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>$$ f_i' = \frac{1}{n}\sum_{j=1}^n w_jf_{(i-1)n+j},\quad x_i = \sum_{k=-K}^Kw_k'f_{i+k}' $$&lt;p>
where $W=[w_1,\dots,w_n]^T\in\mathbb{R}^n$ and $W'=[w_1,\dots,w_n]^T\in\mathbb{R}^{2K}$ are the weights of the convolution layers.&lt;/p>
&lt;p>&lt;strong>D-Abstractor&lt;/strong> aa&lt;/p>
&lt;h1 id="usages">&lt;a href="#usages" class="header-anchor">&lt;/a>Usages
&lt;/h1>&lt;h1 id="comparisons">&lt;a href="#comparisons" class="header-anchor">&lt;/a>Comparisons
&lt;/h1>&lt;h1 id="references">&lt;a href="#references" class="header-anchor">&lt;/a>References
&lt;/h1>&lt;ol>
&lt;li>&lt;a class="link" href="https://papers.nips.cc/paper_files/paper/2023/hash/6dcf277ea32ce3288914faf369fe6de0-Abstract-Conference.html" target="_blank" rel="noopener"
>LLaVA&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Improved_Baselines_with_Visual_Instruction_Tuning_CVPR_2024_paper.pdf" target="_blank" rel="noopener"
>LLaVA 1.5&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://github.com/haotian-liu/LLaVA/blob/main/llava/model/multimodal_projector/builder.py" target="_blank" rel="noopener"
>LLaVA adaption layer code&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://arxiv.org/pdf/2405.10739v1" target="_blank" rel="noopener"
>survey&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>Notes on VITA</title><link>https://maosong.website/p/notes-on-vita/</link><pubDate>Tue, 13 Aug 2024 14:36:45 +0800</pubDate><guid>https://maosong.website/p/notes-on-vita/</guid><description>&lt;h1 id="tldr">&lt;a href="#tldr" class="header-anchor">&lt;/a>TLDR
&lt;/h1>&lt;p>This paper proposes a Multimodal Large Language Model VITA (Video, Image, Text, Audio).
VITA supports non-awakening interaction and audio interruption for better interactive experience.
VITA aims to be an open-sourced version of GPT-4o.&lt;/p>
&lt;h1 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h1>&lt;p>Features of GPT-4o:&lt;/p>
&lt;ol>
&lt;li>a unified framework that processes text, vision, and audio signals in an end-to-end manner,&lt;/li>
&lt;li>the capability to enable natural multimodal human-computer interaction.&lt;/li>
&lt;/ol>
&lt;p>Similar to Mini-GPT4, this paper tries to proposed an open-sourced version of GPT-4o.&lt;/p>
&lt;h1 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h1>&lt;h2 id="model">&lt;a href="#model" class="header-anchor">&lt;/a>Model
&lt;/h2>&lt;p>The architecture of VITA is shown as follows:
&lt;img src="https://maosong.website/p/notes-on-vita/VITA_architecture.png"
width="1282"
height="1069"
loading="lazy"
alt="Architecture of VITA"
class="gallery-image"
data-flex-grow="119"
data-flex-basis="287px"
>&lt;/p>
&lt;ul>
&lt;li>LLM: Mixtral $8\times 7$ B&lt;/li>
&lt;li>Visual Encoder: InternViT-300M-448px&lt;/li>
&lt;li>Audio Encoder: Mel Filter Bank block&lt;/li>
&lt;/ul>
&lt;p>To support audio interruption, the author uses two model at the same time, where the generation model is responsible for handling user queries and the other model monitors the environment. The other models starts to work is there is an audio interruption.&lt;/p>
&lt;h2 id="data">&lt;a href="#data" class="header-anchor">&lt;/a>Data
&lt;/h2>&lt;h3 id="multimodal-instruction-tuning">&lt;a href="#multimodal-instruction-tuning" class="header-anchor">&lt;/a>multimodal instruction tuning
&lt;/h3>&lt;p>Training data of multimodal instruction tuning is given as follows:&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-vita/VITA_SFT_data.png"
width="1286"
height="672"
loading="lazy"
alt="Training data of multimodal instruction tuning"
class="gallery-image"
data-flex-grow="191"
data-flex-basis="459px"
>&lt;/p>
&lt;p>Improvements are made:&lt;/p>
&lt;ol>
&lt;li>The questions are randomly (about half) replaced with their audio versions, using TTS technique such as GPT-SoVITS&lt;/li>
&lt;li>Different system prompts are set to avoid conflicts between different types of data&lt;/li>
&lt;/ol>
&lt;p>To support human-AI interaction, the noisy audio data are also constructed. Noisy audio samples are generated from existed QA data. These negative sample texts aim to improve the ability of VITA to not respond to non-query-related content.&lt;/p>
&lt;p>To distinguish three types of queries, the author uses three state tokens:&lt;/p>
&lt;ul>
&lt;li>Token &lt;code>&amp;lt;1&amp;gt;&lt;/code> denotes that the question input is the query audio&lt;/li>
&lt;li>Token &lt;code>&amp;lt;2&amp;gt;&lt;/code> denotes that the question input is the noisy audio.&lt;/li>
&lt;li>Token &lt;code>&amp;lt;3&amp;gt;&lt;/code> signifies the question of pure text.&lt;/li>
&lt;/ul>
&lt;h2 id="training-pipeline">&lt;a href="#training-pipeline" class="header-anchor">&lt;/a>Training pipeline
&lt;/h2>&lt;p>Training pipeline of VITA consists of three stages:&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/notes-on-vita/VITA_training_pipeline.png"
width="1301"
height="852"
loading="lazy"
alt="Training pipeline of VITA"
class="gallery-image"
data-flex-grow="152"
data-flex-basis="366px"
>&lt;/p>
&lt;h3 id="non-awakening-interaction">&lt;a href="#non-awakening-interaction" class="header-anchor">&lt;/a>Non-awakening Interaction
&lt;/h3>&lt;p>There are following requirements and solutions:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Real-time Tracking of Environmental Sounds. This paper uses SileroVAD to complete the Voice Activity Detection (VAD) task.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Filtering out noisy audio. This is done by making use of token &lt;code>&amp;lt;2&amp;gt;&lt;/code>.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="audio-interrupt-interaction">&lt;a href="#audio-interrupt-interaction" class="header-anchor">&lt;/a>Audio Interrupt Interaction
&lt;/h3>&lt;p>There are following requirements and solutions:&lt;/p>
&lt;ol>
&lt;li>Real-time Tracking and Filtering of External Queries. This is done by use another VITA model as stated in Model section.&lt;/li>
&lt;/ol>
&lt;h1 id="evaluation">&lt;a href="#evaluation" class="header-anchor">&lt;/a>Evaluation
&lt;/h1>&lt;p>&lt;img src="https://maosong.website/p/notes-on-vita/VITA_evaluation.png"
width="1297"
height="536"
loading="lazy"
alt="Evaluation on image and video understanding"
class="gallery-image"
data-flex-grow="241"
data-flex-basis="580px"
>&lt;/p>
&lt;h1 id="conclusion">&lt;a href="#conclusion" class="header-anchor">&lt;/a>Conclusion
&lt;/h1>&lt;p>The paper points out three limitations of VITA:&lt;/p>
&lt;ol>
&lt;li>Enhancement of Foundational Capabilities.&lt;/li>
&lt;li>Refinement of Noisy Audio Construction.&lt;/li>
&lt;li>Building end-to-end TTS in conjunction with LLM.&lt;/li>
&lt;/ol>
&lt;h1 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="http://arxiv.org/abs/2408.05211" target="_blank" rel="noopener"
>Arxiv paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://vita-home.github.io" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>MiniGPT-4-Enhancing Vision-Language Understanding with Advanced Large Language Models</title><link>https://maosong.website/p/minigpt-4-enhancing-vision-language-understanding-with-advanced-large-language-models/</link><pubDate>Thu, 02 May 2024 13:13:12 +0800</pubDate><guid>https://maosong.website/p/minigpt-4-enhancing-vision-language-understanding-with-advanced-large-language-models/</guid><description>&lt;h1 id="tldr">&lt;a href="#tldr" class="header-anchor">&lt;/a>TLDR
&lt;/h1>&lt;p>This paper proposed an open-sourced, GPT-4 liked multimodal language model (MLLM), mini-GPT4, aiming to provide inspiration of how to build an MLLM.&lt;/p>
&lt;h1 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h1>&lt;p>GPT-4 has exhibited remarkable vision language abilities, however, due to it is not open sourced, it is hard to explore the construction and how the GPT-4 is trained.&lt;/p>
&lt;p>To solve this problem, this paper builds mini-GPT4, which utilizes an advanced LLM, Vicuna and BLIP-2, to build an open sourced MLLM. The result show that mini-GPT4 possesses numerous capabilities similar to those demostrated by GPT-4.&lt;/p>
&lt;h1 id="method">&lt;a href="#method" class="header-anchor">&lt;/a>Method
&lt;/h1>&lt;p>The architecture of mini-GPT4 is shown as follows:&lt;/p>
&lt;p>&lt;img src="https://maosong.website/p/minigpt-4-enhancing-vision-language-understanding-with-advanced-large-language-models/mini-gpt4-architecture.png"
width="907"
height="600"
loading="lazy"
alt="mini-GPT4 architecture"
class="gallery-image"
data-flex-grow="151"
data-flex-basis="362px"
>&lt;/p>
&lt;p>mini-GPT4 consists of three parts:&lt;/p>
&lt;ol>
&lt;li>Vision Encoder same as used in BLIP-2, which is built upon a ViT backbone and a Q-former network.&lt;/li>
&lt;li>A single projection layer, which aligns the encoded visual features with the Vicuna language model.&lt;/li>
&lt;li>Language decoder: Vicuna.&lt;/li>
&lt;/ol>
&lt;h2 id="training">&lt;a href="#training" class="header-anchor">&lt;/a>Training
&lt;/h2>&lt;p>mini-GPT4 only trains the linear projection layer, this includes two stages:&lt;/p>
&lt;ol>
&lt;li>First pre-training stage: in this stage, the language decoder and vision encoder are remain frozen, only the linear projection layer is trained. The training dataset contains Conceptual Caption, SBU and LAION. The problem is that the output may be incoherent like repetitive words or sentences.&lt;/li>
&lt;li>Fine-tune dataset generation: the pre-trained mini-GPT4 is used to generate image-text pair as fine-tune data, to improve the text quality, the generated text is polished with the help of ChatGPT. Finally, the dataset is manually refined.&lt;/li>
&lt;li>Second-stage fine-tuning: Fine-tune the mini-GPT-4 with the high quality fine-tune data.&lt;/li>
&lt;/ol>
&lt;h1 id="reference">&lt;a href="#reference" class="header-anchor">&lt;/a>Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://github.com/Vision-CAIR/MiniGPT-4/tree/main" target="_blank" rel="noopener"
>Github&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://openreview.net/forum?id=1tZbq88f27" target="_blank" rel="noopener"
>Paper&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://minigpt-4.github.io/" target="_blank" rel="noopener"
>Homepage&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>